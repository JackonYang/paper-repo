{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700187"
                        ],
                        "name": "Anders S\u00f8gaard",
                        "slug": "Anders-S\u00f8gaard",
                        "structuredName": {
                            "firstName": "Anders",
                            "lastName": "S\u00f8gaard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anders S\u00f8gaard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3053695"
                        ],
                        "name": "Joachim Bingel",
                        "slug": "Joachim-Bingel",
                        "structuredName": {
                            "firstName": "Joachim",
                            "lastName": "Bingel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joachim Bingel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3127682,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b02204b210f822dabf8d68b7e3ea7ac14ee1268",
            "isKey": false,
            "numCitedBy": 199,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Multi-task learning (MTL) in deep neural networks for NLP has recently received increasing interest due to some compelling benefits, including its potential to efficiently regularize models and to reduce the need for labeled data. While it has brought significant improvements in a number of NLP tasks, mixed results have been reported, and little is known about the conditions under which MTL leads to gains in NLP. This paper sheds light on the specific task relations that can lead to gains from MTL models over single-task setups."
            },
            "slug": "Identifying-beneficial-task-relations-for-learning-S\u00f8gaard-Bingel",
            "title": {
                "fragments": [],
                "text": "Identifying beneficial task relations for multi-task learning in deep neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Light is shed on the specific task relations that can lead to gains from MTL models over single-task setups in deep neural networks for NLP."
            },
            "venue": {
                "fragments": [],
                "text": "EACL"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108860856"
                        ],
                        "name": "Xiaodong Liu",
                        "slug": "Xiaodong-Liu",
                        "structuredName": {
                            "firstName": "Xiaodong",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaodong Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800422"
                        ],
                        "name": "Jianfeng Gao",
                        "slug": "Jianfeng-Gao",
                        "structuredName": {
                            "firstName": "Jianfeng",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianfeng Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144137069"
                        ],
                        "name": "Xiaodong He",
                        "slug": "Xiaodong-He",
                        "structuredName": {
                            "firstName": "Xiaodong",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaodong He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800354"
                        ],
                        "name": "Kevin Duh",
                        "slug": "Kevin-Duh",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Duh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Duh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30844359"
                        ],
                        "name": "Ye-Yi Wang",
                        "slug": "Ye-Yi-Wang",
                        "structuredName": {
                            "firstName": "Ye-Yi",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ye-Yi Wang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11754890,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c3b8367a80181e28c95630b9b63060d895de08ff",
            "isKey": false,
            "numCitedBy": 364,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Methods of deep neural networks (DNNs) have recently demonstrated superior performance on a number of natural language processing tasks. However, in most previous work, the models are learned based on either unsupervised objectives, which does not directly optimize the desired task, or singletask supervised objectives, which often suffer from insufficient training data. We develop a multi-task DNN for learning representations across multiple tasks, not only leveraging large amounts of cross-task data, but also benefiting from a regularization effect that leads to more general representations to help tasks in new domains. Our multi-task DNN approach combines tasks of multiple-domain classification (for query classification) and information retrieval (ranking for web search), and demonstrates significant gains over strong baselines in a comprehensive set of domain adaptation."
            },
            "slug": "Representation-Learning-Using-Multi-Task-Deep-for-Liu-Gao",
            "title": {
                "fragments": [],
                "text": "Representation Learning Using Multi-Task Deep Neural Networks for Semantic Classification and Information Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work develops a multi-task DNN for learning representations across multiple tasks, not only leveraging large amounts of cross-task data, but also benefiting from a regularization effect that leads to more general representations to help tasks in new domains."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1806773"
                        ],
                        "name": "Ishan Misra",
                        "slug": "Ishan-Misra",
                        "structuredName": {
                            "firstName": "Ishan",
                            "lastName": "Misra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ishan Misra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781242"
                        ],
                        "name": "Abhinav Shrivastava",
                        "slug": "Abhinav-Shrivastava",
                        "structuredName": {
                            "firstName": "Abhinav",
                            "lastName": "Shrivastava",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Abhinav Shrivastava"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726095131"
                        ],
                        "name": "A. Gupta",
                        "slug": "A.-Gupta",
                        "structuredName": {
                            "firstName": "Abhinav",
                            "lastName": "Gupta",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145670946"
                        ],
                        "name": "M. Hebert",
                        "slug": "M.-Hebert",
                        "structuredName": {
                            "firstName": "Martial",
                            "lastName": "Hebert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hebert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1923223,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f14325ec3041a73118bc4d819204cbbca07d5a71",
            "isKey": false,
            "numCitedBy": 709,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "Multi-task learning in Convolutional Networks has displayed remarkable success in the field of recognition. This success can be largely attributed to learning shared representations from multiple supervisory tasks. However, existing multi-task approaches rely on enumerating multiple network architectures specific to the tasks at hand, that do not generalize. In this paper, we propose a principled approach to learn shared representations in ConvNets using multitask learning. Specifically, we propose a new sharing unit: \"cross-stitch\" unit. These units combine the activations from multiple networks and can be trained end-to-end. A network with cross-stitch units can learn an optimal combination of shared and task-specific representations. Our proposed method generalizes across multiple tasks and shows dramatically improved performance over baseline methods for categories with few training examples."
            },
            "slug": "Cross-Stitch-Networks-for-Multi-task-Learning-Misra-Shrivastava",
            "title": {
                "fragments": [],
                "text": "Cross-Stitch Networks for Multi-task Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper proposes a principled approach to learn shared representations in Convolutional Networks using multitask learning using a new sharing unit: \"cross-stitch\" unit that combines the activations from multiple networks and can be trained end-to-end."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2939803"
                        ],
                        "name": "Ronan Collobert",
                        "slug": "Ronan-Collobert",
                        "structuredName": {
                            "firstName": "Ronan",
                            "lastName": "Collobert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronan Collobert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2617020,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "57458bc1cffe5caa45a885af986d70f723f406b4",
            "isKey": false,
            "numCitedBy": 5024,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in state-of-the-art-performance."
            },
            "slug": "A-unified-architecture-for-natural-language-deep-Collobert-Weston",
            "title": {
                "fragments": [],
                "text": "A unified architecture for natural language processing: deep neural networks with multitask learning"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "This work describes a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense using a language model."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144707379"
                        ],
                        "name": "Brian Kingsbury",
                        "slug": "Brian-Kingsbury",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Kingsbury",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brian Kingsbury"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13953660,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eb9243a3b98a819539ad57b7b4f05b969510d075",
            "isKey": false,
            "numCitedBy": 880,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we provide an overview of the invited and contributed papers presented at the special session at ICASSP-2013, entitled \u201cNew Types of Deep Neural Network Learning for Speech Recognition and Related Applications,\u201d as organized by the authors. We also describe the historical context in which acoustic models based on deep neural networks have been developed. The technical overview of the papers presented in our special session is organized into five ways of improving deep learning methods: (1) better optimization; (2) better types of neural activation function and better network architectures; (3) better ways to determine the myriad hyper-parameters of deep neural networks; (4) more appropriate ways to preprocess speech for deep neural networks; and (5) ways of leveraging multiple languages or dialects that are more easily achieved with deep neural networks than with Gaussian mixture models."
            },
            "slug": "New-types-of-deep-neural-network-learning-for-and-Deng-Hinton",
            "title": {
                "fragments": [],
                "text": "New types of deep neural network learning for speech recognition and related applications: an overview"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "An overview of the invited and contributed papers presented at the special session at ICASSP-2013, entitled \u201cNew Types of Deep Neural Network Learning for Speech Recognition and Related Applications,\u201d as organized by the authors is provided."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35776445"
                        ],
                        "name": "Mingsheng Long",
                        "slug": "Mingsheng-Long",
                        "structuredName": {
                            "firstName": "Mingsheng",
                            "lastName": "Long",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mingsheng Long"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144499343"
                        ],
                        "name": "Jianmin Wang",
                        "slug": "Jianmin-Wang",
                        "structuredName": {
                            "firstName": "Jianmin",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianmin Wang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17901965,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7c61efd58584451b8988c42f2b7006eddbb291f1",
            "isKey": false,
            "numCitedBy": 83,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep neural networks trained on large-scale dataset can learn transferable features that promote learning multiple tasks for inductive transfer and labeling mitigation. As deep features eventually transition from general to specific along the network, a fundamental problem is how to exploit the relationship structure across different tasks while accounting for the feature transferability in the task-specific layers. In this work, we propose a novel Deep Relationship Network (DRN) architecture for multi-task learning by discovering correlated tasks based on multiple task-specific layers of a deep convolutional neural network. DRN models the task relationship by imposing matrix normal priors over the network parameters of all task-specific layers, including higher feature layers and classifier layer that are not transferable safely. By jointly learning the transferable features and task relationships, DRN is able to alleviate the dilemma of negative-transfer in the feature layers and under-transfer in the classifier layer. Empirical evidence shows that DRN yields state-of-the-art classification results on standard multi-domain object recognition datasets."
            },
            "slug": "Learning-Multiple-Tasks-with-Deep-Relationship-Long-Wang",
            "title": {
                "fragments": [],
                "text": "Learning Multiple Tasks with Deep Relationship Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work proposes a novel Deep Relationship Network (DRN) architecture for multi-task learning by discovering correlated tasks based on multiple task-specific layers of a deep convolutional neural network that yields state-of-the-art classification results on standard multi-domain object recognition datasets."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47413820"
                        ],
                        "name": "Hao Cheng",
                        "slug": "Hao-Cheng",
                        "structuredName": {
                            "firstName": "Hao",
                            "lastName": "Cheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hao Cheng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145204655"
                        ],
                        "name": "Hao Fang",
                        "slug": "Hao-Fang",
                        "structuredName": {
                            "firstName": "Hao",
                            "lastName": "Fang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hao Fang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144339506"
                        ],
                        "name": "Mari Ostendorf",
                        "slug": "Mari-Ostendorf",
                        "structuredName": {
                            "firstName": "Mari",
                            "lastName": "Ostendorf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mari Ostendorf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 477021,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a75504adb5773e0b235f8d09655ce2b502193b6f",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Out-of-vocabulary name errors in speech recognition create significant problems for downstream language processing, but the fact that they are rare poses challenges for automatic detection, particularly in an open-domain scenario. To address this problem, a multi-task recurrent neural network language model for sentence-level name detection is proposed for use in combination with out-of-vocabulary word detection. The sentence-level model is also effective for leveraging external text data. Experiments show a 26% improvement in name-error detection F-score over a system using n-gram lexical features."
            },
            "slug": "Open-Domain-Name-Error-Detection-using-a-Multi-Task-Cheng-Fang",
            "title": {
                "fragments": [],
                "text": "Open-Domain Name Error Detection using a Multi-Task RNN"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A multi-task recurrent neural network language model for sentence-level name detection is proposed for use in combination with out-of-vocabulary word detection, which shows a 26% improvement in name-error detection F-score over a system using n-gram lexical features."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2378027"
                        ],
                        "name": "Bharath Ramsundar",
                        "slug": "Bharath-Ramsundar",
                        "structuredName": {
                            "firstName": "Bharath",
                            "lastName": "Ramsundar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bharath Ramsundar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3157072"
                        ],
                        "name": "S. Kearnes",
                        "slug": "S.-Kearnes",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Kearnes",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kearnes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "119508204"
                        ],
                        "name": "Patrick F. Riley",
                        "slug": "Patrick-F.-Riley",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Riley",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Patrick F. Riley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47191829"
                        ],
                        "name": "D. Webster",
                        "slug": "D.-Webster",
                        "structuredName": {
                            "firstName": "Dale",
                            "lastName": "Webster",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Webster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3316826"
                        ],
                        "name": "D. Konerding",
                        "slug": "D.-Konerding",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Konerding",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Konerding"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1806271"
                        ],
                        "name": "V. Pande",
                        "slug": "V.-Pande",
                        "structuredName": {
                            "firstName": "Vijay",
                            "lastName": "Pande",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Pande"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2127453,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "72de1bfa528c9450a57394c900361acf1e85b8d0",
            "isKey": false,
            "numCitedBy": 381,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Massively multitask neural architectures provide a learning framework for drug discovery that synthesizes information from many distinct biological sources. To train these architectures at scale, we gather large amounts of data from public sources to create a dataset of nearly 40 million measurements across more than 200 biological targets. We investigate several aspects of the multitask framework by performing a series of empirical studies and obtain some interesting results: (1) massively multitask networks obtain predictive accuracies significantly better than single-task methods, (2) the predictive power of multitask networks improves as additional tasks and data are added, (3) the total amount of data and the total number of tasks both contribute significantly to multitask improvement, and (4) multitask networks afford limited transferability to tasks not in the training set. Our results underscore the need for greater data sharing and further algorithmic innovation to accelerate the drug discovery process."
            },
            "slug": "Massively-Multitask-Networks-for-Drug-Discovery-Ramsundar-Kearnes",
            "title": {
                "fragments": [],
                "text": "Massively Multitask Networks for Drug Discovery"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The results underscore the need for greater data sharing and further algorithmic innovation to accelerate the drug discovery process and investigate several aspects of the multitask framework."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35776445"
                        ],
                        "name": "Mingsheng Long",
                        "slug": "Mingsheng-Long",
                        "structuredName": {
                            "firstName": "Mingsheng",
                            "lastName": "Long",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mingsheng Long"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3451430"
                        ],
                        "name": "Zhangjie Cao",
                        "slug": "Zhangjie-Cao",
                        "structuredName": {
                            "firstName": "Zhangjie",
                            "lastName": "Cao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhangjie Cao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144499343"
                        ],
                        "name": "Jianmin Wang",
                        "slug": "Jianmin-Wang",
                        "structuredName": {
                            "firstName": "Jianmin",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianmin Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144019071"
                        ],
                        "name": "Philip S. Yu",
                        "slug": "Philip-S.-Yu",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Yu",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip S. Yu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4962395,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b7f9cc57ab8f3f551bdb0d5f153191ec403895e",
            "isKey": false,
            "numCitedBy": 164,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep networks trained on large-scale data can learn transferable features to promote learning multiple tasks. Since deep features eventually transition from general to specific along deep networks, a fundamental problem of multi-task learning is how to exploit the task relatedness underlying parameter tensors and improve feature transferability in the multiple task-specific layers. This paper presents Multilinear Relationship Networks (MRN) that discover the task relationships based on novel tensor normal priors over parameter tensors of multiple task-specific layers in deep convolutional networks. By jointly learning transferable features and multilinear relationships of tasks and features, MRN is able to alleviate the dilemma of negative-transfer in the feature layers and under-transfer in the classifier layer. Experiments show that MRN yields state-of-the-art results on three multi-task learning datasets."
            },
            "slug": "Learning-Multiple-Tasks-with-Multilinear-Networks-Long-Cao",
            "title": {
                "fragments": [],
                "text": "Learning Multiple Tasks with Multilinear Relationship Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "Multilinear Relationship Networks (MRN) is presented that discover the task relationships based on novel tensor normal priors over parameter tensors of multiple task-specific layers in deep convolutional networks and yields state-of-the-art results on three multi-task learning datasets."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700187"
                        ],
                        "name": "Anders S\u00f8gaard",
                        "slug": "Anders-S\u00f8gaard",
                        "structuredName": {
                            "firstName": "Anders",
                            "lastName": "S\u00f8gaard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anders S\u00f8gaard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "79775260"
                        ],
                        "name": "Yoav Goldberg",
                        "slug": "Yoav-Goldberg",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Goldberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoav Goldberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16661147,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "03ad06583c9721855ccd82c3d969a01360218d86",
            "isKey": false,
            "numCitedBy": 392,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "In all previous work on deep multi-task learning we are aware of, all task supervisions are on the same (outermost) layer. We present a multi-task learning architecture with deep bi-directional RNNs, where different tasks supervision can happen at different layers. We present experiments in syntactic chunking and CCG supertagging, coupled with the additional task of POS-tagging. We show that it is consistently better to have POS supervision at the innermost rather than the outermost layer. We argue that this is because \u201clowlevel\u201d tasks are better kept at the lower layers, enabling the higher-level tasks to make use of the shared representation of the lower-level tasks. Finally, we also show how this architecture can be used for domain adaptation."
            },
            "slug": "Deep-multi-task-learning-with-low-level-tasks-at-S\u00f8gaard-Goldberg",
            "title": {
                "fragments": [],
                "text": "Deep multi-task learning with low level tasks supervised at lower layers"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is consistently better to have POS supervision at the innermost rather than the outermost layer, and it is argued that \u201clowlevel\u201d tasks are better kept at the lower layers, enabling the higher- level tasks to make use of the shared representation of the lower-level tasks."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2884561"
                        ],
                        "name": "Sebastian Ruder",
                        "slug": "Sebastian-Ruder",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Ruder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sebastian Ruder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3053695"
                        ],
                        "name": "Joachim Bingel",
                        "slug": "Joachim-Bingel",
                        "structuredName": {
                            "firstName": "Joachim",
                            "lastName": "Bingel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joachim Bingel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736067"
                        ],
                        "name": "Isabelle Augenstein",
                        "slug": "Isabelle-Augenstein",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Augenstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Isabelle Augenstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700187"
                        ],
                        "name": "Anders S\u00f8gaard",
                        "slug": "Anders-S\u00f8gaard",
                        "structuredName": {
                            "firstName": "Anders",
                            "lastName": "S\u00f8gaard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anders S\u00f8gaard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 215776107,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e242ba1a62eb2595d89afbec2657f33d9ab4abe3",
            "isKey": false,
            "numCitedBy": 161,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Multi-task learning is motivated by the observation that humans bring to bear what they know about related problems when solving new ones. Similarly, deep neural networks can profit from related tasks by sharing parameters with other networks. However, humans do not consciously decide to transfer knowledge between tasks. In Natural Language Processing (NLP), it is hard to predict if sharing will lead to improvements, particularly if tasks are only loosely related. To overcome this, we introduce Sluice Networks, a general framework for multi-task learning where trainable parameters control the amount of sharing. Our framework generalizes previous proposals in enabling sharing of all combinations of subspaces, layers, and skip connections. We perform experiments on three task pairs, and across seven different domains, using data from OntoNotes 5.0, and achieve up to 15% average error reductions over common approaches to multi-task learning. We show that a) label entropy is predictive of gains in sluice networks, confirming findings for hard parameter sharing and b) while sluice networks easily fit noise, they are robust across domains in practice."
            },
            "slug": "Sluice-networks:-Learning-what-to-share-between-Ruder-Bingel",
            "title": {
                "fragments": [],
                "text": "Sluice networks: Learning what to share between loosely related tasks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Sluice Networks is introduced, a general framework for multi-task learning where trainable parameters control the amount of sharing and it is shown that a) label entropy is predictive of gains in sluice networks, confirming findings for hard parameter sharing and b) while slUice networks easily fit noise, they are robust across domains in practice."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2653152"
                        ],
                        "name": "Yongxin Yang",
                        "slug": "Yongxin-Yang",
                        "structuredName": {
                            "firstName": "Yongxin",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yongxin Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697755"
                        ],
                        "name": "Timothy M. Hospedales",
                        "slug": "Timothy-M.-Hospedales",
                        "structuredName": {
                            "firstName": "Timothy",
                            "lastName": "Hospedales",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Timothy M. Hospedales"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3047732,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "468a80bcd4ff9b3f47beb9145ff81140777bb3f3",
            "isKey": false,
            "numCitedBy": 174,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "Most contemporary multi-task learning methods assume linear models. This setting is considered shallow in the era of deep learning. In this paper, we present a new deep multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network. Our approach is based on generalising the matrix factorisation techniques explicitly or implicitly used by many conventional MTL algorithms to tensor factorisation, to realise automatic learning of end-to-end knowledge sharing in deep networks. This is in contrast to existing deep learning approaches that need a user-defined multi-task sharing strategy. Our approach applies to both homogeneous and heterogeneous MTL. Experiments demonstrate the efficacy of our deep multi-task representation learning in terms of both higher accuracy and fewer design choices."
            },
            "slug": "Deep-Multi-task-Representation-Learning:-A-Tensor-Yang-Hospedales",
            "title": {
                "fragments": [],
                "text": "Deep Multi-task Representation Learning: A Tensor Factorisation Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "A new deep multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network by generalising the matrix factorisation techniques explicitly or implicitly used by many conventional MTL algorithms to tensor factorisation to realise automatic learning of end-to-end knowledge sharing in deep networks."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2325498"
                        ],
                        "name": "Y. Lu",
                        "slug": "Y.-Lu",
                        "structuredName": {
                            "firstName": "Yongxi",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50333123"
                        ],
                        "name": "Abhishek Kumar",
                        "slug": "Abhishek-Kumar",
                        "structuredName": {
                            "firstName": "Abhishek",
                            "lastName": "Kumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Abhishek Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2443456"
                        ],
                        "name": "Shuangfei Zhai",
                        "slug": "Shuangfei-Zhai",
                        "structuredName": {
                            "firstName": "Shuangfei",
                            "lastName": "Zhai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuangfei Zhai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145215470"
                        ],
                        "name": "Yu Cheng",
                        "slug": "Yu-Cheng",
                        "structuredName": {
                            "firstName": "Yu",
                            "lastName": "Cheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yu Cheng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47197693"
                        ],
                        "name": "T. Javidi",
                        "slug": "T.-Javidi",
                        "structuredName": {
                            "firstName": "Tara",
                            "lastName": "Javidi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Javidi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723233"
                        ],
                        "name": "R. Feris",
                        "slug": "R.-Feris",
                        "structuredName": {
                            "firstName": "Rog\u00e9rio",
                            "lastName": "Feris",
                            "middleNames": [
                                "Schmidt"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Feris"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2057685,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "00a967cb2d18e1394226ad37930524a31351f6cf",
            "isKey": false,
            "numCitedBy": 249,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "Multi-task learning aims to improve generalization performance of multiple prediction tasks by appropriately sharing relevant information across them. In the context of deep neural networks, this idea is often realized by hand-designed network architectures with layers that are shared across tasks and branches that encode task-specific features. However, the space of possible multi-task deep architectures is combinatorially large and often the final architecture is arrived at by manual exploration of this space, which can be both error-prone and tedious. We propose an automatic approach for designing compact multi-task deep learning architectures. Our approach starts with a thin multi-layer network and dynamically widens it in a greedy manner during training. By doing so iteratively, it creates a tree-like deep architecture, on which similar tasks reside in the same branch until at the top layers. Evaluation on person attributes classification tasks involving facial and clothing attributes suggests that the models produced by the proposed method are fast, compact and can closely match or exceed the state-of-the-art accuracy from strong baselines by much more expensive models."
            },
            "slug": "Fully-Adaptive-Feature-Sharing-in-Multi-Task-with-Lu-Kumar",
            "title": {
                "fragments": [],
                "text": "Fully-Adaptive Feature Sharing in Multi-Task Networks with Applications in Person Attribute Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Evaluation on person attributes classification tasks involving facial and clothing attributes suggests that the models produced by the proposed method are fast, compact and can closely match or exceed the state-of-the-art accuracy from strong baselines by much more expensive models."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20851195"
                        ],
                        "name": "Kazuma Hashimoto",
                        "slug": "Kazuma-Hashimoto",
                        "structuredName": {
                            "firstName": "Kazuma",
                            "lastName": "Hashimoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kazuma Hashimoto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2228109"
                        ],
                        "name": "Caiming Xiong",
                        "slug": "Caiming-Xiong",
                        "structuredName": {
                            "firstName": "Caiming",
                            "lastName": "Xiong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Caiming Xiong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143946906"
                        ],
                        "name": "Yoshimasa Tsuruoka",
                        "slug": "Yoshimasa-Tsuruoka",
                        "structuredName": {
                            "firstName": "Yoshimasa",
                            "lastName": "Tsuruoka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshimasa Tsuruoka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2213896,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ade0c116120b54b57a91da51235108b75c28375a",
            "isKey": false,
            "numCitedBy": 461,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. Higher layers include shortcut connections to lower-level task predictions to reflect linguistic hierarchies. We use a simple regularization term to allow for optimizing all model weights to improve one task\u2019s loss without exhibiting catastrophic interference of the other tasks. Our single end-to-end model obtains state-of-the-art or competitive results on five different tasks from tagging, parsing, relatedness, and entailment tasks."
            },
            "slug": "A-Joint-Many-Task-Model:-Growing-a-Neural-Network-Hashimoto-Xiong",
            "title": {
                "fragments": [],
                "text": "A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks and uses a simple regularization term to allow for optimizing all model weights to improve one task\u2019s loss without exhibiting catastrophic interference of the other tasks."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2853049"
                        ],
                        "name": "Z. Kang",
                        "slug": "Z.-Kang",
                        "structuredName": {
                            "firstName": "Zhuoliang",
                            "lastName": "Kang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Kang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794409"
                        ],
                        "name": "K. Grauman",
                        "slug": "K.-Grauman",
                        "structuredName": {
                            "firstName": "Kristen",
                            "lastName": "Grauman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Grauman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145757665"
                        ],
                        "name": "Fei Sha",
                        "slug": "Fei-Sha",
                        "structuredName": {
                            "firstName": "Fei",
                            "lastName": "Sha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fei Sha"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12817931,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d2d35fc47bfcd9bbcdf1905b23be6e5dcee05e9c",
            "isKey": false,
            "numCitedBy": 327,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "In multi-task learning (MTL), multiple tasks are learnt jointly. A major assumption for this paradigm is that all those tasks are indeed related so that the joint training is appropriate and beneficial. In this paper, we study the problem of multi-task learning of shared feature representations among tasks, while simultaneously determining \"with whom\" each task should share. We formulate the problem as a mixed integer programming and provide an alternating minimization technique to solve the optimization problem of jointly identifying grouping structures and parameters. The algorithm mono-tonically decreases the objective function and converges to a local optimum. Compared to the standard MTL paradigm where all tasks are in a single group, our algorithm improves its performance with statistical significance for three out of the four datasets we have studied. We also demonstrate its advantage over other task grouping techniques investigated in literature."
            },
            "slug": "Learning-with-Whom-to-Share-in-Multi-task-Feature-Kang-Grauman",
            "title": {
                "fragments": [],
                "text": "Learning with Whom to Share in Multi-task Feature Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper forms the problem of multi-task learning of shared feature representations among tasks, while simultaneously determining \"with whom\" each task should share as a mixed integer programming and provides an alternating minimization technique to solve the optimization problem of jointly identifying grouping structures and parameters."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2653152"
                        ],
                        "name": "Yongxin Yang",
                        "slug": "Yongxin-Yang",
                        "structuredName": {
                            "firstName": "Yongxin",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yongxin Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697755"
                        ],
                        "name": "Timothy M. Hospedales",
                        "slug": "Timothy-M.-Hospedales",
                        "structuredName": {
                            "firstName": "Timothy",
                            "lastName": "Hospedales",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Timothy M. Hospedales"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8737624,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8341791ba6f7a3845788323bce1a182a6dc0b02b",
            "isKey": false,
            "numCitedBy": 123,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a framework for training multiple neural networks simultaneously. The parameters from all models are regularised by the tensor trace norm, so that each neural network is encouraged to reuse others' parameters if possible -- this is the main motivation behind multi-task learning. In contrast to many deep multi-task learning models, we do not predefine a parameter sharing strategy by specifying which layers have tied parameters. Instead, our framework considers sharing for all shareable layers, and the sharing strategy is learned in a data-driven way."
            },
            "slug": "Trace-Norm-Regularised-Deep-Multi-Task-Learning-Yang-Hospedales",
            "title": {
                "fragments": [],
                "text": "Trace Norm Regularised Deep Multi-Task Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "This work proposes a framework for training multiple neural networks simultaneously, whose parameters from all models are regularised by the tensor trace norm, so that each neural network is encouraged to reuse others' parameters if possible."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1801089"
                        ],
                        "name": "T. Evgeniou",
                        "slug": "T.-Evgeniou",
                        "structuredName": {
                            "firstName": "Theodoros",
                            "lastName": "Evgeniou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Evgeniou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704699"
                        ],
                        "name": "M. Pontil",
                        "slug": "M.-Pontil",
                        "structuredName": {
                            "firstName": "Massimiliano",
                            "lastName": "Pontil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pontil"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 719551,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e219a61354d972a28954e655a7c53373508a08b6",
            "isKey": false,
            "numCitedBy": 1468,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Past empirical work has shown that learning multiple related tasks from data simultaneously can be advantageous in terms of predictive performance relative to learning these tasks independently. In this paper we present an approach to multi--task learning based on the minimization of regularization functionals similar to existing ones, such as the one for Support Vector Machines (SVMs), that have been successfully used in the past for single--task learning. Our approach allows to model the relation between tasks in terms of a novel kernel function that uses a task--coupling parameter. We implement an instance of the proposed approach similar to SVMs and test it empirically using simulated as well as real data. The experimental results show that the proposed method performs better than existing multi--task learning methods and largely outperforms single--task learning using SVMs."
            },
            "slug": "Regularized-multi--task-learning-Evgeniou-Pontil",
            "title": {
                "fragments": [],
                "text": "Regularized multi--task learning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An approach to multi--task learning based on the minimization of regularization functionals similar to existing ones, such as the one for Support Vector Machines, that have been successfully used in the past for single-- task learning is presented."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50033175"
                        ],
                        "name": "Andreas Argyriou",
                        "slug": "Andreas-Argyriou",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Argyriou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andreas Argyriou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1801089"
                        ],
                        "name": "T. Evgeniou",
                        "slug": "T.-Evgeniou",
                        "structuredName": {
                            "firstName": "Theodoros",
                            "lastName": "Evgeniou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Evgeniou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704699"
                        ],
                        "name": "M. Pontil",
                        "slug": "M.-Pontil",
                        "structuredName": {
                            "firstName": "Massimiliano",
                            "lastName": "Pontil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pontil"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7502194,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dbb3342599c9b431a3152a0d5c813d3e56967a27",
            "isKey": false,
            "numCitedBy": 1379,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method for learning a low-dimensional representation which is shared across a set of multiple related tasks. The method builds upon the well-known 1-norm regularization problem using a new regularizer which controls the number of learned features common for all the tasks. We show that this problem is equivalent to a convex optimization problem and develop an iterative algorithm for solving it. The algorithm has a simple interpretation: it alternately performs a supervised and an unsupervised step, where in the latter step we learn commonacross-tasks representations and in the former step we learn task-specific functions using these representations. We report experiments on a simulated and a real data set which demonstrate that the proposed method dramatically improves the performance relative to learning each task independently. Our algorithm can also be used, as a special case, to simply select \u2013 not learn \u2013 a few common features across the tasks."
            },
            "slug": "Multi-Task-Feature-Learning-Argyriou-Evgeniou",
            "title": {
                "fragments": [],
                "text": "Multi-Task Feature Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The method builds upon the well-known 1-norm regularization problem using a new regularizer which controls the number of learned features common for all the tasks, and develops an iterative algorithm for solving it."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145727186"
                        ],
                        "name": "R. Caruana",
                        "slug": "R.-Caruana",
                        "structuredName": {
                            "firstName": "Rich",
                            "lastName": "Caruana",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Caruana"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 43
                            }
                        ],
                        "text": "Most of these have first been proposed by [Caruana, 1998]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 1
                            }
                        ],
                        "text": "[Caruana, 1998] summarizes the goal of MTL succinctly: \u201cMTL improves generalization by leveraging the domain-specific information contained in the training signals of related tasks\"."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 45998148,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "47aaeb6dc682162dfe5659c2cad64e5d825ad910",
            "isKey": false,
            "numCitedBy": 3257,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. This paper reviews prior work on MTL, presents new evidence that MTL in backprop nets discovers task relatedness without the need of supervisory signals, and presents new results for MTL with k-nearest neighbor and kernel regression. In this paper we demonstrate multitask learning in three domains. We explain how multitask learning works, and show that there are many opportunities for multitask learning in real domains. We present an algorithm and results for multitask learning with case-based methods like k-nearest neighbor and kernel regression, and sketch an algorithm for multitask learning in decision trees. Because multitask learning works, can be applied to many different kinds of domains, and can be used with different learning algorithms, we conjecture there will be many opportunities for its use on real-world problems."
            },
            "slug": "Multitask-Learning-Caruana",
            "title": {
                "fragments": [],
                "text": "Multitask Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Prior work on MTL is reviewed, new evidence that MTL in backprop nets discovers task relatedness without the need of supervisory signals is presented, and new results for MTL with k-nearest neighbor and kernel regression are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Encyclopedia of Machine Learning and Data Mining"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2825246"
                        ],
                        "name": "Yaroslav Ganin",
                        "slug": "Yaroslav-Ganin",
                        "structuredName": {
                            "firstName": "Yaroslav",
                            "lastName": "Ganin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yaroslav Ganin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740145"
                        ],
                        "name": "V. Lempitsky",
                        "slug": "V.-Lempitsky",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Lempitsky",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Lempitsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6755881,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2530cfc7764bda1330c48c0c8e2cd0e0c671d7e1",
            "isKey": false,
            "numCitedBy": 3137,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Top-performing deep architectures are trained on massive amounts of labeled data. In the absence of labeled data for a certain task, domain adaptation often provides an attractive option given that labeled data of similar nature but from a different domain (e.g. synthetic images) are available. Here, we propose a new approach to domain adaptation in deep architectures that can be trained on large amount of labeled data from the source domain and large amount of unlabeled data from the target domain (no labeled target-domain data is necessary). \nAs the training progresses, the approach promotes the emergence of \"deep\" features that are (i) discriminative for the main learning task on the source domain and (ii) invariant with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a simple new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation. \nOverall, the approach can be implemented with little effort using any of the deep-learning packages. The method performs very well in a series of image classification experiments, achieving adaptation effect in the presence of big domain shifts and outperforming previous state-of-the-art on Office datasets."
            },
            "slug": "Unsupervised-Domain-Adaptation-by-Backpropagation-Ganin-Lempitsky",
            "title": {
                "fragments": [],
                "text": "Unsupervised Domain Adaptation by Backpropagation"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The method performs very well in a series of image classification experiments, achieving adaptation effect in the presence of big domain shifts and outperforming previous state-of-the-art on Office datasets."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50333123"
                        ],
                        "name": "Abhishek Kumar",
                        "slug": "Abhishek-Kumar",
                        "structuredName": {
                            "firstName": "Abhishek",
                            "lastName": "Kumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Abhishek Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722360"
                        ],
                        "name": "Hal Daum\u00e9",
                        "slug": "Hal-Daum\u00e9",
                        "structuredName": {
                            "firstName": "Hal",
                            "lastName": "Daum\u00e9",
                            "middleNames": [],
                            "suffix": "III"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hal Daum\u00e9"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9494747,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d3c04a424fff21d3d12ff8b0543734cf244d5f67",
            "isKey": false,
            "numCitedBy": 432,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "In the paradigm of multi-task learning, multiple related prediction tasks are learned jointly, sharing information across the tasks. We propose a framework for multi-task learning that enables one to selectively share the information across the tasks. We assume that each task parameter vector is a linear combination of a finite number of underlying basis tasks. The coefficients of the linear combination are sparse in nature and the overlap in the sparsity patterns of two tasks controls the amount of sharing across these. Our model is based on the assumption that task parameters within a group lie in a low dimensional subspace but allows the tasks in different groups to overlap with each other in one or more bases. Experimental results on four datasets show that our approach outperforms competing methods."
            },
            "slug": "Learning-Task-Grouping-and-Overlap-in-Multi-task-Kumar-Daum\u00e9",
            "title": {
                "fragments": [],
                "text": "Learning Task Grouping and Overlap in Multi-task Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "This work proposes a framework for multi-task learning that enables one to selectively share the information across the tasks, based on the assumption that task parameters within a group lie in a low dimensional subspace but allows the tasks in different groups to overlap with each other in one or more bases."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145687301"
                        ],
                        "name": "Marek Rei",
                        "slug": "Marek-Rei",
                        "structuredName": {
                            "firstName": "Marek",
                            "lastName": "Rei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marek Rei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16386838,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ac17cfa150d802750b46220084d850cfdb64d1c1",
            "isKey": false,
            "numCitedBy": 212,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a sequence labeling framework with a secondary training objective, learning to predict surrounding words for every word in the dataset. This language modeling objective incentivises the system to learn general-purpose patterns of semantic and syntactic composition, which are also useful for improving accuracy on different sequence labeling tasks. The architecture was evaluated on a range of datasets, covering the tasks of error detection in learner texts, named entity recognition, chunking and POS-tagging. The novel language modeling objective provided consistent performance improvements on every benchmark, without requiring any additional annotated or unannotated data."
            },
            "slug": "Semi-supervised-Multitask-Learning-for-Sequence-Rei",
            "title": {
                "fragments": [],
                "text": "Semi-supervised Multitask Learning for Sequence Labeling"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "A sequence labeling framework with a secondary training objective, learning to predict surrounding words for every word in the dataset, which incentivises the system to learn general-purpose patterns of semantic and syntactic composition, useful for improving accuracy on different sequence labeling tasks."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143621743"
                        ],
                        "name": "Jianfei Yu",
                        "slug": "Jianfei-Yu",
                        "structuredName": {
                            "firstName": "Jianfei",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianfei Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144924150"
                        ],
                        "name": "Jing Jiang",
                        "slug": "Jing-Jiang",
                        "structuredName": {
                            "firstName": "Jing",
                            "lastName": "Jiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jing Jiang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14015791,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2d38f7aab07d4435b2110602db4138ef20da4cc0",
            "isKey": false,
            "numCitedBy": 144,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we study cross-domain sentiment classification with neural network architectures. We borrow the idea from Structural Correspondence Learning and use two auxiliary tasks to help induce a sentence embedding that supposedly works well across domains for sentiment classification. We also propose to jointly learn this sentence embedding together with the sentiment classifier itself. Experiment results demonstrate that our proposed joint model outperforms several state-of-theart methods on five benchmark datasets."
            },
            "slug": "Learning-Sentence-Embeddings-with-Auxiliary-Tasks-Yu-Jiang",
            "title": {
                "fragments": [],
                "text": "Learning Sentence Embeddings with Auxiliary Tasks for Cross-Domain Sentiment Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This paper borrows the idea from Structural Correspondence Learning and uses two auxiliary tasks to help induce a sentence embedding that supposedly works well across domains for sentiment classification and proposes to jointly learn this sentenceembedding together with the sentiment classifier itself."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143602278"
                        ],
                        "name": "A. Jalali",
                        "slug": "A.-Jalali",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Jalali",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Jalali"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145969795"
                        ],
                        "name": "Pradeep Ravikumar",
                        "slug": "Pradeep-Ravikumar",
                        "structuredName": {
                            "firstName": "Pradeep",
                            "lastName": "Ravikumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pradeep Ravikumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701677"
                        ],
                        "name": "S. Sanghavi",
                        "slug": "S.-Sanghavi",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Sanghavi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sanghavi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052349476"
                        ],
                        "name": "Chao Ruan",
                        "slug": "Chao-Ruan",
                        "structuredName": {
                            "firstName": "Chao",
                            "lastName": "Ruan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chao Ruan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 785634,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "895217d527de919dfdfbfeae5362bf5adba984ce",
            "isKey": false,
            "numCitedBy": 355,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider multi-task learning in the setting of multiple linear regression, and where some relevant features could be shared across the tasks. Recent research has studied the use of l1/lq norm block-regularizations with q > 1 for such block-sparse structured problems, establishing strong guarantees on recovery even under high-dimensional scaling where the number of features scale with the number of observations. However, these papers also caution that the performance of such block-regularized methods are very dependent on the extent to which the features are shared across tasks. Indeed they show [8] that if the extent of overlap is less than a threshold, or even if parameter values in the shared features are highly uneven, then block l1/lq regularization could actually perform worse than simple separate elementwise l1 regularization. Since these caveats depend on the unknown true parameters, we might not know when and which method to apply. Even otherwise, we are far away from a realistic multi-task setting: not only do the set of relevant features have to be exactly the same across tasks, but their values have to as well. \n \nHere, we ask the question: can we leverage parameter overlap when it exists, but not pay a penalty when it does not? Indeed, this falls under a more general question of whether we can model such dirty data which may not fall into a single neat structural bracket (all block-sparse, or all low-rank and so on). With the explosion of such dirty high-dimensional data in modern settings, it is vital to develop tools - dirty models - to perform biased statistical estimation tailored to such data. Here, we take a first step, focusing on developing a dirty model for the multiple regression problem. Our method uses a very simple idea: we estimate a superposition of two sets of parameters and regularize them differently. We show both theoretically and empirically, our method strictly and noticeably outperforms both l1 or l1/lq methods, under high-dimensional scaling and over the entire range of possible overlaps (except at boundary cases, where we match the best method)."
            },
            "slug": "A-Dirty-Model-for-Multi-task-Learning-Jalali-Ravikumar",
            "title": {
                "fragments": [],
                "text": "A Dirty Model for Multi-task Learning"
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145644275"
                        ],
                        "name": "Y. Xue",
                        "slug": "Y.-Xue",
                        "structuredName": {
                            "firstName": "Ya",
                            "lastName": "Xue",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Xue"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2585822"
                        ],
                        "name": "X. Liao",
                        "slug": "X.-Liao",
                        "structuredName": {
                            "firstName": "X.",
                            "lastName": "Liao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Liao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145006560"
                        ],
                        "name": "L. Carin",
                        "slug": "L.-Carin",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Carin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Carin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765150"
                        ],
                        "name": "B. Krishnapuram",
                        "slug": "B.-Krishnapuram",
                        "structuredName": {
                            "firstName": "Balaji",
                            "lastName": "Krishnapuram",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Krishnapuram"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 715932,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "89c808af926ecb20870b2521fbaa7dcbb85be106",
            "isKey": false,
            "numCitedBy": 571,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "Consider the problem of learning logistic-regression models for multiple classification tasks, where the training data set for each task is not drawn from the same statistical distribution. In such a multi-task learning (MTL) scenario, it is necessary to identify groups of similar tasks that should be learned jointly. Relying on a Dirichlet process (DP) based statistical model to learn the extent of similarity between classification tasks, we develop computationally efficient algorithms for two different forms of the MTL problem. First, we consider a symmetric multi-task learning (SMTL) situation in which classifiers for multiple tasks are learned jointly using a variational Bayesian (VB) algorithm. Second, we consider an asymmetric multi-task learning (AMTL) formulation in which the posterior density function from the SMTL model parameters (from previous tasks) is used as a prior for a new task: this approach has the significant advantage of not requiring storage and use of all previous data from prior tasks. The AMTL formulation is solved with a simple Markov Chain Monte Carlo (MCMC) construction. Experimental results on two real life MTL problems indicate that the proposed algorithms: (a) automatically identify subgroups of related tasks whose training data appear to be drawn from similar distributions; and (b) are more accurate than simpler approaches such as single-task learning, pooling of data across all tasks, and simplified approximations to DP."
            },
            "slug": "Multi-Task-Learning-for-Classification-with-Process-Xue-Liao",
            "title": {
                "fragments": [],
                "text": "Multi-Task Learning for Classification with Dirichlet Process Priors"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "Experimental results on two real life MTL problems indicate that the proposed algorithms automatically identify subgroups of related tasks whose training data appear to be drawn from similar distributions are more accurate than simpler approaches such as single-task learning, pooling of data across all tasks, and simplified approximations to DP."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3017695"
                        ],
                        "name": "H\u00e9ctor Mart\u00ednez Alonso",
                        "slug": "H\u00e9ctor-Mart\u00ednez-Alonso",
                        "structuredName": {
                            "firstName": "H\u00e9ctor",
                            "lastName": "Alonso",
                            "middleNames": [
                                "Mart\u00ednez"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H\u00e9ctor Mart\u00ednez Alonso"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2022124"
                        ],
                        "name": "Barbara Plank",
                        "slug": "Barbara-Plank",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Plank",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Barbara Plank"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6045849,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8105e74cab558a82c641f7f0840cdd519281c92e",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Multitask learning has been applied successfully to a range of tasks, mostly morphosyntactic. However, little is known on when MTL works and whether there are data characteristics that help to determine the success of MTL. In this paper we evaluate a range of semantic sequence labeling tasks in a MTL setup. We examine different auxiliary task configurations, amongst which a novel setup, and correlate their impact to data-dependent conditions. Our results show that MTL is not always effective, because significant improvements are obtained only for 1 out of 5 tasks. When successful, auxiliary tasks with compact and more uniform label distributions are preferable."
            },
            "slug": "Multitask-learning-for-semantic-sequence-prediction-Alonso-Plank",
            "title": {
                "fragments": [],
                "text": "Multitask learning for semantic sequence prediction under varying data conditions"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper evaluates a range of semantic sequence labeling tasks in a MTL setup, and shows that MTL is not always effective, because significant improvements are obtained only for 1 out of 5 tasks."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145306271"
                        ],
                        "name": "Neil D. Lawrence",
                        "slug": "Neil-D.-Lawrence",
                        "structuredName": {
                            "firstName": "Neil",
                            "lastName": "Lawrence",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Neil D. Lawrence"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189092"
                        ],
                        "name": "John C. Platt",
                        "slug": "John-C.-Platt",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Platt",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John C. Platt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15764546,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "73e1c4a1152a75ec7310adfb4b8daea16d627bc7",
            "isKey": false,
            "numCitedBy": 342,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes an efficient method for learning the parameters of a Gaussian process (GP). The parameters are learned from multiple tasks which are assumed to have been drawn independently from the same GP prior. An efficient algorithm is obtained by extending the informative vector machine (IVM) algorithm to handle the multi-task learning case. The multi-task IVM (MTIVM) saves computation by greedily selecting the most informative examples from the separate tasks. The MT-IVM is also shown to be more efficient than random sub-sampling on an artificial data-set and more effective than the traditional IVM in a speaker dependent phoneme recognition task."
            },
            "slug": "Learning-to-learn-with-the-informative-vector-Lawrence-Platt",
            "title": {
                "fragments": [],
                "text": "Learning to learn with the informative vector machine"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The multi-task IVM (MTIVM) saves computation by greedily selecting the most informative examples from the separate tasks and is shown to be more efficient than random sub-sampling on an artificial data-set and more effective than the traditional IVM in a speaker dependent phoneme recognition task."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398965769"
                        ],
                        "name": "Y. Abu-Mostafa",
                        "slug": "Y.-Abu-Mostafa",
                        "structuredName": {
                            "firstName": "Yaser",
                            "lastName": "Abu-Mostafa",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Abu-Mostafa"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 45
                            }
                        ],
                        "text": "The easiest way to do this is through hints [Abu-Mostafa, 1990], i.e. directly training the model to predict the most important features."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 319536,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e3cd36c092abd65d6ac8e648f3468eeee90ee1fc",
            "isKey": false,
            "numCitedBy": 258,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-from-hints-in-neural-networks-Abu-Mostafa",
            "title": {
                "fragments": [],
                "text": "Learning from hints in neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "J. Complex."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143626983"
                        ],
                        "name": "B. Bakker",
                        "slug": "B.-Bakker",
                        "structuredName": {
                            "firstName": "Bart",
                            "lastName": "Bakker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Bakker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790356"
                        ],
                        "name": "T. Heskes",
                        "slug": "T.-Heskes",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Heskes",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Heskes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10436583,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a43d7b8e5e1bcb7c3fbf82164cfc9d12737176e8",
            "isKey": false,
            "numCitedBy": 594,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Modeling a collection of similar regression or classification tasks can be improved by making the tasks 'learn from each other'. In machine learning, this subject is approached through 'multitask learning', where parallel tasks are modeled as multiple outputs of the same network. In multilevel analysis this is generally implemented through the mixed-effects linear model where a distinction is made between 'fixed effects', which are the same for all tasks, and 'random effects', which may vary between tasks. In the present article we will adopt a Bayesian approach in which some of the model parameters are shared (the same for all tasks) and others more loosely connected through a joint prior distribution that can be learned from the data. We seek in this way to combine the best parts of both the statistical multilevel approach and the neural network machinery. The standard assumption expressed in both approaches is that each task can learn equally well from any other task. In this article we extend the model by allowing more differentiation in the similarities between tasks. One such extension is to make the prior mean depend on higher-level task characteristics. More unsupervised clustering of tasks is obtained if we go from a single Gaussian prior to a mixture of Gaussians. This can be further generalized to a mixture of experts architecture with the gates depending on task characteristics. All three extensions are demonstrated through application both on an artificial data set and on two real-world problems, one a school problem and the other involving single-copy newspaper sales."
            },
            "slug": "Task-Clustering-and-Gating-for-Bayesian-Multitask-Bakker-Heskes",
            "title": {
                "fragments": [],
                "text": "Task Clustering and Gating for Bayesian Multitask Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A Bayesian approach is adopted in which some of the model parameters are shared and others more loosely connected through a joint prior distribution that can be learned from the data to combine the best parts of both the statistical multilevel approach and the neural network machinery."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46867608"
                        ],
                        "name": "Yu Zhang",
                        "slug": "Yu-Zhang",
                        "structuredName": {
                            "firstName": "Yu",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yu Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739816"
                        ],
                        "name": "D. Yeung",
                        "slug": "D.-Yeung",
                        "structuredName": {
                            "firstName": "Dit-Yan",
                            "lastName": "Yeung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Yeung"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18237764,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c993431e61e524565cd2e86435978e1b47067949",
            "isKey": false,
            "numCitedBy": 422,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Multi-task learning is a learning paradigm which seeks to improve the generalization performance of a learning task with the help of some other related tasks. In this paper, we propose a regularization formulation for learning the relationships between tasks in multi-task learning. This formulation can be viewed as a novel generalization of the regularization framework for single-task learning. Besides modeling positive task correlation, our method, called multi-task relationship learning (MTRL), can also describe negative task correlation and identify outlier tasks based on the same underlying principle. Under this regularization framework, the objective function of MTRL is convex. For efficiency, we use an alternating method to learn the optimal model parameters for each task as well as the relationships between tasks. We study MTRL in the symmetric multi-task learning setting and then generalize it to the asymmetric setting as well. We also study the relationships between MTRL and some existing multi-task learning methods. Experiments conducted on a toy problem as well as several benchmark data sets demonstrate the effectiveness of MTRL."
            },
            "slug": "A-Convex-Formulation-for-Learning-Task-in-Learning-Zhang-Yeung",
            "title": {
                "fragments": [],
                "text": "A Convex Formulation for Learning Task Relationships in Multi-Task Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper proposes a regularization formulation for learning the relationships between tasks in multi-task learning, called MTRL, which can also describe negative task correlation and identify outlier tasks based on the same underlying principle."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693407"
                        ],
                        "name": "K. Crammer",
                        "slug": "K.-Crammer",
                        "structuredName": {
                            "firstName": "Koby",
                            "lastName": "Crammer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Crammer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144830983"
                        ],
                        "name": "Y. Mansour",
                        "slug": "Y.-Mansour",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Mansour",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Mansour"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5686301,
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "id": "0bc45477b85737c5647459bc42b82fd08b19472d",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work we consider a setting where we have a very large number of related tasks with few examples from each individual task. Rather than either learning each task individually (and having a large generalization error) or learning all the tasks together using a single hypothesis (and suffering a potentially large inherent error), we consider learning a small pool of shared hypotheses. Each task is then mapped to a single hypothesis in the pool (hard association). We derive VC dimension generalization bounds for our model, based on the number of tasks, shared hypothesis and the VC dimension of the hypotheses class. We conducted experiments with both synthetic problems and sentiment of reviews, which strongly support our approach."
            },
            "slug": "Learning-Multiple-Tasks-using-Shared-Hypotheses-Crammer-Mansour",
            "title": {
                "fragments": [],
                "text": "Learning Multiple Tasks using Shared Hypotheses"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "This work considers a setting where a very large number of related tasks with few examples from each individual task, and considers learning a small pool of shared hypotheses, which derives VC dimension generalization bounds for the model based on the number of tasks, shared hypothesis and the VC dimension of the hypotheses class."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3157944"
                        ],
                        "name": "A. Saha",
                        "slug": "A.-Saha",
                        "structuredName": {
                            "firstName": "Avishek",
                            "lastName": "Saha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Saha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145593549"
                        ],
                        "name": "Piyush Rai",
                        "slug": "Piyush-Rai",
                        "structuredName": {
                            "firstName": "Piyush",
                            "lastName": "Rai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piyush Rai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722360"
                        ],
                        "name": "Hal Daum\u00e9",
                        "slug": "Hal-Daum\u00e9",
                        "structuredName": {
                            "firstName": "Hal",
                            "lastName": "Daum\u00e9",
                            "middleNames": [],
                            "suffix": "III"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hal Daum\u00e9"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747652"
                        ],
                        "name": "Suresh Venkatasubramanian",
                        "slug": "Suresh-Venkatasubramanian",
                        "structuredName": {
                            "firstName": "Suresh",
                            "lastName": "Venkatasubramanian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Suresh Venkatasubramanian"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7520138,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4356f269ac8724fd2d468efd64fd897aabd85282",
            "isKey": false,
            "numCitedBy": 127,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose an Online MultiTask Learning (Omtl) framework which simultaneously learns the task weight vectors as well as the task relatedness adaptively from the data. Our work is in contrast with prior work on online multitask learning which assumes fixed task relatedness, a priori. Furthermore, whereas prior work in such settings assume only positively correlated tasks, our framework can capture negative correlations as well. Our proposed framework learns the task relationship matrix by framing the objective function as a Bregman divergence minimization problem for positive definite matrices. Subsequently, we exploit this adaptively learned task-relationship matrix to select the most informative samples in an online multitask active learning setting. Experimental results on a number of real-world datasets and comparisons with numerous baselines establish the efficacy of our proposed approach."
            },
            "slug": "Online-Learning-of-Multiple-Tasks-and-Their-Saha-Rai",
            "title": {
                "fragments": [],
                "text": "Online Learning of Multiple Tasks and Their Relationships"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "This work proposes an Online MultiTask Learning (Omtl) framework which simultaneously learns the task weight vectors as well as the task relatedness adaptively from the data, and exploits this adaptively learned task-relationship matrix to select the most informative samples in an online multitask active learning setting."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1801089"
                        ],
                        "name": "T. Evgeniou",
                        "slug": "T.-Evgeniou",
                        "structuredName": {
                            "firstName": "Theodoros",
                            "lastName": "Evgeniou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Evgeniou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708279"
                        ],
                        "name": "C. Micchelli",
                        "slug": "C.-Micchelli",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Micchelli",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Micchelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704699"
                        ],
                        "name": "M. Pontil",
                        "slug": "M.-Pontil",
                        "structuredName": {
                            "firstName": "Massimiliano",
                            "lastName": "Pontil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pontil"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16193644,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0af75728bec67f698a8c619645165de13780c2fa",
            "isKey": false,
            "numCitedBy": 905,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the problem of learning many related tasks simultaneously using kernel methods and regularization. The standard single-task kernel methods, such as support vector machines and regularization networks, are extended to the case of multi-task learning. Our analysis shows that the problem of estimating many task functions with regularization can be cast as a single task learning problem if a family of multi-task kernel functions we define is used. These kernels model relations among the tasks and are derived from a novel form of regularizers. Specific kernels that can be used for multi-task learning are provided and experimentally tested on two real data sets. In agreement with past empirical work on multi-task learning, the experiments show that learning multiple related tasks simultaneously using the proposed approach can significantly outperform standard single-task learning particularly when there are many related tasks but few data per task."
            },
            "slug": "Learning-Multiple-Tasks-with-Kernel-Methods-Evgeniou-Micchelli",
            "title": {
                "fragments": [],
                "text": "Learning Multiple Tasks with Kernel Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "The experiments show that learning multiple related tasks simultaneously using the proposed approach can significantly outperform standard single-task learning particularly when there are many related tasks but few data per task."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38070424"
                        ],
                        "name": "R. Ando",
                        "slug": "R.-Ando",
                        "structuredName": {
                            "firstName": "Rie",
                            "lastName": "Ando",
                            "middleNames": [
                                "Kubota"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ando"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117881943"
                        ],
                        "name": "Tong Zhang",
                        "slug": "Tong-Zhang",
                        "structuredName": {
                            "firstName": "Tong",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tong Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13650160,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "944e1a7b2c5c62e952418d7684e3cade89c76f87",
            "isKey": false,
            "numCitedBy": 1414,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the most important issues in machine learning is whether one can improve the performance of a supervised learning algorithm by including unlabeled data. Methods that use both labeled and unlabeled data are generally referred to as semi-supervised learning. Although a number of such methods are proposed, at the current stage, we still don't have a complete understanding of their effectiveness. This paper investigates a closely related problem, which leads to a novel approach to semi-supervised learning. Specifically we consider learning predictive structures on hypothesis spaces (that is, what kind of classifiers have good predictive power) from multiple learning tasks. We present a general framework in which the structural learning problem can be formulated and analyzed theoretically, and relate it to learning with unlabeled data. Under this framework, algorithms for structural learning will be proposed, and computational issues will be investigated. Experiments will be given to demonstrate the effectiveness of the proposed algorithms in the semi-supervised learning setting."
            },
            "slug": "A-Framework-for-Learning-Predictive-Structures-from-Ando-Zhang",
            "title": {
                "fragments": [],
                "text": "A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper presents a general framework in which the structural learning problem can be formulated and analyzed theoretically, and relate it to learning with unlabeled data, and algorithms for structural learning will be proposed, and computational issues will be investigated."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47645184"
                        ],
                        "name": "Alex Kendall",
                        "slug": "Alex-Kendall",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Kendall",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Kendall"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681954"
                        ],
                        "name": "Y. Gal",
                        "slug": "Y.-Gal",
                        "structuredName": {
                            "firstName": "Yarin",
                            "lastName": "Gal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Gal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745672"
                        ],
                        "name": "R. Cipolla",
                        "slug": "R.-Cipolla",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Cipolla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Cipolla"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4800342,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f98788f32b0d33d200c9bc7d900d0ef39519c927",
            "isKey": false,
            "numCitedBy": 1374,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Numerous deep learning applications benefit from multitask learning with multiple regression and classification objectives. In this paper we make the observation that the performance of such systems is strongly dependent on the relative weighting between each task's loss. Tuning these weights by hand is a difficult and expensive process, making multi-task learning prohibitive in practice. We propose a principled approach to multi-task deep learning which weighs multiple loss functions by considering the homoscedastic uncertainty of each task. This allows us to simultaneously learn various quantities with different units or scales in both classification and regression settings. We demonstrate our model learning per-pixel depth regression, semantic and instance segmentation from a monocular input image. Perhaps surprisingly, we show our model can learn multi-task weightings and outperform separate models trained individually on each task."
            },
            "slug": "Multi-task-Learning-Using-Uncertainty-to-Weigh-for-Kendall-Gal",
            "title": {
                "fragments": [],
                "text": "Multi-task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A principled approach to multi-task deep learning is proposed which weighs multiple loss functions by considering the homoscedastic uncertainty of each task, allowing us to simultaneously learn various quantities with different units or scales in both classification and regression settings."
            },
            "venue": {
                "fragments": [],
                "text": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8603341"
                        ],
                        "name": "Sulin Liu",
                        "slug": "Sulin-Liu",
                        "structuredName": {
                            "firstName": "Sulin",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sulin Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746914"
                        ],
                        "name": "Sinno Jialin Pan",
                        "slug": "Sinno-Jialin-Pan",
                        "structuredName": {
                            "firstName": "Sinno",
                            "lastName": "Pan",
                            "middleNames": [
                                "Jialin"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sinno Jialin Pan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707357"
                        ],
                        "name": "Qirong Ho",
                        "slug": "Qirong-Ho",
                        "structuredName": {
                            "firstName": "Qirong",
                            "lastName": "Ho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qirong Ho"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2953644,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1003b66aeff1bb2b62792069a054b42127113940",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Multi-task learning aims to learn multiple tasks jointly by exploiting their relatedness to improve the generalization performance for each task. Traditionally, to perform multi-task learning, one needs to centralize data from all the tasks to a single machine. However, in many real-world applications, data of different tasks may be geo-distributed over different local machines. Due to heavy communication caused by transmitting the data and the issue of data privacy and security, it is impossible to send data of different task to a master machine to perform multi-task learning. Therefore, in this paper, we propose a distributed multi-task learning framework that simultaneously learns predictive models for each task as well as task relationships between tasks alternatingly in the parameter server paradigm. In our framework, we first offer a general dual form for a family of regularized multi-task relationship learning methods. Subsequently, we propose a communication-efficient primal-dual distributed optimization algorithm to solve the dual problem by carefully designing local subproblems to make the dual problem decomposable. Moreover, we provide a theoretical convergence analysis for the proposed algorithm, which is specific for distributed multi-task relationship learning. We conduct extensive experiments on both synthetic and real-world datasets to evaluate our proposed framework in terms of effectiveness and convergence."
            },
            "slug": "Distributed-Multi-Task-Relationship-Learning-Liu-Pan",
            "title": {
                "fragments": [],
                "text": "Distributed Multi-Task Relationship Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper proposes a distributed multi-task learning framework that simultaneously learns predictive models for each task as well as task relationships between tasks alternatingly in the parameter server paradigm and proposes a communication-efficient primal-dual distributed optimization algorithm to solve theDual problem by carefully designing local subproblems to make the dual problem decomposable."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2022124"
                        ],
                        "name": "Barbara Plank",
                        "slug": "Barbara-Plank",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Plank",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Barbara Plank"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3017695"
                        ],
                        "name": "H\u00e9ctor Mart\u00ednez Alonso",
                        "slug": "H\u00e9ctor-Mart\u00ednez-Alonso",
                        "structuredName": {
                            "firstName": "H\u00e9ctor",
                            "lastName": "Alonso",
                            "middleNames": [
                                "Mart\u00ednez"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H\u00e9ctor Mart\u00ednez Alonso"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2418468,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9405d0388f90ba1432ef13c21309d8363860e22e",
            "isKey": false,
            "numCitedBy": 134,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "Multitask learning has been applied successfully to a range of tasks, mostly morphosyntactic. However, little is known on when MTL works and whether there are data characteristics that help to determine the success of MTL. In this paper we evaluate a range of semantic sequence labeling tasks in a MTL setup. We examine different auxiliary task configurations, amongst which a novel setup, and correlate their impact to data-dependent conditions. Our results show that MTL is not always effective, because significant improvements are obtained only for 1 out of 5 tasks. When successful, auxiliary tasks with compact and more uniform label distributions are preferable."
            },
            "slug": "When-is-multitask-learning-effective-Semantic-under-Plank-Alonso",
            "title": {
                "fragments": [],
                "text": "When is multitask learning effective? Semantic sequence prediction under varying data conditions"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper evaluates a range of semantic sequence labeling tasks in a MTL setup, and shows that MTL is not always effective, because significant improvements are obtained only for 1 out of 5 tasks."
            },
            "venue": {
                "fragments": [],
                "text": "EACL"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145727186"
                        ],
                        "name": "R. Caruana",
                        "slug": "R.-Caruana",
                        "structuredName": {
                            "firstName": "Rich",
                            "lastName": "Caruana",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Caruana"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 102
                            }
                        ],
                        "text": "Hard parameter sharing is the most commonly used approach to MTL in neural networks and goes back to [Caruana, 1993]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 69
                            }
                        ],
                        "text": "Hard parameter sharing, a technique that was originally proposed by [Caruana, 1993], is still the norm 20 years later."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18522085,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9464d15f4f8d578f93332db4aa1c9c182fd51735",
            "isKey": false,
            "numCitedBy": 734,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Multitask-Learning:-A-Knowledge-Based-Source-of-Caruana",
            "title": {
                "fragments": [],
                "text": "Multitask Learning: A Knowledge-Based Source of Inductive Bias"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145462236"
                        ],
                        "name": "Laurent Jacob",
                        "slug": "Laurent-Jacob",
                        "structuredName": {
                            "firstName": "Laurent",
                            "lastName": "Jacob",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Laurent Jacob"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144570279"
                        ],
                        "name": "F. Bach",
                        "slug": "F.-Bach",
                        "structuredName": {
                            "firstName": "Francis",
                            "lastName": "Bach",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Bach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152303545"
                        ],
                        "name": "Jean-Philippe Vert",
                        "slug": "Jean-Philippe-Vert",
                        "structuredName": {
                            "firstName": "Jean-Philippe",
                            "lastName": "Vert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean-Philippe Vert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 488434,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9491bcc1e54b52bea617283f7f716cf009068bce",
            "isKey": false,
            "numCitedBy": 450,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "In multi-task learning several related tasks are considered simultaneously, with the hope that by an appropriate sharing of information across tasks, each task may benefit from the others. In the context of learning linear functions for supervised classification or regression, this can be achieved by including a priori information about the weight vectors associated with the tasks, and how they are expected to be related to each other. In this paper, we assume that tasks are clustered into groups, which are unknown beforehand, and that tasks within a group have similar weight vectors. We design a new spectral norm that encodes this a priori assumption, without the prior knowledge of the partition of tasks into groups, resulting in a new convex optimization formulation for multi-task learning. We show in simulations on synthetic examples and on the IEDB MHC-I binding dataset, that our approach outperforms well-known convex methods for multi-task learning, as well as related non-convex methods dedicated to the same problem."
            },
            "slug": "Clustered-Multi-Task-Learning:-A-Convex-Formulation-Jacob-Bach",
            "title": {
                "fragments": [],
                "text": "Clustered Multi-Task Learning: A Convex Formulation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A new spectral norm is designed that encodes this a priori assumption that tasks are clustered into groups, which are unknown beforehand, and that tasks within a group have similar weight vectors, resulting in a new convex optimization formulation for multi-task learning."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144867807"
                        ],
                        "name": "S. Thrun",
                        "slug": "S.-Thrun",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Thrun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Thrun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1395754302"
                        ],
                        "name": "Joseph O'Sullivan",
                        "slug": "Joseph-O'Sullivan",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "O'Sullivan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joseph O'Sullivan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10420876,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f42a55da3a222184eee20c67d374a9134b77bdc",
            "isKey": false,
            "numCitedBy": 244,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, there has been an increased interest in \u201clifelong\u201d machine learning methods, that transfer knowledge across multiple learning tasks. Such methods have repeatedly been found to outperform conventional, single-task learning algorithms when the learning tasks are appropriately related. To increase robustness of such approaches, methods are desirable that can reason about the relatedness of individual learning tasks, in order to avoid the danger arising from tasks that are unrelated and thus potentially misleading. This paper describes the task-clustering (TC) algorithm. TC clusters learning tasks into classes of mutually related tasks. When facing a new learning task, TC first determines the most related task cluster, then exploits information selectively from this task cluster only. An empirical study carried out in a mobile robot domain shows that TC outperforms its non-selective counterpart in situations where only a small number of tasks is relevant."
            },
            "slug": "Discovering-Structure-in-Multiple-Learning-Tasks:-Thrun-O'Sullivan",
            "title": {
                "fragments": [],
                "text": "Discovering Structure in Multiple Learning Tasks: The TC Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The task-clustering algorithm TC clusters learning tasks into classes of mutually related tasks, and outperforms its non-selective counterpart in situations where only a small number of tasks is relevant."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676352"
                        ],
                        "name": "Sercan \u00d6. Arik",
                        "slug": "Sercan-\u00d6.-Arik",
                        "structuredName": {
                            "firstName": "Sercan",
                            "lastName": "Arik",
                            "middleNames": [
                                "\u00d6."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sercan \u00d6. Arik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35977287"
                        ],
                        "name": "Mike Chrzanowski",
                        "slug": "Mike-Chrzanowski",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Chrzanowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mike Chrzanowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144638694"
                        ],
                        "name": "Adam Coates",
                        "slug": "Adam-Coates",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Coates",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam Coates"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2040049"
                        ],
                        "name": "G. Diamos",
                        "slug": "G.-Diamos",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Diamos",
                            "middleNames": [
                                "Frederick"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Diamos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9544702"
                        ],
                        "name": "Andrew Gibiansky",
                        "slug": "Andrew-Gibiansky",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Gibiansky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Gibiansky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729563"
                        ],
                        "name": "Yongguo Kang",
                        "slug": "Yongguo-Kang",
                        "structuredName": {
                            "firstName": "Yongguo",
                            "lastName": "Kang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yongguo Kang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116235414"
                        ],
                        "name": "Xian Li",
                        "slug": "Xian-Li",
                        "structuredName": {
                            "firstName": "Xian",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xian Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116337165"
                        ],
                        "name": "John Miller",
                        "slug": "John-Miller",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Miller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145175379"
                        ],
                        "name": "Andrew Ng",
                        "slug": "Andrew-Ng",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34042420"
                        ],
                        "name": "Jonathan Raiman",
                        "slug": "Jonathan-Raiman",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Raiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Raiman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2264597"
                        ],
                        "name": "Shubho Sengupta",
                        "slug": "Shubho-Sengupta",
                        "structuredName": {
                            "firstName": "Shubho",
                            "lastName": "Sengupta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shubho Sengupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1911755"
                        ],
                        "name": "M. Shoeybi",
                        "slug": "M.-Shoeybi",
                        "structuredName": {
                            "firstName": "Mohammad",
                            "lastName": "Shoeybi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Shoeybi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5580515,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "63880b57b95de8afd73036e55b9c4bccb7a528b9",
            "isKey": false,
            "numCitedBy": 429,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We present Deep Voice, a production-quality text-to-speech system constructed entirely from deep neural networks. Deep Voice lays the groundwork for truly end-to-end neural speech synthesis. The system comprises five major building blocks: a segmentation model for locating phoneme boundaries, a grapheme-to-phoneme conversion model, a phoneme duration prediction model, a fundamental frequency prediction model, and an audio synthesis model. For the segmentation model, we propose a novel way of performing phoneme boundary detection with deep neural networks using connectionist temporal classification (CTC) loss. For the audio synthesis model, we implement a variant of WaveNet that requires fewer parameters and trains faster than the original. By using a neural network for each component, our system is simpler and more flexible than traditional text-to-speech systems, where each component requires laborious feature engineering and extensive domain expertise. Finally, we show that inference with our system can be performed faster than real time and describe optimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x speedups over existing implementations."
            },
            "slug": "Deep-Voice:-Real-time-Neural-Text-to-Speech-Arik-Chrzanowski",
            "title": {
                "fragments": [],
                "text": "Deep Voice: Real-time Neural Text-to-Speech"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Deep Voice lays the groundwork for truly end-to-end neural speech synthesis and shows that inference with the system can be performed faster than real time and describes optimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x speedups over existing implementations."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3038681"
                        ],
                        "name": "Giovanni Cavallanti",
                        "slug": "Giovanni-Cavallanti",
                        "structuredName": {
                            "firstName": "Giovanni",
                            "lastName": "Cavallanti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Giovanni Cavallanti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388387856"
                        ],
                        "name": "N. Cesa-Bianchi",
                        "slug": "N.-Cesa-Bianchi",
                        "structuredName": {
                            "firstName": "Nicol\u00f2",
                            "lastName": "Cesa-Bianchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cesa-Bianchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895207"
                        ],
                        "name": "C. Gentile",
                        "slug": "C.-Gentile",
                        "structuredName": {
                            "firstName": "Claudio",
                            "lastName": "Gentile",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Gentile"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1443517,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd121bc3b54f847579fd0560f422a1241a03d579",
            "isKey": false,
            "numCitedBy": 184,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce new Perceptron-based algorithms for the online multitask binary classification problem. Under suitable regularity conditions, our algorithms are shown to improve on their baselines by a factor proportional to the number of tasks. We achieve these improvements using various types of regularization that bias our algorithms towards specific notions of task relatedness. More specifically, similarity among tasks is either measured in terms of the geometric closeness of the task reference vectors or as a function of the dimension of their spanned subspace. In addition to adapting to the online setting a mix of known techniques, such as the multitask kernels of Evgeniou et al., our analysis also introduces a matrix-based multitask extension of the p-norm Perceptron, which is used to implement spectral co-regularization. Experiments on real-world data sets complement and support our theoretical findings."
            },
            "slug": "Linear-Algorithms-for-Online-Multitask-Cavallanti-Cesa-Bianchi",
            "title": {
                "fragments": [],
                "text": "Linear Algorithms for Online Multitask Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "Under suitable regularity conditions, new Perceptron-based algorithms for the online multitask binary classification problem are shown to improve on their baselines by a factor proportional to the number of tasks."
            },
            "venue": {
                "fragments": [],
                "text": "COLT"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3152448"
                        ],
                        "name": "Zhanpeng Zhang",
                        "slug": "Zhanpeng-Zhang",
                        "structuredName": {
                            "firstName": "Zhanpeng",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhanpeng Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47571885"
                        ],
                        "name": "Ping Luo",
                        "slug": "Ping-Luo",
                        "structuredName": {
                            "firstName": "Ping",
                            "lastName": "Luo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ping Luo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717179"
                        ],
                        "name": "Chen Change Loy",
                        "slug": "Chen-Change-Loy",
                        "structuredName": {
                            "firstName": "Chen",
                            "lastName": "Loy",
                            "middleNames": [
                                "Change"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chen Change Loy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50295995"
                        ],
                        "name": "Xiaoou Tang",
                        "slug": "Xiaoou-Tang",
                        "structuredName": {
                            "firstName": "Xiaoou",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaoou Tang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14181993,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f500b1a7df00f67c417673e0538d86abb8a333fa",
            "isKey": false,
            "numCitedBy": 1180,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Facial landmark detection has long been impeded by the problems of occlusion and pose variation. Instead of treating the detection task as a single and independent problem, we investigate the possibility of improving detection robustness through multi-task learning. Specifically, we wish to optimize facial landmark detection together with heterogeneous but subtly correlated tasks, e.g. head pose estimation and facial attribute inference. This is non-trivial since different tasks have different learning difficulties and convergence rates. To address this problem, we formulate a novel tasks-constrained deep model, with task-wise early stopping to facilitate learning convergence. Extensive evaluations show that the proposed task-constrained learning (i) outperforms existing methods, especially in dealing with faces with severe occlusion and pose variation, and (ii) reduces model complexity drastically compared to the state-of-the-art method based on cascaded deep model [21]."
            },
            "slug": "Facial-Landmark-Detection-by-Deep-Multi-task-Zhang-Luo",
            "title": {
                "fragments": [],
                "text": "Facial Landmark Detection by Deep Multi-task Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A novel tasks-constrained deep model is formulated, with task-wise early stopping to facilitate learning convergence and reduces model complexity drastically compared to the state-of-the-art method based on cascaded deep model."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145378739"
                        ],
                        "name": "Long Duong",
                        "slug": "Long-Duong",
                        "structuredName": {
                            "firstName": "Long",
                            "lastName": "Duong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Long Duong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143620680"
                        ],
                        "name": "Trevor Cohn",
                        "slug": "Trevor-Cohn",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Cohn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Cohn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21308992"
                        ],
                        "name": "Steven Bird",
                        "slug": "Steven-Bird",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Bird",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven Bird"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145130572"
                        ],
                        "name": "Paul Cook",
                        "slug": "Paul-Cook",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Cook",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul Cook"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17263016,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ea4b9f05d91c91c99a654b623c0ed87de4427ed9",
            "isKey": false,
            "numCitedBy": 243,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Training a high-accuracy dependency parser requires a large treebank. However, these are costly and time-consuming to build. We propose a learning method that needs less data, based on the observation that there are underlying shared structures across languages. We exploit cues from a different source language in order to guide the learning process. Our model saves at least half of the annotation effort to reach the same accuracy compared with using the purely supervised method."
            },
            "slug": "Low-Resource-Dependency-Parsing:-Cross-lingual-in-a-Duong-Cohn",
            "title": {
                "fragments": [],
                "text": "Low Resource Dependency Parsing: Cross-lingual Parameter Sharing in a Neural Network Parser"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "This work proposes a learning method that needs less data, based on the observation that there are underlying shared structures across languages, and exploits cues from a different source language in order to guide the learning process."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1401829700"
                        ],
                        "name": "Shai Ben-David",
                        "slug": "Shai-Ben-David",
                        "structuredName": {
                            "firstName": "Shai",
                            "lastName": "Ben-David",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shai Ben-David"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2952147"
                        ],
                        "name": "Reba Schuller Borbely",
                        "slug": "Reba-Schuller-Borbely",
                        "structuredName": {
                            "firstName": "Reba",
                            "lastName": "Borbely",
                            "middleNames": [
                                "Schuller"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Reba Schuller Borbely"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13967968,
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "id": "3a4551508f84a3f5447d3490b2db95b4d87a7969",
            "isKey": false,
            "numCitedBy": 345,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "The approach of learning of multiple \u201crelated\u201d tasks simultaneously has proven quite successful in practice; however, theoretical justification for this success has remained elusive. The starting point for previous work on multiple task learning has been that the tasks to be learned jointly are somehow \u201calgorithmically related\u201d, in the sense that the results of applying a specific learning algorithm to these tasks are assumed to be similar. We offer an alternative approach, defining relatedness of tasks on the basis of similarity between the example generating distributions that underline these task."
            },
            "slug": "Exploiting-Task-Relatedness-for-Mulitple-Task-Ben-David-Borbely",
            "title": {
                "fragments": [],
                "text": "Exploiting Task Relatedness for Mulitple Task Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work offers an alternative approach to multiple task learning, defining relatedness of tasks on the basis of similarity between the example generating distributions that underline these task."
            },
            "venue": {
                "fragments": [],
                "text": "COLT"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683647"
                        ],
                        "name": "X. Chen",
                        "slug": "X.-Chen",
                        "structuredName": {
                            "firstName": "X.",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1692950"
                        ],
                        "name": "Seyoung Kim",
                        "slug": "Seyoung-Kim",
                        "structuredName": {
                            "firstName": "Seyoung",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seyoung Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39436683"
                        ],
                        "name": "Qihang Lin",
                        "slug": "Qihang-Lin",
                        "structuredName": {
                            "firstName": "Qihang",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qihang Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143712374"
                        ],
                        "name": "J. Carbonell",
                        "slug": "J.-Carbonell",
                        "structuredName": {
                            "firstName": "Jaime",
                            "lastName": "Carbonell",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Carbonell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143977260"
                        ],
                        "name": "E. Xing",
                        "slug": "E.-Xing",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Xing",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Xing"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2243731,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c509e5b5eca48651b30fc29fad22ced4b29b6eec",
            "isKey": false,
            "numCitedBy": 97,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of learning a structured multi-task regression, where the output consists of multiple responses that are related by a graph and the correlated response variables are dependent on the common inputs in a sparse but synergistic manner. Previous methods such as l1/l2-regularized multi-task regression assume that all of the output variables are equally related to the inputs, although in many real-world problems, outputs are related in a complex manner. In this paper, we propose graph-guided fused lasso (GFlasso) for structured multi-task regression that exploits the graph structure over the output variables. We introduce a novel penalty function based on fusion penalty to encourage highly correlated outputs to share a common set of relevant inputs. In addition, we propose a simple yet efficient proximal-gradient method for optimizing GFlasso that can also be applied to any optimization problems with a convex smooth loss and the general class of fusion penalty defined on arbitrary graph structures. By exploiting the structure of the non-smooth ''fusion penalty'', our method achieves a faster convergence rate than the standard first-order method, sub-gradient method, and is significantly more scalable than the widely adopted second-order cone-programming and quadratic-programming formulations. In addition, we provide an analysis of the consistency property of the GFlasso model. Experimental results not only demonstrate the superiority of GFlasso over the standard lasso but also show the efficiency and scalability of our proximal-gradient method."
            },
            "slug": "Graph-Structured-Multi-task-Regression-and-an-for-Chen-Kim",
            "title": {
                "fragments": [],
                "text": "Graph-Structured Multi-task Regression and an Efficient Optimization Method for General Fused Lasso"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This paper proposes graph-guided fused lasso (GFlasso) for structured multi-task regression that exploits the graph structure over the output variables and introduces a novel penalty function based on fusion penalty to encourage highly correlated outputs to share a common set of relevant inputs."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1692950"
                        ],
                        "name": "Seyoung Kim",
                        "slug": "Seyoung-Kim",
                        "structuredName": {
                            "firstName": "Seyoung",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seyoung Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143977260"
                        ],
                        "name": "E. Xing",
                        "slug": "E.-Xing",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Xing",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Xing"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7894269,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4f5e2d78128805249cff3075dc7e8e526f0e4fb1",
            "isKey": false,
            "numCitedBy": 428,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of learning a sparse multi-task regression, where the structure in the outputs can be represented as a tree with leaf nodes as outputs and internal nodes as clusters of the outputs at multiple granularity. Our goal is to recover the common set of relevant inputs for each output cluster. Assuming that the tree structure is available as prior knowledge, we formulate this problem as a new multi-task regularized regression called tree-guided group lasso. Our structured regularization is based on a group-lasso penalty, where groups are defined with respect to the tree structure. We describe a systematic weighting scheme for the groups in the penalty such that each output variable is penalized in a balanced manner even if the groups overlap. We present an efficient optimization method that can handle a large-scale problem. Using simulated and yeast datasets, we demonstrate that our method shows a superior performance in terms of both prediction errors and recovery of true sparsity patterns compared to other methods for multi-task learning."
            },
            "slug": "Tree-Guided-Group-Lasso-for-Multi-Task-Regression-Kim-Xing",
            "title": {
                "fragments": [],
                "text": "Tree-Guided Group Lasso for Multi-Task Regression with Structured Sparsity"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "This work considers the problem of learning a sparse multi-task regression, where the structure in the outputs can be represented as a tree with leaf nodes as outputs and internal nodes as clusters of the outputs at multiple granularity, and proposes a structured regularization based on a group-lasso penalty."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790356"
                        ],
                        "name": "T. Heskes",
                        "slug": "T.-Heskes",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Heskes",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Heskes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 95
                            }
                        ],
                        "text": "Much other work on learning task relationships for multi-task learning uses Bayesian methods: [Heskes, 2000] propose a Bayesian neural network for multi-task learning by placing a prior on the model parameters to encourage similar parameters across tasks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1376989,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "446d6b48f79fce24cb12f293e3b161112be261a6",
            "isKey": false,
            "numCitedBy": 105,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new model for studying mul-titask learning, linking theoretical results to practical simulations. In our model all tasks are combined in a single feedforward neu-ral network. Learning is implemented in a Bayesian fashion. In this Bayesian framework the hidden-to-output weights, being speciic to each task, play the role of model parameters. The input-to-hidden weights, which are shared between all tasks, are treated as hyperparameters. Other hyper-parameters describe error variance and correlations and priors for the model parameters. An important feature of our model is that the probability of these hyperparam-eters given the data can be computed ex-plicitely and only depends on a set of suu-cient statistics. None of these statistics scales with the number of tasks or patterns, which makes empirical Bayes for multitask learning a relatively straightforward optimization problem. Simulations on real-world data sets on single-copy newspaper and magazine sales illustrate properties of multitask learning. Most notably we derive experimental curves for \\learning to learn\" that can be linked to theoretical results obtained elsewhere."
            },
            "slug": "Empirical-Bayes-for-Learning-to-Learn-Heskes",
            "title": {
                "fragments": [],
                "text": "Empirical Bayes for Learning to Learn"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "A new model for studying mul-titask learning is presented, linking theoretical results to practical simulations, and experimental curves for \"learning to learn\" that can be linked to theoretical results obtained elsewhere are derived."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114075909"
                        ],
                        "name": "Kai Yu",
                        "slug": "Kai-Yu",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700754"
                        ],
                        "name": "Volker Tresp",
                        "slug": "Volker-Tresp",
                        "structuredName": {
                            "firstName": "Volker",
                            "lastName": "Tresp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Volker Tresp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2071649"
                        ],
                        "name": "Anton Schwaighofer",
                        "slug": "Anton-Schwaighofer",
                        "structuredName": {
                            "firstName": "Anton",
                            "lastName": "Schwaighofer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anton Schwaighofer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13895067,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "135687fa2e5573037748dcf44d1c3d6766809f57",
            "isKey": false,
            "numCitedBy": 406,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of multi-task learning, that is, learning multiple related functions. Our approach is based on a hierarchical Bayesian framework, that exploits the equivalence between parametric linear models and nonparametric Gaussian processes (GPs). The resulting models can be learned easily via an EM-algorithm. Empirical studies on multi-label text categorization suggest that the presented models allow accurate solutions of these multi-task problems."
            },
            "slug": "Learning-Gaussian-processes-from-multiple-tasks-Yu-Tresp",
            "title": {
                "fragments": [],
                "text": "Learning Gaussian processes from multiple tasks"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "This work considers the problem of multi-task learning, that is, learning multiple related functions, and presents a hierarchical Bayesian framework, that exploits the equivalence between parametric linear models and nonparametric Gaussian processes."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47392513"
                        ],
                        "name": "Jonathan Baxter",
                        "slug": "Jonathan-Baxter",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Baxter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Baxter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12565208,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1bd6e929ed8384ea2212d50ab3c103ec018cc9fd",
            "isKey": false,
            "numCitedBy": 382,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "A Bayesian model of learning to learn by sampling from multiple tasks is presented. The multiple tasks are themselves generated by sampling from a distribution over an environment of related tasks. Such an environment is shown to be naturally modelled within a Bayesian context by the concept of an objective prior distribution. It is argued that for many common machine learning problems, although in general we do not know the true (objective) prior for the problem, we do have some idea of a set of possible priors to which the true prior belongs. It is shown that under these circumstances a learner can use Bayesian inference to learn the true prior by learning sufficiently many tasks from the environment. In addition, bounds are given on the amount of information required to learn a task when it is simultaneously learnt with several other tasks. The bounds show that if the learner has little knowledge of the true prior, but the dimensionality of the true prior is small, then sampling multiple tasks is highly advantageous. The theory is applied to the problem of learning a common feature set or equivalently a low-dimensional-representation (LDR) for an environment of related tasks."
            },
            "slug": "A-Bayesian/Information-Theoretic-Model-of-Learning-Baxter",
            "title": {
                "fragments": [],
                "text": "A Bayesian/Information Theoretic Model of Learning to Learn via Multiple Task Sampling"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is argued that for many common machine learning problems, although in general the authors do not know the true (objective) prior for the problem, they do have some idea of a set of possible priors to which the true prior belongs."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2777800"
                        ],
                        "name": "Karim Lounici",
                        "slug": "Karim-Lounici",
                        "structuredName": {
                            "firstName": "Karim",
                            "lastName": "Lounici",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karim Lounici"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704699"
                        ],
                        "name": "M. Pontil",
                        "slug": "M.-Pontil",
                        "structuredName": {
                            "firstName": "Massimiliano",
                            "lastName": "Pontil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pontil"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2667773"
                        ],
                        "name": "A. Tsybakov",
                        "slug": "A.-Tsybakov",
                        "structuredName": {
                            "firstName": "Alexandre",
                            "lastName": "Tsybakov",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tsybakov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "88941006"
                        ],
                        "name": "S. Geer",
                        "slug": "S.-Geer",
                        "structuredName": {
                            "firstName": "Sara",
                            "lastName": "Geer",
                            "middleNames": [
                                "A.",
                                "van",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Geer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5783396,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "382e2f23f36d57656eb815bb70e094e3c48f94b6",
            "isKey": false,
            "numCitedBy": 261,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the problem of estimating multiple linear regression equations for the purpose of both prediction and variable selection. Following recent work on multi-task learning Argyriou et al. [2008], we assume that the regression vectors share the same sparsity pattern. This means that the set of relevant predictor variables is the same across the different equations. This assumption leads us to consider the Group Lasso as a candidate estimation method. We show that this estimator enjoys nice sparsity oracle inequalities and variable selection properties. The results hold under a certain restricted eigenvalue condition and a coherence condition on the design matrix, which naturally extend recent work in Bickel et al. [2007], Lounici [2008]. In particular, in the multi-task learning scenario, in which the number of tasks can grow, we are able to remove completely the effect of the number of predictor variables in the bounds. Finally, we show how our results can be extended to more general noise distributions, of which we only require the variance to be finite."
            },
            "slug": "Taking-Advantage-of-Sparsity-in-Multi-Task-Learning-Lounici-Pontil",
            "title": {
                "fragments": [],
                "text": "Taking Advantage of Sparsity in Multi-Task Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The Group Lasso is considered as a candidate estimation method and it is shown that this estimator enjoys nice sparsity oracle inequalities and variable selection properties and can be extended to more general noise distributions, of which it only requires the variance to be finite."
            },
            "venue": {
                "fragments": [],
                "text": "COLT"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722360"
                        ],
                        "name": "Hal Daum\u00e9",
                        "slug": "Hal-Daum\u00e9",
                        "structuredName": {
                            "firstName": "Hal",
                            "lastName": "Daum\u00e9",
                            "middleNames": [],
                            "suffix": "III"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hal Daum\u00e9"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6257661,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b5043c731758b60f22a439afb322df83cfeb8c7c",
            "isKey": false,
            "numCitedBy": 121,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We learn multiple hypotheses for related tasks under a latent hierarchical relationship between tasks. We exploit the intuition that for domain adaptation, we wish to share classifier structure, but for multitask learning, we wish to share covariance structure. Our hierarchical model is seen to subsume several previously proposed multitask learning models and performs well on three distinct real-world data sets."
            },
            "slug": "Bayesian-Multitask-Learning-with-Latent-Hierarchies-Daum\u00e9",
            "title": {
                "fragments": [],
                "text": "Bayesian Multitask Learning with Latent Hierarchies"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A hierarchical model is seen to subsume several previously proposed multitask learning models and performs well on three distinct real-world data sets."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 217
                            }
                        ],
                        "text": "Multi-task learning has been used successfully across all applications of machine learning, from natural language processing [Collobert and Weston, 2008] and speech recognition [Deng et al., 2013] to computer vision [Girshick, 2015] and drug discovery [Ramsundar et al., 2015]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206770307,
            "fieldsOfStudy": [
                "Computer Science",
                "Environmental Science"
            ],
            "id": "7ffdbc358b63378f07311e883dddacc9faeeaf4b",
            "isKey": false,
            "numCitedBy": 14080,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn."
            },
            "slug": "Fast-R-CNN-Girshick",
            "title": {
                "fragments": [],
                "text": "Fast R-CNN"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection that builds on previous work to efficiently classify object proposals using deep convolutional networks."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47392513"
                        ],
                        "name": "Jonathan Baxter",
                        "slug": "Jonathan-Baxter",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Baxter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Baxter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9803204,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "727e1e16ede6eaad241bad11c525da07b154c688",
            "isKey": false,
            "numCitedBy": 973,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "A major problem in machine learning is that of inductive bias: how to choose a learner's hypothesis space so that it is large enough to contain a solution to the problem being learnt, yet small enough to ensure reliable generalization from reasonably-sized training sets. Typically such bias is supplied by hand through the skill and insights of experts. In this paper a model for automatically learning bias is investigated. The central assumption of the model is that the learner is embedded within an environment of related learning tasks. Within such an environment the learner can sample from multiple tasks, and hence it can search for a hypothesis space that contains good solutions to many of the problems in the environment. Under certain restrictions on the set of all hypothesis spaces available to the learner, we show that a hypothesis space that performs well on a sufficiently large number of training tasks will also perform well when learning novel tasks in the same environment. Explicit bounds are also derived demonstrating that learning multiple tasks within an environment of related tasks can potentially give much better generalization than learning a single task."
            },
            "slug": "A-Model-of-Inductive-Bias-Learning-Baxter",
            "title": {
                "fragments": [],
                "text": "A Model of Inductive Bias Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Under certain restrictions on the set of all hypothesis spaces available to the learner, it is shown that a hypothesis space that performs well on a sufficiently large number of training tasks will also perform well when learning novel tasks in the same environment."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145727186"
                        ],
                        "name": "R. Caruana",
                        "slug": "R.-Caruana",
                        "structuredName": {
                            "firstName": "Rich",
                            "lastName": "Caruana",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Caruana"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780015"
                        ],
                        "name": "V. D. Sa",
                        "slug": "V.-D.-Sa",
                        "structuredName": {
                            "firstName": "Virginia",
                            "lastName": "Sa",
                            "middleNames": [
                                "R.",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. D. Sa"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 24334306,
            "fieldsOfStudy": [
                "Economics",
                "Computer Science"
            ],
            "id": "73efa0c00cf2709cd574af68664fd08673006d44",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "In supervised learning there is usually a clear distinction between inputs and outputs - inputs are what you will measure, outputs are what you will predict from those measurements. This paper shows that the distinction between inputs and outputs is not this simple. Some features are more useful as extra outputs than as inputs. By using a feature as an output we get more than just the case values but can learn a mapping from the other inputs to that feature. For many features this mapping may be more useful than the feature value itself. We present two regression problems and one classification problem where performance improves if features that could have been used as inputs are used as extra outputs instead. This result is surprising since a feature used as an output is not used during testing."
            },
            "slug": "Promoting-Poor-Features-to-Supervisors:-Some-Inputs-Caruana-Sa",
            "title": {
                "fragments": [],
                "text": "Promoting Poor Features to Supervisors: Some Inputs Work Better as Outputs"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper presents two regression problems and one classification problem where performance improves if features that could have been used as inputs are used as extra outputs instead and is surprising since a feature used as an output is not used during testing."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145537739"
                        ],
                        "name": "S. Negahban",
                        "slug": "S.-Negahban",
                        "structuredName": {
                            "firstName": "Sahand",
                            "lastName": "Negahban",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Negahban"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721860"
                        ],
                        "name": "M. Wainwright",
                        "slug": "M.-Wainwright",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Wainwright",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Wainwright"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 1
                            }
                        ],
                        "text": "[Negahban and Wainwright, 2008] show that if features do not overlap by much, `1/`q regularization might actually be worse than element-wise `1 regularization."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13957291,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0c2aea1b3b7ccd49f7f7b37416e3cb089554593f",
            "isKey": false,
            "numCitedBy": 84,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Given a collection of r \u2265 2 linear regression problems in p dimensions, suppose that the regression coefficients share partially common supports. This set-up suggests the use of l1/l\u221e-regularized regression for joint estimation of the p x r matrix of regression coefficients. We analyze the high-dimensional scaling of l1/l\u221e-regularized quadratic programming, considering both consistency rates in l\u221e-norm, and also how the minimal sample size n required for performing variable selection grows as a function of the model dimension, sparsity, and overlap between the supports. We begin by establishing bounds on the l\u221e-error as well sufficient conditions for exact variable selection for fixed design matrices, as well as designs drawn randomly from general Gaussian matrices. These results show that the high-dimensional scaling of l1/l\u221e-regularization is qualitatively similar to that of ordinary l1-regularization. Our second set of results applies to design matrices drawn from standard Gaussian ensembles, for which we provide a sharp set of necessary and sufficient conditions: the l1/l\u221e-regularized method undergoes a phase transition characterized by the rescaled sample size \u03b81,\u221e(n,p, s, \u03b1) = n/{(4 - 3\u03b1)s log(p - (2 - \u03b1) s)}. More precisely, for any \u03b4 > 0, the probability of successfully recovering both supports converges to 1 for scalings such that \u03b81,\u221e \u2265 1 + \u03b4, and converges to 0 for scalings for which \u03b81,\u221e \u2264 1-\u03b4. An implication of this threshold is that use of l1,\u221e-regularization yields improved statistical efficiency if the overlap parameter is large enough (\u03b1 > 2/3), but performs worse than a naive Lasso-based approach for moderate to small overlap (\u03b1 < 2/3). We illustrate the close agreement between these theoretical predictions, and the actual behavior in simulations."
            },
            "slug": "Joint-support-recovery-under-high-dimensional-and-\u2113-Negahban-Wainwright",
            "title": {
                "fragments": [],
                "text": "Joint support recovery under high-dimensional scaling: Benefits and perils of \u2113 1,\u221e -regularization"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The high-dimensional scaling of l1/l\u221e-regularized quadratic programming is analyzed, considering both consistency rates in l \u221e-norm, and also how the minimal sample size n required for performing variable selection grows as a function of the model dimension, sparsity, and overlap between the supports."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS 2008"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153884534"
                        ],
                        "name": "Cun-Hui Zhang",
                        "slug": "Cun-Hui-Zhang",
                        "structuredName": {
                            "firstName": "Cun-Hui",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cun-Hui Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732234"
                        ],
                        "name": "Jian Huang",
                        "slug": "Jian-Huang",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Huang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13997789,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d4ff7059aead6b8771f2259bed1c9232d95d28dc",
            "isKey": false,
            "numCitedBy": 775,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Meinshausen and Buhlmann [Ann. Statist. 34 (2006) 1436-1462] showed that, for neighborhood selection in Gaussian graphical models, under a neighborhood stability condition, the LASSO is consistent, even when the number of variables is of greater order than the sample size. Zhao and Yu [(2006) J. Machine Learning Research 7 2541-2567] formalized the neighborhood stability condition in the context of linear regression as a strong irrepresentable condition. That paper showed that under this condition, the LASSO selects exactly the set of nonzero regression coefficients, provided that these coefficients are bounded away from zero at a certain rate. In this paper, the regression coefficients outside an ideal model are assumed to be small, but not necessarily zero. Under a sparse Riesz condition on the correlation of design variables, we prove that the LASSO selects a model of the correct order of dimensionality, controls the bias of the selected model at a level determined by the contributions of small regression coefficients and threshold bias, and selects all coefficients of greater order than the bias of the selected model. Moreover, as a consequence of this rate consistency of the LASSO in model selection, it is proved that the sum of error squares for the mean response and the l \u03b1 -loss for the regression coefficients converge at the best possible rates under the given conditions. An interesting aspect of our results is that the logarithm of the number of variables can be of the same order as the sample size for certain random dependent designs."
            },
            "slug": "The-sparsity-and-bias-of-the-Lasso-selection-in-Zhang-Huang",
            "title": {
                "fragments": [],
                "text": "The sparsity and bias of the Lasso selection in high-dimensional linear regression"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144171839"
                        ],
                        "name": "M. Yuan",
                        "slug": "M.-Yuan",
                        "structuredName": {
                            "firstName": "Ming",
                            "lastName": "Yuan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Yuan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108066200"
                        ],
                        "name": "Yi Lin",
                        "slug": "Yi-Lin",
                        "structuredName": {
                            "firstName": "Yi",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yi Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6162124,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d98ef875e2cbde3e2cc8fad521e3cbfe1bddbd69",
            "isKey": false,
            "numCitedBy": 6387,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Summary.\u2002 We consider the problem of selecting grouped variables (factors) for accurate prediction in regression. Such a problem arises naturally in many practical situations with the multifactor analysis\u2010of\u2010variance problem as the most important and well\u2010known example. Instead of selecting factors by stepwise backward elimination, we focus on the accuracy of estimation and consider extensions of the lasso, the LARS algorithm and the non\u2010negative garrotte for factor selection. The lasso, the LARS algorithm and the non\u2010negative garrotte are recently proposed regression methods that can be used to select individual variables. We study and propose efficient algorithms for the extensions of these methods for factor selection and show that these extensions give superior performance to the traditional stepwise backward elimination method in factor selection problems. We study the similarities and the differences between these methods. Simulations and real examples are used to illustrate the methods."
            },
            "slug": "Model-selection-and-estimation-in-regression-with-Yuan-Lin",
            "title": {
                "fragments": [],
                "text": "Model selection and estimation in regression with grouped variables"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Identifying bene\ufffdcial task relations for multi-task learning in deep neural networks. In EACL"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2017
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 11
                            }
                        ],
                        "text": "[Zhang and Huang, 2008] use `1/`\u221e regularization, while [Argyriou and Pontil, 2007] use a mixed `1/`2 norm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Model selection consistency of the lasso selection in high - dimensional linear regression"
            },
            "venue": {
                "fragments": [],
                "text": "Annals of Statistics"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Joint support recovery under high-dimensional scaling: Benefits and perils of $ell_{1,infty}$-regularization"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems,"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 126
                            }
                        ],
                        "text": "Multi-task learning has been used successfully across all applications of machine learning, from natural language processing [Collobert and Weston, 2008] and speech recognition [Deng et al., 2013] to computer vision [Girshick, 2015] and drug discovery [Ramsundar et al., 2015]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A uni\ufffded architecture for natural language processing"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 25th International Conference on Machine Learning - ICML \u201908,"
            },
            "year": 2008
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 2,
            "methodology": 6
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 62,
        "totalPages": 7
    },
    "page_url": "https://www.semanticscholar.org/paper/An-Overview-of-Multi-Task-Learning-in-Deep-Neural-Ruder/6d431f835c06afdea45dff6b24486bf301ebdef0?sort=total-citations"
}