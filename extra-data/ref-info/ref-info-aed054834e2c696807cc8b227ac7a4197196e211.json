{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115356552"
                        ],
                        "name": "Tsungnan Lin",
                        "slug": "Tsungnan-Lin",
                        "structuredName": {
                            "firstName": "Tsungnan",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tsungnan Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35216199"
                        ],
                        "name": "B. Horne",
                        "slug": "B.-Horne",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Horne",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Horne"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4023505"
                        ],
                        "name": "P. Ti\u0148o",
                        "slug": "P.-Ti\u0148o",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Ti\u0148o",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Ti\u0148o"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145157784"
                        ],
                        "name": "C. Lee Giles",
                        "slug": "C.-Lee-Giles",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Giles",
                            "middleNames": [
                                "Lee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lee Giles"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[17] also propose variants of time-delay networks, called NARX networks (crossreference see also Chapter 11)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6638216,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f6e91c9e7e8f8a577a98ecfcfa998212a683195a",
            "isKey": false,
            "numCitedBy": 629,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "It has previously been shown that gradient-descent learning algorithms for recurrent neural networks can perform poorly on tasks that involve long-term dependencies, i.e. those problems for which the desired output depends on inputs presented at times far in the past. We show that the long-term dependencies problem is lessened for a class of architectures called nonlinear autoregressive models with exogenous (NARX) recurrent neural networks, which have powerful representational capabilities. We have previously reported that gradient descent learning can be more effective in NARX networks than in recurrent neural network architectures that have \"hidden states\" on problems including grammatical inference and nonlinear system identification. Typically, the network converges much faster and generalizes better than other networks. The results in this paper are consistent with this phenomenon. We present some experimental results which show that NARX networks can often retain information for two to three times as long as conventional recurrent neural networks. We show that although NARX networks do not circumvent the problem of long-term dependencies, they can greatly improve performance on long-term dependency problems. We also describe in detail some of the assumptions regarding what it means to latch information robustly and suggest possible ways to loosen these assumptions."
            },
            "slug": "Learning-long-term-dependencies-in-NARX-recurrent-Lin-Horne",
            "title": {
                "fragments": [],
                "text": "Learning long-term dependencies in NARX recurrent neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that the long-term dependencies problem is lessened for a class of architectures called nonlinear autoregressive models with exogenous (NARX) recurrent neural networks, which have powerful representational capabilities."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308557"
                        ],
                        "name": "S. Hochreiter",
                        "slug": "S.-Hochreiter",
                        "structuredName": {
                            "firstName": "Sepp",
                            "lastName": "Hochreiter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hochreiter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 90
                            }
                        ],
                        "text": "There is a novel, efficient, gradient-based method called \u201cLong Short-Term Memory\u201d (LSTM) [13]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 50
                            }
                        ],
                        "text": "It will be interesting to examine to which extent LSTM is applicable to real world problems such as language separation from prosody (first results in [7]) and speech recognition."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 109
                            }
                        ],
                        "text": "In comparisons with RTRL, BPTT, Recurrent Cascade-Correlation, Elman networks, and Neural Sequence Chunking, LSTM led to many more successful runs, and learned much faster."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "LSTM also solved complex, artificial long time lag tasks that have never been solved by previous recurrent network algorithms."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 53
                            }
                        ],
                        "text": "Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete time steps by enforcing constant error flow through \u201cconstant error carrousels\u201d within special units."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "LSTM is designed to get rid of the vanishing error problem."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "LSTM is local in space and time; its computational complexity per time step and weight is O(1)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1915014,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "44d2abe2175df8153f465f6c39b68b76a0d40ab9",
            "isKey": true,
            "numCitedBy": 51653,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms."
            },
            "slug": "Long-Short-Term-Memory-Hochreiter-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Long Short-Term Memory"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A novel, efficient, gradient based method called long short-term memory (LSTM) is introduced, which can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115356552"
                        ],
                        "name": "Tsungnan Lin",
                        "slug": "Tsungnan-Lin",
                        "structuredName": {
                            "firstName": "Tsungnan",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tsungnan Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35216199"
                        ],
                        "name": "B. Horne",
                        "slug": "B.-Horne",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Horne",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Horne"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145157784"
                        ],
                        "name": "C. Lee Giles",
                        "slug": "C.-Lee-Giles",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Giles",
                            "middleNames": [
                                "Lee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lee Giles"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 151
                            }
                        ],
                        "text": "The same idea can be applied to other architectures, by inserting multiple delays in the connections among hidden state units rather than output units [16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14241577,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4e2b855834df1f800414aa4950a22deae63577d6",
            "isKey": false,
            "numCitedBy": 82,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "How-embedded-memory-in-recurrent-neural-network-Lin-Horne",
            "title": {
                "fragments": [],
                "text": "How embedded memory in recurrent neural network architectures helps learning long-term temporal dependencies"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2088368"
                        ],
                        "name": "F. Gers",
                        "slug": "F.-Gers",
                        "structuredName": {
                            "firstName": "Felix",
                            "lastName": "Gers",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Gers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780235"
                        ],
                        "name": "Fred Cummins",
                        "slug": "Fred-Cummins",
                        "structuredName": {
                            "firstName": "Fred",
                            "lastName": "Cummins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fred Cummins"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 11598600,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "11540131eae85b2e11d53df7f1360eeb6476e7f4",
            "isKey": false,
            "numCitedBy": 3203,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "Long short-term memory (LSTM; Hochreiter & Schmidhuber, 1997) can solve numerous tasks not solvable by previous learning algorithms for recurrent neural networks (RNNs). We identify a weakness of LSTM networks processing continual input streams that are not a priori segmented into subsequences with explicitly marked ends at which the network's internal state could be reset. Without resets, the state may grow indefinitely and eventually cause the network to break down. Our remedy is a novel, adaptive forget gate that enables an LSTM cell to learn to reset itself at appropriate times, thus releasing internal resources. We review illustrative benchmark problems on which standard LSTM outperforms other RNN algorithms. All algorithms (including LSTM) fail to solve continual versions of these problems. LSTM with forget gates, however, easily solves them, and in an elegant way."
            },
            "slug": "Learning-to-Forget:-Continual-Prediction-with-LSTM-Gers-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Learning to Forget: Continual Prediction with LSTM"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This work identifies a weakness of LSTM networks processing continual input streams that are not a priori segmented into subsequences with explicitly marked ends at which the network's internal state could be reset, and proposes a novel, adaptive forget gate that enables an LSTm cell to learn to reset itself at appropriate times, thus releasing internal resources."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144473519"
                        ],
                        "name": "M. Mozer",
                        "slug": "M.-Mozer",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Mozer",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mozer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "sequences causing the time lags (see also [18])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 74
                            }
                        ],
                        "text": "For long time lags, however, the time constants need external fine tuning [18]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": "To deal with long time lags, Mozer [18] uses time constants influencing changes of unit activations (deVries and Principe\u2019s related approach [8] may be viewed as a mixture of time-delay neural networks (TDNN) [15] and time constants)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5355536,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e141d68065ce638f9fc4f006eab2f66711e89768",
            "isKey": false,
            "numCitedBy": 160,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning structure in temporally-extended sequences is a difficult computational problem because only a fraction of the relevant information is available at any instant. Although variants of back propagation can in principle be used to find structure in sequences, in practice they are not sufficiently powerful to discover arbitrary contingencies, especially those spanning long temporal intervals or involving high order statistics. For example, in designing a connectionist network for music composition, we have encountered the problem that the net is able to learn musical structure that occurs locally in time--e.g., relations among notes within a musical phrase--but not structure that occurs over longer time periods--e.g., relations among phrases. To address this problem, we require a means of constructing a reduced description of the sequence that makes global aspects more explicit or more readily detectable. I propose to achieve this using hidden units that operate with different time constants. Simulation experiments indicate that slower time-scale hidden units are able to pick up global structure, structure that simply can not be learned by standard back propagation."
            },
            "slug": "Induction-of-Multiscale-Temporal-Structure-Mozer",
            "title": {
                "fragments": [],
                "text": "Induction of Multiscale Temporal Structure"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Simulation experiments indicate that slower time-scale hidden units are able to pick up global structure, structure that simply can not be learned by standard back propagation, using hidden units that operate with different time constants."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688235"
                        ],
                        "name": "P. Frasconi",
                        "slug": "P.-Frasconi",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Frasconi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Frasconi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[6] investigate methods such as simulated annealing, multi-grid random search, and discrete error propagation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 192
                            }
                        ],
                        "text": ", [22]) error signals \u201cflowing backwards in time\u201d tend to either (1) blow up or (2) vanish: the temporal evolution of the backpropagated error exponentially depends on the size of the weights [12, 6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 9
                            }
                        ],
                        "text": "\u2019s paper [6], the analysis of the problem of gradient decays is generalized to parameterized dynamical systems (hence including second order and other recurrent architectures)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 31
                            }
                        ],
                        "text": "Using the analysis reported in [6] we show that one of the following two undesirable situations necessarily arise: either the system is unable to robustly store past information about its inputs, or gradients vanish exponentially."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206457500,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d0be39ee052d246ae99c082a565aba25b811be2d",
            "isKey": false,
            "numCitedBy": 6142,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered."
            },
            "slug": "Learning-long-term-dependencies-with-gradient-is-Bengio-Simard",
            "title": {
                "fragments": [],
                "text": "Learning long-term dependencies with gradient descent is difficult"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work shows why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases, and exposes a trade-off between efficient learning by gradient descent and latching on information for long periods."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2308463"
                        ],
                        "name": "Salah El Hihi",
                        "slug": "Salah-El-Hihi",
                        "structuredName": {
                            "firstName": "Salah",
                            "lastName": "Hihi",
                            "middleNames": [
                                "El"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Salah El Hihi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 22
                            }
                        ],
                        "text": "long-term latching\nIn Bengio et al.\u2019s paper [6], the analysis of the problem of gradient decays is generalized to parameterized dynamical systems (hence including second order and other recurrent architectures)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 0
                            }
                        ],
                        "text": "Bengio et al. [6] investigate methods such as simulated annealing, multi-grid random search, and discrete error propagation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 0
                            }
                        ],
                        "text": "Bengio and Frasconi [4] propose a probabilistic approach for propagating targets."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 26
                            }
                        ],
                        "text": "Finally, El Hihi & Bengio [10] looked at hierarchically organized recurrent neural networks with different levels of time-constants or time-delays."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 136
                            }
                        ],
                        "text": "Sepp Hochreiter\nFakulta\u0308t fu\u0308r Informatik Technische Universita\u0308t Mu\u0308nchen\n80290 Mu\u0308nchen, Germany hochreit@informatik.tu-muenchen.de\nYoshua Bengio\nDept. Informatique et Recherche Ope\u0301rationnelle Universite\u0301 de Montre\u0301al, CP 6128, Succ."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2843869,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b13813b49f160e1a2010c44bd4fb3d09a28446e3",
            "isKey": false,
            "numCitedBy": 332,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We have already shown that extracting long-term dependencies from sequential data is difficult, both for determimstic dynamical systems such as recurrent networks, and probabilistic models such as hidden Markov models (HMMs) or input/output hidden Markov models (IOHMMs). In practice, to avoid this problem, researchers have used domain specific a-priori knowledge to give meaning to the hidden or state variables representing past context. In this paper, we propose to use a more general type of a-priori knowledge, namely that the temporal dependencies are structured hierarchically. This implies that long-term dependencies are represented by variables with a long time scale. This principle is applied to a recurrent network which includes delays and multiple time scales. Experiments confirm the advantages of such structures. A similar approach is proposed for HMMs and IOHMMs."
            },
            "slug": "Hierarchical-Recurrent-Neural-Networks-for-Hihi-Bengio",
            "title": {
                "fragments": [],
                "text": "Hierarchical Recurrent Neural Networks for Long-Term Dependencies"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper proposes to use a more general type of a-priori knowledge, namely that the temporal dependencies are structured hierarchically, which implies that long-term dependencies are represented by variables with a long time scale."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34922532"
                        ],
                        "name": "Guo-Zheng Sun",
                        "slug": "Guo-Zheng-Sun",
                        "structuredName": {
                            "firstName": "Guo-Zheng",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guo-Zheng Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115401300"
                        ],
                        "name": "Hsing-Hen Chen",
                        "slug": "Hsing-Hen-Chen",
                        "structuredName": {
                            "firstName": "Hsing-Hen",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hsing-Hen Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2552960"
                        ],
                        "name": "Yee-Chun Lee",
                        "slug": "Yee-Chun-Lee",
                        "structuredName": {
                            "firstName": "Yee-Chun",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yee-Chun Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 24
                            }
                        ],
                        "text": "\u2019s alternative approach [26] updates the activation of a recurrent unit by adding the old activation and the (scaled) current net input."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17076103,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b1ae0fb208fd389d2ff723e5442f9ca7896cb0a4",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We proposed a model of Time Warping Invariant Neural Networks (TWINN) to handle the time warped continuous signals. Although TWINN is a simple modification of well known recurrent neural network, analysis has shown that TWINN completely removes time warping and is able to handle difficult classification problem. It is also shown that TWINN has certain advantages over the current available sequential processing schemes: Dynamic Programming(DP)[1], Hidden Markov Model(HMM)[2], Time Delayed Neural Networks(TDNN) [3] and Neural Network Finite Automata(NNFA)[4]. \n \nWe also analyzed the time continuity employed in TWINN and pointed out that this kind of structure can memorize longer input history compared with Neural Network Finite Automata (NNFA). This may help to understand the well accepted fact that for learning grammatical reference with NNFA one had to start with very short strings in training set. \n \nThe numerical example we used is a trajectory classification problem. This problem, making a feature of variable sampling rates, having internal states, continuous dynamics, heavily time-warped data and deformed phase space trajectories, is shown to be difficult to other schemes. With TWINN this problem has been learned in 100 iterations. For benchmark we also trained the exact same problem with TDNN and completely failed as expected."
            },
            "slug": "Time-Warping-Invariant-Neural-Networks-Sun-Chen",
            "title": {
                "fragments": [],
                "text": "Time Warping Invariant Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Analysis has shown that TWINN completely removes time warping and is able to handle difficult classification problem, and has certain advantages over the current available sequential processing schemes."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 43
                            }
                        ],
                        "text": "Schmidhuber\u2019s hierarchical chunker systems [24, 25] can in principle bridge arbitrary time lags, but only if there is local predictability across the sub-"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18271205,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "50c770b425a5bb25c77387f687a9910a9d130722",
            "isKey": false,
            "numCitedBy": 428,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous neural network learning algorithms for sequence processing are computationally expensive and perform poorly when it comes to long time lags. This paper first introduces a simple principle for reducing the descriptions of event sequences without loss of information. A consequence of this principle is that only unexpected inputs can be relevant. This insight leads to the construction of neural architectures that learn to divide and conquer by recursively decomposing sequences. I describe two architectures. The first functions as a self-organizing multilevel hierarchy of recurrent networks. The second, involving only two recurrent networks, tries to collapse a multilevel predictor hierarchy into a single recurrent net. Experiments show that the system can require less computation per time step and many fewer training sequences than conventional training algorithms for recurrent nets."
            },
            "slug": "Learning-Complex,-Extended-Sequences-Using-the-of-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Learning Complex, Extended Sequences Using the Principle of History Compression"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A simple principle for reducing the descriptions of event sequences without loss of information is introduced and this insight leads to the construction of neural architectures that learn to divide and conquer by recursively decomposing sequences."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47055692"
                        ],
                        "name": "P. Werbos",
                        "slug": "P.-Werbos",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Werbos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Werbos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 2
                            }
                        ],
                        "text": ", [23, 28, 27]) or \u201cReal-Time Recurrent Learning\u201d (RTRL, e."
                    },
                    "intents": []
                }
            ],
            "corpusId": 205118721,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "266e07d0dd9a75b61e3632e9469993dbaf063f1c",
            "isKey": false,
            "numCitedBy": 882,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Generalization-of-backpropagation-with-application-Werbos",
            "title": {
                "fragments": [],
                "text": "Generalization of backpropagation with application to a recurrent gas market model"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688235"
                        ],
                        "name": "P. Frasconi",
                        "slug": "P.-Frasconi",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Frasconi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Frasconi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 20
                            }
                        ],
                        "text": "Bengio and Frasconi [4] propose a probabilistic approach for propagating targets."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 79
                            }
                        ],
                        "text": "Centre-Ville,\nMontre\u0301al, Que\u0301bec, Canada, H3C 3J7 bengioy@iro.umontreal.ca\nPaolo Frasconi Dept. of Systems and Computer Science\nUniversity of Florence via di Santa Marta 3, 50139 Firenze (Italy)\npaolo@dsi.unifi.it\nJu\u0308rgen Schmidhuber"
                    },
                    "intents": []
                }
            ],
            "corpusId": 15753355,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "13369d124474b5f8dcbc70d12296a185832192b2",
            "isKey": false,
            "numCitedBy": 51,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning to recognize or predict sequences using long-term context has many applications. However, practical and theoretical problems are found in training recurrent neural networks to perform tasks in which input/output dependencies span long intervals. Starting from a mathematical analysis of the problem, we consider and compare alternative algorithms and architectures on tasks for which the span of the input/output dependencies can be controlled. Results on the new algorithms show performance qualitatively superior to that obtained with backpropagation."
            },
            "slug": "Credit-Assignment-through-Time:-Alternatives-to-Bengio-Frasconi",
            "title": {
                "fragments": [],
                "text": "Credit Assignment through Time: Alternatives to Backpropagation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work considers and compares alternative algorithms and architectures on tasks for which the span of the input/output dependencies can be controlled and shows performance qualitatively superior to that obtained with backpropagation."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144902513"
                        ],
                        "name": "P. Baldi",
                        "slug": "P.-Baldi",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Baldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Baldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30320952"
                        ],
                        "name": "F. Pineda",
                        "slug": "F.-Pineda",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pineda",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Pineda"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 221
                            }
                        ],
                        "text": "That is, the error blows up, and conflicting error signals (with opposite signs) arriving at unit v can lead to oscillating weights and unstable learning due to \u201covershooting\u201d (for error blow-ups or bifurcations see also [20, 2, 9])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14926573,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9b7861d28f653ead3e02f1ae5c07540b2d07346d",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The concept of Contrastive Learning (CL) is developed as a family of possible learning algorithms for neural networks. CL is an extension of Deterministic Boltzmann Machines to more general dynamical systems. During learning, the network oscillates between two phases. One phase has a teacher signal and one phase has no teacher signal. The weights are updated using a learning rule that corresponds to gradient descent on a contrast function that measures the discrepancy between the free network and the network with a teacher signal. The CL approach provides a general unified framework for developing new learning algorithms. It also shows that many different types of clamping and teacher signals are possible. Several examples are given and an analysis of the landscape of the contrast function is proposed with some relevant predictions for the CL curves. An approach that may be suitable for collective analog implementations is described. Simulation results and possible extensions are briefly discussed together with a new conjecture regarding the function of certain oscillations in the brain. In the appendix, we also examine two extensions of contrastive learning to time-dependent trajectories."
            },
            "slug": "Contrastive-Learning-and-Neural-Oscillations-Baldi-Pineda",
            "title": {
                "fragments": [],
                "text": "Contrastive Learning and Neural Oscillations"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The CL approach provides a general unified framework for developing new learning algorithms and shows that many different types of clamping and teacher signals are possible, as well as examining two extensions of contrastive learning to time-dependent trajectories."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11023955"
                        ],
                        "name": "Mark B. Ring",
                        "slug": "Mark-B.-Ring",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Ring",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark B. Ring"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 5
                            }
                        ],
                        "text": "Ring [21] also proposes a method for bridging long time lags."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14593743,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d0dd604b2b29bbc0adee2b71bbabca5d5ad3cd54",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "An incremental, higher-order, non-recurrent network combines two properties found to be useful for learning sequential tasks: higher-order connections and incremental introduction of new units. The network adds higher orders when needed by adding new units that dynamically modify connection weights. Since the new units modify the weights at the next time-step with information from the previous step, temporal tasks can be learned without the use of feedback, thereby greatly simplifying training. Furthermore, a theoretically unlimited number of units can be added to reach into the arbitrarily distant past. Experiments with the Reber grammar have demonstrated speedups of two orders of magnitude over recurrent networks."
            },
            "slug": "Learning-Sequential-Tasks-by-Incrementally-Adding-Ring",
            "title": {
                "fragments": [],
                "text": "Learning Sequential Tasks by Incrementally Adding Higher Orders"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "An incremental, higher-order, non-recurrent network combines two properties found to be useful for learning sequential tasks: higher- order connections and incremental introduction of new units."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780235"
                        ],
                        "name": "Fred Cummins",
                        "slug": "Fred-Cummins",
                        "structuredName": {
                            "firstName": "Fred",
                            "lastName": "Cummins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fred Cummins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2088368"
                        ],
                        "name": "F. Gers",
                        "slug": "F.-Gers",
                        "structuredName": {
                            "firstName": "Felix",
                            "lastName": "Gers",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Gers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 151
                            }
                        ],
                        "text": "It will be interesting to examine to which extent LSTM is applicable to real world problems such as language separation from prosody (first results in [7]) and speech recognition."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2763470,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "2a9ce6eeada970ec7a494a3efc2b333e3b943ab2",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Most current language identi cation (LID) systems make little or no use of prosodic information, despite the importance of prosody in LID by humans. The greatest obstacle has been that of nding an appropriate feature set which captures linguistically relevant prosodic information. The only system to attempt LID entirely on the basis of prosodic variables uses a set of over 200 features which are selected and combined in a task-speci c manner [12]. We apply a novel recurrent neural network model to the task of pairwise discrimination among languages. Network inputs are limited to delta-F0 and the rst di erence of the band limited amplitude envelope. Initial results are based on all pairwise combinations of English, German, Japanese, Mandarin and Spanish, with 90 speakers per language."
            },
            "slug": "Language-identification-from-prosody-without-Cummins-Gers",
            "title": {
                "fragments": [],
                "text": "Language identification from prosody without explicit features"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work applies a novel recurrent neural network model to the task of pairwise discrimination among languages, based on all pairwise combinations of English, German, Japanese, Mandarin and Spanish, with 90 speakers per language."
            },
            "venue": {
                "fragments": [],
                "text": "EUROSPEECH"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1718584"
                        ],
                        "name": "P. Angeline",
                        "slug": "P.-Angeline",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Angeline",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Angeline"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33047146"
                        ],
                        "name": "G. M. Saunders",
                        "slug": "G.-M.-Saunders",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Saunders",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. M. Saunders"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145539951"
                        ],
                        "name": "J. Pollack",
                        "slug": "J.-Pollack",
                        "structuredName": {
                            "firstName": "Jordan",
                            "lastName": "Pollack",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pollack"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[1] (see also crossreference Chapter 15) propose a genetic approach that also avoids gradient computation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 44767,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fba6007d482db5fbe8cd6c3af90fe0922453e1d2",
            "isKey": false,
            "numCitedBy": 1068,
            "numCiting": 69,
            "paperAbstract": {
                "fragments": [],
                "text": "Standard methods for simultaneously inducing the structure and weights of recurrent neural networks limit every task to an assumed class of architectures. Such a simplification is necessary since the interactions between network structure and function are not well understood. Evolutionary computations, which include genetic algorithms and evolutionary programming, are population-based search methods that have shown promise in many similarly complex tasks. This paper argues that genetic algorithms are inappropriate for network acquisition and describes an evolutionary program, called GNARL, that simultaneously acquires both the structure and weights for recurrent networks. GNARL's empirical acquisition method allows for the emergence of complex behaviors and topologies that are potentially excluded by the artificial architectural constraints imposed in standard network induction methods."
            },
            "slug": "An-evolutionary-algorithm-that-constructs-recurrent-Angeline-Saunders",
            "title": {
                "fragments": [],
                "text": "An evolutionary algorithm that constructs recurrent neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is argued that genetic algorithms are inappropriate for network acquisition and an evolutionary program is described, called GNARL, that simultaneously acquires both the structure and weights for recurrent networks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688235"
                        ],
                        "name": "P. Frasconi",
                        "slug": "P.-Frasconi",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Frasconi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Frasconi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 107
                            }
                        ],
                        "text": "Belief propagation over long time lags does not effectively occur, a phenomenon called diffusion of credit [5], which closely resembles the vanishing gradients problem in RNNs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1758609,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "914fa99500eb779e33d22609f8a0fdf3fd2799e1",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper studies the problem of ergodicity of transition probability matrices in Markovian models, such as hidden Markov models (HMMs), and how it makes very difficult the task of learning to represent long-term context for sequential data. This phenomenon hurts the forward propagation of long-term context information, as well as learning a hidden state representation to represent long-term context, which depends on propagating credit information backwards in time. Using results from Markov chain theory, we show that this problem of diffusion of context and credit is reduced when the transition probabilities approach 0 or 1, i.e., the transition probability matrices are sparse and the model essentially deterministic. The results found in this paper apply to learning approaches based on continuous optimization, such as gradient descent and the Baum-Welch algorithm."
            },
            "slug": "Diffusion-of-Context-and-Credit-Information-in-Bengio-Frasconi",
            "title": {
                "fragments": [],
                "text": "Diffusion of Context and Credit Information in Markovian Models"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "This paper shows that the problem of diffusion of context and credit is reduced when the transition probabilities approach 0 or 1, i.e., the transition probability matrices are sparse and the model essentially deterministic."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308557"
                        ],
                        "name": "S. Hochreiter",
                        "slug": "S.-Hochreiter",
                        "structuredName": {
                            "firstName": "Sepp",
                            "lastName": "Hochreiter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hochreiter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 215
                            }
                        ],
                        "text": "In fact, as discussed in crossreference Chapter 9 of this book, simple weight guessing solves several popular benchmarks described in previous work faster than the recurrent net algorithms proposed therein (compare [14])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7452865,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b158a006bebb619e2ea7bf0a22c27d45c5d19004",
            "isKey": false,
            "numCitedBy": 617,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Standard recurrent nets cannot deal with long minimal time lags between relevant signals. Several recent NIPS papers propose alternative methods. We first show: problems used to promote various previous algorithms can be solved more quickly by random weight guessing than by the proposed algorithms. We then use LSTM, our own recent algorithm, to solve a hard problem that can neither be quickly solved by random search nor by any other recurrent net algorithm we are aware of."
            },
            "slug": "LSTM-can-Solve-Hard-Long-Time-Lag-Problems-Hochreiter-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "LSTM can Solve Hard Long Time Lag Problems"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work shows that problems used to promote various previous algorithms can be solved more quickly by random weight guessing than by the proposed algorithms, and uses LSTM, its own recent algorithm, to solve a hard problem."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30320952"
                        ],
                        "name": "F. Pineda",
                        "slug": "F.-Pineda",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pineda",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Pineda"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 221
                            }
                        ],
                        "text": "That is, the error blows up, and conflicting error signals (with opposite signs) arriving at unit v can lead to oscillating weights and unstable learning due to \u201covershooting\u201d (for error blow-ups or bifurcations see also [20, 2, 9])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 27867182,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5146d7902132bcb0b2e6fe5f607358768fc47323",
            "isKey": false,
            "numCitedBy": 207,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Dynamics-and-architecture-for-neural-computation-Pineda",
            "title": {
                "fragments": [],
                "text": "Dynamics and architecture for neural computation"
            },
            "venue": {
                "fragments": [],
                "text": "J. Complex."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49464494"
                        ],
                        "name": "Kevin J. Lang",
                        "slug": "Kevin-J.-Lang",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Lang",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin J. Lang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 209
                            }
                        ],
                        "text": "To deal with long time lags, Mozer [18] uses time constants influencing changes of unit activations (deVries and Principe\u2019s related approach [8] may be viewed as a mixture of time-delay neural networks (TDNN) [15] and time constants)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1234937,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e08d090d1e586610d636a46004876e9f3ded8209",
            "isKey": false,
            "numCitedBy": 640,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-time-delay-neural-network-architecture-for-word-Lang-Waibel",
            "title": {
                "fragments": [],
                "text": "A time-delay neural network architecture for isolated word recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3327108"
                        ],
                        "name": "B. D. Vries",
                        "slug": "B.-D.-Vries",
                        "structuredName": {
                            "firstName": "Bert",
                            "lastName": "Vries",
                            "middleNames": [
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. D. Vries"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143961030"
                        ],
                        "name": "J. Pr\u00edncipe",
                        "slug": "J.-Pr\u00edncipe",
                        "structuredName": {
                            "firstName": "Jos\u00e9",
                            "lastName": "Pr\u00edncipe",
                            "middleNames": [
                                "Carlos"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pr\u00edncipe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 141
                            }
                        ],
                        "text": "To deal with long time lags, Mozer [18] uses time constants influencing changes of unit activations (deVries and Principe\u2019s related approach [8] may be viewed as a mixture of time-delay neural networks (TDNN) [15] and time constants)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7830172,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c115f0d793225c515ebce6be91521fcb8374ad6b",
            "isKey": false,
            "numCitedBy": 78,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new neural network model for processing of temporal patterns. This model, the gamma neural model, is as general as a convolution delay model with arbitrary weight kernels w(t). We show that the gamma model can be formulated as a (partially prewired) additive model. A temporal hebbian learning rule is derived and we establish links to related existing models for temporal processing."
            },
            "slug": "A-Theory-for-Neural-Networks-with-Time-Delays-Vries-Pr\u00edncipe",
            "title": {
                "fragments": [],
                "text": "A Theory for Neural Networks with Time Delays"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "This model, the gamma neural model, is as general as a convolution delay model with arbitrary weight kernels w(t) and it is shown that the gamma model can be formulated as a (partially prewired) additive model."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714997"
                        ],
                        "name": "K. Doya",
                        "slug": "K.-Doya",
                        "structuredName": {
                            "firstName": "Kenji",
                            "lastName": "Doya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Doya"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 221
                            }
                        ],
                        "text": "That is, the error blows up, and conflicting error signals (with opposite signs) arriving at unit v can lead to oscillating weights and unstable learning due to \u201covershooting\u201d (for error blow-ups or bifurcations see also [20, 2, 9])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15069221,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f55e5107f756e29f1e6ac6109dc44d698d6301fb",
            "isKey": false,
            "numCitedBy": 145,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Gradient descent algorithms in recurrent neural networks can have problems when the network dynamics experience bifurcations in the course of learning. The possible hazards caused by the bifurcations of the network dynamics and the learning equations are investigated. The roles of teacher forcing, preprogramming of network structures, and the approximate learning algorithms are discussed.<<ETX>>"
            },
            "slug": "Bifurcations-in-the-learning-of-recurrent-neural-Doya",
            "title": {
                "fragments": [],
                "text": "Bifurcations in the learning of recurrent neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "The roles of teacher forcing, preprogramming of network structures, and the approximate learning algorithms are discussed and the possible hazards caused by the bifurcations of the network dynamics and the learning equations are investigated."
            },
            "venue": {
                "fragments": [],
                "text": "[Proceedings] 1992 IEEE International Symposium on Circuits and Systems"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 2
                            }
                        ],
                        "text": ", [23, 28, 27]) or \u201cReal-Time Recurrent Learning\u201d (RTRL, e."
                    },
                    "intents": []
                }
            ],
            "corpusId": 62245742,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "111fd833a4ae576cfdbb27d87d2f8fc0640af355",
            "isKey": false,
            "numCitedBy": 19355,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-internal-representations-by-error-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning internal representations by error propagation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055652029"
                        ],
                        "name": "J\u00fcrgen Schmidhuber",
                        "slug": "J\u00fcrgen-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J\u00fcrgen Schmidhuber"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 43
                            }
                        ],
                        "text": "Schmidhuber\u2019s hierarchical chunker systems [24, 25] can in principle bridge arbitrary time lags, but only if there is local predictability across the sub-"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 41
                            }
                        ],
                        "text": "For instance, in his postdoctoral thesis [25], Schmidhuber uses hierarchical recurrent networks with self-organizing time scales to rapidly solve certain grammar learning tasks involving minimal time lags in excess of 1000 steps."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 196113676,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a4a9af3c0418bceef7e057db0f08fdfeab4cdff5",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Netzwerkarchitekturen,-Zielfunktionen-und-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Netzwerkarchitekturen, Zielfunktionen und Kettenregel"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066667188"
                        ],
                        "name": "Sepp Hochreiter",
                        "slug": "Sepp-Hochreiter",
                        "structuredName": {
                            "firstName": "Sepp",
                            "lastName": "Hochreiter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sepp Hochreiter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "We refer to Hochreiter\u2019s thesis [12] for more details."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 192
                            }
                        ],
                        "text": ", [22]) error signals \u201cflowing backwards in time\u201d tend to either (1) blow up or (2) vanish: the temporal evolution of the backpropagated error exponentially depends on the size of the weights [12, 6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 116
                            }
                        ],
                        "text": "In Section 2, we consider the case of standard RNNs and derive the main result using the approach first proposed in [12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60091947,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3f3d13e95c25a8f6a753e38dfce88885097cbd43",
            "isKey": false,
            "numCitedBy": 603,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Untersuchungen-zu-dynamischen-neuronalen-Netzen-Hochreiter",
            "title": {
                "fragments": [],
                "text": "Untersuchungen zu dynamischen neuronalen Netzen"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895771"
                        ],
                        "name": "D. Zipser",
                        "slug": "D.-Zipser",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Zipser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Zipser"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14792754,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "10dae7fca6b65b61d155a622f0c6ca2bc3922251",
            "isKey": false,
            "numCitedBy": 567,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Gradient-based-learning-algorithms-for-recurrent-Williams-Zipser",
            "title": {
                "fragments": [],
                "text": "Gradient-based learning algorithms for recurrent networks and their computational complexity"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning to forget: Con-  tinual prediction with lstm"
            },
            "venue": {
                "fragments": [],
                "text": "In Proc. ICANN'99, Int. Conf. on Arti cial  Neural Networks,"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 2
                            }
                        ],
                        "text": ", [21]) error signals \\ owing backwards in time\" tend to either (1) blow up or (2) vanish: the temporal evolution of the backpropagated error exponentially depends on the size of the weights [11, 6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The utility driven dynamic error propa-  gation network"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report CUED/F-INFENG/TR.1,"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The cascade - correlation learning architecture . Advances in neural information processing systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Untersuchungen zu dynamischen neuronalen Netzen. Diploma thesis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Untersuchungen zu dynamischen neuronalen Netzen . Diploma thesis, Institut f ur Informatik, Lehrstuhl Prof. Brauer, Technische Universitt at M unchen"
            },
            "venue": {
                "fragments": [],
                "text": "Untersuchungen zu dynamischen neuronalen Netzen . Diploma thesis, Institut f ur Informatik, Lehrstuhl Prof. Brauer, Technische Universitt at M unchen"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 171
                            }
                        ],
                        "text": "In regions where |M \u2032| > 1 it can be shown that arbitrarily small perturbations (for example due to the inputs) can eventually kick the state out of a basin of attraction [19] (see the sample trajectory on the right of Figure 1)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Iterative Solution of Non-linear Equations in Several Variables and Systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 191
                            }
                        ],
                        "text": ", [21]) error signals \\ owing backwards in time\" tend to either (1) blow up or (2) vanish: the temporal evolution of the backpropagated error exponentially depends on the size of the weights [11, 6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 196
                            }
                        ],
                        "text": "We observe that if jwijj wmax < 4:0 n 8i; j; then kWkA n wmax < 4:0 will result in exponential decay; by setting := nwmax 4:0 < 1:0, we obtain @ v(s) @ k(t) n t s: We refer to Hochreiter's thesis [11] for more details."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 114
                            }
                        ],
                        "text": "In Section 2, we consider the case of standard RNNs and derive the main result using the approach rst proposed in [11]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Untersuchungen zu dynamischen neuronalen Net-  zen. Diploma thesis, Institut f\u007f  ur Informatik, Lehrstuhl Prof. Brauer,  Technische Universit\u007fat M\u007f"
            },
            "venue": {
                "fragments": [],
                "text": "unchen,"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 70
                            }
                        ],
                        "text": "Adaptive sequence chunkers Schmidhuber's hierarchical chunker systems [23, 24] can in principle bridge arbitrary time lags, but only if there is local predictability across the sub10"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Netzwerkarchitekturen, Zielfunktionen und Ketten-  regel. Habilitationsschrift, Institut f\u007f  ur Informatik, Technische Univer-  sit\u007fat M\u007f"
            },
            "venue": {
                "fragments": [],
                "text": "unchen,"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "J. Schmidhuber. Netzwerkarchitekturen, Zielfunktionen und Kettenregel . Habilitationsschrift, Institut f ur Informatik Technische Universitt at M unchen"
            },
            "venue": {
                "fragments": [],
                "text": "J. Schmidhuber. Netzwerkarchitekturen, Zielfunktionen und Kettenregel . Habilitationsschrift, Institut f ur Informatik Technische Universitt at M unchen"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 2
                            }
                        ],
                        "text": ", [22]) error signals \u201cflowing backwards in time\u201d tend to either (1) blow up or (2) vanish: the temporal evolution of the backpropagated error exponentially depends on the size of the weights [12, 6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The utility driven dynamic error propagation network"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report CUED/F-INFENG/TR.1,"
            },
            "year": 1987
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 22,
            "methodology": 7
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 35,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Gradient-Flow-in-Recurrent-Nets:-the-Difficulty-of-Hochreiter-Bengio/aed054834e2c696807cc8b227ac7a4197196e211?sort=total-citations"
}