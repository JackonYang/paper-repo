{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145319478"
                        ],
                        "name": "Michael J. Jones",
                        "slug": "Michael-J.-Jones",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jones",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 36
                            }
                        ],
                        "text": "(1995), Olshausen and Field (1996), Harpur and Prager (1996), Daubechies (1992), Mallat and Zhang (1993), and Coifman and Wickerhauser (1992) for further details."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 90
                            }
                        ],
                        "text": "In the case in which V(x) = x2 we obtain the standard regularization theory solution (see Girosi et al., 1995, for an alternative derivation):\n(K + \u03b3 I) a = y where we have defined \u03b3 \u2261 1C ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 114
                            }
                        ],
                        "text": "Notice that the norm induced by the scalar product (see equation A.10) is the smoothness functional considered by Girosi et al. (1995) in their approach to regularization theory for function approximation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 49743910,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "eae2430d9a984120bf511655a03c15089b007499",
            "isKey": false,
            "numCitedBy": 1365,
            "numCiting": 198,
            "paperAbstract": {
                "fragments": [],
                "text": "We had previously shown that regularization principles lead to approximation schemes that are equivalent to networks with one layer of hidden units, called regularization networks. In particular, standard smoothness functionals lead to a subclass of regularization networks, the well known radial basis functions approximation schemes. This paper shows that regularization networks encompass a much broader range of approximation schemes, including many of the popular general additive models and some of the neural networks. In particular, we introduce new classes of smoothness functionals that lead to different classes of basis functions. Additive splines as well as some tensor product splines can be obtained from appropriate classes of smoothness functionals. Furthermore, the same generalization that extends radial basis functions (RBF) to hyper basis functions (HBF) also leads from additive models to ridge approximation models, containing as special cases Breiman's hinge functions, some forms of projection pursuit regression, and several types of neural networks. We propose to use the term generalized regularization networks for this broad class of approximation schemes that follow from an extension of regularization. In the probabilistic interpretation of regularization, the different classes of basis functions correspond to different classes of prior probabilities on the approximating function spaces, and therefore to different types of smoothness assumptions. In summary, different multilayer networks with one hidden layer, which we collectively call generalized regularization networks, correspond to different classes of priors and associated smoothness functionals in a classical regularization principle. Three broad classes are (1) radial basis functions that can be generalized to hyper basis functions, (2) some tensor product splines, and (3) additive splines that can be generalized to schemes of the type of ridge approximation, hinge functions, and several perceptron-like neural networks with one hidden layer."
            },
            "slug": "Regularization-Theory-and-Neural-Networks-Girosi-Jones",
            "title": {
                "fragments": [],
                "text": "Regularization Theory and Neural Networks Architectures"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "This paper shows that regularization networks encompass a much broader range of approximation schemes, including many of the popular general additive models and some of the neural networks, and introduces new classes of smoothness functionals that lead to different classes of basis functions."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727797"
                        ],
                        "name": "S. Chen",
                        "slug": "S.-Chen",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Chen",
                            "middleNames": [
                                "Saobing"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145621255"
                        ],
                        "name": "M. Saunders",
                        "slug": "M.-Saunders",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Saunders",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Saunders"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2429822,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9af121fbed84c3484ab86df8f17f1f198ed790a0",
            "isKey": false,
            "numCitedBy": 9739,
            "numCiting": 81,
            "paperAbstract": {
                "fragments": [],
                "text": "The time-frequency and time-scale communities have recently developed a large number of overcomplete waveform dictionaries --- stationary wavelets, wavelet packets, cosine packets, chirplets, and warplets, to name a few. Decomposition into overcomplete systems is not unique, and several methods for decomposition have been proposed, including the method of frames (MOF), Matching pursuit (MP), and, for special dictionaries, the best orthogonal basis (BOB). \nBasis Pursuit (BP) is a principle for decomposing a signal into an \"optimal\" superposition of dictionary elements, where optimal means having the smallest l1 norm of coefficients among all such decompositions. We give examples exhibiting several advantages over MOF, MP, and BOB, including better sparsity and superresolution. BP has interesting relations to ideas in areas as diverse as ill-posed problems, in abstract harmonic analysis, total variation denoising, and multiscale edge denoising. \nBP in highly overcomplete dictionaries leads to large-scale optimization problems. With signals of length 8192 and a wavelet packet dictionary, one gets an equivalent linear program of size 8192 by 212,992. Such problems can be attacked successfully only because of recent advances in linear programming by interior-point methods. We obtain reasonable success with a primal-dual logarithmic barrier method and conjugate-gradient solver."
            },
            "slug": "Atomic-Decomposition-by-Basis-Pursuit-Chen-Donoho",
            "title": {
                "fragments": [],
                "text": "Atomic Decomposition by Basis Pursuit"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Basis Pursuit (BP) is a principle for decomposing a signal into an \"optimal\" superposition of dictionary elements, where optimal means having the smallest l1 norm of coefficients among all such decompositions."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Sci. Comput."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746242"
                        ],
                        "name": "S. Mallat",
                        "slug": "S.-Mallat",
                        "structuredName": {
                            "firstName": "St\u00e9phane",
                            "lastName": "Mallat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mallat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109019649"
                        ],
                        "name": "Zhifeng Zhang",
                        "slug": "Zhifeng-Zhang",
                        "structuredName": {
                            "firstName": "Zhifeng",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhifeng Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 264,
                                "start": 244
                            }
                        ],
                        "text": "\u2026with the use of overcomplete or redundant representations, in which a signal is approximated as a linear superposition of basis functions taken from a large dictionary (Chen, 1995; Chen et al., 1995; Olshausen & Field, 1996; Daubechies, 1992; Mallat & Zhang, 1993; Coifman & Wickerhauser, 1992)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 241,
                                "start": 218
                            }
                        ],
                        "text": "\u2026et al.\nD ow nloaded from http://direct.m it.edu/neco/article-pdf/10/6/1455/813908/089976698300017269.pdf by guest on 01 Septem ber 2021\n(1995), Olshausen and Field (1996), Harpur and Prager (1996), Daubechies (1992), Mallat and Zhang (1993), and Coifman and Wickerhauser (1992) for further details."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14427335,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2210a7157565422261b03cf2cdf4e91b583df5a0",
            "isKey": false,
            "numCitedBy": 8851,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors introduce an algorithm, called matching pursuit, that decomposes any signal into a linear expansion of waveforms that are selected from a redundant dictionary of functions. These waveforms are chosen in order to best match the signal structures. Matching pursuits are general procedures to compute adaptive signal representations. With a dictionary of Gabor functions a matching pursuit defines an adaptive time-frequency transform. They derive a signal energy distribution in the time-frequency plane, which does not include interference terms, unlike Wigner and Cohen class distributions. A matching pursuit isolates the signal structures that are coherent with respect to a given dictionary. An application to pattern extraction from noisy signals is described. They compare a matching pursuit decomposition with a signal expansion over an optimized wavepacket orthonormal basis, selected with the algorithm of Coifman and Wickerhauser see (IEEE Trans. Informat. Theory, vol. 38, Mar. 1992). >"
            },
            "slug": "Matching-pursuits-with-time-frequency-dictionaries-Mallat-Zhang",
            "title": {
                "fragments": [],
                "text": "Matching pursuits with time-frequency dictionaries"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "The authors introduce an algorithm, called matching pursuit, that decomposes any signal into a linear expansion of waveforms that are selected from a redundant dictionary of functions, chosen in order to best match the signal structures."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Signal Process."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713577"
                        ],
                        "name": "D. Mattera",
                        "slug": "D.-Mattera",
                        "structuredName": {
                            "firstName": "Davide",
                            "lastName": "Mattera",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mattera"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2058020346"
                        ],
                        "name": "F. Palmieri",
                        "slug": "F.-Palmieri",
                        "structuredName": {
                            "firstName": "Francesco",
                            "lastName": "Palmieri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Palmieri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735300"
                        ],
                        "name": "S. Haykin",
                        "slug": "S.-Haykin",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Haykin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Haykin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 24905794,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "31cbfc4672b6bc177c2822817f4723ad8c0c0c61",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Most support vector (SV) methods proposed in the recent literature can be viewed in a unified framework with great flexibility in terms of the choice of the kernel functions and their constraints. We show that all these problems can be solved within a unique approach if we are equipped with a robust method for finding a sparse solution of a linear system. Moreover, for such a purpose, we propose an iterative algorithm that can be simply implemented. Finally, we compare the classical SV approach with other, recently proposed, cross-correlation based, alternative methods. The simplicity of their implementation and the possibility of exactly calculating their computational complexity constitute important advantages in a real-time signal processing scenario."
            },
            "slug": "Simple-and-robust-methods-for-support-vector-Mattera-Palmieri",
            "title": {
                "fragments": [],
                "text": "Simple and robust methods for support vector expansions"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work proposes an iterative algorithm that can be simply implemented for finding a sparse solution of a linear system and compares the classical SV approach with other, recently proposed, cross-correlation based, alternative methods."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 110
                            }
                        ],
                        "text": "In this section we briefly sketch the ideas behind SVMs for regression and refer readers to Vapnik (1995) and Vapnik et al. (1997) for a full description of the technique."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 44422404,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "ad3fdd0b3746845b69f21a4ee30de0dd0f74c3c6",
            "isKey": false,
            "numCitedBy": 84,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "The Support Vector (SV) method is a new general method of function estimation which does not depend explicitly on the dimensionality of input space. It was applied for pattern recognition, regression estimation, and density estimation problems as well as for problems of solving linear operator equations. In this article we describe the general idea of the SV method and present theorems demonstrating that the generalization ability of the SV method is based on factors which classical statistics do not take into account. We also describe the SV method for density estimation in a set of functions defined by a mixture of an infinite number of Gaussians."
            },
            "slug": "The-Support-Vector-Method-Vapnik",
            "title": {
                "fragments": [],
                "text": "The Support Vector Method"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The general idea of the Support Vector method is described and theorems demonstrating that the generalization ability of the SV method is based on factors which classical statistics do not take into account are presented."
            },
            "venue": {
                "fragments": [],
                "text": "ICANN"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145627013"
                        ],
                        "name": "J. Stewart",
                        "slug": "J.-Stewart",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Stewart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Stewart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 120093770,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "400ca5d045588b887ec8949662dff5436bbc2461",
            "isKey": false,
            "numCitedBy": 189,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "2 fi COS Xi + i X 6 S i n i = \u00b0i = l ' ' i = l ' Likewise it is easily verified directly that e is p.d. for real \\ , but it is not so straightforward to see that such functions as e~H e~*, and (1 4x)\" ? e p.d. These and other examples are discussed in \u00a7 3. Positive definite functions and their various analogues and generalizations have arisen in diverse parts of mathematics since the beginning of this century. They occur naturally in Fourier analysis, probability theory, operator theory, complex function-theory, moment problems, integral equations, boundary-value problems for partial differential equations, embedding problems, information theory, and other areas. Their history constitutes a good illustration of the words of Hobson [51, p. 290] : \"Not only are special results, obtained independently of one another, frequently seen to be really included in"
            },
            "slug": "Positive-definite-functions-and-generalizations,-an-Stewart",
            "title": {
                "fragments": [],
                "text": "Positive definite functions and generalizations, an historical survey"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1976
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2076400"
                        ],
                        "name": "B. Caprile",
                        "slug": "B.-Caprile",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Caprile",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Caprile"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 11
                            }
                        ],
                        "text": "i=1 j ij : (14) If we now let the variables i assume values over the all real line, and assuming that the coe cients ai are bounded, it is clear that the coe cients ai are redundant, and can be dropped from the cost function."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10243731,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "898c01de58eb3b8e790b60e0fe0db2230d88f15b",
            "isKey": false,
            "numCitedBy": 698,
            "numCiting": 152,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning an input-output mapping from a set of examples, of the type that many neural networks have been constructed to perform, can be regarded as synthesizing an approximation of a multi-dimensional function. We develop a theoretical framework for approximation based on regularization techniques that leads to a class of three-layer networks that we call Generalized Radial Basis Functions (GRBF). GRBF networks are not only equivalent to generalized splines, but are also closely related to several pattern recognition methods and neural network algorithms. The paper introduces several extensions and applications of the technique and discusses intriguing analogies with neurobiological data."
            },
            "slug": "Extensions-of-a-Theory-of-Networks-for-and-Learning-Girosi-Poggio",
            "title": {
                "fragments": [],
                "text": "Extensions of a Theory of Networks for Approximation and Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A theoretical framework for approximation based on regularization techniques that leads to a class of three-layer networks that is called Generalized Radial Basis Functions (GRBF), which is not only equivalent to generalized splines, but is closely related to several pattern recognition methods and neural network algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2459012"
                        ],
                        "name": "S. Mika",
                        "slug": "S.-Mika",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Mika",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mika"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3125657"
                        ],
                        "name": "Phil Knirsch",
                        "slug": "Phil-Knirsch",
                        "structuredName": {
                            "firstName": "Phil",
                            "lastName": "Knirsch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Phil Knirsch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152597562"
                        ],
                        "name": "Gunnar R\u00e4tsch",
                        "slug": "Gunnar-R\u00e4tsch",
                        "structuredName": {
                            "firstName": "Gunnar",
                            "lastName": "R\u00e4tsch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gunnar R\u00e4tsch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14669541,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b6a3e0028d99439ce2741d0e147b6e9a34bc4267",
            "isKey": false,
            "numCitedBy": 1231,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper collects some ideas targeted at advancing our understanding of the feature spaces associated with support vector (SV) kernel functions. We first discuss the geometry of feature space. In particular, we review what is known about the shape of the image of input space under the feature space map, and how this influences the capacity of SV methods. Following this, we describe how the metric governing the intrinsic geometry of the mapped surface can be computed in terms of the kernel, using the example of the class of inhomogeneous polynomial kernels, which are often used in SV pattern recognition. We then discuss the connection between feature space and input space by dealing with the question of how one can, given some vector in feature space, find a preimage (exact or approximate) in input space. We describe algorithms to tackle this issue, and show their utility in two applications of kernel methods. First, we use it to reduce the computational complexity of SV decision functions; second, we combine it with the Kernel PCA algorithm, thereby constructing a nonlinear statistical denoising technique which is shown to perform well on real-world data."
            },
            "slug": "Input-space-versus-feature-space-in-kernel-based-Sch\u00f6lkopf-Mika",
            "title": {
                "fragments": [],
                "text": "Input space versus feature space in kernel-based methods"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The geometry of feature space is reviewed, and the connection between feature space and input space is discussed by dealing with the question of how one can, given some vector in feature space, find a preimage in input space."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "92579735"
                        ],
                        "name": "Peter Craven",
                        "slug": "Peter-Craven",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Craven",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Craven"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 147
                            }
                        ],
                        "text": "In order to make this problem well posed, we follow the approach of regularization theory (Tikhonov & Arsenin, 1977; Morozov, 1984; Bertero, 1986; Wahba, 1975, 1990) and impose an additional smoothness constraint on the solution of the approximation problem."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 124
                            }
                        ],
                        "text": "We will take a viewpoint closer to classical regularization theory (Tikhonov & Arsenin, 1977; Morozov, 1984; Bertero, 1986; Wahba, 1975, 1990), which might be more familiar to readers, rather than the theory of uniform conver-\nD ow nloaded from http://direct.m\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "n=0 n cos(nx) : (32) An example of such a function is"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 29
                            }
                        ],
                        "text": "It is easy to check that, if (32) holds, then we have:"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14094416,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b477dd12dd49e44a62c1a303501df5fb6706c7e9",
            "isKey": true,
            "numCitedBy": 3541,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "SummarySmoothing splines are well known to provide nice curves which smooth discrete, noisy data. We obtain a practical, effective method for estimating the optimum amount of smoothing from the data. Derivatives can be estimated from the data by differentiating the resulting (nearly) optimally smoothed spline.We consider the modelyi(ti)+\u03b5i,i=1, 2, ...,n,ti\u2208[0, 1], whereg\u2208W2(m)={f:f,f\u2032, ...,f(m\u22121) abs. cont.,f(m)\u2208\u21122[0,1]}, and the {\u03b5i} are random errors withE\u03b5i=0,E\u03b5i\u03b5j=\u03c32\u03b4ij. The error variance \u03c32 may be unknown. As an estimate ofg we take the solutiongn, \u03bb to the problem: Findf\u2208W2(m) to minimize\n$$\\frac{1}{n}\\sum\\limits_{j = 1}^n {(f(t_j ) - y_j )^2 + \\lambda \\int\\limits_0^1 {(f^{(m)} (u))^2 du} }$$\n. The functiongn, \u03bb is a smoothing polynomial spline of degree 2m\u22121. The parameter \u03bb controls the tradeoff between the \u201croughness\u201d of the solution, as measured by\n$$\\int\\limits_0^1 {[f^{(m)} (u)]^2 du}$$\n, and the infidelity to the data as measured by\n$$\\frac{1}{n}\\sum\\limits_{j = 1}^n {(f(t_j ) - y_j )^2 }$$\n, and so governs the average square errorR(\u03bb; g)=R(\u03bb) defined by\n$$R(\\lambda ) = \\frac{1}{n}\\sum\\limits_{j = 1}^n {(g_{n,\\lambda } (t_j ) - g(t_j ))^2 }$$\n. We provide an estimate\n$$\\hat \\lambda$$\n, called the generalized cross-validation estimate, for the minimizer ofR(\u03bb). The estimate\n$$\\hat \\lambda$$\n is the minimizer ofV(\u03bb) defined by\n$$V(\\lambda ) = \\frac{1}{n}\\parallel (I - A(\\lambda ))y\\parallel ^2 /\\left[ {\\frac{1}{n}{\\text{Trace(}}I - A(\\lambda ))} \\right]^2$$\n, wherey=(y1, ...,yn)t andA(\u03bb) is then\u00d7n matrix satisfying(gn, \u03bb (t1), ...,gn, \u03bb (tn))t=A (\u03bb) y. We prove that there exist a sequence of minimizers\n$$\\tilde \\lambda = \\tilde \\lambda (n)$$\n ofEV(\u03bb), such that as the (regular) mesh{ti}i=1n becomes finer,\n$$\\mathop {\\lim }\\limits_{n \\to \\infty } ER(\\tilde \\lambda )/\\mathop {\\min }\\limits_\\lambda ER(\\lambda ) \\downarrow 1$$\n. A Monte Carlo experiment with several smoothg's was tried withm=2,n=50 and several values of \u03c32, and typical values of\n$$R(\\hat \\lambda )/\\mathop {\\min }\\limits_\\lambda R(\\lambda )$$\n were found to be in the range 1.01\u20131.4. The derivativeg\u2032 ofg can be estimated by\n$$g'_{n,\\hat \\lambda } (t)$$\n. In the Monte Carlo examples tried, the minimizer of\n$$R_D (\\lambda ) = \\frac{1}{n}\\sum\\limits_{j = 1}^n {(g'_{n,\\lambda } (t_j ) - } g'(t_j ))$$\n tended to be close to the minimizer ofR(\u03bb), so that\n$$\\hat \\lambda$$\n was also a good value of the smoothing parameter for estimating the derivative."
            },
            "slug": "Smoothing-noisy-data-with-spline-functions-Craven-Wahba",
            "title": {
                "fragments": [],
                "text": "Smoothing noisy data with spline functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780112"
                        ],
                        "name": "R. Coifman",
                        "slug": "R.-Coifman",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Coifman",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Coifman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709398"
                        ],
                        "name": "M. Wickerhauser",
                        "slug": "M.-Wickerhauser",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Wickerhauser",
                            "middleNames": [
                                "Victor"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Wickerhauser"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 294,
                                "start": 266
                            }
                        ],
                        "text": "\u2026with the use of overcomplete or redundant representations, in which a signal is approximated as a linear superposition of basis functions taken from a large dictionary (Chen, 1995; Chen et al., 1995; Olshausen & Field, 1996; Daubechies, 1992; Mallat & Zhang, 1993; Coifman & Wickerhauser, 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 278,
                                "start": 247
                            }
                        ],
                        "text": "\u2026et al.\nD ow nloaded from http://direct.m it.edu/neco/article-pdf/10/6/1455/813908/089976698300017269.pdf by guest on 01 Septem ber 2021\n(1995), Olshausen and Field (1996), Harpur and Prager (1996), Daubechies (1992), Mallat and Zhang (1993), and Coifman and Wickerhauser (1992) for further details."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 546882,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5478a91c183c3a460bd4098acb8927bfc671367c",
            "isKey": false,
            "numCitedBy": 3339,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Adapted waveform analysis uses a library of orthonormal bases and an efficiency functional to match a basis to a given signal or family of signals. It permits efficient compression of a variety of signals, such as sound and images. The predefined libraries of modulated waveforms include orthogonal wavelet-packets and localized trigonometric functions, and have reasonably well-controlled time-frequency localization properties. The idea is to build out of the library functions an orthonormal basis relative to which the given signal or collection of signals has the lowest information cost. The method relies heavily on the remarkable orthogonality properties of the new libraries: all expansions in a given library conserve energy and are thus comparable. Several cost functionals are useful; one of the most attractive is Shannon entropy, which has a geometric interpretation in this context. >"
            },
            "slug": "Entropy-based-algorithms-for-best-basis-selection-Coifman-Wickerhauser",
            "title": {
                "fragments": [],
                "text": "Entropy-based algorithms for best basis selection"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "Adapted waveform analysis uses a library of orthonormal bases and an efficiency functional to match a basis to a given signal or family of signals, and relies heavily on the remarkable orthogonality properties of the new libraries."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6294728,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4609f6bdc3beab00c9beceaa12dd8101fefe6f1c",
            "isKey": false,
            "numCitedBy": 4843,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical learning theory was introduced in the late 1960's. Until the 1990's it was a purely theoretical analysis of the problem of function estimation from a given collection of data. In the middle of the 1990's new types of learning algorithms (called support vector machines) based on the developed theory were proposed. This made statistical learning theory not only a tool for the theoretical analysis but also a tool for creating practical algorithms for estimating multidimensional functions. This article presents a very general overview of statistical learning theory including both theoretical and algorithmic aspects of the theory. The goal of this overview is to demonstrate how the abstract learning theory established conditions for generalization which are more general than those discussed in classical statistical paradigms and how the understanding of these conditions inspired new algorithmic approaches to function estimation problems. A more detailed overview of the theory (without proofs) can be found in Vapnik (1995). In Vapnik (1998) one can find detailed description of the theory (including proofs)."
            },
            "slug": "An-overview-of-statistical-learning-theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "An overview of statistical learning theory"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "How the abstract learning theory established conditions for generalization which are more general than those discussed in classical statistical paradigms are demonstrated and how the understanding of these conditions inspired new algorithmic approaches to function estimation problems are demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "LETTER Communicated by Bruno Olshausen"
                    },
                    "intents": []
                }
            ],
            "corpusId": 207165665,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2599131a4bc2fa957338732a37c744cfe3e17b24",
            "isKey": false,
            "numCitedBy": 10840,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms."
            },
            "slug": "A-training-algorithm-for-optimal-margin-classifiers-Boser-Guyon",
            "title": {
                "fragments": [],
                "text": "A training algorithm for optimal margin classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented, applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '92"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144575699"
                        ],
                        "name": "S. Vijayakumar",
                        "slug": "S.-Vijayakumar",
                        "structuredName": {
                            "firstName": "Sethu",
                            "lastName": "Vijayakumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Vijayakumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143881860"
                        ],
                        "name": "H. Ogawa",
                        "slug": "H.-Ogawa",
                        "structuredName": {
                            "firstName": "Hidemitsu",
                            "lastName": "Ogawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ogawa"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16372164,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "528d4ccd56caaef8e01549c3cf7c4253892c1899",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "RKHS-based-functional-analysis-for-exact-learning-Vijayakumar-Ogawa",
            "title": {
                "fragments": [],
                "text": "RKHS-based functional analysis for exact incremental learning"
            },
            "venue": {
                "fragments": [],
                "text": "Neurocomputing"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2378798"
                        ],
                        "name": "L. Schumaker",
                        "slug": "L.-Schumaker",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Schumaker",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Schumaker"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 95
                            }
                        ],
                        "text": "The functions Bn are piecewise polynomials of degree n, whose exact definition can be found in Schumaker (1981)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 123077071,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8fa7e5d7ef68525b53a2de82c7b7dfdfeb781511",
            "isKey": false,
            "numCitedBy": 2928,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This classic work continues to offer a comprehensive treatment of the theory of univariate and tensor-product splines. It will be of interest to researchers and students working in applied analysis, numerical analysis, computer science, and engineering. The material covered provides the reader with the necessary tools for understanding the many applications of splines in such diverse areas as approximation theory, computer-aided geometric design, curve and surface design and fitting, image processing, numerical solution of differential equations, and increasingly in business and the biosciences. This new edition includes a supplement outlining some of the major advances in the theory since 1981, and some 250 new references. It can be used as the main or supplementary text for courses in splines, approximation theory or numerical analysis."
            },
            "slug": "Spline-Functions:-Basic-Theory-Schumaker",
            "title": {
                "fragments": [],
                "text": "Spline Functions: Basic Theory"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The material covered provides the reader with the necessary tools for understanding the many applications of splines in such diverse areas as approximation theory, computer-aided geometric design, curve and surface design and fitting, image processing, numerical solution of differential equations, and increasingly in business and the biosciences."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16697538"
                        ],
                        "name": "N. Aronszajn",
                        "slug": "N.-Aronszajn",
                        "structuredName": {
                            "firstName": "N.",
                            "lastName": "Aronszajn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Aronszajn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 26
                            }
                        ],
                        "text": "In this article, an RKHS (Aronszajn, 1950) is defined as a Hilbert space of functions defined over some domain \u00c4 \u2282 Rd with the property that, for\nD ow nloaded from http://direct.m it.edu/neco/article-pdf/10/6/1455/813908/089976698300017269.pdf by guest on 01 Septem ber 2021\neach x \u2208 \u00c4, the\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 25
                            }
                        ],
                        "text": "In this article, an RKHS (Aronszajn, 1950) is defined as a Hilbert space of functions defined over some domain \u00c4 \u2282 Rd with the property that, for D ow naded rom httpdirect."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 54040858,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "fe697b4e2cb4c132da39aed8b8266a0e6113f9f2",
            "isKey": false,
            "numCitedBy": 5083,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : The present paper may be considered as a sequel to our previous paper in the Proceedings of the Cambridge Philosophical Society, Theorie generale de noyaux reproduisants-Premiere partie (vol. 39 (1944)) which was written in 1942-1943. In the introduction to this paper we outlined the plan of papers which were to follow. In the meantime, however, the general theory has been developed in many directions, and our original plans have had to be changed. Due to wartime conditions we were not able, at the time of writing the first paper, to take into account all the earlier investigations which, although sometimes of quite a different character, were, nevertheless, related to our subject. Our investigation is concerned with kernels of a special type which have been used under different names and in different ways in many domains of mathematical research. We shall therefore begin our present paper with a short historical introduction in which we shall attempt to indicate the different manners in which these kernels have been used by various investigators, and to clarify the terminology. We shall also discuss the more important trends of the application of these kernels without attempting, however, a complete bibliography of the subject matter. (KAR) P. 2"
            },
            "slug": "Theory-of-Reproducing-Kernels.-Aronszajn",
            "title": {
                "fragments": [],
                "text": "Theory of Reproducing Kernels."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1950
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708655"
                        ],
                        "name": "B. Olshausen",
                        "slug": "B.-Olshausen",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Olshausen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Olshausen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49649079"
                        ],
                        "name": "D. Field",
                        "slug": "D.-Field",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Field",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Field"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4358477,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8012c4a1e2ca663f1a04e80cbb19631a00cbab27",
            "isKey": false,
            "numCitedBy": 5639,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "THE receptive fields of simple cells in mammalian primary visual cortex can be characterized as being spatially localized, oriented1\u20134 and bandpass (selective to structure at different spatial scales), comparable to the basis functions of wavelet transforms5,6. One approach to understanding such response properties of visual neurons has been to consider their relationship to the statistical structure of natural images in terms of efficient coding7\u201312. Along these lines, a number of studies have attempted to train unsupervised learning algorithms on natural images in the hope of developing receptive fields with similar properties13\u201318, but none has succeeded in producing a full set that spans the image space and contains all three of the above properties. Here we investigate the proposal8,12 that a coding strategy that maximizes sparseness is sufficient to account for these properties. We show that a learning algorithm that attempts to find sparse linear codes for natural scenes will develop a complete family of localized, oriented, bandpass receptive fields, similar to those found in the primary visual cortex. The resulting sparse image code provides a more efficient representation for later stages of processing because it possesses a higher degree of statistical independence among its outputs."
            },
            "slug": "Emergence-of-simple-cell-receptive-field-properties-Olshausen-Field",
            "title": {
                "fragments": [],
                "text": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that a learning algorithm that attempts to find sparse linear codes for natural scenes will develop a complete family of localized, oriented, bandpass receptive fields, similar to those found in the primary visual cortex."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 147
                            }
                        ],
                        "text": "In order to make this problem well posed, we follow the approach of regularization theory (Tikhonov & Arsenin, 1977; Morozov, 1984; Bertero, 1986; Wahba, 1975, 1990) and impose an additional smoothness constraint on the solution of the approximation problem."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "It is easy to check that, if ( 32 ) holds, then we have:"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 124
                            }
                        ],
                        "text": "We will take a viewpoint closer to classical regularization theory (Tikhonov & Arsenin, 1977; Morozov, 1984; Bertero, 1986; Wahba, 1975, 1990), which might be more familiar to readers, rather than the theory of uniform conver-\nD ow nloaded from http://direct.m\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "X n=0 \u201an cos(nx) : ( 32 ) An example of such a function is"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 189781595,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "02cb8ef325adcc6f29e0b1759920527836ea8b2b",
            "isKey": true,
            "numCitedBy": 1334,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "It is shown how to choose the smoothing parameter when a smoothing periodic spline of degree 2m\u22121 is used to reconstruct a smooth periodic curve from noisy ordinate data. The noise is assumed \u201cwhite\u201d, and the true curve is assumed to be in the Sobolev spaceW2(2m) of periodic functions with absolutely continuousv-th derivative,v=0, 1, ..., 2m\u22121 and square integrable 2m-th derivative. The criteria is minimum expected square error, averaged over the data points. The dependency of the optimum smoothing parameter on the sample size, the noise variance, and the smoothness of the true curve is found explicitly."
            },
            "slug": "Smoothing-noisy-data-with-spline-functions-Wahba",
            "title": {
                "fragments": [],
                "text": "Smoothing noisy data with spline functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2155551370"
                        ],
                        "name": "Shaobing Chen",
                        "slug": "Shaobing-Chen",
                        "structuredName": {
                            "firstName": "Shaobing",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaobing Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "122514140"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Donoho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 170
                            }
                        ],
                        "text": "What is more surprising, and less obvious, is that SVM and a specific model of sparse approximation, which is a modified version of the basis pursuit denoising algorithm (Chen, 1995; Chen et al., 1995), are actually equivalent in the case of noiseless data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 344,
                                "start": 218
                            }
                        ],
                        "text": "Sparse approximation often appears in conjunction with the use of overcomplete or redundant representations, in which a signal is approximated as a linear superposition of basis functions taken from a large dictionary (Chen, 1995; Chen et al., 1995; Olshausen & Field, 1996; Daubechies, 1992; Mallat & Zhang, 1993; Coifman & Wickerhauser, 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 13
                            }
                        ],
                        "text": "For example, Chen (1995) and Chen et al. (1995) use the L1 norm as an approximation of the L0, obtaining an approximation scheme that they call basis pursuit denoising."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 86
                            }
                        ],
                        "text": "We consider this to be a modified version of the basis pursuit denoising technique of Chen (1995) and Chen et al. (1995). Notice that it looks from equation 4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 67
                            }
                        ],
                        "text": "which is the one proposed in the basis pursuit denoising method of Chen et al. (1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 37
                            }
                        ],
                        "text": "The approximation scheme proposed by Chen et al. (1995) has the form described by equation 3."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 97
                            }
                        ],
                        "text": "It is not our purpose to discuss the motivations that lead to this approach and refer readers to Chen (1995), Chen et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 13
                            }
                        ],
                        "text": "For example, Chen (1995) and Chen et al. (1995) use the L1 norm as an approximation of the L0, obtaining an approximation scheme that they call basis pursuit denoising. In related work, Olshausen and Field (1996) enforce sparsity by considering D ow naded rom httpdirect."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 101
                            }
                        ],
                        "text": "Vapnik (1995) and a sparse approximation scheme that resembles the basis pursuit denoising algorithm (Chen, 1995; Chen, Donoho, & Saunders, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 86
                            }
                        ],
                        "text": "We consider this to be a modified version of the basis pursuit denoising technique of Chen (1995) and Chen et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 96447294,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "30f3567eeb13079a1a02ac1342f610cbd95df3bc",
            "isKey": true,
            "numCitedBy": 655,
            "numCiting": 96,
            "paperAbstract": {
                "fragments": [],
                "text": "The time-frequency and time-scale communities have recently developed an enormous number of over-complete signal dictionaries, wavelets, wavelet packets, cosine packets, Wilson bases, chirplets, warped bases, and hyperbolic cross bases being a few examples. Basis pursuit is a technique for decomposing a signal into an \"optimal\" superposition of dictionary elements. The optimization criterion is the l/sup 1/ norm of coefficients. The method has several advantages over matching pursuit and best ortho basis, including super-resolution and stability.<<ETX>>"
            },
            "slug": "Basis-pursuit-Chen-Donoho",
            "title": {
                "fragments": [],
                "text": "Basis pursuit"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Basis pursuit is a technique for decomposing a signal into an \"optimal\" superposition of dictionary elements, which has several advantages over matching pursuit and best ortho basis, including super-resolution and stability."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1994 28th Asilomar Conference on Signals, Systems and Computers"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112452392"
                        ],
                        "name": "Si Wu",
                        "slug": "Si-Wu",
                        "structuredName": {
                            "firstName": "Si",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Si Wu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14294002,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9ddd24ea0d9f266383464d0f462579d83dcd0b06",
            "isKey": false,
            "numCitedBy": 845,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Improving-support-vector-machine-classifiers-by-Amari-Wu",
            "title": {
                "fragments": [],
                "text": "Improving support vector machine classifiers by modifying kernel functions"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 167309,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "20f6d89f13d8397b51f938f795e2666b4c0f33a9",
            "isKey": false,
            "numCitedBy": 97,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We derive the correspondence between regularization operators used in Regularization Networks and Hilbert Schmidt Kernels appearing in Support Vector Machines. More specifically, we prove that the Green's Functions associated with regularization operators are suitable Support Vector Kernels with equivalent regularization properties. As a by-product we show that a large number of Radial Basis Functions namely conditionally positive definite functions may be used as Support Vector kernels."
            },
            "slug": "From-Regularization-Operators-to-Support-Vector-Smola-Sch\u00f6lkopf",
            "title": {
                "fragments": [],
                "text": "From Regularization Operators to Support Vector Kernels"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "It is proved that the Green's Functions associated with regularization operators are suitable Support Vector Kernels with equivalent regularization properties and a large number of Radial Basis Functions namely conditionally positive definite functions may be used as Support Vector kernels."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145016534"
                        ],
                        "name": "J. Moody",
                        "slug": "J.-Moody",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Moody",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Moody"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2319258"
                        ],
                        "name": "C. Darken",
                        "slug": "C.-Darken",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Darken",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Darken"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 31251383,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e7c4f513f24c3b82a1138b9f22ed87ed00cbe76",
            "isKey": false,
            "numCitedBy": 4527,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a network architecture which uses a single internal layer of locally-tuned processing units to learn both classification tasks and real-valued function approximations (Moody and Darken 1988). We consider training such networks in a completely supervised manner, but abandon this approach in favor of a more computationally efficient hybrid learning method which combines self-organized and supervised learning. Our networks learn faster than backpropagation for two reasons: the local representations ensure that only a few units respond to any given input, thus reducing computational overhead, and the hybrid learning rules are linear rather than nonlinear, thus leading to faster convergence. Unlike many existing methods for data analysis, our network architecture and learning rules are truly adaptive and are thus appropriate for real-time use."
            },
            "slug": "Fast-Learning-in-Networks-of-Locally-Tuned-Units-Moody-Darken",
            "title": {
                "fragments": [],
                "text": "Fast Learning in Networks of Locally-Tuned Processing Units"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "This work proposes a network architecture which uses a single internal layer of locally-tuned processing units to learn both classification tasks and real-valued function approximations (Moody and Darken 1988)."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35224059"
                        ],
                        "name": "S. Golowich",
                        "slug": "S.-Golowich",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Golowich",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Golowich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 19196574,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "43ffa2c1a06a76e58a333f2e7d0bd498b24365ca",
            "isKey": false,
            "numCitedBy": 2603,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "The Support Vector (SV) method was recently proposed for estimating regressions, constructing multidimensional splines, and solving linear operator equations [Vapnik, 1995]. In this presentation we report results of applying the SV method to these problems."
            },
            "slug": "Support-Vector-Method-for-Function-Approximation,-Vapnik-Golowich",
            "title": {
                "fragments": [],
                "text": "Support Vector Method for Function Approximation, Regression Estimation and Signal Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "This presentation reports results of applying the Support Vector method to problems of estimating regressions, constructing multidimensional splines, and solving linear operator equations."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 12841311,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "188bcf4e56b6437098c612c063e0f97d4e8d6e9c",
            "isKey": false,
            "numCitedBy": 97,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We derive a new general representation for a function as a linear combination of local correlation kernels at optimal sparse locations (and scales) and characterize its relation to principal component analysis, regularization, sparsity principles, and support vector machines."
            },
            "slug": "A-Sparse-Representation-for-Function-Approximation-Poggio-Girosi",
            "title": {
                "fragments": [],
                "text": "A Sparse Representation for Function Approximation"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A new general representation for a function is derived as a linear combination of local correlation kernels at optimal sparse locations (and scales) and its relation to principal component analysis, regularization, sparsity principles, and support vector machines is characterized."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6059616"
                        ],
                        "name": "G. F. Harpur",
                        "slug": "G.-F.-Harpur",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Harpur",
                            "middleNames": [
                                "Francis"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. F. Harpur"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680968"
                        ],
                        "name": "R. Prager",
                        "slug": "R.-Prager",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Prager",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Prager"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 13911187,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0590940043b88992c4417d239c01c282cd31309",
            "isKey": false,
            "numCitedBy": 117,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present an unsupervised neural network which exhibits competition between units via inhibitory feedback. The operation is such as to minimize reconstruction error, both for individual patterns, and over the entire training set. A key difference from networks which perform principal components analysis, or one of its variants, is the ability to converge to non-orthogonal weight values. We discuss the network's operation in relation to the twin goals of maximizing information transfer and minimizing code entropy, and show how the assignment of prior probabilities to network outputs can help to reduce entropy. We present results from two binary coding problems, and from experiments with image coding."
            },
            "slug": "Development-of-low-entropy-coding-in-a-recurrent-Harpur-Prager",
            "title": {
                "fragments": [],
                "text": "Development of low entropy coding in a recurrent network."
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "An unsupervised neural network which exhibits competition between units via inhibitory feedback is presented, and it is shown how the assignment of prior probabilities to network outputs can help to reduce entropy."
            },
            "venue": {
                "fragments": [],
                "text": "Network"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737063"
                        ],
                        "name": "I. Daubechies",
                        "slug": "I.-Daubechies",
                        "structuredName": {
                            "firstName": "Ingrid",
                            "lastName": "Daubechies",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Daubechies"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 242,
                                "start": 226
                            }
                        ],
                        "text": "\u2026with the use of overcomplete or redundant representations, in which a signal is approximated as a linear superposition of basis functions taken from a large dictionary (Chen, 1995; Chen et al., 1995; Olshausen & Field, 1996; Daubechies, 1992; Mallat & Zhang, 1993; Coifman & Wickerhauser, 1992)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 344,
                                "start": 218
                            }
                        ],
                        "text": "Sparse approximation often appears in conjunction with the use of overcomplete or redundant representations, in which a signal is approximated as a linear superposition of basis functions taken from a large dictionary (Chen, 1995; Chen et al., 1995; Olshausen & Field, 1996; Daubechies, 1992; Mallat & Zhang, 1993; Coifman & Wickerhauser, 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 199
                            }
                        ],
                        "text": "\u2026et al.\nD ow nloaded from http://direct.m it.edu/neco/article-pdf/10/6/1455/813908/089976698300017269.pdf by guest on 01 Septem ber 2021\n(1995), Olshausen and Field (1996), Harpur and Prager (1996), Daubechies (1992), Mallat and Zhang (1993), and Coifman and Wickerhauser (1992) for further details."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 58524360,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "7e63bf9af3f70abd5771c06d459a0d3fbfbb2909",
            "isKey": false,
            "numCitedBy": 15556,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Introduction Preliminaries and notation The what, why, and how of wavelets The continuous wavelet transform Discrete wavelet transforms: Frames Time-frequency density and orthonormal bases Orthonormal bases of wavelets and multiresolutional analysis Orthonormal bases of compactly supported wavelets More about the regularity of compactly supported wavelets Symmetry for compactly supported wavelet bases Characterization of functional spaces by means of wavelets Generalizations and tricks for orthonormal wavelet bases References Indexes."
            },
            "slug": "Ten-Lectures-on-Wavelets-Daubechies",
            "title": {
                "fragments": [],
                "text": "Ten Lectures on Wavelets"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper presents a meta-analyses of the wavelet transforms of Coxeter\u2019s inequality and its applications to multiresolutional analysis and orthonormal bases."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 249258,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "f55156fedf0bb586273f835c8987546b6bd65249",
            "isKey": false,
            "numCitedBy": 194,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Independent component analysis or blind source separation is a new technique of extracting independent signals from mixtures. It is applicable even when the number of independent sources is unknown and is larger or smaller than the number of observed mixture signals. This article extends the natural gradient learning algorithm to be applicable to these overcomplete and undercomplete cases. Here, the observed signals are assumed to be whitened by preprocessing, so that we use the natural Riemannian gradient in Stiefel manifolds."
            },
            "slug": "Natural-Gradient-Learning-for-Over-and-Bases-in-ICA-Amari",
            "title": {
                "fragments": [],
                "text": "Natural Gradient Learning for Over- and Under-Complete Bases in ICA"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The natural gradient learning algorithm is extended to be applicable to these overcomplete and undercomplete cases, where the observed signals are assumed to be whitened by preprocessing, so that the natural Riemannian gradient in Stiefel manifolds is used."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144083823"
                        ],
                        "name": "V. Hutson",
                        "slug": "V.-Hutson",
                        "structuredName": {
                            "firstName": "Vivian",
                            "lastName": "Hutson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Hutson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 339,
                                "start": 43
                            }
                        ],
                        "text": "In fact, it follows from Mercer\u2019s theorem (Hochstadt, 1973) that any function K(x;y) that is the kernel of a positive operator2 in L2(\u00c4) has an expansion of the form in equation A.2, in which the \u03c6i and the \u03bbi are, respectively, the orthogonal eigenfunctions and the positive eigenvalues of the operator corresponding to K. Stewart (1976) reports that the positivity of the operator associated with K is equivalent to the statement that the kernel K is positive definite, that is, the matrix Kij = K(xi; xj) is positive definite for all choices of distinct points xi."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 42
                            }
                        ],
                        "text": "In fact, it follows from Mercer\u2019s theorem (Hochstadt, 1973) that any function K(x;y) that is the kernel of a positive operator2 in L2(\u00c4) has an expansion of the form in equation A.2, in which the \u03c6i and the \u03bbi are, respectively, the orthogonal eigenfunctions and the positive eigenvalues of the\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 123
                            }
                        ],
                        "text": "The mathematical details (such the convergence or not of certain series) can be found in the theory of integral equations (Hochstadt, 1973; Cochran, 1972; Courant & Hilbert, 1962), which is very well established, so we do not discuss them here."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 42
                            }
                        ],
                        "text": "In fact, it follows from Mercer\u2019s theorem (Hochstadt, 1973) that any function K(x;y) that is the kernel of a positive operator2 in L2(\u00c4) has an expansion of the form in equation A."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4298975,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "22388b80c859a567548052bba5787d0df4676bab",
            "isKey": true,
            "numCitedBy": 1037,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Integral Equations and their ApplicationsVol. 1. By W. Pogorzelski. (International Series of Monographs in Pure and Applied Mathematics, Vol. 88.) Pp. xiii + 714. (London and New York: Pergamon Press Ltd.; Warszawa: PWN-Polish Scientific Publishers, 1966.) 120s."
            },
            "slug": "Integral-Equations-Hutson",
            "title": {
                "fragments": [],
                "text": "Integral Equations"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708279"
                        ],
                        "name": "C. Micchelli",
                        "slug": "C.-Micchelli",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Micchelli",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Micchelli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14461054,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "9d700e611ee7ffdf54873684a9e8883d3da0bcd7",
            "isKey": false,
            "numCitedBy": 1193,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Among other things, we prove that multiquadric surface interpolation is always solvable, thereby settling a conjecture of R. Franke."
            },
            "slug": "Interpolation-of-scattered-data:-Distance-matrices-Micchelli",
            "title": {
                "fragments": [],
                "text": "Interpolation of scattered data: Distance matrices and conditionally positive definite functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38751698"
                        ],
                        "name": "V. Morozov",
                        "slug": "V.-Morozov",
                        "structuredName": {
                            "firstName": "Vitali",
                            "lastName": "Morozov",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Morozov"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 117
                            }
                        ],
                        "text": "In order to make this problem well posed, we follow the approach of regularization theory (Tikhonov & Arsenin, 1977; Morozov, 1984; Bertero, 1986; Wahba, 1975, 1990) and impose an additional smoothness constraint on the solution of the approximation problem."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 94
                            }
                        ],
                        "text": "We will take a viewpoint closer to classical regularization theory (Tikhonov & Arsenin, 1977; Morozov, 1984; Bertero, 1986; Wahba, 1975, 1990), which might be more familiar to readers, rather than the theory of uniform conver-\nD ow nloaded from http://direct.m\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 74
                            }
                        ],
                        "text": "Basis pursuit denoising is a sparse approximation technique in which a function is reconstructed by using a small number of basis functions chosen from a large set (the dictionary)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 118764354,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "b65f5f60618304c2cfa8431488f481888e1cc10c",
            "isKey": false,
            "numCitedBy": 967,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Spend your time even for only few minutes to read a book. Reading a book will never reduce and waste your time to be useless. Reading, for some people become a need that is to do every day such as spending time for eating. Now, what about you? Do you like to read a book? Now, we will show you a new book enPDFd methods for solving incorrectly posed problems that can be a new way to explore the knowledge. When reading this book, you can get one thing to always remember in every reading time, even step by step."
            },
            "slug": "Methods-for-Solving-Incorrectly-Posed-Problems-Morozov",
            "title": {
                "fragments": [],
                "text": "Methods for Solving Incorrectly Posed Problems"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A new book enPDFd methods for solving incorrectly posed problems that can be a new way to explore the knowledge and get one thing to always remember in every reading time, even step by step is shown."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 92
                            }
                        ],
                        "text": "In this section we briefly sketch the ideas behind SVMs for regression and refer readers to Vapnik (1995) and Vapnik et al. (1997) for a full description of the technique."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 192
                            }
                        ],
                        "text": "In this article, we discuss the relationship between an approximation technique based on the principle of sparsity and the support vector machines (SVM) technique recently proposed by Vapnik (Vapnik, 1995; Vapnik, Golowich, & Smola, 1997; Cortes & Vapnik, 1995; Boser, Guyon, & Vapnik, 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 34
                            }
                        ],
                        "text": "gence in probability developed by Vapnik (1982, 1995). A similar approach is described in Smola and Sch\u00f6lkopf (1998), although with a different formalism."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 10
                            }
                        ],
                        "text": "Following Vapnik (1995) we now consider the case of the \u00b2-insensitive cost function V(x) = |x|\u00b2 ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 132
                            }
                        ],
                        "text": "This article shows a relationship between two different approximation techniques: the support vector machines (SVM), proposed by V. Vapnik (1995) and a sparse approximation scheme that resembles the basis pursuit denoising algorithm (Chen, 1995; Chen, Donoho, & Saunders, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 3
                            }
                        ],
                        "text": "V. Vapnik (1995) proposed using a particularly interesting form for the function V, which he calls the \u00b2-insensitive cost function, which we plot in Figure 1:\nV(x) = |x|\u00b2 \u2261 {\n0 if |x|   \u00b2 |x| \u2212 \u00b2 otherwise."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 5
                            }
                        ],
                        "text": "(See Vapnik, 1995, for more details about SVM and for the original derivation of the technique.)"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 290,
                                "start": 271
                            }
                        ],
                        "text": "\u20261986; Wahba, 1975, 1990), which might be more familiar to readers, rather than the theory of uniform conver-\nD ow nloaded from http://direct.m it.edu/neco/article-pdf/10/6/1455/813908/089976698300017269.pdf by guest on 01 Septem ber 2021\ngence in probability developed by Vapnik (1982, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 10
                            }
                        ],
                        "text": "Following Vapnik (1995) we now consider the case of the 2-insensitive cost function V(x) = |x|2 ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7138354,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8213dbed4db44e113af3ed17d6dad57471a0c048",
            "isKey": true,
            "numCitedBy": 38755,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Setting of the learning problem consistency of learning processes bounds on the rate of convergence of learning processes controlling the generalization ability of learning processes constructing learning algorithms what is important in learning theory?."
            },
            "slug": "The-Nature-of-Statistical-Learning-Theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "The Nature of Statistical Learning Theory"
            },
            "venue": {
                "fragments": [],
                "text": "Statistics for Engineering and Information Science"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 196
                            }
                        ],
                        "text": "Renaming the variables \u03bei as ai, we then have that the approximated cost function is\nE[a] = \u2225\u2225\u2225\u2225\u2225 f (x)\u2212 n\u2211\ni=1 ai\u03d5i(x)\n\u2225\u2225\u2225\u2225\u2225 2\nL2\n+ \u03bb\u2016a\u2016L1 , (3.6)\nwhich is the one proposed in the basis pursuit denoising method of Chen et al. (1995)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 183
                            }
                        ],
                        "text": "What is more surprising, and less obvious, is that SVM and a specific model of sparse approximation, which is a modified version of the basis pursuit denoising algorithm (Chen, 1995; Chen et al., 1995), are actually equivalent in the case of noiseless data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 344,
                                "start": 218
                            }
                        ],
                        "text": "Sparse approximation often appears in conjunction with the use of overcomplete or redundant representations, in which a signal is approximated as a linear superposition of basis functions taken from a large dictionary (Chen, 1995; Chen et al., 1995; Olshausen & Field, 1996; Daubechies, 1992; Mallat & Zhang, 1993; Coifman & Wickerhauser, 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 102
                            }
                        ],
                        "text": "We consider this to be a modified version of the basis pursuit denoising technique of Chen (1995) and Chen et al. (1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 182
                            }
                        ],
                        "text": "\u2026with the use of overcomplete or redundant representations, in which a signal is approximated as a linear superposition of basis functions taken from a large dictionary (Chen, 1995; Chen et al., 1995; Olshausen & Field, 1996; Daubechies, 1992; Mallat & Zhang, 1993; Coifman & Wickerhauser, 1992)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 29
                            }
                        ],
                        "text": "For example, Chen (1995) and Chen et al. (1995) use the L1 norm as an approximation of the L0, obtaining an approximation scheme that they call basis pursuit denoising."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 125
                            }
                        ],
                        "text": "In section 4 we present a sparse approximation model, which is similar in spirit to the basis pursuit denoising technique of Chen et al. (1995), and show that, in the case of noiseless data, it is equivalent to SVM. Section 5 concludes the article and contains a series of remarks and observations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 37
                            }
                        ],
                        "text": "The approximation scheme proposed by Chen et al. (1995) has the form described by equation 3.1, where the coefficients are found by minimizing the cost function (see equation 3.6)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Atomic decomposition by basis pursuit (Tech"
            },
            "venue": {
                "fragments": [],
                "text": "(May,"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 607,
                                "start": 47
                            }
                        ],
                        "text": "10) is the smoothness functional considered by Girosi et al. (1995) in their approach to regularization theory for function approximation. This is not surprising, since RKHS have been known to play a central role in spline theory (Wahba, 1990). Notice also that in spline theory, one actually deals with semi-RKHS, in which the norm \u2016\u00b7\u2016H has been substituted with a seminorm. Semi-RKHS share most of the properties of RKHS, but their theory becomes a little more complicated because of the null space of the seminorm, which has to be taken into account. Details about semi-RKHS can be found in Wahba (1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 788,
                                "start": 77
                            }
                        ],
                        "text": "Details about RKHS and examples of kernels can be found in appendix A and in Girosi (1997). If the cost function V is quadratic, the unknown coefficients in equation 2.4 can be found by solving a linear system. When the kernel K is a radially symmetric function, equation 2.4 describes a radial basis functions approximation scheme, which is closely related to smoothing splines, and when K is of the form K(x \u2212 y), equation 2.4 is a regularization network (Girosi, Jones, & Poggio, 1995). When the cost function V is not quadratic anymore, the solution of the variational problem (see equation 2.2) still has the form of equation 2.4 (Smola & Sch\u00f6lkopf, 1998; Girosi, Poggio, & Caprile, 1991), but the coefficients ai cannot be found anymore by solving a linear system. V. Vapnik (1995) proposed using a particularly interesting form for the function V, which he calls the 2-insensitive cost function, which we plot in Figure 1:"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 47
                            }
                        ],
                        "text": "10) is the smoothness functional considered by Girosi et al. (1995) in their approach to regularization theory for function approximation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 77
                            }
                        ],
                        "text": "Details about RKHS and examples of kernels can be found in appendix A and in Girosi (1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 77
                            }
                        ],
                        "text": "Details about RKHS and examples of kernels can be found in appendix A and in Girosi (1997). If the cost function V is quadratic, the unknown coefficients in equation 2."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An equivalence between sparse approximation and support vector machines (A.I"
            },
            "venue": {
                "fragments": [],
                "text": "Memo No. 1606)"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 9
                            }
                        ],
                        "text": "i=1 i) p (11) where f igi=1 is a set of binary variables, with values in f0; 1g ,k kL2 is the usual L2 norm, and p is a positive number that we set to one unless otherwise stated."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 77
                            }
                        ],
                        "text": "Details about RKHS and examples of kernels can be found in appendix A and in Girosi (1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 109
                            }
                        ],
                        "text": "In order to see this, let us allow the variables i to assume values in f 1; 0; 1g, so that the cost function (11) can be rewritten as E[a; ] = kf(x) n X"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 164
                            }
                        ],
                        "text": "In the case in which S(x) = jxj, that is the Basis Pursuit De-Noising case, it is simple to see how the cost function (13) is an approximated version of the one in (11)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An equivalence between sparse approximation and Support Vector Machines"
            },
            "venue": {
                "fragments": [],
                "text": "A.I. Memo 1606, MIT Arti cial Intelligence Laboratory"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 143
                            }
                        ],
                        "text": "n=1 n n(x) n(y) (5) The kernelK can be seen as the kernel of a Reproducing Kernel Hilbert Space (RKHS), a concept that will be used in section (4)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "(4) is a Regularization Network (Girosi, Jones and Poggio, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 18
                            }
                        ],
                        "text": "i=1 aiK(x;xi) + b (4) where we have de ned the (symmetric) kernel function K as:"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 113
                            }
                        ],
                        "text": "When the cost function V is not quadratic anymore the solution of the variational problem (2) has still the form (4) (Smola and Sch\u007folkopf, 1998; Girosi, Poggio and Caprile, 1991), but the coe cients ai cannot be found anymore by solving a linear system."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 284,
                                "start": 271
                            }
                        ],
                        "text": "\u2026ow nloaded from http://direct.m it.edu/neco/article-pdf/10/6/1455/813908/089976698300017269.pdf by guest on 01 Septem ber 2021\nselection, in order to produce linear models that use a small number of variables and therefore have greater interpretability (Tibshirani, 1994; Breiman, 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "(4) describe a Radial Basis Functions approximation scheme, which is closely related to smoothing splines, and when K is of the form K(x y) eq."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 62
                            }
                        ],
                        "text": "If the cost function V is quadratic the unknown coe cients in (4) can be found by solving a linear system."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Better subset selection using the non-negative garotte"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 246,
                                "start": 231
                            }
                        ],
                        "text": "(4.1)\nThis model is similar to the one of SVM (see equation 2.7) except for the constant b, and if K(x;y) = G(\u2016x\u2212y\u2016), where G is a positive definite function, it corresponds to a classical radial basis functions approximation scheme (Micchelli, 1986; Moody & Darken, 1989; Powell, 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Interpolation of scattered data: distance matrices and conditionally positive deenite functions. Constructive Approximation"
            },
            "venue": {
                "fragments": [],
                "text": "Interpolation of scattered data: distance matrices and conditionally positive deenite functions. Constructive Approximation"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 171
                            }
                        ],
                        "text": "What is more surprising, and less obvious, is that SVM and a specific model of sparse approximation, which is a modified version of the basis pursuit denoising algorithm (Chen, 1995; Chen et al., 1995), are actually equivalent in the case of noiseless data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 86
                            }
                        ],
                        "text": "We consider this to be a modified version of the basis pursuit denoising technique of Chen (1995) and Chen et al. (1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 244,
                                "start": 234
                            }
                        ],
                        "text": "This article shows a relationship between two different approximation techniques: the support vector machines (SVM), proposed by V. Vapnik (1995) and a sparse approximation scheme that resembles the basis pursuit denoising algorithm (Chen, 1995; Chen, Donoho, & Saunders, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 170
                            }
                        ],
                        "text": "\u2026with the use of overcomplete or redundant representations, in which a signal is approximated as a linear superposition of basis functions taken from a large dictionary (Chen, 1995; Chen et al., 1995; Olshausen & Field, 1996; Daubechies, 1992; Mallat & Zhang, 1993; Coifman & Wickerhauser, 1992)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 13
                            }
                        ],
                        "text": "For example, Chen (1995) and Chen et al. (1995) use the L1 norm as an approximation of the L0, obtaining an approximation scheme that they call basis pursuit denoising."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 97
                            }
                        ],
                        "text": "It is not our purpose to discuss the motivations that lead to this approach and refer readers to Chen (1995), Chen et al.\nD ow nloaded from http://direct.m it.edu/neco/article-pdf/10/6/1455/813908/089976698300017269.pdf by guest on 01 Septem ber 2021\n(1995), Olshausen and Field (1996), Harpur and\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Basis pursuit. Unpublished doctoral dissertation, Stanford University"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 282,
                                "start": 270
                            }
                        ],
                        "text": "(4.1)\nThis model is similar to the one of SVM (see equation 2.7) except for the constant b, and if K(x;y) = G(\u2016x\u2212y\u2016), where G is a positive definite function, it corresponds to a classical radial basis functions approximation scheme (Micchelli, 1986; Moody & Darken, 1989; Powell, 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The theory of radial basis functions approximation in 1990 Advances in Numerical Analysis Volume II: Wavelets, Subdivision Algorithms and Radial Basis Functions, pages 105{210"
            },
            "venue": {
                "fragments": [],
                "text": "The theory of radial basis functions approximation in 1990 Advances in Numerical Analysis Volume II: Wavelets, Subdivision Algorithms and Radial Basis Functions, pages 105{210"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 282,
                                "start": 270
                            }
                        ],
                        "text": "(4.1)\nThis model is similar to the one of SVM (see equation 2.7) except for the constant b, and if K(x;y) = G(\u2016x\u2212y\u2016), where G is a positive definite function, it corresponds to a classical radial basis functions approximation scheme (Micchelli, 1986; Moody & Darken, 1989; Powell, 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The theory of radial basis functions approximation in 1990 Advances in Numerical Analysis Volume II: Wavelets, Subdivision Algorithms and Radial Basis Functions"
            },
            "venue": {
                "fragments": [],
                "text": "The theory of radial basis functions approximation in 1990 Advances in Numerical Analysis Volume II: Wavelets, Subdivision Algorithms and Radial Basis Functions"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "(23), disregarding the constant term in kfk 2 H, and de ning"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 282,
                                "start": 270
                            }
                        ],
                        "text": "(4.1)\nThis model is similar to the one of SVM (see equation 2.7) except for the constant b, and if K(x;y) = G(\u2016x\u2212y\u2016), where G is a positive definite function, it corresponds to a classical radial basis functions approximation scheme (Micchelli, 1986; Moody & Darken, 1989; Powell, 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The theory of radial basis functions approximation in 1990"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 77
                            }
                        ],
                        "text": "Details about RKHS and examples of kernels can be found in appendix A and in Girosi (1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 149
                            }
                        ],
                        "text": "\u2026that an approximating function of the form of equation 3.1 is sparse if the coefficients have been chosen so that they minimize the following cost function:\nE[a, \u03be] = \u2225\u2225\u2225\u2225\u2225 f (x)\u2212 n\u2211\ni=1 \u03beiai\u03d5i(x)\n\u2225\u2225\u2225\u2225\u2225 2\nL2\n+ \u03bb (\nn\u2211 i=1 \u03bei\n)p , (3.2)\nwhere {\u03bei}ni=1 is a set of binary variables, with values in\u2026"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An equivalence between sparse approximation and Support Vector Machines. A.I. Memo 1606"
            },
            "venue": {
                "fragments": [],
                "text": "MIT Artificial Intelligence Laboratory"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 91
                            }
                        ],
                        "text": "This is not surprising, since RKHS have been known to play a central role in spline theory (Wahba, 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 195857038,
            "fieldsOfStudy": [],
            "id": "b7294ac444ae927f71ac442372903ff5e2a763e1",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Spline Models for Observational Data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16729445"
                        ],
                        "name": "R. Courant",
                        "slug": "R.-Courant",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Courant",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Courant"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6234493"
                        ],
                        "name": "D. Hilbert",
                        "slug": "D.-Hilbert",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Hilbert",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hilbert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 62
                            }
                        ],
                        "text": "(1995), Olshausen and Field (1996), Harpur and Prager (1996), Daubechies (1992), Mallat and Zhang (1993), and Coifman and Wickerhauser (1992) for further details."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 155
                            }
                        ],
                        "text": "The mathematical details (such the convergence or not of certain series) can be found in the theory of integral equations (Hochstadt, 1973; Cochran, 1972; Courant & Hilbert, 1962), which is very well established, so we do not discuss them here."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 125547342,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "1f5ba2142aa78a34a4077dafb337caea0aefaa04",
            "isKey": false,
            "numCitedBy": 865,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Methods-of-Mathematical-Physics,-Vol.-I-Courant-Hilbert",
            "title": {
                "fragments": [],
                "text": "Methods of Mathematical Physics, Vol. I"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1954
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143639508"
                        ],
                        "name": "A. Tikhonov",
                        "slug": "A.-Tikhonov",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Tikhonov",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tikhonov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102992139"
                        ],
                        "name": "Vasiliy Yakovlevich Arsenin",
                        "slug": "Vasiliy-Yakovlevich-Arsenin",
                        "structuredName": {
                            "firstName": "Vasiliy",
                            "lastName": "Arsenin",
                            "middleNames": [
                                "Yakovlevich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vasiliy Yakovlevich Arsenin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 91
                            }
                        ],
                        "text": "In order to make this problem well posed, we follow the approach of regularization theory (Tikhonov & Arsenin, 1977; Morozov, 1984; Bertero, 1986; Wahba, 1975, 1990) and impose an additional smoothness constraint on the solution of the approximation problem."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 68
                            }
                        ],
                        "text": "We will take a viewpoint closer to classical regularization theory (Tikhonov & Arsenin, 1977; Morozov, 1984; Bertero, 1986; Wahba, 1975, 1990), which might be more familiar to readers, rather than the theory of uniform conver-\nD ow nloaded from http://direct.m\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 46
                            }
                        ],
                        "text": "Basis pursuit denoising is a sparse approximation technique in which a function is reconstructed by using a small number of basis functions chosen from a large set (the dictionary)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 122072756,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "bc14819e745cd7af37efd09ea29773dc0065119e",
            "isKey": false,
            "numCitedBy": 7884,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Solutions-of-ill-posed-problems-Tikhonov-Arsenin",
            "title": {
                "fragments": [],
                "text": "Solutions of ill-posed problems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69335340"
                        ],
                        "name": "J. A. Cochran",
                        "slug": "J.-A.-Cochran",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Cochran",
                            "middleNames": [
                                "Alan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. A. Cochran"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 140
                            }
                        ],
                        "text": "The mathematical details (such the convergence or not of certain series) can be found in the theory of integral equations (Hochstadt, 1973; Cochran, 1972; Courant & Hilbert, 1962), which is very well established, so we do not discuss them here."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 120984150,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e3736b70c8a204972e8fc05479df72498c21bdcc",
            "isKey": false,
            "numCitedBy": 140,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-analysis-of-linear-integral-equations-Cochran",
            "title": {
                "fragments": [],
                "text": "The analysis of linear integral equations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46933679"
                        ],
                        "name": "M. Bertero",
                        "slug": "M.-Bertero",
                        "structuredName": {
                            "firstName": "Mario",
                            "lastName": "Bertero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bertero"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117703131,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "bd7efb0ea1893522220b4cf54d71269b77e80deb",
            "isKey": false,
            "numCitedBy": 102,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Regularization-methods-for-linear-inverse-problems-Bertero",
            "title": {
                "fragments": [],
                "text": "Regularization methods for linear inverse problems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 144
                            }
                        ],
                        "text": "\u2026A.2, in which the \u03c6i and the \u03bbi are, respectively, the orthogonal eigenfunctions and the positive eigenvalues of the operator corresponding to K. Stewart (1976) reports that the positivity of the operator associated with K is equivalent to the statement that the kernel K is positive\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 124
                            }
                        ],
                        "text": "If we now rename the coefficients as follows: ai \u21d2 \u03b1\u2217 i ai \u21d2 \u03b1i we notice that the QP problem defined by equations (25) and (26) is the same QP problem that we need to solve for training a SVM with kernel K (see eq."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Positive definite functions and generalizations"
            },
            "venue": {
                "fragments": [],
                "text": "an historical survey. Rocky Mountain J. Math., 6:409\u2013434"
            },
            "year": 1976
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 86
                            }
                        ],
                        "text": "The kernel of H has the following reproducing property: f(x) =< f(y);K(y;x) >H 8f 2 H (27) where < ; >H denotes the scalar product in H."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Regression selection and shrinkage via the lasso"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 91
                            }
                        ],
                        "text": "In order to make this problem well posed, we follow the approach of regularization theory (Tikhonov & Arsenin, 1977; Morozov, 1984; Bertero, 1986; Wahba, 1975, 1990) and impose an additional smoothness constraint on the solution of the approximation problem."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 68
                            }
                        ],
                        "text": "We will take a viewpoint closer to classical regularization theory (Tikhonov & Arsenin, 1977; Morozov, 1984; Bertero, 1986; Wahba, 1975, 1990), which might be more familiar to readers, rather than the theory of uniform conver-\nD ow nloaded from http://direct.m\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Solutions of ill-posed problems. Washington"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 77
                            }
                        ],
                        "text": "Details about RKHS and examples of kernels can be found in appendix A and in Girosi (1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An equivalence between sparse approximation and support vector machines (A.I. Memo No. 1606)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Matching Pursuit in a time - frequency dictionary"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactionson Signal Processing"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 173
                            }
                        ],
                        "text": "\u2026et al.\nD ow nloaded from http://direct.m it.edu/neco/article-pdf/10/6/1455/813908/089976698300017269.pdf by guest on 01 Septem ber 2021\n(1995), Olshausen and Field (1996), Harpur and Prager (1996), Daubechies (1992), Mallat and Zhang (1993), and Coifman and Wickerhauser (1992) for further details."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Development o f l o w e n tropy coding in a recurrent network"
            },
            "venue": {
                "fragments": [],
                "text": "Network"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 144
                            }
                        ],
                        "text": "\u2026A.2, in which the \u03c6i and the \u03bbi are, respectively, the orthogonal eigenfunctions and the positive eigenvalues of the operator corresponding to K. Stewart (1976) reports that the positivity of the operator associated with K is equivalent to the statement that the kernel K is positive\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 87
                            }
                        ],
                        "text": "subject to the constraints: a;a 0 Pl i=1(a + i a i ) = 0 a+i a i = 0 8i = 1; : : : ; l (26)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 119
                            }
                        ],
                        "text": "If we now rename the coe cients as follows: a+i ) i a i ) i we notice that the QP problem de ned by equations (25) and (26) is the same QP problem that we need to solve for training a SVM with kernel K (see eq."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Positive de nite functions and generalizations"
            },
            "venue": {
                "fragments": [],
                "text": "an historical survey. Rocky Mountain J. Math., 6:409{434"
            },
            "year": 1976
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Solutions of Illposed Problems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Extensions of a theory of networks for approximationand learning : outliers and negative examples"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 264,
                                "start": 244
                            }
                        ],
                        "text": "\u2026with the use of overcomplete or redundant representations, in which a signal is approximated as a linear superposition of basis functions taken from a large dictionary (Chen, 1995; Chen et al., 1995; Olshausen & Field, 1996; Daubechies, 1992; Mallat & Zhang, 1993; Coifman & Wickerhauser, 1992)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 18
                            }
                        ],
                        "text": "i=1 (yi f (xi)) 2 (18) However, because we used the norm k kH, we will see in the following that (surprisingly) no approximation is required, and the expression (17) can be computed exactly, up to a constant (which is obviously irrelevant for the minimization process)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 241,
                                "start": 218
                            }
                        ],
                        "text": "\u2026et al.\nD ow nloaded from http://direct.m it.edu/neco/article-pdf/10/6/1455/813908/089976698300017269.pdf by guest on 01 Septem ber 2021\n(1995), Olshausen and Field (1996), Harpur and Prager (1996), Daubechies (1992), Mallat and Zhang (1993), and Coifman and Wickerhauser (1992) for further details."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Matching pursuit in a time - frequency dictionary"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Signal Processing"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2054,
                                "start": 145
                            }
                        ],
                        "text": "selection, in order to produce linear models that use a small number of variables and therefore have greater interpretability (Tibshirani, 1994; Breiman, 1993). In this article, we discuss the relationship between an approximation technique based on the principle of sparsity and the support vector machines (SVM) technique recently proposed by Vapnik (Vapnik, 1995; Vapnik, Golowich, & Smola, 1997; Cortes & Vapnik, 1995; Boser, Guyon, & Vapnik, 1992). SVM is a classification and approximation technique derived by V. Vapnik in the framework of structural risk minimization, which aims at building parsimonious models, in the sense of VC-dimension. Sparse approximation techniques are also parsimonious, in the sense that they try to minimize the number of parameters of the model, so it is not surprising that some connections between SVM and sparse approximation exist. What is more surprising, and less obvious, is that SVM and a specific model of sparse approximation, which is a modified version of the basis pursuit denoising algorithm (Chen, 1995; Chen et al., 1995), are actually equivalent in the case of noiseless data. By equivalent, we mean the following: if applied to the same data set, they give the same solution, which is obtained by solving the same quadratic programming problem. The equivalence between sparse approximation and SVM for noiseless data is the main point of this article, but we also include a derivation of the SVM different from the one given by V. Vapnik and that fits very well in the framework of regularization theory, the same one used to derive techniques like splines or radial basis functions. In section 2 in this article, we introduce the technique of SVM in the framework of regularization theory (the mathematical details are set out in appendix B). Section 3 introduces the notion of sparsity and presents an exact and approximate formulation of the problem. In section 4 we present a sparse approximation model, which is similar in spirit to the basis pursuit denoising technique of Chen et al. (1995), and show that, in the case of noiseless data, it is equivalent to SVM."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 126
                            }
                        ],
                        "text": "selection, in order to produce linear models that use a small number of variables and therefore have greater interpretability (Tibshirani, 1994; Breiman, 1993)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 284,
                                "start": 271
                            }
                        ],
                        "text": "\u2026ow nloaded from http://direct.m it.edu/neco/article-pdf/10/6/1455/813908/089976698300017269.pdf by guest on 01 Septem ber 2021\nselection, in order to produce linear models that use a small number of variables and therefore have greater interpretability (Tibshirani, 1994; Breiman, 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Better subset selection using the non-negative garotte (Tech"
            },
            "venue": {
                "fragments": [],
                "text": "Rep.). Berkeley: Department of Statistics, University of California, Berkeley."
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Aronszajn . Theory of reproducing kernels"
            },
            "venue": {
                "fragments": [],
                "text": "Trans . Amer . Math . Soc ."
            },
            "year": 1950
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Regression selection and shrinkage via the lasso (Tech"
            },
            "venue": {
                "fragments": [],
                "text": "survey. Rocky Mountain J. Math.,"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An equivalence between sparse approximation and Support Vector Machines. A.I. Memo 1606, MIT Artiicial Intelligence Laboratory, 1 9 9 7 . (available at the URL"
            },
            "venue": {
                "fragments": [],
                "text": "An equivalence between sparse approximation and Support Vector Machines. A.I. Memo 1606, MIT Artiicial Intelligence Laboratory, 1 9 9 7 . (available at the URL"
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 31,
            "methodology": 19
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 59,
        "totalPages": 6
    },
    "page_url": "https://www.semanticscholar.org/paper/An-Equivalence-Between-Sparse-Approximation-and-Girosi/d27c7569fdbcbb57ff511f5293e32b547acca7b3?sort=total-citations"
}