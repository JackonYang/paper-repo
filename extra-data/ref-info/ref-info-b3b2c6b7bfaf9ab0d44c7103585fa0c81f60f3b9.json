{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2124754"
                        ],
                        "name": "U. Gargi",
                        "slug": "U.-Gargi",
                        "structuredName": {
                            "firstName": "Ullas",
                            "lastName": "Gargi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Gargi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721328"
                        ],
                        "name": "Sameer Kiran Antani",
                        "slug": "Sameer-Kiran-Antani",
                        "structuredName": {
                            "firstName": "Sameer",
                            "lastName": "Antani",
                            "middleNames": [
                                "Kiran"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sameer Kiran Antani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3110392"
                        ],
                        "name": "R. Kasturi",
                        "slug": "R.-Kasturi",
                        "structuredName": {
                            "firstName": "Rangachar",
                            "lastName": "Kasturi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kasturi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 132
                            }
                        ],
                        "text": "Further, we would like to use text detection for video analysis purposes as opposed to its more traditional usage of video indexing [6,10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 30086203,
            "fieldsOfStudy": [
                "Computer Science",
                "Art"
            ],
            "id": "7e3ed958f25974dab09b445fbd6f297852b761fc",
            "isKey": false,
            "numCitedBy": 69,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Like shot changes, the presence of text in digital video is an important event that can be used to index digital video and provide extremely useful semantic information about the scene content. The special characteristics of digital video compared to document images both require and allow new robust approaches to recognition of text in video. We discuss the characteristics and special challenges of text in video and present a strategy of detecting, localizing, and segmenting text from video data for the text indexing problem. Preliminary results from our approach are presented."
            },
            "slug": "Indexing-text-events-in-digital-video-databases-Gargi-Antani",
            "title": {
                "fragments": [],
                "text": "Indexing text events in digital video databases"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The characteristics and special challenges of text in video are discussed and a strategy of detecting, localizing, and segmenting text from video data for the text indexing problem is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116415943"
                        ],
                        "name": "B. Yu",
                        "slug": "B.-Yu",
                        "structuredName": {
                            "firstName": "Bin",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Yu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5196787,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f565f502ad1acb81c5659b051c04683a34ed138f",
            "isKey": false,
            "numCitedBy": 576,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatic text location (without character recognition capabilities) deals with extracting image regions that contain text only. The images of these regions can then be fed to an optical character recognition module or highlighted for users. This is very useful in a number of applications such as database indexing and converting paper documents to their electronic versions. The performance of our automatic text location algorithm is shown in several applications. Compared with some traditional text location methods, our method has the following advantages: 1) low computational cost; 2) robust to font size; and 3) high accuracy."
            },
            "slug": "Automatic-text-location-in-images-and-video-frames-Jain-Yu",
            "title": {
                "fragments": [],
                "text": "Automatic text location in images and video frames"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Compared with some traditional text location methods, this method has the following advantages: 1) low computational cost; 2) robust to font size; and 3) high accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2907696"
                        ],
                        "name": "J. Shim",
                        "slug": "J.-Shim",
                        "structuredName": {
                            "firstName": "Jae",
                            "lastName": "Shim",
                            "middleNames": [
                                "Chang"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145253787"
                        ],
                        "name": "C. Dorai",
                        "slug": "C.-Dorai",
                        "structuredName": {
                            "firstName": "Chitra",
                            "lastName": "Dorai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Dorai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70029967"
                        ],
                        "name": "R. Bolle",
                        "slug": "R.-Bolle",
                        "structuredName": {
                            "firstName": "Ruud",
                            "lastName": "Bolle",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bolle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 111
                            }
                        ],
                        "text": ", use a generalized region labeling algorithm to find homogeneous regions for text segmentation and extraction [18]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12062439,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1eb854ce0539b6fd18dbba942f52a80a735f5c8e",
            "isKey": false,
            "numCitedBy": 142,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Efficient content-based retrieval of image and video databases is an important application due to rapid proliferation of digital video data on the Internet and corporate intranets. Text either embedded or superimposed within video frames is very useful for describing the contents of the frames, as it enables both keyword and free-text based search, automatic video logging, and video cataloging. We have developed a scheme for automatically extracting text from digital images and videos for content annotation and retrieval. We present our approach to robust text extraction from video frames, which can handle complex image backgrounds, deal with different font sizes, font styles, and font appearances such as normal and inverse video. Our algorithm results in segmented characters that can be directly processed by an OCR system to produce ASCII text. Results from our experiments with over 5000 frames obtained from twelve MPEG video streams demonstrate the good performance of our system in terms of text identification accuracy and computational efficiency."
            },
            "slug": "Automatic-text-extraction-from-video-for-annotation-Shim-Dorai",
            "title": {
                "fragments": [],
                "text": "Automatic text extraction from video for content-based annotation and retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work has developed a scheme for automatically extracting text from digital images and videos for content annotation and retrieval that results in segmented characters that can be directly processed by an OCR system to produce ASCII text."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. Fourteenth International Conference on Pattern Recognition (Cat. No.98EX170)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144739319"
                        ],
                        "name": "R. Lienhart",
                        "slug": "R.-Lienhart",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Lienhart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lienhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2083891040"
                        ],
                        "name": "F. Stuber",
                        "slug": "F.-Stuber",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Stuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Stuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14147742,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "778a307aa0cf8b2ed273b9089cb9aa8210f49f24",
            "isKey": false,
            "numCitedBy": 205,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We have developed algorithms for automatic character segmentation in motion pictures which extract automatically and reliably the text in pre-title sequences, credit titles, and closing sequences with title and credits. The algorithms we propose make use of typical characteristics of text in videos in order to enhance segmentation and, consequently, recognition performance. As a result, we get segmented characters from video pictures. These can be parsed by any OCR software. The recognition results of multiple instances of the same character throughout subsequent frames are combined to enhance recognition result and to compute the final output. We have tested our segmentation algorithms in a series of experiments with video clips recorded from television and achieved good segmentation results."
            },
            "slug": "Automatic-text-recognition-in-digital-videos-Lienhart-Stuber",
            "title": {
                "fragments": [],
                "text": "Automatic text recognition in digital videos"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "Algorithms for automatic character segmentation in motion pictures which extract automatically and reliably the text in pre-title sequences, credit titles, and closing sequences with title and credits are developed."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7661726"
                        ],
                        "name": "Alexander Hauptmann",
                        "slug": "Alexander-Hauptmann",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Hauptmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander Hauptmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116645662"
                        ],
                        "name": "Michael Smith",
                        "slug": "Michael-Smith",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Smith",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 20
                            }
                        ],
                        "text": "Hauptmann and Smith [8] use the spatial context of text and high contrast of text regions in scene images to merge large numbers of horizontal and vertical edges in spatial proximity to detect text."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 20
                            }
                        ],
                        "text": "A. Hauptmann and M. Smith, \u201cText, Speech, and Vision for Video Segmentation: The Informedia Project,\u201d In AAAI Fall 1995 Symposium on Computational Models for Integrating Language and Vision 1995."
                    },
                    "intents": []
                }
            ],
            "corpusId": 36119549,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "02e95ad680fc3b216a11181638ef5d31f66f423a",
            "isKey": false,
            "numCitedBy": 118,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe three technologies involved in creating a digital video library suitable for fullcontent search and retrieval. Image processing analyzes scenes, speech processing transcribes the audio signal, and natural language processing determines word relevance. The integration of these technologies enables us to include vast amounts of video data in the library."
            },
            "slug": "Text,-Speech,-and-Vision-for-Video-Segmentation:-Hauptmann-Smith",
            "title": {
                "fragments": [],
                "text": "Text, Speech, and Vision for Video Segmentation: The InformediaTM Project"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "Three technologies involved in creating a digital video library suitable for fullcontent search and retrieval are described: image processing analyzes scenes, speech processing transcribes the audio signal, and natural language processing determines word relevance."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144739319"
                        ],
                        "name": "R. Lienhart",
                        "slug": "R.-Lienhart",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Lienhart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lienhart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8416045,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0dd1def5778f24c2c5a5f1c114846326e8f86123",
            "isKey": false,
            "numCitedBy": 143,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Efficient indexing and retrieval of digital video is an important aspect of video databases. One powerful index for retrieval is the text appearing in them. It enables content- based browsing. We present our methods for automatic segmentation and recognition of text in digital videos. The algorithms we propose make use of typical characteristics of text in videos in order to enable and enhance segmentation and recognition performance. Especially the inter-frame dependencies of the characters provide new possibilities for their refinement. Then, a straightforward indexing and retrieval scheme is introduced. It is used in the experiments to demonstrate that the proposed text segmentation and text recognition algorithms are suitable for indexing and retrieval of relevant video scenes in and from a video data base. Our experimental results are very encouraging and suggest that these algorithms can be used in video retrieval applications as well as to recognize higher semantics in video."
            },
            "slug": "Automatic-text-recognition-for-video-indexing-Lienhart",
            "title": {
                "fragments": [],
                "text": "Automatic text recognition for video indexing"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The proposed text segmentation and text recognition algorithms are suitable for indexing and retrieval of relevant video scenes in and from a video data base and suggest that they can be used in video retrieval applications as well as to recognize higher semantics in video."
            },
            "venue": {
                "fragments": [],
                "text": "MULTIMEDIA '96"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "79306765"
                        ],
                        "name": "T. McGee",
                        "slug": "T.-McGee",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "McGee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. McGee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2160177"
                        ],
                        "name": "N. Dimitrova",
                        "slug": "N.-Dimitrova",
                        "structuredName": {
                            "firstName": "Nevenka",
                            "lastName": "Dimitrova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Dimitrova"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 54
                            }
                        ],
                        "text": "Also, commercial detection is an active research area [10,14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12688171,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3cbd5989b55aa67cc1349c096e1daa5ec54518e3",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstracting video information automatically from TV broadcast, requires reliable methods for isolating program and commercial segments out of the full broadcast material. In this paper, we present the results from cut, static sequence, black frame, and text detection, for the purpose of isolating non-program segments. These results are evaluated, by comparison, to human visual inspection using more than 13 hours of varied program content. Using cut rate detection alone, produced a high recall with medium precision. Text detection was performed on the commercials, and the false positive segments. Adding text detection slightly lowers the recall. However, much higher precision is achieved. A new fast black frame detector algorithm is presented. Black frame detection is important for identifying commercial boundaries. Results indicate that adding detection of text, in addition to cut rate, to reduce the number of false positives, appears to be a promising method. Furthermore, by adding the information about position and size of text, and tracking it through an area, should further increase reliability."
            },
            "slug": "Parsing-TV-programs-for-identification-and-removal-McGee-Dimitrova",
            "title": {
                "fragments": [],
                "text": "Parsing TV programs for identification and removal of nonstory segments"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Results indicate that adding detection of text, in addition to cut rate, to reduce the number of false positives, appears to be a promising method that should further increase reliability."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2160177"
                        ],
                        "name": "N. Dimitrova",
                        "slug": "N.-Dimitrova",
                        "structuredName": {
                            "firstName": "Nevenka",
                            "lastName": "Dimitrova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Dimitrova"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "79306765"
                        ],
                        "name": "T. McGee",
                        "slug": "T.-McGee",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "McGee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. McGee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1989828"
                        ],
                        "name": "H. Elenbaas",
                        "slug": "H.-Elenbaas",
                        "structuredName": {
                            "firstName": "Herman",
                            "lastName": "Elenbaas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Elenbaas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6274599"
                        ],
                        "name": "Jacquelyn Martino",
                        "slug": "Jacquelyn-Martino",
                        "structuredName": {
                            "firstName": "Jacquelyn",
                            "lastName": "Martino",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jacquelyn Martino"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 112
                            }
                        ],
                        "text": "The indexing and classification of video is an important problem that has been discussed in recent publications [1,3,4,5,12,17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 85
                            }
                        ],
                        "text": "For the commercials, the keyframes were extracted using our visual summary algorithm [5]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 37390031,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dca4ee816a6a58f6db4e029d21114f4c1a052c29",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Methods for video content analysis are necessary for the growing amount of video information delivered to consumers today. In this paper, we present a system for video content analysis called Vitamin, which provides management of a home video library. The system presents the user with a visual table of contents that provides an overview of the video content and direct access to particular points in the stored video. In this process, we apply a computationally inexpensive, simple, yet powerful mechanism for cut detection and keyframe filtering. Our initial implementation and results show that this system can perform video content extraction in real time on a low-end platform that matches a visual table of contents extracted by an expert."
            },
            "slug": "Video-Content-Management-in-Consumer-Devices-Dimitrova-McGee",
            "title": {
                "fragments": [],
                "text": "Video Content Management in Consumer Devices"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper presents a system for video content analysis called Vitamin, which provides management of a home video library and applies a computationally inexpensive, simple, yet powerful mechanism for cut detection and keyframe filtering."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Knowl. Data Eng."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9546964"
                        ],
                        "name": "Shih-Fu Chang",
                        "slug": "Shih-Fu-Chang",
                        "structuredName": {
                            "firstName": "Shih-Fu",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shih-Fu Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109082247"
                        ],
                        "name": "William Chen",
                        "slug": "William-Chen",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069339"
                        ],
                        "name": "H. Meng",
                        "slug": "H.-Meng",
                        "structuredName": {
                            "firstName": "Horace",
                            "lastName": "Meng",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Meng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145372008"
                        ],
                        "name": "H. Sundaram",
                        "slug": "H.-Sundaram",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Sundaram",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Sundaram"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40588714"
                        ],
                        "name": "D. Zhong",
                        "slug": "D.-Zhong",
                        "structuredName": {
                            "firstName": "Di",
                            "lastName": "Zhong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Zhong"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 112
                            }
                        ],
                        "text": "The indexing and classification of video is an important problem that has been discussed in recent publications [1,3,4,5,12,17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 88
                            }
                        ],
                        "text": "More algorithms that could be useful for character segmentation are described elsewhere [3,13,16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4389174,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "125250e97714592b8437031de631bcda469d02a6",
            "isKey": false,
            "numCitedBy": 403,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": "The rapidity with which digitat information, particularly video, is being generated, has necessitated the development of tools for efficient search of these media. Content based visual queries have been primarily focussed on still image retrieval. In this papel; we propose a novel, real-time, interactive system on the Web, based on the visual paradigm, with spatio-temporal attributesplaying a key role in video retrieval. We have developed algorithms for automated video object segmentation and tracking and use real-time video editing techniques while responding to user queries. The resulting system pe$orms well, with the user being able to retrieve complex video clips such as those of skiers, baseball players, with ease."
            },
            "slug": "VideoQ:-an-automated-content-based-video-search-Chang-Chen",
            "title": {
                "fragments": [],
                "text": "VideoQ: an automated content based video search system using visual cues"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A novel, real-time, interactive system on the Web, based on the visual paradigm, with spatio-temporal attributesplaying a key role in video retrieval, with the user being able to retrieve complex video clips such as those of skiers, baseball players, with ease."
            },
            "venue": {
                "fragments": [],
                "text": "MULTIMEDIA '97"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "92329551"
                        ],
                        "name": "M. Abdel-Mottaleb",
                        "slug": "M.-Abdel-Mottaleb",
                        "structuredName": {
                            "firstName": "Mohamed",
                            "lastName": "Abdel-Mottaleb",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Abdel-Mottaleb"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2160177"
                        ],
                        "name": "N. Dimitrova",
                        "slug": "N.-Dimitrova",
                        "structuredName": {
                            "firstName": "Nevenka",
                            "lastName": "Dimitrova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Dimitrova"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "75177975"
                        ],
                        "name": "Ranjit P. Desai",
                        "slug": "Ranjit-P.-Desai",
                        "structuredName": {
                            "firstName": "Ranjit",
                            "lastName": "Desai",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ranjit P. Desai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6274599"
                        ],
                        "name": "Jacquelyn Martino",
                        "slug": "Jacquelyn-Martino",
                        "structuredName": {
                            "firstName": "Jacquelyn",
                            "lastName": "Martino",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jacquelyn Martino"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 112
                            }
                        ],
                        "text": "The indexing and classification of video is an important problem that has been discussed in recent publications [1,3,4,5,12,17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15226248,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "859e1a1ebd139673c60eb503b849734e83df8347",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "We have developed a CO Ntent-based Image and Video Access System (CO NIVAS). The system consists of a collection of tools to help users retrieve still images and videos from databases by content. This system includes new algorithms for image and video retrieval in addition to algorithms that are adopted from the literature. The system can be used in many applications that require searching large collections of images and video clips such as digital video library, professional video editing, TV news retrieval and copyright protection applications,"
            },
            "slug": "CONIVAS:-content-based-image-and-video-access-Abdel-Mottaleb-Dimitrova",
            "title": {
                "fragments": [],
                "text": "CONIVAS: content-based image and video access system"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "The CO NIVAS system is a collection of tools to help users retrieve still images and videos from databases by content that includes new algorithms for image and video retrieval in addition to algorithms that are adopted from the literature."
            },
            "venue": {
                "fragments": [],
                "text": "MULTIMEDIA '96"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708785"
                        ],
                        "name": "J. Ohya",
                        "slug": "J.-Ohya",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Ohya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ohya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2019875"
                        ],
                        "name": "A. Shio",
                        "slug": "A.-Shio",
                        "structuredName": {
                            "firstName": "Akio",
                            "lastName": "Shio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Shio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49052113"
                        ],
                        "name": "S. Akamatsu",
                        "slug": "S.-Akamatsu",
                        "structuredName": {
                            "firstName": "Shigeru",
                            "lastName": "Akamatsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Akamatsu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[15], perform character extraction by local thresholding and detect character candidate regions by evaluating gray level difference between adjacent regions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1565945,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e94d1ff801fce49eea8d8aa51a477b130ca755de",
            "isKey": false,
            "numCitedBy": 263,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "An effective algorithm for character recognition in scene images is studied. Scene images are segmented into regions by an image segmentation method based on adaptive thresholding. Character candidate regions are detected by observing gray-level differences between adjacent regions. To ensure extraction of multisegment characters as well as single-segment characters, character pattern candidates are obtained by associating the detected regions according to their positions and gray levels. A character recognition process selects patterns with high similarities by calculating the similarities between character pattern candidates and the standard patterns in a dictionary and then comparing the similarities to the thresholds. A relaxational approach to determine character patterns updates the similarities by evaluating the interactions between categories of patterns, and finally character patterns and their recognition results are obtained. Highly promising experimental results have been obtained using the method on 100 images involving characters of different sizes and formats under uncontrolled lighting. >"
            },
            "slug": "Recognizing-Characters-in-Scene-Images-Ohya-Shio",
            "title": {
                "fragments": [],
                "text": "Recognizing Characters in Scene Images"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "An effective algorithm for character recognition in scene images is studied and highly promising experimental results have been obtained using the method on 100 images involving characters of different sizes and formats under uncontrolled lighting."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713407"
                        ],
                        "name": "S. Pfeiffer",
                        "slug": "S.-Pfeiffer",
                        "structuredName": {
                            "firstName": "Silvia",
                            "lastName": "Pfeiffer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Pfeiffer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144739319"
                        ],
                        "name": "R. Lienhart",
                        "slug": "R.-Lienhart",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Lienhart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lienhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2061646861"
                        ],
                        "name": "Stephan Fischer",
                        "slug": "Stephan-Fischer",
                        "structuredName": {
                            "firstName": "Stephan",
                            "lastName": "Fischer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephan Fischer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750165"
                        ],
                        "name": "W. Effelsberg",
                        "slug": "W.-Effelsberg",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Effelsberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Effelsberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 112
                            }
                        ],
                        "text": "The indexing and classification of video is an important problem that has been discussed in recent publications [1,3,4,5,12,17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 43383351,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "01f7baa4bfc99e0b6805a7e10300e96ac1839cf3",
            "isKey": false,
            "numCitedBy": 191,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Large video on demand databases consisting of thousands of digital movies are not easy to handle: the user must have an attractive means to retrieve his movie of choice. For analog video, movie trailers are produced to allow a quick preview and perhaps stimulate possible buyers. This paper presents techniques to automatically produce such movie abstracts of digtial videos. We define a video abstract to be a sequence of still or moving images presenting the content of a video in such a way that the resprective target groupis rapidly provided with concise information about the content while the essential message of the original is preserved. We therefore mainly distinguish video abstracts consisting of a collection of salient still images and video abstracts consisting of a collection of scenes (sequences of images) which are therefore a video themselves. Still-images abstracting systems have been reported often in the literature. We propose a moving-images abstracting system, called VAbstract, and explain its concept, algorithmic realization and advantages. The paper also describes a series of abstracting experiments in which we compared our automatically produced abstracts to manually produced trailers of TV series."
            },
            "slug": "Abstracting-Digital-Movies-Automatically-Pfeiffer-Lienhart",
            "title": {
                "fragments": [],
                "text": "Abstracting Digital Movies Automatically"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A moving-images abstracting system, called VAbstract, is proposed, and its concept, algorithmic realization and advantages are explained, and a series of abstracting experiments are described in which the automatically produced abstracts are compared to manually produced trailers of TV series."
            },
            "venue": {
                "fragments": [],
                "text": "J. Vis. Commun. Image Represent."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745366"
                        ],
                        "name": "Doug Beeferman",
                        "slug": "Doug-Beeferman",
                        "structuredName": {
                            "firstName": "Doug",
                            "lastName": "Beeferman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Doug Beeferman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710580"
                        ],
                        "name": "A. Berger",
                        "slug": "A.-Berger",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Berger",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8620787,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "119051c3344c6863daed8a26e139fd136e436c18",
            "isKey": false,
            "numCitedBy": 143,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces a new statistical approach to partitioning text automatically into coherent segments. Our approach enlists both short-range and long-range language models to help it sniff out likely sites of topic changes in text. To aid its search, the system consults a set of simple lexical hints it has learned to associate with the presence of boundaries through inspection of a large corpus of annotated data. We also propose a new probabilistically mot ivated error metric for use by the natural language processing and information retrieval communities, intended to supersede precision and recall for appraising segmentation algorithms. Qualitative assessment of our algorithm as well as evaluation using this new metric demonstrate the effectiveness of our approach in two very different domains, Wall Street Journal articles and the T D T Corpus, a collection of newswire articles and broadcast news transcripts. 1 I n t r o d u c t i o n The task we address in this paper might seem on the face of it rather elementary: identify where one region of text ends and another begins. This work was motivated by the observations that such a seemingly simple problem can actually prove quite difficult to automate, and that a tool for partitioning a stream of undifferentiated text (or multimedia) into coherent regions would be of great benefit to a number of existing applications. The task itself is ill-defined: what exactly is meant by a \"region\" of text? We confront this issue by *Research supported in part by NSF grant IRI9314969, DARPA AASERT award DAAH04-95-1-0475, and the ATR Interpreting Telecommunications Research Laboratories. adopting an empirical definition of segment. At our disposal is a collection of online data (38 million words of Wall Street Journal archives and another 150 million words from selected news broadcasts) annotated with the boundaries between regions-articles or news reports, respectively. Given this input, the task of constructing a segmenter may be cast as a problem in machine learning: glean from the data a set of hints about where boundaries occur, and use these hints to inform a decision on where to place breaks in unsegmented data. A general-purpose tool for partitioning expository text or multimedia data into coherent regions would have a number of immediate practical uses. In fact, this research was inspired by a problem in information retrieval: given a large unpartit ioned collection of expository text and a user's query, return a collection of coherent segments matching the query. Lacking a segmenting tool, an II:t application may be able to locate positions in its database which are strong matches with the user's query, but be unable to determine how much of the surrounding data to provide to the user. This can manifest itself in quite unfortunate ways. For example, a video-on-demand application (such as the one described in (Christel et al., 1995)) responding to a query about a recent news event may provide the user with a news clip related to the event, followed or preceded by part of an unrelated story or even a commercial. Document summarization is another fertile area for an automatic segmenter. Summarization tools often work by breaking the input into \"topics\" and then summarizing each topic independently. A segmentation tool has obvious applications to the first of these tasks. The output of a segmenter could also serve as input to various language-modeling tools. For instance, one could envision segmenting a corpus, classifying the segments by topic, and then constructing topic-dependent language models from the generated classes. The paper will proceed as follows. In Section 2 we"
            },
            "slug": "Text-Segmentation-Using-Exponential-Models-Beeferman-Berger",
            "title": {
                "fragments": [],
                "text": "Text Segmentation Using Exponential Models"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "This work enlists both short-range and long-range language models to help it sniff out likely sites of topic changes in text, and proposes a new probabilistically motivated error metric for use by the natural language processing and information retrieval communities."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34767796"
                        ],
                        "name": "M. Mandal",
                        "slug": "M.-Mandal",
                        "structuredName": {
                            "firstName": "Mrinal",
                            "lastName": "Mandal",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mandal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777011"
                        ],
                        "name": "T. Aboulnasr",
                        "slug": "T.-Aboulnasr",
                        "structuredName": {
                            "firstName": "Tyseer",
                            "lastName": "Aboulnasr",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Aboulnasr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743991"
                        ],
                        "name": "S. Panchanathan",
                        "slug": "S.-Panchanathan",
                        "structuredName": {
                            "firstName": "Sethuraman",
                            "lastName": "Panchanathan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Panchanathan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 7436820,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "26c1daac90b3a4e5ba1d70cdbfc980b9148ce5ba",
            "isKey": false,
            "numCitedBy": 215,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Histogram comparison is a popular technique for image and video indexing. The complexity of the technique can be reduced by representing the histogram by its moments. In this paper, we propose two techniques to improve the performance of the basic histogram/moment-based technique. First, we propose to use orthogonal Legendre moments for representing histograms. Since Legendre moments are orthogonal, they provide superior indexing performance compared to regular moments at a similar complexity. Secondly, we propose to compare the histograms of wavelet coefficients at different scales. The wavelet coefficients provide important directional information, and hence improve the performance of the basic histogram-based technique. The proposed scheme can be easily extended to color images and also be integrated into a wavelet-based image coder."
            },
            "slug": "Image-Indexing-Using-Moments-and-Wavelets-Mandal-Aboulnasr",
            "title": {
                "fragments": [],
                "text": "Image Indexing Using Moments and Wavelets"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Two techniques to improve the performance of the basic histogram/moment-based technique by using orthogonal Legendre moments for representing histograms and comparing the histograms of wavelet coefficients at different scales are proposed."
            },
            "venue": {
                "fragments": [],
                "text": "1996. Digest of Technical Papers., International Conference on Consumer Electronics"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144739319"
                        ],
                        "name": "R. Lienhart",
                        "slug": "R.-Lienhart",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Lienhart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lienhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784075"
                        ],
                        "name": "Christoph Kuhm\u00fcnch",
                        "slug": "Christoph-Kuhm\u00fcnch",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Kuhm\u00fcnch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christoph Kuhm\u00fcnch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750165"
                        ],
                        "name": "W. Effelsberg",
                        "slug": "W.-Effelsberg",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Effelsberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Effelsberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 19
                            }
                        ],
                        "text": "Lienhart and Suber [10] use a non-linear R\u2019G\u2019B\u2019 color system to reduce the number of colors."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 19
                            }
                        ],
                        "text": "R. Lienhart and F. Suber, \u201cAutomatic Text Recognition for Video Indexing,\u201d SPIE conference on image and video processing, Jan 1996."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 132
                            }
                        ],
                        "text": "Further, we would like to use text detection for video analysis purposes as opposed to its more traditional usage of video indexing [6,10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 54
                            }
                        ],
                        "text": "Also, commercial detection is an active research area [10,14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1782321,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c5e55f0915720fb262be6fcf5c664ba1d20cb78b",
            "isKey": true,
            "numCitedBy": 273,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "TV commercials are interesting in many respects: advertisers and psychologists are interested in their influence on human purchasing habits, while parents might be interested in shielding their children from their influence. In this paper, two methods for detecting and extracting commercials in digital videos are described. The first method is based on statistics of measurable features and enables the detection of commercial blocks within TV broadcasts. The second method performs detection and recognition of known commercials with high accuracy. Finally, we show how both approaches can be combined into a self-learning system. Our experimental results underline the practicality of the methods."
            },
            "slug": "On-the-detection-and-recognition-of-television-Lienhart-Kuhm\u00fcnch",
            "title": {
                "fragments": [],
                "text": "On the detection and recognition of television commercials"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Two methods for detecting and extracting commercials in digital videos are described and it is shown how both approaches can be combined into a self-learning system."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE International Conference on Multimedia Computing and Systems"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067184490"
                        ],
                        "name": "V.",
                        "slug": "V.",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "V.",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V."
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "65990038"
                        ],
                        "name": "Donghyeok An",
                        "slug": "Donghyeok-An",
                        "structuredName": {
                            "firstName": "Donghyeok",
                            "lastName": "An",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Donghyeok An"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2070444158"
                        ],
                        "name": "J.",
                        "slug": "J.",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "J.",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J."
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14814398,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "dd9b0c0e19c20afc51698626832d189a2388d86f",
            "isKey": false,
            "numCitedBy": 133,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "There has been recent interest in the segmentation of images by thresholding. We present several model based algorithms for threshold selection. We concentrate on the important two population univariate case when an image contains an object and background. However the methods are applicable to multispectral k-population images. We show how the main ideas behind two important nonspatial thresholding algorithms follow from classical discriminant analysis. We then give various new thresholding algorithms which make use of available IocaVspatial information. We consider one FLIR image and two artificial examples. A comparative study indicates that a new \" alternating mean thresholding and median filtering \" algorithm provides an acceptable method when the image is relatively highly contaminated. This method seems to depend less on initial values. I. INTRODUCTION We will consider the problem of image segmentation by thresh-olding. This problem and its importance were fully described recently in [l], [2], and [3]. Our main interest is with the k = 2 population case which is related to object identification. However, we shall also consider the extension to the general k-population problem. We first show that the threshold value in the segmentation algorithms of [ l ] and [2] can be deduced from the well-known statistical discriminant rule. Unlike [3], their rule is not spatial, i.e., it does not use contextual information. We give a spatial allocation rule based on the work of [4] and [ 5 ]. This is utilized to give a new thresholding algorithm. We also consider the iterated conditional modes (ICM) method [6]. Section I1 describes the nonspatial allocation rule, and shows how the allocation rules of [I] and [2] are particular cases. We summarize their iterative thresholding method in Section 111. In Section IV we give a spatial allocation rule which takes into account the spatial relationship between neighboring pixels and describe the modified iterative algorithms in Section V. In Section VI we describe ICM and its implementation. The methods are compared using synthetic images (following [3]) and one \" real \" FLIR (forward looking infrared) image. We conclude with a discussion of the methods in Section VIII. Our method follows naturally from a model introduced in Section 11. The method in [3] is not discussed here since it is not model orientated. Also our method applies to multispectral data, i.e., color images. In all the algorithms considered here, we do not require any prior"
            },
            "slug": "A-Spatial-Thresholding-Method-for-Image-V.-An",
            "title": {
                "fragments": [],
                "text": "A Spatial Thresholding Method for Image Segmentation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3209910"
                        ],
                        "name": "K. Mardia",
                        "slug": "K.-Mardia",
                        "structuredName": {
                            "firstName": "Kanti",
                            "lastName": "Mardia",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Mardia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054455054"
                        ],
                        "name": "T. J. Hainsworth",
                        "slug": "T.-J.-Hainsworth",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Hainsworth",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. J. Hainsworth"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "More algorithms that could be useful for character segmentation are described elsewhere [3, 13 ,16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 46473981,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "212cfa0b00cbbeb719b3876301ff258e9cf65d0e",
            "isKey": false,
            "numCitedBy": 110,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Several model-based algorithms for threshold selection are presented, concentrating on the two-population univariate case in which an image contains an object and background. It is shown how the main ideas behind two important nonspatial thresholding algorithms follow from classical discriminant analysis. Novel thresholding algorithms that make use of available local/spatial information are then given. It is found that an algorithm using alternating mean thresholding and median filtering provides an acceptable method when the image is relatively highly contaminated, and seems to depend less on initial values than other procedures. The methods are also applicable to multispectral k-population images. >"
            },
            "slug": "A-Spatial-Thresholding-Method-for-Image-Mardia-Hainsworth",
            "title": {
                "fragments": [],
                "text": "A Spatial Thresholding Method for Image Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is found that an algorithm using alternating mean thresholding and median filtering provides an acceptable method when the image is relatively highly contaminated, and seems to depend less on initial values than other procedures."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110380565"
                        ],
                        "name": "Arnulfo Perez",
                        "slug": "Arnulfo-Perez",
                        "structuredName": {
                            "firstName": "Arnulfo",
                            "lastName": "Perez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Arnulfo Perez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145061328"
                        ],
                        "name": "R. Gonz\u00e1lez",
                        "slug": "R.-Gonz\u00e1lez",
                        "structuredName": {
                            "firstName": "Rafael",
                            "lastName": "Gonz\u00e1lez",
                            "middleNames": [
                                "Corsino"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Gonz\u00e1lez"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 88
                            }
                        ],
                        "text": "More algorithms that could be useful for character segmentation are described elsewhere [3,13,16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 227763,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5180a52ababc4303a68dabd7d8bf04a6ddc1af69",
            "isKey": false,
            "numCitedBy": 116,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "A thresholding technique is developed for segmenting digital images with bimodal reflectance distributions under nonuniform illumination. The algorithm works in a raster format, thus making it an attractive segmentation tool in situations requiring fast data throughput. The theoretical base of the algorithm is a recursive Taylor expansion of a continuously varying threshold tracking function."
            },
            "slug": "An-Iterative-Thresholding-Algorithm-for-Image-Perez-Gonz\u00e1lez",
            "title": {
                "fragments": [],
                "text": "An Iterative Thresholding Algorithm for Image Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "A thresholding technique is developed for segmenting digital images with bimodal reflectance distributions under nonuniform illumination in a raster format, thus making it an attractive segmentation tool in situations requiring fast data throughput."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1772930"
                        ],
                        "name": "Michael G. Christel",
                        "slug": "Michael-G.-Christel",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Christel",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael G. Christel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35497738"
                        ],
                        "name": "M. Mauldin",
                        "slug": "M.-Mauldin",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Mauldin",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mauldin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055696387"
                        ],
                        "name": "Raj Reddy",
                        "slug": "Raj-Reddy",
                        "structuredName": {
                            "firstName": "Raj",
                            "lastName": "Reddy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Raj Reddy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753583"
                        ],
                        "name": "M. Sirbu",
                        "slug": "M.-Sirbu",
                        "structuredName": {
                            "firstName": "Marvin",
                            "lastName": "Sirbu",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sirbu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48817314"
                        ],
                        "name": "S. Stevens",
                        "slug": "S.-Stevens",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Stevens",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Stevens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679880"
                        ],
                        "name": "H. Wactlar",
                        "slug": "H.-Wactlar",
                        "structuredName": {
                            "firstName": "Howard",
                            "lastName": "Wactlar",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Wactlar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 112
                            }
                        ],
                        "text": "The indexing and classification of video is an important problem that has been discussed in recent publications [1,3,4,5,12,17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 30216392,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "891455e066098aaf609268857b556be20899967e",
            "isKey": false,
            "numCitedBy": 124,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The Informedia Digital Video Library Project is developing new technologies for creating full-content search and retrieval digital video libraries. Working in collaboration with WQED Pittsburgh, the project is creating a testbed that will enable K-12 students to access, explore, and retrieve science and mathematics materials from the digital video library. The library will initially contain 1,000 hours of video from the archives of project partners: WQED, Fairfax Co. VA Schools' Electronic BBC-produced video courses. (Industrial partners include Digital Equipment Corp., Bell Atlantic, Intel Corp., and Microsoft, Inc.) This library will be installed at Winchester Thurston School, an independent K-12 school in Pittsburgh."
            },
            "slug": "Informedia-Digital-Video-Library-Christel-Kanade",
            "title": {
                "fragments": [],
                "text": "Informedia Digital Video Library"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "The Informedia Digital Video Library Project is developing new technologies for creating full-content search and retrieval digital video libraries and is creating a testbed that will enable K-12 students to access, explore, and retrieve science and mathematics materials from the digital video library."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Content - based Image and Video Access System Text Segmentation Using Exponential Models \u201d"
            },
            "venue": {
                "fragments": [],
                "text": "Proc . of Empirical Methods in Natural Language Processing"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 88
                            }
                        ],
                        "text": "More algorithms that could be useful for character segmentation are described elsewhere [3,13,16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hainsworth, \u201cA Spatial Thresholding Method for Image Segmentation,"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transa. on Pattern Analysis and Machine Intelligence,"
            },
            "year": 1988
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 8,
            "methodology": 9
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 21,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Text-detection-for-video-analysis-Agnihotri-Dimitrova/b3b2c6b7bfaf9ab0d44c7103585fa0c81f60f3b9?sort=total-citations"
}