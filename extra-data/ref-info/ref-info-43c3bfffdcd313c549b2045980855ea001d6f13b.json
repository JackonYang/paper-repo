{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2294095"
                        ],
                        "name": "M. Powell",
                        "slug": "M.-Powell",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Powell",
                            "middleNames": [
                                "J.",
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Powell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 57917027,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c514310fe4550f947f2aaaef88e56610daa5047a",
            "isKey": false,
            "numCitedBy": 478,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Many different procedures have been proposed for optimization calculations when first derivatives are not available. Further, several researchers have contributed to the subject, including some who wish to prove convergence theorems, and some who wish to make any reduction in the least calculated value of the objective function. There is not even a key idea that can be used as a foundation of a review, except for the problem itself, which is the adjustment of variables so that a function becomes least, where each value of the function is returned by a subroutine for each trial vector of variables. Therefore the paper is a collection of essays on particular strategies and algorithms, in order to consider the advantages, limitations and theory of several techniques. The subjects addressed are line search methods, the restriction of vectors of variables to discrete grids, the use of geometric simplices, conjugate direction procedures, trust region algorithms that form linear or quadratic approximations to the objective function, and simulated annealing. We study the main features of the methods themselves, instead of providing a catalogue of references to published work, because an understanding of these features may be very helpful to future research."
            },
            "slug": "Direct-search-algorithms-for-optimization-Powell",
            "title": {
                "fragments": [],
                "text": "Direct search algorithms for optimization calculations"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Line search methods, the restriction of vectors of variables to discrete grids, the use of geometric simplices, conjugate direction procedures, trust region algorithms that form linear or quadratic approximations to the objective function, and simulated annealing are addressed."
            },
            "venue": {
                "fragments": [],
                "text": "Acta Numerica"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685738"
                        ],
                        "name": "M. H. Wright",
                        "slug": "M.-H.-Wright",
                        "structuredName": {
                            "firstName": "Margaret",
                            "lastName": "Wright",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. H. Wright"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121970817,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f28d164d99e0f1b8ca237fa6e062aad17e96242",
            "isKey": false,
            "numCitedBy": 277,
            "numCiting": 77,
            "paperAbstract": {
                "fragments": [],
                "text": "Interior methods for optimization were widely used in the 1960s, primarily in the form of barrier methods. However, they were not seriously applied to linear programming because of the dominance of the simplex method. Barrier methods fell from favour during the 1970s for a variety of reasons, including their apparent inefficiency compared with the best available alternatives. In 1984, Karmarkar's announcement of a fast polynomial-time interior method for linear programming caused tremendous excitement in the field of optimization. A formal connection can be shown between his method and classical barrier methods, which have consequently undergone a renaissance in interest and popularity. Most papers published since 1984 have concentrated on issues of computational complexity in interior methods for linear programming. During the same period, implementations of interior methods have displayed great efficiency in solving many large linear programs of ever-increasing size. Interior methods have also been applied with notable success to nonlinear and combinatorial problems. This paper presents a self-contained survey of major themes in both classical material and recent developments related to the theory and practice of interior methods."
            },
            "slug": "Interior-methods-for-constrained-optimization-Wright",
            "title": {
                "fragments": [],
                "text": "Interior methods for constrained optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A self-contained survey of major themes in both classical material and recent developments related to the theory and practice of interior methods for linear programming is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Acta Numerica"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800149"
                        ],
                        "name": "J. Dennis",
                        "slug": "J.-Dennis",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Dennis",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dennis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46190125"
                        ],
                        "name": "R. Schnabel",
                        "slug": "R.-Schnabel",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schnabel",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schnabel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 20
                            }
                        ],
                        "text": "Dennis and Schnabel [93] survey trust-region methods as part of their overview of unconstrained optimization, providing pointers to many important developments in the literature."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 53416228,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "02c87d7506454c947c92cfb593ac3cab1b1b2ab3",
            "isKey": false,
            "numCitedBy": 100,
            "numCiting": 109,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : Finding the unconstrained minimizer of a function of more than one variable is an important problem with many practical applications, including data fitting, engineering design, and process control. In addition, techniques for solving unconstrained optimization problems form the basis for most methods for solving constrained optimization problems. This paper surveys the state of the art for solving unconstrained optimization problems and the closely related problem of solving systems of nonlinear equations. First the authors briefly give some mathematical background. Then they discuss Newton's method, the fundamental method underlying most approaches to these problems, as well as the inexact Newton method. The two main practical deficiencies of Newton's method, the need for analytic derivatives and the possible failure to converge to the solution from poor starting points, are the key issues in unconstrained optimization, and the extension of these techniques to solving large, sparse problems. Then this document discusses the main methods used to ensure convergence from poor starting points, line search methods and trust region methods. Also discusses are two rather different approaches to unconstrained optimization, the Nelder-Meade simplex method and conjugate direction methods."
            },
            "slug": "A-view-of-unconstrained-optimization-Dennis-Schnabel",
            "title": {
                "fragments": [],
                "text": "A view of unconstrained optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The main methods used to ensure convergence from poor starting points, line search methods and trust region methods are discussed, and two rather different approaches to unconstrained optimization, the Nelder-Meade simplex method and conjugate direction methods."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3175188"
                        ],
                        "name": "R. Sundaram",
                        "slug": "R.-Sundaram",
                        "structuredName": {
                            "firstName": "Rangarajan",
                            "lastName": "Sundaram",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sundaram"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 221925755,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "f8907802d8f749af8d3a31435645c98646e77f59",
            "isKey": false,
            "numCitedBy": 257,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This book, first published in 1996, introduces students to optimization theory and its use in economics and allied disciplines. The first of its three parts examines the existence of solutions to optimization problems in Rn, and how these solutions may be identified. The second part explores how solutions to optimization problems change with changes in the underlying parameters, and the last part provides an extensive description of the fundamental principles of finite- and infinite-horizon dynamic programming. Each chapter contains a number of detailed examples explaining both the theory and its applications for first-year master's and graduate students. 'Cookbook' procedures are accompanied by a discussion of when such methods are guaranteed to be successful, and, equally importantly, when they could fail. Each result in the main body of the text is also accompanied by a complete proof. A preliminary chapter and three appendices are designed to keep the book mathematically self-contained."
            },
            "slug": "A-First-Course-in-Optimization-Theory-Sundaram",
            "title": {
                "fragments": [],
                "text": "A First Course in Optimization Theory"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699285"
                        ],
                        "name": "N. Higham",
                        "slug": "N.-Higham",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Higham",
                            "middleNames": [
                                "John"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Higham"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 27183478,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "005585e6dda67b5e3340c7f15d358b2716093c34",
            "isKey": false,
            "numCitedBy": 3022,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nWhat is the most accurate way to sum floating point numbers? What are the advantages of IEEE arithmetic? How accurate is Gaussian elimination and what were the key breakthroughs in the development of error analysis for the method? The answers to these and many related questions are included here. \nThis book gives a thorough, up-to-date treatment of the behavior of numerical algorithms in finite precision arithmetic. It combines algorithmic derivations, perturbation theory, and rounding error analysis. Software practicalities are emphasized throughout, with particular reference to LAPACK and MATLAB. The best available error bounds, some of them new, are presented in a unified format with a minimum of jargon. Because of its central role in revealing problem sensitivity and providing error bounds, perturbation theory is treated in detail. \nHistorical perspective and insight are given, with particular reference to the fundamental work of Wilkinson and Turing, and the many quotations provide further information in an accessible format. \nThe book is unique in that algorithmic developments and motivations are given succinctly and implementation details minimized, so that attention can be concentrated on accuracy and stability results. Here, in one place and in a unified notation, is error analysis for most of the standard algorithms in matrix computations. Not since Wilkinson's Rounding Errors in Algebraic Processes (1963) and The Algebraic Eigenvalue Problem (1965) has any volume treated this subject in such depth. A number of topics are treated that are not usually covered in numerical analysis textbooks, including floating point summation, block LU factorization, condition number estimation, the Sylvester equation, powers of matrices, finite precision behavior of stationary iterative methods, Vandermonde systems, and fast matrix multiplication. \nAlthough not designed specifically as a textbook, this volume is a suitable reference for an advanced course, and could be used by instructors at all levels as a supplementary text from which to draw examples, historical perspective, statements of results, and exercises (many of which have never before appeared in textbooks). The book is designed to be a comprehensive reference and its bibliography contains more than 1100 references from the research literature. \nAudience \nSpecialists in numerical analysis as well as computational scientists and engineers concerned about the accuracy of their results will benefit from this book. Much of the book can be understood with only a basic grounding in numerical analysis and linear algebra. \nAbout the Author \nNicholas J. Higham is a Professor of Applied Mathematics at the University of Manchester, England. He is the author of more than 40 publications and is a member of the editorial boards of the SIAM Journal on Matrix Analysis and Applications and the IMA Journal of Numerical Analysis. His book Handbook of Writing for the Mathematical Sciences was published by SIAM in 1993."
            },
            "slug": "Accuracy-and-stability-of-numerical-algorithms-Higham",
            "title": {
                "fragments": [],
                "text": "Accuracy and stability of numerical algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This book gives a thorough, up-to-date treatment of the behavior of numerical algorithms in finite precision arithmetic by combining algorithmic derivations, perturbation theory, and rounding error analysis."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "73484911"
                        ],
                        "name": "A. Ben-Tal",
                        "slug": "A.-Ben-Tal",
                        "structuredName": {
                            "firstName": "Aharon",
                            "lastName": "Ben-Tal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ben-Tal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145853268"
                        ],
                        "name": "A. Nemirovski",
                        "slug": "A.-Nemirovski",
                        "structuredName": {
                            "firstName": "Arkadi",
                            "lastName": "Nemirovski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Nemirovski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 59
                            }
                        ],
                        "text": "Robust optimization is discussed in Ben-Tal and Nemirovski [15]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 118626807,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "013545b0ab886f6a9b47c19737e1c80273fda6b5",
            "isKey": false,
            "numCitedBy": 2249,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "This is a book devoted to well-structured and thus efficiently solvable convex optimization problems, with emphasis on conic quadratic and semidefinite programming. The authors present the basic theory underlying these problems as well as their numerous applications in engineering, including synthesis of filters, Lyapunov stability analysis, and structural design. The authors also discuss the complexity issues and provide an overview of the basic theory of state-of-the-art polynomial time interior point methods for linear, conic quadratic, and semidefinite programming. The book's focus on well-structured convex problems in conic form allows for unified theoretical and algorithmical treatment of a wide spectrum of important optimization problems arising in applications."
            },
            "slug": "Lectures-on-modern-convex-optimization-analysis,-Ben-Tal-Nemirovski",
            "title": {
                "fragments": [],
                "text": "Lectures on modern convex optimization - analysis, algorithms, and engineering applications"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "The authors present the basic theory of state-of-the-art polynomial time interior point methods for linear, conic quadratic, and semidefinite programming as well as their numerous applications in engineering."
            },
            "venue": {
                "fragments": [],
                "text": "MPS-SIAM series on optimization"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2784955"
                        ],
                        "name": "J. Nocedal",
                        "slug": "J.-Nocedal",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Nocedal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nocedal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12443837,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "45ac05d71c59a250aa445798421983f18cf291fd",
            "isKey": false,
            "numCitedBy": 352,
            "numCiting": 212,
            "paperAbstract": {
                "fragments": [],
                "text": "A few months ago, while preparing a lecture to an audience that included engineers and numerical analysts, I asked myself the question: from the point of view of a user of nonlinear optimization routines, how interesting and practical is the body of theoretical analysis developed in this field? To make the question a bit more precise, I decided to select the best optimization methods known to date \u2013 those methods that deserve to be in a subroutine library \u2013 and for each method ask: what do we know about the behaviour of this method, as implemented in practice? To make my task more tractable, I decided to consider only algorithms for unconstrained optimization."
            },
            "slug": "Theory-of-algorithms-for-unconstrained-optimization-Nocedal",
            "title": {
                "fragments": [],
                "text": "Theory of algorithms for unconstrained optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "To make the task more tractable, I decided to consider only algorithms for unconstrained optimization, and select the best optimization methods known to date \u2013 those methods that deserve to be in a subroutine library."
            },
            "venue": {
                "fragments": [],
                "text": "Acta Numerica"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2484420"
                        ],
                        "name": "B. M. Averick",
                        "slug": "B.-M.-Averick",
                        "structuredName": {
                            "firstName": "Brett",
                            "lastName": "Averick",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. M. Averick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144805121"
                        ],
                        "name": "R. Carter",
                        "slug": "R.-Carter",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Carter",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Carter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143969518"
                        ],
                        "name": "G. Xue",
                        "slug": "G.-Xue",
                        "structuredName": {
                            "firstName": "Guoliang",
                            "lastName": "Xue",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Xue"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2062748962"
                        ],
                        "name": "J. J. Mor\u00e9",
                        "slug": "J.-J.-Mor\u00e9",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Mor\u00e9",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. J. Mor\u00e9"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 60780962,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "108c35a4d2187159bf29024fb5e466fbac9408f2",
            "isKey": false,
            "numCitedBy": 189,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Optimization software has often been developed without any specific application in mind. This generic approach has worked well in many cases, but as we seek the solution of larger and more complex optimization problems on high-performance computers, the development of optimization software should take into account specific optimization problems that arise in a wide range of applications. This observation was the motivation for the development of the MINPACK-2 test problem collection. Each of the problems in this collection comes from a real application and is representative of other commonly encountered problems. There are problems from such diverse fields as fluid dynamics, medicine, elasticity, combustion, molecular conformation, nondestructive testing, chemical kinetics, lubrication, and superconductivity."
            },
            "slug": "The-MINPACK-2-test-problem-collection-Averick-Carter",
            "title": {
                "fragments": [],
                "text": "The MINPACK-2 test problem collection"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The motivation for the development of the MINPACK-2 test problem collection was the desire to take into account specific optimization problems that arise in a wide range of applications."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16729445"
                        ],
                        "name": "R. Courant",
                        "slug": "R.-Courant",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Courant",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Courant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16547277,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "efa32848ae69aba45d37e02b872272b87e36d020",
            "isKey": false,
            "numCitedBy": 1585,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "As Henri Poincare once remarked, \"solution of a mathematical problem\" is a phrase of indefinite meaning. Pure mathematicians sometimes are satisfied with showing that the non-existence of a solution implies a logical contradiction, while engineers might consider a numerical result as the only reasonable goal. Such one sided views seem to reflect human limitations rather than objective values. In itself mathematics is an indivisible organism uniting theoretical contemplation and active application. This address will deal with a topic in which such a synthesis of theoretical and applied mathematics has become particularly convincing. Since Gauss and W. Thompson, the equivalence between boundary value problems of partial differential equations on the one hand and problems of the calculus of variations on the other hand has been a central point in analysis. At first, the theoretical interest in existence proofs dominated and only much later were practical applications envisaged by two physicists, Lord Rayleigh and Walther Ritz ; they independently conceived the idea of utilizing this equivalence for numerical calculation of the solutions, by substituting for the variational problems simpler approximating extremum problems in which but a finite number of parameters need be determined. Rayleigh, in his classical work\u2014Theory of sound\u2014and in other publications, was the first to use such a procedure. But only the spectacular success of Walther Ritz and its tragic circumstances caught the general interest. In two publications of 1908 and 1909 [39], Ritz, conscious of his imminent death from consumption, gave a masterly account of the theory, and at the same time applied his method to the calculation of the nodal lines of vibrating plates, a problem of classical physics that previously had not been satisfactorily treated. Thus methods emerged which could not fail to attract engineers and physicists; after all, the minimum principles of mechanics are more suggestive than the differential equations. Great successes in applications were soon followed by further progress in the understanding of the theoretical background, and such progress in turn must result in advantages for the applications."
            },
            "slug": "Variational-methods-for-the-solution-of-problems-of-Courant",
            "title": {
                "fragments": [],
                "text": "Variational methods for the solution of problems of equilibrium and vibrations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1943
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2877506"
                        ],
                        "name": "S. Nash",
                        "slug": "S.-Nash",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Nash",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nash"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 42
                            }
                        ],
                        "text": "Freely available packages include TN/TNBC [220] and TNPACK [275]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 121393558,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e027b578a08a05d3542ad0921240f84a54c3cc29",
            "isKey": false,
            "numCitedBy": 319,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper discusses the use of the linear conjugate-gradient method (developed via the Lanczos method) in the solution of large-scale unconstrained minimization problems. At each iteration of a Newton-type method, the direction of search is defined as the solution of a quadratic subproblem. When the number of variables is very large, this subproblem may be solved using the linear conjugate-gradient method of Hestenes and Stiefel. We show how the equivalent Lanczos characterization of the linear conjugate-gradient method may be exploited to define a modified Newton method which can be applied to problems that do not necessarily have positive-definite Hessian matrices at all points of the region of interest. This derivation also makes it possible to compute a negative-curvature direction at a stationary point.The idea of a truncated Newton method is to perform only a limited number of iterations of the quadratic subproblem. This effectively gives a search direction that interpolates between the steepest-de..."
            },
            "slug": "Newton-Type-Minimization-via-the-Lanczos-Method-Nash",
            "title": {
                "fragments": [],
                "text": "Newton-Type Minimization via the Lanczos Method"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2403331"
                        ],
                        "name": "Ingrid Bongartz",
                        "slug": "Ingrid-Bongartz",
                        "structuredName": {
                            "firstName": "Ingrid",
                            "lastName": "Bongartz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ingrid Bongartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723559"
                        ],
                        "name": "A. Conn",
                        "slug": "A.-Conn",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Conn",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Conn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144874680"
                        ],
                        "name": "N. Gould",
                        "slug": "N.-Gould",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Gould",
                            "middleNames": [
                                "I.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Gould"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145758858"
                        ],
                        "name": "P. Toint",
                        "slug": "P.-Toint",
                        "structuredName": {
                            "firstName": "Philippe",
                            "lastName": "Toint",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Toint"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 53
                            }
                        ],
                        "text": "The test problems are taken from the CUTE collection [35], the number of variables is indicated by n, and the termination criterion \u2016\u2207 fk\u2016 \u2264 10\u22125 is used."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 36228328,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e11ad55398210a70777467bd8055aeb81e5eb625",
            "isKey": false,
            "numCitedBy": 833,
            "numCiting": 92,
            "paperAbstract": {
                "fragments": [],
                "text": "The purpose of this article is to discuss the scope and functionality of a versatile environment for testing small- and large-scale nonlinear optimization algorithms. Although many of these facilities were originally produced by the authors in conjunction with the software package LANCELOT, we believe that they will be useful in their own right and should be available to researchers for their development of optimization software. The tools can be obtained by anonymous ftp from a number of sources and may, in many cases, be installed automatically. The scope of a major collection of test problems written in the standard input format (SIF) used by the LANCELOT software package is described. Recognizing that most software was not written with the SIF in mind, we provide tools to assist in building an interface between this input format and other optimization packages. These tools provide a link between the SIF and a number of existing packages, including MINOS and OSL. Additionally, as each problem includes a specific classification that is designed to be useful in identifying particular classes of problems, facilities are provided to build and manage a database of this information. There is a Unix and C shell bias to many of the descriptions in the article, since, for the sake of simplicity, we do not illustrate everything in its fullest generality. We trust that the majority of potential users are sufficiently familiar with Unix that these examples will not lead to undue confusion."
            },
            "slug": "CUTE:-constrained-and-unconstrained-testing-Bongartz-Conn",
            "title": {
                "fragments": [],
                "text": "CUTE: constrained and unconstrained testing environment"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "The scope and functionality of a versatile environment for testing small- and large-scale nonlinear optimization algorithms, and tools to assist in building an interface between this input format and other optimization packages are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "TOMS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2587310"
                        ],
                        "name": "R. Rockafellar",
                        "slug": "R.-Rockafellar",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Rockafellar",
                            "middleNames": [
                                "Tyrrell"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rockafellar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14342885,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b55eee3cc0831f4400e04e1d4384197de1cfa636",
            "isKey": false,
            "numCitedBy": 506,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": "Lagrange multipliers used to be viewed as auxiliary variables introduced in a problem of constrained minimization in order to write first-order optimality conditions formally as a system of equations. Modern applications, with their emphasis on numerical methods and more complicated side conditions than equations, have demanded deeper understanding of the concept and how it fits into a larger theoretical picture. A major line of research has been the nonsmooth geometry of one-sided tangent and normal vectors to the set of points satisfying the given constraints. Another has been the game-theoretic role of multiplier vectors as solutions to a dual problem. Interpretations as generalized derivatives of the optimal value with respect to problem parameters have also been explored. Lagrange multipliers are now being seen as arising from a general rule for the subdifferentiation of a nonsmooth objective function which allows black-and-white constraints to be replaced by penalty expressions. This paper traces su..."
            },
            "slug": "Lagrange-Multipliers-and-Optimality-Rockafellar",
            "title": {
                "fragments": [],
                "text": "Lagrange Multipliers and Optimality"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "Lagrange multipliers are now being seen as arising from a general rule for the subdifferentiation of a nonsmooth objective function which allows black-and-white constraints to be replaced by penalty expressions."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM Rev."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2580666"
                        ],
                        "name": "N. Maratos",
                        "slug": "N.-Maratos",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Maratos",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Maratos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 30507117,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "1c44110c5ef90e868907c6820699ed58108f7cf7",
            "isKey": false,
            "numCitedBy": 258,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "In this thesis first and second order algorithms are proposed for solving equality constrained finite dimensional minimization problems and optimal control problems with terminal equality constraints. The algorithms are based on the exact penalty function approach, i. e. they replace the original constrained optimization problem by the unconstrained problem of minimizing a non-differentiable exact penalty function (which depends on the penalty parameter 0. For a sufficiently large but finite value of the parameter c the two problems are locally equivalent. Two difficulties arise with this approach: (i) choosing a suitable value for c, and (ii) minimizing the non-differentiable exact penalty function. A procedure for automatically increasing c is used to cope with the first difficulty. At-each iteration a test is performed on the current value of c; if necessary, c is increased in order to satisfy the test. In normal operation (i. e. when the algorithms construct compact sequences), c is increased at a finite number of iterations only, and remains constant eventually. To deal with the second difficulty, the search direction*is defined by solving an auxiliary problem which is a (first or second order) local approximation to the original (constrained) problem. The step-length is determined by an one-dimensional approximate minimization of the exact penalty function (using the Armijo rule). Algorithms 1,2 and 3 solve the finite dimensional problem. Algorithm 1 makes use of a first order search direction which occurs from solving a linearized version of the original problem. Global convergence is established and computational results for a number of test problems are given. Algorithm 2 is a modified Newton method, Global convergence is established, the algorithm's rate of convergence is examined and some numerical results are given. Algorithm 3 is an arc method; at-each iteration a quadratic arc is generated which (for unit step-length) satisfies the constraints to second order. Global convergence and a quadratic convergence rate are established; numerical results are also given. Algorithm 4 is a first order method (an infinite dimensional extension of Algorithm 1) for the control optimization problem. Convergence is proven in L. O."
            },
            "slug": "Exact-penalty-function-algorithms-for-finite-and-Maratos",
            "title": {
                "fragments": [],
                "text": "Exact penalty function algorithms for finite dimensional and control optimization problems"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "In this thesis first and second order algorithms are proposed for solving equality constrained finite dimensional minimization problems and optimal control problems with terminal equality constraints using the exact penalty function approach."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102235031"
                        ],
                        "name": "Kenneth Levenberg",
                        "slug": "Kenneth-Levenberg",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Levenberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth Levenberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 124308544,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b1afd5740cdb03295f14e6c993b8d36844956dce",
            "isKey": false,
            "numCitedBy": 10367,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The standard method for solving least squares problems which lead to non-linear normal equations depends upon a reduction of the residuals to linear form by first order Taylor approximations taken about an initial or trial solution for the parameters.2 If the usual least squares procedure, performed with these linear approximations, yields new values for the parameters which are not sufficiently close to the initial values, the neglect of second and higher order terms may invalidate the process, and may actually give rise to a larger value of the sum of the squares of the residuals than that corresponding to the initial solution. This failure of the standard method to improve the initial solution has received some notice in statistical applications of least squares3 and has been encountered rather frequently in connection with certain engineering applications involving the approximate representation of one function by another. The purpose of this article is to show how the problem may be solved by an extension of the standard method which insures improvement of the initial solution.4 The process can also be used for solving non-linear simultaneous equations, in which case it may be considered an extension of Newton's method. Let the function to be approximated be h{x, y, z, \u2022 \u2022 \u2022 ), and let the approximating function be H{oc, y, z, \u2022 \u2022 \u25a0 ; a, j3, y, \u25a0 \u2022 \u25a0 ), where a, /3, 7, \u2022 \u25a0 \u25a0 are the unknown parameters. Then the residuals at the points, yit zit \u2022 \u2022 \u2022 ), i = 1, 2, \u25a0 \u2022 \u2022 , n, are"
            },
            "slug": "A-METHOD-FOR-THE-SOLUTION-OF-CERTAIN-NON-\u2013-LINEAR-Levenberg",
            "title": {
                "fragments": [],
                "text": "A METHOD FOR THE SOLUTION OF CERTAIN NON \u2013 LINEAR PROBLEMS IN LEAST SQUARES"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1944
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108505561"
                        ],
                        "name": "Yin Zhang",
                        "slug": "Yin-Zhang",
                        "structuredName": {
                            "firstName": "Yin",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yin Zhang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14580420,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9b97cb57282f8f80f1383a314f95251f4e4502a1",
            "isKey": false,
            "numCitedBy": 363,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we describe our implementation of a primal-dual infeasible-interior-point algorithm for large-scale linear programming under the MATLAB environment. The resulting software is called LIPSOL \u2014 Linear-programming Interior-Point SOLvers. LIPSOL is designed to take the advantages of MATLAB's sparse-matrix functions and external interface facilities, and of existing Fortran sparse Cholesky codes. Under the MATLAB environment, LIPSOL inherits a high degree of simplicity and versatility in comparison to its counterparts in Fortran or C language. More importantly, our extensive computational results demonstrate that LIPSOL also attains an impressive performance comparable with that of efficient Fortran or C codes in solving large-scale problems. In addition, we discuss in detail a technique for overcoming numerical instability in Cholesky factorization at the end-stage of iterations in interior-point algorithms."
            },
            "slug": "Solving-large-scale-linear-programs-by-methods-the-Zhang",
            "title": {
                "fragments": [],
                "text": "Solving large-scale linear programs by interior-point methods under the Matlab \u2217 Environment \u2020"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "This paper describes the implementation of a primal-dual infeasible-interior-point algorithm for large-scale linear programming under the MATLAB environment, and discusses in detail a technique for overcoming numerical instability in Cholesky factorization at the end-stage of iterations in interior-point algorithms."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35074400"
                        ],
                        "name": "Jean Charles Gilbert",
                        "slug": "Jean-Charles-Gilbert",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Gilbert",
                            "middleNames": [
                                "Charles"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean Charles Gilbert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8773884"
                        ],
                        "name": "C. Lemar\u00e9chal",
                        "slug": "C.-Lemar\u00e9chal",
                        "structuredName": {
                            "firstName": "Claude",
                            "lastName": "Lemar\u00e9chal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lemar\u00e9chal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 69
                            }
                        ],
                        "text": "Limited-memory BFGS methods are implemented in LBFGS [194] and M1QN3 [122]; see Gill and Leonard [125] for a variant that requires less storage and appears to be quite efficient."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 113
                            }
                        ],
                        "text": "For further discussion on the L-BFGS method see Nocedal [228], Liu and Nocedal [194], and Gilbert and Lemar\u00e9chal [122]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2893408,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "6d78e5111b35dac99bd2cddddf6e8d9253e49d25",
            "isKey": false,
            "numCitedBy": 749,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes some numerical experiments with variable-storage quasi-Newton methods for the optimization of some large-scale models (coming from fluid mechanics and molecular biology). In addition to assessing these kinds of methods in real-life situations, we compare an algorithm of A. Buckley with a proposal by J. Nocedal. The latter seems generally superior, provided that careful attention is given to some nontrivial implementation aspects, which concern the general question of properly initializing a quasi-Newton matrix. In this context, we find it appropriate to use a diagonal matrix, generated by an update of the identity matrix, so as to fit the Rayleigh ellipsoid of the local Hessian in the direction of the change in the gradient.Also, a variational derivation of some rank one and rank two updates in Hilbert spaces is given."
            },
            "slug": "Some-numerical-experiments-with-variable-storage-Gilbert-Lemar\u00e9chal",
            "title": {
                "fragments": [],
                "text": "Some numerical experiments with variable-storage quasi-Newton algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is found appropriate to use a diagonal matrix, generated by an update of the identity matrix, so as to fit the Rayleigh ellipsoid of the local Hessian in the direction of the change in the gradient."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35282765"
                        ],
                        "name": "Sanjay Mehrotra",
                        "slug": "Sanjay-Mehrotra",
                        "structuredName": {
                            "firstName": "Sanjay",
                            "lastName": "Mehrotra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sanjay Mehrotra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7845529,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "73cf86998a77c3dfd42b7ab574239879faf96f6f",
            "isKey": false,
            "numCitedBy": 1610,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper gives an approach to implementing a second-order primal-dual interior point method. It uses a Taylor polynomial of second order to approximate a primal-dual trajectory. The computations for the second derivative are combined with the computations for the centering direction. Computations in this approach do not require that primal and dual solutions be feasible. Expressions are given to compute all the higher-order derivatives of the trajectory of interest. The implementation ensures that a suitable potential function is reduced by a constant amount at each iteration.There are several salient features of this approach. An adaptive heuristic for estimating the centering parameter is given. The approach used to compute the step length is also adaptive. A new practical approach to compute the starting point is given. This approach treats primal and dual problems symmetrically.Computational results on a subset of problems available from netlib are given. On mutually tested problems the results show..."
            },
            "slug": "On-the-Implementation-of-a-Primal-Dual-Interior-Mehrotra",
            "title": {
                "fragments": [],
                "text": "On the Implementation of a Primal-Dual Interior Point Method"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A Taylor polynomial of second order is used to approximate a primal-dual trajectory and an adaptive heuristic for estimating the centering parameter is given, which treats primal and dual problems symmetrically."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Optim."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "73748106"
                        ],
                        "name": "K. Tanabe",
                        "slug": "K.-Tanabe",
                        "structuredName": {
                            "firstName": "Kunio",
                            "lastName": "Tanabe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Tanabe"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 115690624,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "04624a8136b5ff265319e5c713091be230433b0d",
            "isKey": false,
            "numCitedBy": 104,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The purpose of this paper is to introduce a generic class of algorithms for solving a system of nonlinear equations, Linear Programming problems, Quadratic Programming problems, Nonlinear Programming problems and general complementarity problems. The algorithms were obtained by modifying the standard Newton-Raphson method applied to a system of nonlinear equations in the complementarity conditions so that it is biased towards \u2018center curve\u2019 passing through the solutions. The search direction of the methods is a positive combination of the Newton direction and a \u2018centering\u2019 direction which is also given by applying the Newton method to a projected system of the complementarity equations. These two directions guide the generated sequence of the approximations towards the solution and the center variety respectively. A class of \u2018penalized norms\u2019 and \u2018guiding cones\u2019 is also introduced for choosing step lengths in bivariate search."
            },
            "slug": "Centered-newton-method-for-mathematical-programming-Tanabe",
            "title": {
                "fragments": [],
                "text": "Centered newton method for mathematical programming"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "A generic class of algorithms for solving a system of nonlinear equations, Linear Programming problems, Quadratic programming problems, Nonlinear Programming problems and general complementarity problems is introduced by modifying the standard Newton-Raphson method."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144874680"
                        ],
                        "name": "N. Gould",
                        "slug": "N.-Gould",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Gould",
                            "middleNames": [
                                "I.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Gould"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145758858"
                        ],
                        "name": "P. Toint",
                        "slug": "P.-Toint",
                        "structuredName": {
                            "firstName": "Philippe",
                            "lastName": "Toint",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Toint"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 55361150,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2274c08c95d69f156c66e110447a255a8c7db8d4",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-iterative-working-set-method-for-large-scale-Gould-Toint",
            "title": {
                "fragments": [],
                "text": "An iterative working-set method for large-scale nonconvex quadratic programming"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107603179"
                        ],
                        "name": "D. R. Jones",
                        "slug": "D.-R.-Jones",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Jones",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. R. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2563797"
                        ],
                        "name": "C. D. Perttunen",
                        "slug": "C.-D.-Perttunen",
                        "structuredName": {
                            "firstName": "Cary",
                            "lastName": "Perttunen",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. D. Perttunen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2424124"
                        ],
                        "name": "B. Stuckman",
                        "slug": "B.-Stuckman",
                        "structuredName": {
                            "firstName": "Bruce",
                            "lastName": "Stuckman",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Stuckman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 123674634,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d7808f2ad77de7b71e83a2e79d27f2e3e12be8d5",
            "isKey": false,
            "numCitedBy": 1838,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new algorithm for finding the global minimum of a multivariate function subject to simple bounds. The algorithm is a modification of the standard Lipschitzian approach that eliminates the need to specify a Lipschitz constant. This is done by carrying out simultaneous searches using all possible constants from zero to infinity. On nine standard test functions, the new algorithm converges in fewer function evaluations than most competing methods.The motivation for the new algorithm stems from a different way of looking at the Lipschitz constant. In particular, the Lipschitz constant is viewed as a weighting parameter that indicates how much emphasis to place on global versus local search. In standard Lipschitzian methods, this constant is usually large because it must equal or exceed the maximum rate of change of the objective function. As a result, these methods place a high emphasis on global search and exhibit slow convergence. In contrast, the new algorithm carries out simultaneous searches using all possible constants, and therefore operates at both the global and local level. Once the global part of the algorithm finds the basin of convergence of the optimum, the local part of the algorithm quickly and automatically exploits it. This accounts for the fast convergence of the new algorithm on the test functions."
            },
            "slug": "Lipschitzian-optimization-without-the-Lipschitz-Jones-Perttunen",
            "title": {
                "fragments": [],
                "text": "Lipschitzian optimization without the Lipschitz constant"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2294095"
                        ],
                        "name": "M. Powell",
                        "slug": "M.-Powell",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Powell",
                            "middleNames": [
                                "J.",
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Powell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117078247,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "bc679a91edb305eb3b09ead40003d9697cfcb5a6",
            "isKey": false,
            "numCitedBy": 315,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "CONVERGENCE-PROPERTIES-OF-A-CLASS-OF-MINIMIZATION-Powell",
            "title": {
                "fragments": [],
                "text": "CONVERGENCE PROPERTIES OF A CLASS OF MINIMIZATION ALGORITHMS"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723559"
                        ],
                        "name": "A. Conn",
                        "slug": "A.-Conn",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Conn",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Conn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2005127"
                        ],
                        "name": "K. Scheinberg",
                        "slug": "K.-Scheinberg",
                        "structuredName": {
                            "firstName": "Katya",
                            "lastName": "Scheinberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Scheinberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145758858"
                        ],
                        "name": "P. Toint",
                        "slug": "P.-Toint",
                        "structuredName": {
                            "firstName": "Philippe",
                            "lastName": "Toint",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Toint"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14075960,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "701891a9274cdd446621fe1ce5f0bf599881beb2",
            "isKey": false,
            "numCitedBy": 277,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an introduction to a new class of derivative free methods for unconstrained optimization. We start by discussing the motivation for such methods and why they are in high demand by practitioners. We then review the past developments in this field, before introducing the features that characterize the newer algorithms. In the context of a trust region framework, we focus on techniques that ensure a suitable \u201cgeometric quality\u201d of the considered models. We then outline the class of algorithms based on these techniques, as well as their respective merits. We finally conclude the paper with a discussion of open questions and perspectives."
            },
            "slug": "Recent-progress-in-unconstrained-nonlinear-without-Conn-Scheinberg",
            "title": {
                "fragments": [],
                "text": "Recent progress in unconstrained nonlinear optimization without derivatives"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "An introduction to a new class of derivative free methods for unconstrained optimization in the context of a trust region framework that focuses on techniques that ensure a suitable \u201cgeometric quality\u201d of the considered models."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703847"
                        ],
                        "name": "Boris Polyak",
                        "slug": "Boris-Polyak",
                        "structuredName": {
                            "firstName": "Boris",
                            "lastName": "Polyak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Boris Polyak"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122143037,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "69a77be57f8d51f8a6416a3db2693af1120ce6dc",
            "isKey": false,
            "numCitedBy": 752,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-conjugate-gradient-method-in-extremal-problems-Polyak",
            "title": {
                "fragments": [],
                "text": "The conjugate gradient method in extremal problems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1969
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52067790"
                        ],
                        "name": "N. Gould",
                        "slug": "N.-Gould",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Gould",
                            "middleNames": [
                                "I.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Gould"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14083163,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ec51f279a5b192ffaf1d610169602b4d1337e196",
            "isKey": false,
            "numCitedBy": 68,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "We are particularly concerned in solving (1.1) when n is large and the vectors a, and matrix H are sparse. We do not restrict H to being positive (semi-)definite and consequently are content with finding local solutions to (1.1). Of course, for many classes of problem, it is known a priori that any local solution is a global one. Our method is fundamentally related to that proposed by Fletcher (1971), but makes use of sparse matrix technology (in particular, linear programming basis handling techniques) to exploit the nature of the problem. In Section 2, we describe a general framework for our method. Linear algebraic issues are considered in Section 3 together with a description of how these issues relate to solving more specific quadratic programming problems of the form"
            },
            "slug": "An-Algorithm-for-Large-Scale-Quadratic-Programming-Gould",
            "title": {
                "fragments": [],
                "text": "An Algorithm for Large-Scale Quadratic Programming"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "The method is fundamentally related to that proposed by Fletcher (1971), but makes use of sparse matrix technology (in particular, linear programming basis handling techniques) to exploit the nature of the problem."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144731788"
                        ],
                        "name": "Stephen J. Wright",
                        "slug": "Stephen-J.-Wright",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Wright",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen J. Wright"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8172320,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f0c35717ae2b08a372e04a90ff20d86acdcc0996",
            "isKey": false,
            "numCitedBy": 145,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "The connections between optimization and control theory have been explored by many researchers and optimization algorithms have been applied with success to optimal control. The rapid pace of developments in model predictive control has given rise to a host of new problems to which optimization has yet to be applied. Concurrently, developments in optimization, and especially in interior-point methods, have produced a new set of algorithms that may be especially helpful in this context. In this paper, we reexamine the relatively simple problem of control of linear processes subject to quadratic objectives and general linear constraints. We show how new algorithms for quadratic programming can be applied efficiently to this problem. The approach extends to several more general problems in straightforward ways."
            },
            "slug": "Applying-new-optimization-algorithms-to-more-Wright",
            "title": {
                "fragments": [],
                "text": "Applying new optimization algorithms to more predictive control"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper reexamine the relatively simple problem of control of linear processes subject to quadratic objectives and general linear constraints and shows how new algorithms forquadratic programming can be applied efficiently to this problem."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2294095"
                        ],
                        "name": "M. Powell",
                        "slug": "M.-Powell",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Powell",
                            "middleNames": [
                                "J.",
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Powell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 206800411,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "be3fc6cc4520f32dbee045c1cf4071fb35aae8ee",
            "isKey": false,
            "numCitedBy": 449,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Lagrangian functions are the basis of many of the more successful methods for nonlinear constraints in optimization calculations. Sometimes they are used in conjunction with linear approximations to the constraints and sometimes penalty terms are included to allow the use of algorithms for unconstrained optimization. Much has been discovered about these techniques during the last eight years and this paper gives a view of the progress and understanding that has been achieved and its relevance to practical algorithms. A particular method is recommended that seems to be more powerful than the author believed to be possible at the beginning of 1976."
            },
            "slug": "Algorithms-for-nonlinear-constraints-that-use-Powell",
            "title": {
                "fragments": [],
                "text": "Algorithms for nonlinear constraints that use lagrangian functions"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A view of the progress and understanding that has been achieved during the last eight years about Lagrangian functions and its relevance to practical algorithms is given."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710850"
                        ],
                        "name": "C. Bischof",
                        "slug": "C.-Bischof",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Bischof",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bischof"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730833"
                        ],
                        "name": "A. Bouaricha",
                        "slug": "A.-Bouaricha",
                        "structuredName": {
                            "firstName": "Ali",
                            "lastName": "Bouaricha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bouaricha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2363532"
                        ],
                        "name": "P. Khademi",
                        "slug": "P.-Khademi",
                        "structuredName": {
                            "firstName": "Peyvand",
                            "lastName": "Khademi",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Khademi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144241870"
                        ],
                        "name": "J. J. Mor\u00e9",
                        "slug": "J.-J.-Mor\u00e9",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Mor\u00e9",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. J. Mor\u00e9"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 10333557,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d7e331050ff52cad635d26199341265f8dc446b1",
            "isKey": false,
            "numCitedBy": 55,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": "The accurate and efficient computation of gradients for partially separable functions is central to the solution of large-scale optimization problems, because these functions are ubiquitous in large-scale problems. We describe two approaches for computing gradients of partially separable functions via automatic differentiation. In our experiments we employ the ADIFOR (automatic differentiation of Fortran) tool and the SparsLinC (sparse linear combination) library. We use applications from the MINPACK-2 test problem collection to compare the numerical reliability and computational efficiency of these approaches with hand-coded derivatives and approximations based on differences of function values. Our conclusion is that automatic differentiation is the method of choice, providing code for the efficient computation of the gradient without the need for tedious hand-coding."
            },
            "slug": "Computing-Gradients-in-Large-Scale-Optimization-Bischof-Bouaricha",
            "title": {
                "fragments": [],
                "text": "Computing Gradients in Large-Scale Optimization Using Automatic Differentiation"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "This work describes two approaches for computing gradients of partially separable functions via automatic differentiation, providing code for the efficient computation of the gradient without the need for tedious hand-coding."
            },
            "venue": {
                "fragments": [],
                "text": "INFORMS J. Comput."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144733293"
                        ],
                        "name": "R. Fletcher",
                        "slug": "R.-Fletcher",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Fletcher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fletcher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2767643"
                        ],
                        "name": "E. S. D. L. Maza",
                        "slug": "E.-S.-D.-L.-Maza",
                        "structuredName": {
                            "firstName": "Eduardo",
                            "lastName": "Maza",
                            "middleNames": [
                                "S\u00e1inz",
                                "de",
                                "la"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. S. D. L. Maza"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 2397490,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "16f41f152bcf270a1428301a5967443ccad65a59",
            "isKey": false,
            "numCitedBy": 113,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Methods are considered for solving nonlinear programming problems using an exactl1 penalty function. LP-like subproblems incorporating a trust region constraint are solved successively both to estimate the active set and to provide a foundation for proving global convergence. In one particular method, second order information is represented by approximating the reduced Hessian matrix, and Coleman-Conn steps are taken. A criterion for accepting these steps is given which enables the superlinear convergence properties of the Coleman-Conn method to be retained whilst preserving global convergence and avoiding the Maratos effect. The methods generalize to solve a wide range of composite nonsmooth optimization problems and the theory is presented in this general setting. A range of numerical experiments on small test problems is described."
            },
            "slug": "Nonlinear-programming-and-nonsmooth-optimization-by-Fletcher-Maza",
            "title": {
                "fragments": [],
                "text": "Nonlinear programming and nonsmooth optimization by successive linear programming"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "Methods are considered for solving nonlinear programming problems using an exactl1 penalty function that generalize to solve a wide range of composite nonsmooth optimization problems and the theory is presented in this general setting."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696678"
                        ],
                        "name": "D. Wolpert",
                        "slug": "D.-Wolpert",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Wolpert",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Wolpert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2957256"
                        ],
                        "name": "W. Macready",
                        "slug": "W.-Macready",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Macready",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Macready"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5553697,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8315dff3d304baf47c025f4b33535b9d693350c1",
            "isKey": false,
            "numCitedBy": 9277,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "A framework is developed to explore the connection between effective optimization algorithms and the problems they are solving. A number of \"no free lunch\" (NFL) theorems are presented which establish that for any algorithm, any elevated performance over one class of problems is offset by performance over another class. These theorems result in a geometric interpretation of what it means for an algorithm to be well suited to an optimization problem. Applications of the NFL theorems to information-theoretic aspects of optimization and benchmark measures of performance are also presented. Other issues addressed include time-varying optimization problems and a priori \"head-to-head\" minimax distinctions between optimization algorithms, distinctions that result despite the NFL theorems' enforcing of a type of uniformity over all algorithms."
            },
            "slug": "No-free-lunch-theorems-for-optimization-Wolpert-Macready",
            "title": {
                "fragments": [],
                "text": "No free lunch theorems for optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "A framework is developed to explore the connection between effective optimization algorithms and the problems they are solving and a number of \"no free lunch\" (NFL) theorems are presented which establish that for any algorithm, any elevated performance over one class of problems is offset by performance over another class."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Evol. Comput."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2294095"
                        ],
                        "name": "M. Powell",
                        "slug": "M.-Powell",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Powell",
                            "middleNames": [
                                "J.",
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Powell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 119842532,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a1ffeeeeda0624930648a86be170394f537144be",
            "isKey": false,
            "numCitedBy": 183,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper reviews some of the most successful methods for unconstrained, constrained and nondifferentiable optimization calculations. Particular attention is given to the contribution that theoretical analysis has made to the development of algorithms. It seems that practical considerations provide the main new ideas, and that subsequent theoretical studies give improvements to algorithms, coherence to the subject, and better understanding."
            },
            "slug": "Convergence-properties-of-algorithms-for-nonlinear-Powell",
            "title": {
                "fragments": [],
                "text": "Convergence properties of algorithms for nonlinear optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This paper reviews some of the most successful methods for unconstrained, constrained and nondifferentiable optimization calculations and suggests that practical considerations provide the main new ideas, and that subsequent theoretical studies give improvements to algorithms, coherence to the subject, and better understanding."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144102674"
                        ],
                        "name": "C. Papadimitriou",
                        "slug": "C.-Papadimitriou",
                        "structuredName": {
                            "firstName": "Christos",
                            "lastName": "Papadimitriou",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Papadimitriou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2996331"
                        ],
                        "name": "K. Steiglitz",
                        "slug": "K.-Steiglitz",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Steiglitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Steiglitz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 136
                            }
                        ],
                        "text": "Discrete optimization problems are not addressed directly in this book; we refer the reader to the texts by Papadimitriou and Steiglitz [235], Nemhauser and Wolsey [224], Cook et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 218,
                                "start": 213
                            }
                        ],
                        "text": "Network and integer optimization are described in some excellent texts: for instance, Ahuja, Magnanti, and Orlin [1] in the case of network optimization and Nemhauser and Wolsey [224], Papadimitriou and Steiglitz [235], and Wolsey [312] in the case of integer programming."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 29222216,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1565c128b727550a73bc4e105a3353420112485d",
            "isKey": false,
            "numCitedBy": 7242,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "This clearly written , mathematically rigorous text includes a novel algorithmic exposition of the simplex method and also discusses the Soviet ellipsoid algorithm for linear programming; efficient algorithms for network flow, matching, spanning trees, and matroids; the theory of NP-complete problems; approximation algorithms, local search heuristics for NPcomplete problems, more. All chapters are supplemented by thoughtprovoking problems. A useful work for graduate-level students with backgrounds in computer science, operations research, and electrical engineering. Mathematicians wishing a self-contained introduction need look no further.\u2014American Mathematical Monthly. 1982 ed."
            },
            "slug": "Combinatorial-Optimization:-Algorithms-and-Papadimitriou-Steiglitz",
            "title": {
                "fragments": [],
                "text": "Combinatorial Optimization: Algorithms and Complexity"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This clearly written, mathematically rigorous text includes a novel algorithmic exposition of the simplex method and also discusses the Soviet ellipsoid algorithm for linear programming; efficient algorithms for network flow, matching, spanning trees, and matroids; the theory of NP-complete problems; approximation algorithms, local search heuristics for NPcomplete problems, more."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48306096"
                        ],
                        "name": "M. Hestenes",
                        "slug": "M.-Hestenes",
                        "structuredName": {
                            "firstName": "Magnus",
                            "lastName": "Hestenes",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hestenes"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121584579,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bfb4b1b8d4f54d16f6496dffc118a434fc2c1069",
            "isKey": false,
            "numCitedBy": 2029,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The main purpose of this paper is to suggest a method for finding the minimum of a functionf(x) subject to the constraintg(x)=0. The method consists of replacingf byF=f+\u03bbg+1/2cg2, wherec is a suitably large constant, and computing the appropriate value of the Lagrange multiplier. Only the simplest algorithm is presented. The remaining part of the paper is devoted to a survey of known methods for finding unconstrained minima, with special emphasis on the various gradient techniques that are available. This includes Newton's method and the method of conjugate gradients."
            },
            "slug": "Multiplier-and-gradient-methods-Hestenes",
            "title": {
                "fragments": [],
                "text": "Multiplier and gradient methods"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "The main purpose of this paper is to suggest a method for finding the minimum of a functionf(x) subject to the constraintg(x)=0, which consists of replacingf byF=f+\u03bbg+1/2cg2, and computing the appropriate value of the Lagrange multiplier."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1969
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2784955"
                        ],
                        "name": "J. Nocedal",
                        "slug": "J.-Nocedal",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Nocedal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nocedal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 6
                            }
                        ],
                        "text": "Jorge Nocedal Stephen J. Wright\nNumerical Optimization\nWith 85 Illustrations\n, Springer\nContents\nPreface vii\n1 Introduction, 1 Mathematical Formulation 2 Example: A Transportation Problem 4 Continuous versus Discrete Optimization 4 Constrained and Unconstrained Optimization 6 Global and Local Optimization 6 Stochastic and Deterministic Optimization 7 Optimization Algorithms 7 Convexity 8\nNotes and References 9\n2 Fundamentals of Unconstrained Optimization 10 2.1 What Is a Solution?"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 56
                            }
                        ],
                        "text": "For further discussion on the L-BFGS method see Nocedal [228], Liu and Nocedal [194], and Gilbert and Lemar\u00e9chal [122]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9033333,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d2aaf56fb183ad66d099ac6c9110c5c365ab27f3",
            "isKey": false,
            "numCitedBy": 2414,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We study how to use the BFGS quasi-Newton matrices to precondition minimization methods for problems where the storage is critical. We give an update formula which generates matrices using information from the last m iterations, where m is any number supplied by the user. The quasi-Newton matrix is updated at every iteration by dropping the oldest information and replacing it by the newest informa- tion. It is shown that the matrices generated have some desirable properties. The resulting algorithms are tested numerically and compared with several well- known methods. 1. Introduction. For the problem of minimizing an unconstrained function / of n variables, quasi-Newton methods are widely employed (4). They construct a se- quence of matrices which in some way approximate the hessian of /(or its inverse). These matrices are symmetric; therefore, it is necessary to have n(n + l)/2 storage locations for each one. For large dimensional problems it will not be possible to re- tain the matrices in the high speed storage of a computer, and one has to resort to other kinds of algorithms. For example, one could use the methods (Toint (15), Shanno (12)) which preserve the sparsity structure of the hessian, or conjugate gradient methods (CG) which only have to store 3 or 4 vectors. Recently, some CG algorithms have been developed which use a variable amount of storage and which do not require knowledge about the sparsity structure of the problem (2), (7), (8). A disadvantage of these methods is that after a certain number of iterations the quasi-Newton matrix is discarded, and the algorithm is restarted using an initial matrix (usually a diagonal matrix). We describe an algorithm which uses a limited amount of storage and where the quasi-Newton matrix is updated continuously. At every step the oldest information contained in the matrix is discarded and replaced by new one. In this way we hope to have a more up to date model of our function. We will concentrate on the BFGS method since it is considered to be the most efficient. We believe that similar algo- rithms cannot be developed for the other members of the Broyden 0-class (1). Let / be the function to be nnnimized, g its gradient and h its hessian. We define"
            },
            "slug": "Updating-Quasi-Newton-Matrices-With-Limited-Storage-Nocedal",
            "title": {
                "fragments": [],
                "text": "Updating Quasi-Newton Matrices With Limited Storage"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An update formula which generates matrices using information from the last m iterations, where m is any number supplied by the user, and the BFGS method is considered to be the most efficient."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710850"
                        ],
                        "name": "C. Bischof",
                        "slug": "C.-Bischof",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Bischof",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bischof"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2363532"
                        ],
                        "name": "P. Khademi",
                        "slug": "P.-Khademi",
                        "structuredName": {
                            "firstName": "Peyvand",
                            "lastName": "Khademi",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Khademi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102871513"
                        ],
                        "name": "Ali Buaricha",
                        "slug": "Ali-Buaricha",
                        "structuredName": {
                            "firstName": "Ali",
                            "lastName": "Buaricha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ali Buaricha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123937253"
                        ],
                        "name": "Carle Alan",
                        "slug": "Carle-Alan",
                        "structuredName": {
                            "firstName": "Carle",
                            "lastName": "Alan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carle Alan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121472788,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cdb84e22f3ff2090dae64427858e81664dc4cfe7",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatic differentiation (AD) is a technique that augments computer codes with statements for the computation of derivatives. The computational workhorse of AD-generated codes for first-order derivatives is the linear combination of vectors. For many large-scale problems, the vectors involved in this operation are inherently sparse. If the underlying function is a partially separable one (e.g., if its Hessian is sparse), many of the intermediate gradient vectors computed by AD will also be sparse, even though the final gradient is likely to be dense. For large Jacobians computations, every intermediate derivative vector is usually at least as sparse as the least sparse row of the final Jacobian. In this paper, we show that dynamic exploitation of the sparsity inherent in derivative computation can result in dramatic gains in runtime and memory savings. For a set of gradient problems exhibiting implicit sparsity, we report on the runtime and memory requirements of computing the gradients with the ADIFOR (..."
            },
            "slug": "Efficient-computation-of-gradients-and-Jacobians-by-Bischof-Khademi",
            "title": {
                "fragments": [],
                "text": "Efficient computation of gradients and Jacobians by dynamic exploitation of sparsity in automatic differentiation"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper shows that dynamic exploitation of the sparsity inherent in derivative computation can result in dramatic gains in runtime and memory savings, and reports on the runtime andMemory requirements of computing the gradients with the ADIFOR."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30175095"
                        ],
                        "name": "H. Khalfan",
                        "slug": "H.-Khalfan",
                        "structuredName": {
                            "firstName": "Humaid",
                            "lastName": "Khalfan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Khalfan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144448267"
                        ],
                        "name": "R. Byrd",
                        "slug": "R.-Byrd",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Byrd",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Byrd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795397"
                        ],
                        "name": "Bobby Schnabel",
                        "slug": "Bobby-Schnabel",
                        "structuredName": {
                            "firstName": "Bobby",
                            "lastName": "Schnabel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bobby Schnabel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 93
                            }
                        ],
                        "text": "Computational experiments by Conn, Gould, and Toint [70, 73] and Khalfan, Byrd, and Schnabel [181], using both line search and trust-region approaches, indicate that the SR1 method appears to be competitive with the BFGS method."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 28835336,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f35606a3b85fbaf4ba9efdc70bb04387db9d3895",
            "isKey": false,
            "numCitedBy": 112,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper first discusses computational experience using the SRi update in conventional line search and trust region algorithms for unconstrained optimization. The experiments show that the SRi is very competitive with the widely used BFGS method. They also indicate two interesting features: the final Hessian approximations produced by the SRi method are not generally appreciably better than those produced by the BFGS, and the sequences of steps produced by the SRi do not usually seem to have the \u201cuniform linear independence\u201d property that is assumed in recent convergence analysis. This paper presents a new analysis that shows that the SRi method with a line search is $( n + 1)$-step q-superlinearly convergent without the assumption of linearly independent iterates. This analysis assumes that the Hessian approximations are positive definite and bounded asymptotically, which, from computational experience, are reasonable assumptions."
            },
            "slug": "A-Theoretical-and-Experimental-Study-of-the-Update-Khalfan-Byrd",
            "title": {
                "fragments": [],
                "text": "A Theoretical and Experimental Study of the Symmetric Rank-One Update"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A new analysis is presented that shows that the SRi method with a line search is $( n + 1)$-step q-superlinearly convergent without the assumption of linearly independent iterates."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Optim."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2294095"
                        ],
                        "name": "M. Powell",
                        "slug": "M.-Powell",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Powell",
                            "middleNames": [
                                "J.",
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Powell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118162286,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bfecbc33f26e6ef2605869f8e846a76b0dfa965d",
            "isKey": false,
            "numCitedBy": 541,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "THE-CONVERGENCE-OF-VARIABLE-METRIC-METHODS-FOR-Powell",
            "title": {
                "fragments": [],
                "text": "THE CONVERGENCE OF VARIABLE METRIC METHODS FOR NONLINEARLY CONSTRAINED OPTIMIZATION CALCULATIONS"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726721"
                        ],
                        "name": "J. F. Bonnans",
                        "slug": "J.-F.-Bonnans",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Bonnans",
                            "middleNames": [
                                "Fr\u00e9d\u00e9ric"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. F. Bonnans"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809172"
                        ],
                        "name": "E. Panier",
                        "slug": "E.-Panier",
                        "structuredName": {
                            "firstName": "Eliane",
                            "lastName": "Panier",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Panier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3112829"
                        ],
                        "name": "A. Tits",
                        "slug": "A.-Tits",
                        "structuredName": {
                            "firstName": "Andr\u00e9",
                            "lastName": "Tits",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tits"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "100772319"
                        ],
                        "name": "Jian L. Zhou",
                        "slug": "Jian-L.-Zhou",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Zhou",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian L. Zhou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 56135987,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "dffb312e73a17ebd0096899b7c351e3526a04f12",
            "isKey": false,
            "numCitedBy": 105,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "When solving inequality constrained optimization problems via Sequential Quadratic Programming (SQP), it is potentially advantageous to generate iterates that all satisfy the constraints: all quadratic programs encountered are then feasible and there is no need for a surrogate merit function. (Feasibility of the successive iterates is in fact required in many contexts such as in real-time applications or when the objective function is not defined outside the feasible set.) It has recently been shown that this is, indeed, possible, by means of a suitable perturbation of the original SQP iteration, without losing superlinear convergence. In this context, the well-known Maratos effect is compounded by the possible infeasibility of the full step of one even close to a solution. These difficulties have been accommodated by making use of a suitable modification of a \u201cbending\u201d technique proposed by Mayne and Polak, requiring evaluation of the constraints function at an auxiliary point at each iteration.In Part I..."
            },
            "slug": "Avoiding-the-Maratos-effect-by-means-of-a-line-II.-Bonnans-Panier",
            "title": {
                "fragments": [],
                "text": "Avoiding the Maratos effect by means of a nonmonotone line search II. Inequality constrained problems\u2014feasible iterates"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3158732"
                        ],
                        "name": "P. Boggs",
                        "slug": "P.-Boggs",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Boggs",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Boggs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144448267"
                        ],
                        "name": "R. Byrd",
                        "slug": "R.-Byrd",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Byrd",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Byrd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46190125"
                        ],
                        "name": "R. Schnabel",
                        "slug": "R.-Schnabel",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schnabel",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schnabel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123238459,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "71292b5be957353ab2640aa1a5d73647ca1d6322",
            "isKey": false,
            "numCitedBy": 322,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the most widely used methodologies in scientific and engineering research is the fitting of equations to data by least squares. In cases where significant observation errors exist in the independent variables as well as the dependent variables, however, the ordinary least squares (OLS) approach, where all errors are attributed to the dependent variable, is often inappropriate. An alternate approach, suggested by several researchers, involves minimizing the sum of squared orthogonal distances between each data point and the curve described by the model equation. We refer to this as orthogonal distance regression (ODR). This paper describes a method for solving the orthogonal distance regression problem that is a direct analog of the trust region Levenberg-Marquardt algorithm. The number of unknowns involved is the number of model parameters plus the number of data points, often a very large number. By exploiting sparsity, however, our algorithm has a computational effort per step which is of the same order as required for the Levenberg-Marquardt method for ordinary least squares. We prove our algorithm to be globally and locally convergent, and perform computational tests that illustrate some differences between ODR and OLS."
            },
            "slug": "A-Stable-and-Efficient-Algorithm-for-Nonlinear-Boggs-Byrd",
            "title": {
                "fragments": [],
                "text": "A Stable and Efficient Algorithm for Nonlinear Orthogonal Distance Regression"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper describes a method for solving the orthogonal distance regression problem that is a direct analog of the trust region Levenberg-Marquardt algorithm, and proves the algorithm to be globally and locally convergent, and performs computational tests that illustrate some differences between ODR and OLS."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144241870"
                        ],
                        "name": "J. J. Mor\u00e9",
                        "slug": "J.-J.-Mor\u00e9",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Mor\u00e9",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. J. Mor\u00e9"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1787519"
                        ],
                        "name": "D. Thuente",
                        "slug": "D.-Thuente",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Thuente",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Thuente"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 81
                            }
                        ],
                        "text": "See Dennis and Schnabel [92], Lemar\u00e9chal [189], Fletcher [101], Mor\u00e9 and Thuente [216] (in particular), and Hager and Zhang [161]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 169
                            }
                        ],
                        "text": "Practical line search algorithms also make use of the properties of the interpolating polynomials to make educated guesses of where the next step length should lie; see [39, 216]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207184388,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d2581519560dd59e5b92599f9711aa1ab249ff86",
            "isKey": false,
            "numCitedBy": 556,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "The development of software for minimization problems is often based on a line search method. We consider line search methods that satisfy sufficient decrease and curvature conditions, and formulate the problem of determining a point that satisfies these two conditions in terms of finding a point in a set T(\u03bc). We describe a search algorithm for this problem that produces a sequence of iterates that converge to a point in T(\u03bc) and that, except for pathological cases, terminates in a finite number of steps. Numerical results for an implementation of the search algorithm on a set of test functions show that the algorithm terminates within a small number of iterations."
            },
            "slug": "Line-search-algorithms-with-guaranteed-sufficient-Mor\u00e9-Thuente",
            "title": {
                "fragments": [],
                "text": "Line search algorithms with guaranteed sufficient decrease"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A search algorithm is described for this problem that produces a sequence of iterates that converge to a point in T(\u03bc) and that, except for pathological cases, terminates in a finite number of steps."
            },
            "venue": {
                "fragments": [],
                "text": "TOMS"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3158732"
                        ],
                        "name": "P. Boggs",
                        "slug": "P.-Boggs",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Boggs",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Boggs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1858423"
                        ],
                        "name": "J. Tolle",
                        "slug": "J.-Tolle",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "Tolle",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tolle"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 123
                            }
                        ],
                        "text": "A study of the convergence of BFGS matrices for nonlinear problems can be found in Ge and Powell [119] and Boggs and Tolle [32]; however, the results are not as satisfactory as for SR1 updating."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 11376237,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7890da4b0540cf57929f74ee8b3679660a4a3c78",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Many optimization algorithms generate, at each iteration, a pair $( x_k ,H_k )$ consisting of an approximation to the solution $x_k $ and a Hessian matrix approximation $H_k $ that contains local second-order information about the problem. Much is known about the convergence of $x_k $ to the solution of the problem, but relatively little is known about the behavior of the sequence of matrix approximations. The sequence $\\{ H_k \\}$, generated by the extended Broyden class of updating schemes independently of the optimization setting in which they are used, is analyzed. Various conditions under which convergence is assured are derived, and the structure of the limits is delineated. Rates of convergence are also obtained. These results extend and clarify those already in the literature."
            },
            "slug": "Convergence-Properties-of-a-Class-of-Rank-two-Boggs-Tolle",
            "title": {
                "fragments": [],
                "text": "Convergence Properties of a Class of Rank-two Updates"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The sequence of matrix approximations generated by the extended Broyden class of updating schemes independently of the optimization setting in which they are used, is analyzed and various conditions under which convergence is assured are derived and the structure of the limits is delineated."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Optim."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2587310"
                        ],
                        "name": "R. Rockafellar",
                        "slug": "R.-Rockafellar",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Rockafellar",
                            "middleNames": [
                                "Tyrrell"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rockafellar"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121931445,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2586d52153babf1315f7f83d94b4b2697a9de8d5",
            "isKey": false,
            "numCitedBy": 642,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "For nonlinear programming problems with equality constraints, Hestenes and Powell have independently proposed a dual method of solution in which squares of the constraint functions are added as penalties to the Lagrangian, and a certain simple rule is used for updating the Lagrange multipliers after each cycle. Powell has essentially shown that the rate of convergence is linear if one starts with a sufficiently high penalty factor and sufficiently near to a local solution satisfying the usual second-order sufficient conditions for optimality. This paper furnishes the corresponding method for inequality-constrained problems. Global convergence to an optimal solution is established in the convex case for an arbitrary penalty factor and without the requirement that an exact minimum be calculated at each cycle. Furthermore, the Lagrange multipliers are shown to converge, even though the optimal multipliers may not be unique."
            },
            "slug": "The-multiplier-method-of-Hestenes-and-Powell-to-Rockafellar",
            "title": {
                "fragments": [],
                "text": "The multiplier method of Hestenes and Powell applied to convex programming"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1932759"
                        ],
                        "name": "M. Todd",
                        "slug": "M.-Todd",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Todd",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Todd"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 39627975,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "030d17bce6b8bb27ac7f023cb7896d6529f805d3",
            "isKey": false,
            "numCitedBy": 62,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We provide a survey of interior-point methods for linear programming and its extensions that are based on reducing a suitable potential function at each iteration. We give a fairly complete overview of potential-reduction methods for linear programming, focusing on the possibility of taking long steps and the properties of the barrier function that are necessary for the analysis. We then describe briefly how the methods and results can be extended to certain convex programming problems, following the approach of Nesterov and Todd. We conclude with some open problems."
            },
            "slug": "Potential-reduction-methods-in-mathematical-Todd",
            "title": {
                "fragments": [],
                "text": "Potential-reduction methods in mathematical programming"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A survey of interior-point methods for linear programming and its extensions that are based on reducing a suitable potential function at each iteration, focusing on the possibility of taking long steps and the properties of the barrier function that are necessary for the analysis."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1787993"
                        ],
                        "name": "R. Fourer",
                        "slug": "R.-Fourer",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Fourer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fourer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1847322"
                        ],
                        "name": "B. Kernighan",
                        "slug": "B.-Kernighan",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Kernighan",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Kernighan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 67
                            }
                        ],
                        "text": "We refer the reader to Dantzig [86] and Fourer, Gay, and Kernighan [112] for more comprehensive discussion of this issue."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 157
                            }
                        ],
                        "text": "Information about modeling techniques for various application areas can be found in Dantzig [86], Ahuja, Magnanti, and Orlin [1], Fourer, Gay, and Kernighan [112], Winston [308], and Rardin [262]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 60753226,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "912a2dc05a3e0f5fc778f4d0ed18286d005c1ab1",
            "isKey": false,
            "numCitedBy": 3537,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Practical large-scale mathematical programming involves more than just the application of an algorithm to minimize or maximize an objective function. Before any optimizing routine can be invoked, considerable effort must be expended to formulate the underlying model and to generate the requisite computational data structures. AMPL is a new language designed to make these steps easier and less error-prone. AMPL closely resembles the symbolic algebraic notation that many modelers use to describe mathematical programs, yet it is regular and formal enough to be processed by a computer system; it is particularly notable for the generality of its syntax and for the variety of its indexing operations. We have implemented an efficient translator that takes as input a linear AMPL model and associated data, and produces output suitable for standard linear programming optimizers. Both the language and the translator admit straightforward extensions to more general mathematical programs that incorporate nonlinear expressions or discrete variables."
            },
            "slug": "AMPL:-A-Modeling-Language-for-Mathematical-Fourer-Kernighan",
            "title": {
                "fragments": [],
                "text": "AMPL: A Modeling Language for Mathematical Programming"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "An efficient translator is implemented that takes as input a linear AMPL model and associated data, and produces output suitable for standard linear programming optimizers."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3158732"
                        ],
                        "name": "P. Boggs",
                        "slug": "P.-Boggs",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Boggs",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Boggs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1858423"
                        ],
                        "name": "J. Tolle",
                        "slug": "J.-Tolle",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "Tolle",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tolle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16697278,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "197179262942eec4207cd2f8f45938ab903823d0",
            "isKey": false,
            "numCitedBy": 1582,
            "numCiting": 132,
            "paperAbstract": {
                "fragments": [],
                "text": "Since its popularization in the late 1970s, Sequential Quadratic Programming (SQP) has arguably become the most successful method for solving nonlinearly constrained optimization problems. As with most optimization methods, SQP is not a single algorithm, but rather a conceptual method from which numerous specific algorithms have evolved. Backed by a solid theoretical and computational foundation, both commercial and public-domain SQP algorithms have been developed and used to solve a remarkably large set of important practical problems. Recently large-scale versions have been devised and tested with promising results."
            },
            "slug": "Sequential-Quadratic-Programming-Boggs-Tolle",
            "title": {
                "fragments": [],
                "text": "Sequential Quadratic Programming"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Since its popularization in the late 1970s, Sequential Quadratic Programming (SQP) has arguably become the most successful method for solving nonlinearly constrained optimization problems."
            },
            "venue": {
                "fragments": [],
                "text": "Acta Numerica"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144733293"
                        ],
                        "name": "R. Fletcher",
                        "slug": "R.-Fletcher",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Fletcher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fletcher"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 42290598,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a9384f25bf630df9a7f84766775a9f6409e22c83",
            "isKey": false,
            "numCitedBy": 57,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A Hessian update is described that preserves sparsity and positive definiteness and satisfies a minimal change property. The update reduces to the BFGS update in the dense case and generalises a recent result in [SIAM J. Nnmer. Anal., 26 (1989), pp. 727\u2013739] relating to the Byrd and Nocedal measure function. A surprising outcome is that a sparsity projection of the inverse Hessian plays a major role. It is shown that the Hessian itself can be recovered from this information under mild assumptions.The update is computed by solving a concave programming problem derived by using the Wolfe dual. The Hessian of the dual is important and plays a similar role to the matrix Q that arises in the sparse PSB update of Toint [Math. Comp., 31 (1977), pp. 954\u2013961]. This matrix is shown to satisfy the same structural and definiteness conditions as Toint's matrix. The update has been implemented for tridiagonal systems and some numerical experiments are described. These experiments indicate that there is potential for a ..."
            },
            "slug": "An-Optimal-Positive-Definite-Update-for-Sparse-Fletcher",
            "title": {
                "fragments": [],
                "text": "An Optimal Positive Definite Update for Sparse Hessian Matrices"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "A Hessian update is described that preserves sparsity and positive definiteness and satisfies a minimal change property and reduces to the BFGS update in the dense case and generalises a recent result in SIAM J. Nnmer."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Optim."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2378746"
                        ],
                        "name": "N. Karmarkar",
                        "slug": "N.-Karmarkar",
                        "structuredName": {
                            "firstName": "Narendra",
                            "lastName": "Karmarkar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Karmarkar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7257867,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "d8bb567103352bf5746c61a445d7c3700430b793",
            "isKey": false,
            "numCitedBy": 4006,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new polynomial-time algorithm for linear programming. In the worst case, the algorithm requiresO(n3.5L) arithmetic operations onO(L) bit numbers, wheren is the number of variables andL is the number of bits in the input. The running-time of this algorithm is better than the ellipsoid algorithm by a factor ofO(n2.5). We prove that given a polytopeP and a strictly interior point a \u03b5P, there is a projective transformation of the space that mapsP, a toP\u2032, a\u2032 having the following property. The ratio of the radius of the smallest sphere with center a\u2032, containingP\u2032 to the radius of the largest sphere with center a\u2032 contained inP\u2032 isO(n). The algorithm consists of repeated application of such projective transformations each followed by optimization over an inscribed sphere to create a sequence of points which converges to the optimal solution in polynomial time."
            },
            "slug": "A-new-polynomial-time-algorithm-for-linear-Karmarkar",
            "title": {
                "fragments": [],
                "text": "A new polynomial-time algorithm for linear programming"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is proved that given a polytopeP and a strictly interior point a \u03b5P, there is a projective transformation of the space that mapsP, a toP\u2032, a\u2032 having the following property: the ratio of the radius of the smallest sphere with center a\u2032, containingP\u2032 to theradius of the largest sphere withCenter a\u2032 contained inP\u2032 isO(n)."
            },
            "venue": {
                "fragments": [],
                "text": "STOC '84"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51043392"
                        ],
                        "name": "J. Gondzio",
                        "slug": "J.-Gondzio",
                        "structuredName": {
                            "firstName": "Jacek",
                            "lastName": "Gondzio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gondzio"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 38607897,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0b30290d95b4963c10dd436beb9df106f179af8d",
            "isKey": false,
            "numCitedBy": 250,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "A modification of the (infeasible) primal-dual interior point method is developed. The method uses multiple corrections to improve the centrality of the current iterate. The maximum number of corrections the algorithm is encouraged to make depends on the ratio of the efforts to solve and to factorize the KKT systems. For any LP problem, this ratio is determined right after preprocessing the KKT system and prior to the optimization process. The harder the factorization, the more advantageous the higher-order corrections might prove to be.The computational performance of the method is studied on more difficult Netlib problems as well as on tougher and larger real-life LP models arising from applications. The use of multiple centrality corrections gives on the average a 25% to 40% reduction in the number of iterations compared with the widely used second-order predictor-corrector method. This translates into 20% to 30% savings in CPU time."
            },
            "slug": "Multiple-centrality-corrections-in-a-primal-dual-Gondzio",
            "title": {
                "fragments": [],
                "text": "Multiple centrality corrections in a primal-dual method for linear programming"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A modification of the (infeasible) primal-dual interior point method that uses multiple corrections to improve the centrality of the current iterate and gives on the average a 25% to 40% reduction in the number of iterations compared with the widely used second-order predictor-corrector method."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Optim. Appl."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144312915"
                        ],
                        "name": "J. Ortega",
                        "slug": "J.-Ortega",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Ortega",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ortega"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3001942"
                        ],
                        "name": "W. Rheinboldt",
                        "slug": "W.-Rheinboldt",
                        "structuredName": {
                            "firstName": "Werner",
                            "lastName": "Rheinboldt",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Rheinboldt"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 92
                            }
                        ],
                        "text": "For an extensive discussion of line search termination conditions see Ortega and Rheinboldt [230]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 39585209,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ef098e0154da4b210a6ee11b84ca30bd3e445ac6",
            "isKey": false,
            "numCitedBy": 7887,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface to the Classics Edition Preface Acknowledgments Glossary of Symbols Introduction Part I. Background Material. 1. Sample Problems 2. Linear Algebra 3. Analysis Part II. Nonconstructive Existence Theorems. 4. Gradient Mappings and Minimization 5. Contractions and the Continuation Property 6. The Degree of a Mapping Part III. Iterative Methods. 7. General Iterative Methods 8. Minimization Methods Part IV. Local Convergence. 9. Rates of Convergence-General 10. One-Step Stationary Methods 11. Multistep Methods and Additional One-Step Methods Part V. Semilocal and Global Convergence. 12. Contractions and Nonlinear Majorants 13. Convergence under Partial Ordering 14. Convergence of Minimization Methods An Annotated List of Basic Reference Books Bibliography Author Index Subject Index."
            },
            "slug": "Iterative-solution-of-nonlinear-equations-in-Ortega-Rheinboldt",
            "title": {
                "fragments": [],
                "text": "Iterative solution of nonlinear equations in several variables"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "Convergence of Minimization Methods An Annotated List of Basic Reference Books Bibliography Author Index Subject Index."
            },
            "venue": {
                "fragments": [],
                "text": "Computer science and applied mathematics"
            },
            "year": 1970
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2294095"
                        ],
                        "name": "M. Powell",
                        "slug": "M.-Powell",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Powell",
                            "middleNames": [
                                "J.",
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Powell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145758858"
                        ],
                        "name": "P. Toint",
                        "slug": "P.-Toint",
                        "structuredName": {
                            "firstName": "Philippe",
                            "lastName": "Toint",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Toint"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 119813163,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cb34072db6ed38f6af2cbc04c6954b459c381346",
            "isKey": false,
            "numCitedBy": 130,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper studies automatic procedures for estimating second derivatives of a real valued function of several variables. The estimates are obtained from differences in first derivative vectors, and it is supposed that the required matrix is sparse and that its sparsity structure is known. Our main purpose is to find ways of taking advantage of the sparsity structure and the symmetry of the second derivative matrix, in order to make small the number of first derivative vectors that have to be calculated. Two new algorithms are proposed, which seem to be very successful in practice and which do not require much computer arithmetic. One is a direct method and the other is a substitution method, these terms being explained in the paper. Some examples show, however, that the given methods may not minimize the number of first derivative vector calculations."
            },
            "slug": "On-the-Estimation-of-Sparse-Hessian-Matrices-Powell-Toint",
            "title": {
                "fragments": [],
                "text": "On the Estimation of Sparse Hessian Matrices"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The main purpose is to find ways of taking advantage of the sparsity structure and the symmetry of the second derivative matrix, in order to make small the number of first derivative vectors that have to be calculated."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33225083"
                        ],
                        "name": "D. Gay",
                        "slug": "D.-Gay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Gay",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 157
                            }
                        ],
                        "text": "Since an automatic procedure for detecting the decomposition of a given function f into its partially separable representation was developed recently by Gay [118], it has become possible to exploit the efficiencies that accrue from this property without asking much information from the user."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2066155,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9720bd4880026aba5465cbbba0ded67cfd878ae6",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe computational experience with automatic differentiation of mathematical programming problems expressed in the modeling language AMPL. Nonlinear expressions are translated to loop-free code, which makes it easy to compute gradients and Jacobians by backward automatic differentiation. The nonlinear expressions may be interpreted or, to gain some evaluation speed at the cost of increased preparation time, converted to Fortran or C. We have extended the interpretive scheme to evaluate Hessian (of Lagrangian) times vector. Detecting partially separable structure (sums of terms, each depending, perhaps after a linear transformation, on only a few variables) is of independent interest, as some solvers exploit this structure. It can be detected automatically by suitable \u2018\u2018tree walks\u2019\u2019. Exploiting this structure permits an AD computation of the entire Hessian matrix by accumulating Hessian times vector computations for each term, and can lead to a much faster computation of the Hessian than by computing the whole Hessian times each unit vector."
            },
            "slug": "More-AD-of-Nonlinear-AMPL-Models:-Computing-Hessian-Gay",
            "title": {
                "fragments": [],
                "text": "More AD of Nonlinear AMPL Models: Computing Hessian Information and Exploiting Partial Separability\u2020"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "Computational experience with automatic differentiation of mathematical programming problems expressed in the modeling language AMPL is described, which makes it easy to compute gradients and Jacobians by backward automatic differentiation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33133899"
                        ],
                        "name": "E. Andersen",
                        "slug": "E.-Andersen",
                        "structuredName": {
                            "firstName": "Erling",
                            "lastName": "Andersen",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Andersen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735908"
                        ],
                        "name": "Knud D. Andersen",
                        "slug": "Knud-D.-Andersen",
                        "structuredName": {
                            "firstName": "Knud",
                            "lastName": "Andersen",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Knud D. Andersen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15368870,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "245671cd4a68680b31a8bff97a344994b3250a3a",
            "isKey": false,
            "numCitedBy": 219,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Most modern linear programming solvers analyze the LP problem before submitting it to optimization. Some examples are the solvers WHIZARD (Tomlin and Welch, 1983), OB1 (Lustig et al., 1994), OSL (Forrest and Tomlin, 1992), Sciconic (1990) and CPLEX (Bixby, 1994). The purpose of the presolve phase is to reduce the problem size and to discover whether the problem is unbounded or infeasible.In this paper we present a comprehensive survey of presolve methods. Moreover, we discuss the restoration procedure in detail, i.e., the procedure that undoes the presolve.Computational results on the NETLIB problems (Gay, 1985) are reported to illustrate the efficiency of the presolve methods."
            },
            "slug": "Presolving-in-linear-programming-Andersen-Andersen",
            "title": {
                "fragments": [],
                "text": "Presolving in linear programming"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper presents a comprehensive survey of presolve methods and discusses the restoration procedure in detail, i.e., the procedure that undoes the presolve."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1986103"
                        ],
                        "name": "R. Vanderbei",
                        "slug": "R.-Vanderbei",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Vanderbei",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Vanderbei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2213123"
                        ],
                        "name": "D. Shanno",
                        "slug": "D.-Shanno",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Shanno",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Shanno"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 29
                            }
                        ],
                        "text": "Other packages, such as LOQO [294] implement Newton methods with a sparse factorization modified to ensure positive definiteness."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6610058,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "268be450a0466c6130dd0a67a5f7b95d695da393",
            "isKey": false,
            "numCitedBy": 584,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper describes an interior-point algorithm for nonconvex nonlinear programming which is a direct extension of interior-point methods for linear and quadratic programming. Major modifications include a merit function and an altered search direction to ensure that a descent direction for the merit function is obtained. Preliminary numerical testing indicates that the method is robust. Further, numerical comparisons with MINOS and LANCELOT show that the method is efficient, and has the promise of greatly reducing solution times on at least some classes of models."
            },
            "slug": "An-Interior-Point-Algorithm-for-Nonconvex-Nonlinear-Vanderbei-Shanno",
            "title": {
                "fragments": [],
                "text": "An Interior-Point Algorithm for Nonconvex Nonlinear Programming"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "Numerical comparisons with MINOS and LANCELOT show that the interior-point algorithm for nonconvex nonlinear programming is efficient, and has the promise of greatly reducing solution times on at least some classes of models."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Optim. Appl."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2849034"
                        ],
                        "name": "H. Crowder",
                        "slug": "H.-Crowder",
                        "structuredName": {
                            "firstName": "Harlan",
                            "lastName": "Crowder",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Crowder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145820374"
                        ],
                        "name": "P. Wolfe",
                        "slug": "P.-Wolfe",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Wolfe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Wolfe"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 18
                            }
                        ],
                        "text": "Crowder and Wolfe [82] show that the rate of convergence is linear, and show by constructing an example that Q-superlinear convergence is not achievable."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 184
                            }
                        ],
                        "text": "21 Models for Trust-Region Methods 26 Scaling 27 Rates of Convergence 28 R-Rates of Convergence 29 Notes and References 30 Exercises 30\n3 Line Search Methods 34 3.1 Step Length 36\nThe Wolfe Conditions 37 The Goldstein Conditions 41 Sufficient Decrease and Backtracking 41\n3.2 Convergence of Line Search Methods 43 3.3 Rate of Convergence 46\nConvergence Rate of Steepest Descent 47 Quasi-Newton Methods 49 Newton's Method 51 Coordinate Descent Methods 53\n3.4 Step-Length Selection Algorithms 55 Interpolation 56 The Initial Step Length 58 A Line Search Algorithm for the Wolfe Conditions 58 Notes and References 61 Exercises 62\n4 Trust-Region Methods 64 Outline of the Algorithm 67\n4.1 The Cauchy Point and Related Algorithms 69 The Cauchy Point 69 Improving on the Cauchy Point 70 The Dogleg Method 71 Two-Dimensional Subspace Minimization 74 Steihaug's Approach 75 4.2 Using Nearly Exact Solutions to the Subproblem 77 Characterizing Exact Solutions 77 Calculating Nearly Exact Solutions 78 The Hard Case 82 Proof of Theorem 4.3 84 4.3 Global Convergence 87\nCONTENTS xiii\nReduction Obtained by the Cauchy Point 87 Convergence to Stationary Points 89 Convergence of Algorithms Based on Nearly Exact Solutions 93\n4.4 Other Enhancements 94 Scaling 94 Non-Euclidean Trust Regions 96 Notes and References 97 Exercises 97\nConjugate Gradient Methods 100 5.1 The Linear Conjugate Gradient Method 102\nConjugate Direction Methods 102 Basic Properties of the Conjugate Gradient Method 107 A Practical Form of the Conjugate Gradient Method I l l Rate of Convergence 112 Preconditioning 118 Practical Preconditioners 119\n5.2 Nonlinear Conjugate Gradient Methods 120 The Fletcher-Reeves Method 120 The Polak-Ribiere Method 121 Quadratic Termination and Restarts 122 Numerical Performance 124 Behavior of the Fletcher-Reeves Method 124 Global Convergence 127 Notes and References 131 Exercises 132\nPractical Newton'Methods 134 6.1 Inexact Newton Steps 136 6.2 Line Search Newton Methods 139\nLine Search Newton-CG Method 139 Modified Newton's Method 141\n6.3 Hessian Modifications ."
                    },
                    "intents": []
                }
            ],
            "corpusId": 122350390,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "65a4424d3f17cc1bf79e339a977f14972e92325f",
            "isKey": true,
            "numCitedBy": 69,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "There are two procedures for applying the method of conjugate gradients to the problem of minimizing a convex, nonlinear function: the \"continued\" method, and the \"restarted\" method in which all the data except the best previous point are discarded, and the procedure is begun a new from that point. It is demonstrated by example that in the absence of the standard initial starting condition on a quadratic function, the continued conjugate gradient method will converge to the solution no better than linearly. Furthermore, it is shown that for a general nonlinear function, the nonrestarted conjugate gradient method converges no worse than linearly."
            },
            "slug": "Linear-convergence-of-the-conjugate-gradient-method-Crowder-Wolfe",
            "title": {
                "fragments": [],
                "text": "Linear convergence of the conjugate gradient method"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1409964566"
                        ],
                        "name": "Dong C. Liu",
                        "slug": "Dong-C.-Liu",
                        "structuredName": {
                            "firstName": "Dong C.",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong C. Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2784955"
                        ],
                        "name": "J. Nocedal",
                        "slug": "J.-Nocedal",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Nocedal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nocedal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 6
                            }
                        ],
                        "text": "Jorge Nocedal Stephen J. Wright\nNumerical Optimization\nWith 85 Illustrations\n, Springer\nContents\nPreface vii\n1 Introduction, 1 Mathematical Formulation 2 Example: A Transportation Problem 4 Continuous versus Discrete Optimization 4 Constrained and Unconstrained Optimization 6 Global and Local Optimization 6 Stochastic and Deterministic Optimization 7 Optimization Algorithms 7 Convexity 8\nNotes and References 9\n2 Fundamentals of Unconstrained Optimization 10 2.1 What Is a Solution?"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 53
                            }
                        ],
                        "text": "Limited-memory BFGS methods are implemented in LBFGS [194] and M1QN3 [122]; see Gill and Leonard [125] for a variant that requires less storage and appears to be quite efficient."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 79
                            }
                        ],
                        "text": "For further discussion on the L-BFGS method see Nocedal [228], Liu and Nocedal [194], and Gilbert and Lemar\u00e9chal [122]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5681609,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "1267fe36b5ece49a9d8f913eb67716a040bbcced",
            "isKey": false,
            "numCitedBy": 5864,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the numerical performance of a limited memory quasi-Newton method for large scale optimization, which we call the L-BFGS method. We compare its performance with that of the method developed by Buckley and LeNir (1985), which combines cycles of BFGS steps and conjugate direction steps. Our numerical tests indicate that the L-BFGS method is faster than the method of Buckley and LeNir, and is better able to use additional storage to accelerate convergence. We show that the L-BFGS method can be greatly accelerated by means of a simple scaling. We then compare the L-BFGS method with the partitioned quasi-Newton method of Griewank and Toint (1982a). The results show that, for some problems, the partitioned quasi-Newton method is clearly superior to the L-BFGS method. However we find that for other problems the L-BFGS method is very competitive due to its low iteration cost. We also study the convergence properties of the L-BFGS method, and prove global convergence on uniformly convex problems."
            },
            "slug": "On-the-limited-memory-BFGS-method-for-large-scale-Liu-Nocedal",
            "title": {
                "fragments": [],
                "text": "On the limited memory BFGS method for large scale optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The numerical tests indicate that the L-BFGS method is faster than the method of Buckley and LeNir, and is better able to use additional storage to accelerate convergence, and the convergence properties are studied to prove global convergence on uniformly convex problems."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710850"
                        ],
                        "name": "C. Bischof",
                        "slug": "C.-Bischof",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Bischof",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bischof"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3157288"
                        ],
                        "name": "L. Roh",
                        "slug": "L.-Roh",
                        "structuredName": {
                            "firstName": "Lucas",
                            "lastName": "Roh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Roh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1406445135"
                        ],
                        "name": "A. Mauer-Oats",
                        "slug": "A.-Mauer-Oats",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Mauer-Oats",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Mauer-Oats"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9699067,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c19cb622723bc595f5c19af47ff867235a99064c",
            "isKey": false,
            "numCitedBy": 192,
            "numCiting": 71,
            "paperAbstract": {
                "fragments": [],
                "text": "In scientific computing, we often require the derivatives \u2202f/\u2202x of a function f expressed as a program with respect to some input parameter(s) x, say. Automatic Differentiation (AD) techniques augment the program with derivative computation by applying the chain rule of calculus to elementary operations in an automated fashion. This article introduces ADIC (Automatic Differentiation of C), a new AD tool for ANSI-C programs. ADIC is currently the only tool for ANSI-C that employs a source-to-source program transformation approach; that is, it takes a C code and produces a new C code that computes the original results as well as the derivatives. We first present ADIC \u2018by example\u2019 to illustrate the functionality and ease of use of ADIC and then describe in detail the architecture of ADIC. ADIC incorporates a modular design that provides a foundation for both rapid prototyping of better AD algorithms and their sharing across AD tools for different languages. A component architecture called AIF (Automatic Differentiation Intermediate Form) separates core AD concepts from their language-specific implementation and allows the development of generic AD modules that can be reused directly in other AIF-based AD tools. The language-specific ADIC front-end and back-end canonicalize C programs to make them fit for semantic augmentation and manage, for example, the association of a program variable with its derivative object. We also report on applications of ADIC to a semiconductor device simulator, 3-D CFD grid generator, vehicle simulator, and neural network code. \u00a9 1997 John Wiley & Sons, Ltd."
            },
            "slug": "ADIC:-An-Extensible-Automatic-Differentiation-Tool-Bischof-Roh",
            "title": {
                "fragments": [],
                "text": "ADIC: An Extensible Automatic Differentiation Tool for ANSI-C"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "ADIC (Automatic Differentiation of C), a new AD tool for ANSI-C programs, is introduced and a modular design that provides a foundation for both rapid prototyping of better AD algorithms and their sharing across AD tools for different languages is described."
            },
            "venue": {
                "fragments": [],
                "text": "Softw. Pract. Exp."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723559"
                        ],
                        "name": "A. Conn",
                        "slug": "A.-Conn",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Conn",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Conn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144874680"
                        ],
                        "name": "N. Gould",
                        "slug": "N.-Gould",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Gould",
                            "middleNames": [
                                "I.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Gould"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145758858"
                        ],
                        "name": "P. Toint",
                        "slug": "P.-Toint",
                        "structuredName": {
                            "firstName": "Philippe",
                            "lastName": "Toint",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Toint"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9898484,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "425ac58bf0c38db31447d3a17560e1ad70d8da64",
            "isKey": false,
            "numCitedBy": 81,
            "numCiting": 88,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we describe the algorithmic options of Release A of LANCELOT, a Fortran package for large-scale nonlinear optimization. We then present the results of intensitive numberical tests and discuss the relative merits of the options. The experiments described involve both academic and applied problems. Finally, we propose conclusion, both specific to LANCELOT and of more general scope."
            },
            "slug": "Numerical-experiments-with-the-LANCELOT-package-A)-Conn-Gould",
            "title": {
                "fragments": [],
                "text": "Numerical experiments with the LANCELOT package (release A) for large-scale nonlinear optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "The algorithmic options of Release A of LANCELOT, a Fortran package for large-scale nonlinear optimization, are described and the results of intensitive numberical tests are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2294095"
                        ],
                        "name": "M. Powell",
                        "slug": "M.-Powell",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Powell",
                            "middleNames": [
                                "J.",
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Powell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117994662,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e436af183ca09fc85810a134895809eb75c97363",
            "isKey": false,
            "numCitedBy": 413,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Variable metric methods solve nonlinearly constrained optimization problems, using calculated first derivatives and a single positive definite matrix, which holds second derivative information that is obtained automatically. The theory of these methods is shown by analysing the global and local convergence properties of a basic algorithm, and we find that superlinear convergence requires less second derivative information than in the unconstrained case. Moreover, in order to avoid the difficulties of inconsistent linear approximations to constraints, careful consideration is given to the calculation of search directions by unconstrained minimization subproblems. The Maratos effect and relations to reduced gradient algorithms are studied briefly."
            },
            "slug": "Variable-Metric-Methods-for-Constrained-Powell",
            "title": {
                "fragments": [],
                "text": "Variable Metric Methods for Constrained Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "Variable metric methods solve nonlinearly constrained optimization problems, using calculated first derivatives and a single positive definite matrix, which holds second derivative information that is obtained automatically, and it is found that superlinear convergence requires less second derivatives information than in the unconstrained case."
            },
            "venue": {
                "fragments": [],
                "text": "ISMP"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144874680"
                        ],
                        "name": "N. Gould",
                        "slug": "N.-Gould",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Gould",
                            "middleNames": [
                                "I.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Gould"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2593984"
                        ],
                        "name": "S. Lucidi",
                        "slug": "S.-Lucidi",
                        "structuredName": {
                            "firstName": "Stefano",
                            "lastName": "Lucidi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lucidi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145954360"
                        ],
                        "name": "M. Roma",
                        "slug": "M.-Roma",
                        "structuredName": {
                            "firstName": "Massimo",
                            "lastName": "Roma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Roma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145758858"
                        ],
                        "name": "P. Toint",
                        "slug": "P.-Toint",
                        "structuredName": {
                            "firstName": "Philippe",
                            "lastName": "Toint",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Toint"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 5
                            }
                        ],
                        "text": "GLTR [145] offers a Newton\u2013Lanczos method."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 50
                            }
                        ],
                        "text": "For a discussion of the Newton\u2013Lanczos method see [145]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 103
                            }
                        ],
                        "text": "A sophisticated implementation of the Newton\u2013Lanczos approach has been implemented in the GLTR package [145]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 39756229,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9503605fb606e153a105c09049a5736632482085",
            "isKey": true,
            "numCitedBy": 277,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "The approximate minimization of a quadratic function within an ellipsoidal trust region is an important subproblem for many nonlinear programming methods. When the number of variables is large, the most widely used strategy is to trace the path of conjugate gradient iterates either to convergence or until it reaches the trust-region boundary. In this paper, we investigate ways of continuing the process once the boundary has been encountered. The key is to observe that the trust-region problem within the currently generated Krylov subspace has a very special structure which enables it to be solved very efficiently. We compare the new strategy with existing methods. The resulting software package is available as HSL_VF05 within the Harwell Subroutine Library."
            },
            "slug": "Solving-the-Trust-Region-Subproblem-using-the-Gould-Lucidi",
            "title": {
                "fragments": [],
                "text": "Solving the Trust-Region Subproblem using the Lanczos Method"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "The key is to observe that the trust-region problem within the currently generated Krylov subspace has a very special structure which enables it to be solved very efficiently."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Optim."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1787993"
                        ],
                        "name": "R. Fourer",
                        "slug": "R.-Fourer",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Fourer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fourer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35282765"
                        ],
                        "name": "Sanjay Mehrotra",
                        "slug": "Sanjay-Mehrotra",
                        "structuredName": {
                            "firstName": "Sanjay",
                            "lastName": "Mehrotra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sanjay Mehrotra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16319200,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "17267eb128b00bf92cf0e21e1e37322bd94fa100",
            "isKey": false,
            "numCitedBy": 77,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an implementation of a primal\u2014dual path following method for linear programming that solves symmetric indefinite \u201caugmented\u201d systems directly by Bunch\u2014Parlett factorization, rather than reducing these systems to the positive definite \u201cnormal equations\u201d that are solved by Cholesky factorization in many existing implementations. The augmented system approach is seen to avoid difficulties of numerical instability and inefficiency associated with free variables and with dense columns in the normal equations approach. Solving the indefinite systems does incur an extra overhead, whose median is about 40% in our tests; but the augmented system approach proves to be faster for a minority of cases in which the normal equations have relatively dense Cholesky factors. A detailed analysis shows that the augmented system factorization is reliable over a fairly large range of the parameter settings that control the tradeoff between sparsity and numerical stability."
            },
            "slug": "Solving-symmetric-indefinite-systems-in-an-method-Fourer-Mehrotra",
            "title": {
                "fragments": [],
                "text": "Solving symmetric indefinite systems in an interior-point method for linear programming"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "An implementation of a primal\u2014dual path following method for linear programming that solves symmetric indefinite \u201caugmented\u201d systems directly by Bunch\u2014Parlett factorization, rather than reducing these systems to the positive definite \u201cnormal equations\u201d that are solved by Cholesky factorization in many existing implementations is described."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145830268"
                        ],
                        "name": "J. Morales",
                        "slug": "J.-Morales",
                        "structuredName": {
                            "firstName": "Jos\u00e9",
                            "lastName": "Morales",
                            "middleNames": [
                                "Luis"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Morales"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2784955"
                        ],
                        "name": "J. Nocedal",
                        "slug": "J.-Nocedal",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Nocedal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nocedal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 96
                            }
                        ],
                        "text": "A preconditioner for Newton\u2013CG based on limited-memory BFGS approximations is provided in PREQN [209]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14501222,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a409091edc6bef52c3486f041e213252dbd39fad",
            "isKey": false,
            "numCitedBy": 162,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a preconditioner for the conjugate gradient method (CG) that is designed for solving systems of equations Ax=bi with different right-hand-side vectors or for solving a sequence of slowly varying systems Ak x = bk. The preconditioner has the form of a limited memory quasi-Newton matrix and is generated using information from the CG iteration. The automatic preconditioner does not require explicit knowledge of the coefficient matrix A and is therefore suitable for problems where only products of A times a vector can be computed. Numerical experiments indicate that the preconditioner has most to offer when these matrix-vector products are expensive to compute and when low accuracy in the solution is required. The effectiveness of the preconditioner is tested within a Hessian-free Newton method for optimization and by solving certain linear systems arising in finite element models."
            },
            "slug": "Automatic-Preconditioning-by-Limited-Memory-Morales-Nocedal",
            "title": {
                "fragments": [],
                "text": "Automatic Preconditioning by Limited Memory Quasi-Newton Updating"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "A preconditioner for the conjugate gradient method that is designed for solving systems of equations Ax=bi with different right-hand-side vectors or for solving a sequence of slowly varying systems Ak x = bk is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Optim."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50018942"
                        ],
                        "name": "H. Markowitz",
                        "slug": "H.-Markowitz",
                        "structuredName": {
                            "firstName": "Harry",
                            "lastName": "Markowitz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Markowitz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123177183,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5c7c2b1a9234d3116926bd1bfcfcb2a45b7cbfca",
            "isKey": false,
            "numCitedBy": 507,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "It is common for matrices in industrial applications of linear programming to have a large proportion of zero coefficients. While every item raw material, intermediate material, end item, equipment item in, say, a petroleum refinery may be indirectly related to every other, any particular process uses few of these. Thus the matrix describing petroleum technology has a small percentage of non-zeros. If spacial or temporal distinctions are introduced into the model the percentage of non-zeros generally falls further. \n \nThe present paper discusses a form of inverse which is especially convenient to obtain and use for matrices with a high percentage of zeros. The application of this form of inverse in linear programming is also discussed."
            },
            "slug": "The-Elimination-form-of-the-Inverse-and-its-to-Markowitz",
            "title": {
                "fragments": [],
                "text": "The Elimination form of the Inverse and its Application to Linear Programming"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1957
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33133899"
                        ],
                        "name": "E. Andersen",
                        "slug": "E.-Andersen",
                        "structuredName": {
                            "firstName": "Erling",
                            "lastName": "Andersen",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Andersen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735908"
                        ],
                        "name": "Knud D. Andersen",
                        "slug": "Knud-D.-Andersen",
                        "structuredName": {
                            "firstName": "Knud",
                            "lastName": "Andersen",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Knud D. Andersen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117484770,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d2b98fbce490feb02f24013413835736af319da6",
            "isKey": false,
            "numCitedBy": 584,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "The purpose of this work is to present the MOSEK optimizer intended for solution of large-scale sparse linear programs. The optimizer is based on the homogeneous interior-point algorithm which in contrast to the primal-dual algorithm detects a possible primal or dual infeasibility reliably. It employs advanced (parallelized) linear algebra, it handles dense columns in the constraint matrix efficiently, and it has a basis identification procedure."
            },
            "slug": "The-Mosek-Interior-Point-Optimizer-for-Linear-An-of-Andersen-Andersen",
            "title": {
                "fragments": [],
                "text": "The Mosek Interior Point Optimizer for Linear Programming: An Implementation of the Homogeneous Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The MOSEK optimizer is based on the homogeneous interior-point algorithm which in contrast to the primal-dual algorithm detects a possible primal or dual infeasibility reliably reliably."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800149"
                        ],
                        "name": "J. Dennis",
                        "slug": "J.-Dennis",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Dennis",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dennis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795397"
                        ],
                        "name": "Bobby Schnabel",
                        "slug": "Bobby-Schnabel",
                        "structuredName": {
                            "firstName": "Bobby",
                            "lastName": "Schnabel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bobby Schnabel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 24
                            }
                        ],
                        "text": "See Dennis and Schnabel [92], Lemar\u00e9chal [189], Fletcher [101], Mor\u00e9 and Thuente [216] (in particular), and Hager and Zhang [161]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 127
                            }
                        ],
                        "text": "For a more thorough treatment of the modified Cholesky factorization see Gill, Murray, and Wright [130] or Dennis and Schnabel [92]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 78
                            }
                        ],
                        "text": "For a comprehensive treatment of quasi-Newton methods see Dennis and Schnabel [92], Dennis and Mor\u00e9 [91], and Fletcher [101]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 61
                            }
                        ],
                        "text": "For details, see Dennis and Mor\u00e9 [91] or Dennis and Schnabel [92]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 97
                            }
                        ],
                        "text": "A formula for updating the Cholesky factors of the BFGS matrices is given in Dennis and Schnabel [92]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 60
                            }
                        ],
                        "text": "Our discussion of interpolation follows Dennis and Schnabel [92], and the algorithm for finding a step length satisfying the strong Wolfe conditions can be found in Fletcher [101]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 27578127,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e1053197256c6c3c0631377ec23a3f7dc1cb4781",
            "isKey": true,
            "numCitedBy": 7616,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface 1. Introduction. Problems to be considered Characteristics of 'real-world' problems Finite-precision arithmetic and measurement of error Exercises 2. Nonlinear Problems in One Variable. What is not possible Newton's method for solving one equation in one unknown Convergence of sequences of real numbers Convergence of Newton's method Globally convergent methods for solving one equation in one uknown Methods when derivatives are unavailable Minimization of a function of one variable Exercises 3. Numerical Linear Algebra Background. Vector and matrix norms and orthogonality Solving systems of linear equations-matrix factorizations Errors in solving linear systems Updating matrix factorizations Eigenvalues and positive definiteness Linear least squares Exercises 4. Multivariable Calculus Background Derivatives and multivariable models Multivariable finite-difference derivatives Necessary and sufficient conditions for unconstrained minimization Exercises 5. Newton's Method for Nonlinear Equations and Unconstrained Minimization. Newton's method for systems of nonlinear equations Local convergence of Newton's method The Kantorovich and contractive mapping theorems Finite-difference derivative methods for systems of nonlinear equations Newton's method for unconstrained minimization Finite difference derivative methods for unconstrained minimization Exercises 6. Globally Convergent Modifications of Newton's Method. The quasi-Newton framework Descent directions Line searches The model-trust region approach Global methods for systems of nonlinear equations Exercises 7. Stopping, Scaling, and Testing. Scaling Stopping criteria Testing Exercises 8. Secant Methods for Systems of Nonlinear Equations. Broyden's method Local convergence analysis of Broyden's method Implementation of quasi-Newton algorithms using Broyden's update Other secant updates for nonlinear equations Exercises 9. Secant Methods for Unconstrained Minimization. The symmetric secant update of Powell Symmetric positive definite secant updates Local convergence of positive definite secant methods Implementation of quasi-Newton algorithms using the positive definite secant update Another convergence result for the positive definite secant method Other secant updates for unconstrained minimization Exercises 10. Nonlinear Least Squares. The nonlinear least-squares problem Gauss-Newton-type methods Full Newton-type methods Other considerations in solving nonlinear least-squares problems Exercises 11. Methods for Problems with Special Structure. The sparse finite-difference Newton method Sparse secant methods Deriving least-change secant updates Analyzing least-change secant methods Exercises Appendix A. A Modular System of Algorithms for Unconstrained Minimization and Nonlinear Equations (by Robert Schnabel) Appendix B. Test Problems (by Robert Schnabel) References Author Index Subject Index."
            },
            "slug": "Numerical-methods-for-unconstrained-optimization-Dennis-Schnabel",
            "title": {
                "fragments": [],
                "text": "Numerical methods for unconstrained optimization and nonlinear equations"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Newton's Method for Nonlinear Equations and Unconstrained Minimization and methods for solving nonlinear least-squares problems with Special Structure."
            },
            "venue": {
                "fragments": [],
                "text": "Prentice Hall series in computational mathematics"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4745449"
                        ],
                        "name": "D. Goldfarb",
                        "slug": "D.-Goldfarb",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Goldfarb",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Goldfarb"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144942152"
                        ],
                        "name": "J. Reid",
                        "slug": "J.-Reid",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Reid",
                            "middleNames": [
                                "Ker"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Reid"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 40451994,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d6f7f74433636245252a6006797126f79cb78574",
            "isKey": false,
            "numCitedBy": 208,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "It is shown that suitable recurrences may be used in order to implement in practice the steepest-edge simplex linear programming algorithm. In this algorithm each iteration is along an edge of the polytope of feasible solutions on which the objective function decreases most rapidly with respect to distance in the space of all the variables. Results of computer comparisons on medium-scale problems indicate that the resulting algorithm requires less iterations but about the same overall time as the algorithm of Harris [8], which may be regarded as approximating the steepest-edge algorithm. Both show a worthwhile advantage over the standard algorithm."
            },
            "slug": "A-practicable-steepest-edge-simplex-algorithm-Goldfarb-Reid",
            "title": {
                "fragments": [],
                "text": "A practicable steepest-edge simplex algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Results of computer comparisons on medium-scale problems indicate that the resulting algorithm requires less iterations but about the same overall time as the algorithm of Harris [8], which may be regarded as approximating the steepest-edge algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46190125"
                        ],
                        "name": "R. Schnabel",
                        "slug": "R.-Schnabel",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schnabel",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schnabel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704765"
                        ],
                        "name": "P. Frank",
                        "slug": "P.-Frank",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Frank",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Frank"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 119704128,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "c26cf41e14c33c9f6ab6fcd62fa8935f0a139045",
            "isKey": false,
            "numCitedBy": 137,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "A new class of methods for solving systems of nonlinear equations, called tensor methods, is introduced. Tensor methods are general purpose methods intended especially for problems where the Jacobian matrix at the solution is singular or ill-conditioned. They base each iteration on a quadratic model of the nonlinear function, the standard linear model augmented by a simple second order term. The second order term is selected so that the model interpolates function values from several previous iterations, as well as the current function value and Jacobian. The tensor method requires no more function and derivative information per iteration and hardly more storage or arithmetic per iteration, than a standard method based on Newton\u2019s method. In extensive computational tests, a tensor algorithm is significantly more efficient than a similar algorithm based on the standard linear model, both on standard nonsingular test problems and on problems where the Jacobian at the solution is singular."
            },
            "slug": "Tensor-Methods-for-Nonlinear-Equations.-Schnabel-Frank",
            "title": {
                "fragments": [],
                "text": "Tensor Methods for Nonlinear Equations."
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "In extensive computational tests, a tensor algorithm is significantly more efficient than a similar algorithm based on the standard linear model, both on standard nonsingular test problems and on problems where the Jacobian at the solution is singular."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145221576"
                        ],
                        "name": "H. Walker",
                        "slug": "H.-Walker",
                        "structuredName": {
                            "firstName": "Homer",
                            "lastName": "Walker",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Walker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 100
                            }
                        ],
                        "text": "For a comprehensive treatment of quasi-Newton methods see Dennis and Schnabel [92], Dennis and Mor\u00e9 [91], and Fletcher [101]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 33
                            }
                        ],
                        "text": "For details, see Dennis and Mor\u00e9 [91] or Dennis and Schnabel [92]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 118177162,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "1b074b4b415380ed3270bb828085124796429e53",
            "isKey": false,
            "numCitedBy": 126,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem to be solved is formulated precisely and the introduction of quasi-Newton methods is motivated by considering the classical Newton and secant methods and their properties. Three highly successful quasi-Newton methods are surveyed: Broyden's method for the solution of general nonlinear equations, and the Davidon-Fletcher-Powell and Broyden-Fletcher-Goldfarb-Shanno procedures for unconstrained minimization. Finally, the properties of these methods are compared to those of Newton's method and UHMLE in potential applications to maximum-likelihood estimation of parameters in mixture distributions."
            },
            "slug": "Quasi-Newton-Methods-Walker",
            "title": {
                "fragments": [],
                "text": "Quasi-Newton Methods"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2572541"
                        ],
                        "name": "T. Steihaug",
                        "slug": "T.-Steihaug",
                        "structuredName": {
                            "firstName": "Trond",
                            "lastName": "Steihaug",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Steihaug"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 32
                            }
                        ],
                        "text": "This algorithm, due to Steihaug [281], is specified below as Algorithm 7."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 851,
                                "start": 843
                            }
                        ],
                        "text": "21 Models for Trust-Region Methods 26 Scaling 27 Rates of Convergence 28 R-Rates of Convergence 29 Notes and References 30 Exercises 30\n3 Line Search Methods 34 3.1 Step Length 36\nThe Wolfe Conditions 37 The Goldstein Conditions 41 Sufficient Decrease and Backtracking 41\n3.2 Convergence of Line Search Methods 43 3.3 Rate of Convergence 46\nConvergence Rate of Steepest Descent 47 Quasi-Newton Methods 49 Newton's Method 51 Coordinate Descent Methods 53\n3.4 Step-Length Selection Algorithms 55 Interpolation 56 The Initial Step Length 58 A Line Search Algorithm for the Wolfe Conditions 58 Notes and References 61 Exercises 62\n4 Trust-Region Methods 64 Outline of the Algorithm 67\n4.1 The Cauchy Point and Related Algorithms 69 The Cauchy Point 69 Improving on the Cauchy Point 70 The Dogleg Method 71 Two-Dimensional Subspace Minimization 74 Steihaug's Approach 75 4.2 Using Nearly Exact Solutions to the Subproblem 77 Characterizing Exact Solutions 77 Calculating Nearly Exact Solutions 78 The Hard Case 82 Proof of Theorem 4.3 84 4.3 Global Convergence 87\nCONTENTS xiii\nReduction Obtained by the Cauchy Point 87 Convergence to Stationary Points 89 Convergence of Algorithms Based on Nearly Exact Solutions 93\n4.4 Other Enhancements 94 Scaling 94 Non-Euclidean Trust Regions 96 Notes and References 97 Exercises 97\nConjugate Gradient Methods 100 5.1 The Linear Conjugate Gradient Method 102\nConjugate Direction Methods 102 Basic Properties of the Conjugate Gradient Method 107 A Practical Form of the Conjugate Gradient Method I l l Rate of Convergence 112 Preconditioning 118 Practical Preconditioners 119\n5.2 Nonlinear Conjugate Gradient Methods 120 The Fletcher-Reeves Method 120 The Polak-Ribiere Method 121 Quadratic Termination and Restarts 122 Numerical Performance 124 Behavior of the Fletcher-Reeves Method 124 Global Convergence 127 Notes and References 131 Exercises 132\nPractical Newton'Methods 134 6.1 Inexact Newton Steps 136 6.2 Line Search Newton Methods 139\nLine Search Newton-CG Method 139 Modified Newton's Method 141\n6.3 Hessian Modifications ."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2832391,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0be514b583f7af1448f0873a1728ce658278527f",
            "isKey": true,
            "numCitedBy": 800,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Algorithms based on trust regions have been shown to be robust methods for unconstrained optimization problems. All existing methods, either based on the dogleg strategy or Hebden-More iterations, require solution of system of linear equations. In large scale optimization this may be prohibitively expensive. It is shown in this paper that an approximate solution of the trust region problem may be found by the preconditioned conjugate gradient method. This may be regarded as a generalized dogleg technique where we asymptotically take the inexact quasi-Newton step. We also show that we have the same convergence properties as existing methods based on the dogleg strategy using an approximate Hessian."
            },
            "slug": "The-Conjugate-Gradient-Method-and-Trust-Regions-in-Steihaug",
            "title": {
                "fragments": [],
                "text": "The Conjugate Gradient Method and Trust Regions in Large Scale Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is shown in this paper that an approximate solution of the trust region problem may be found by the preconditioned conjugate gradient method, and it is shown that the method has the same convergence properties as existing methods based on the dogleg strategy using an approximate Hessian."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1986103"
                        ],
                        "name": "R. Vanderbei",
                        "slug": "R.-Vanderbei",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Vanderbei",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Vanderbei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 119722553,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "640bb6a37dd6fe0b209373de4d5e32011f22d35a",
            "isKey": false,
            "numCitedBy": 1208,
            "numCiting": 123,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface. Part 1: Basic Theory - The Simplex Method and Duality. 1. Introduction. 2. The Simplex Method. 3. Degeneracy. 4. Efficiency of the Simplex Method. 5. Duality Theory. 6. The Simplex Method in Matrix Notation. 7. Sensitivity and Parametric Analyses. 8. Implementation Issues. 9. Problems in General Form. 10. Convex Analysis. 11. Game Theory. 12. Regression. Part 2: Network-Type Problems. 13. Network Flow Problems. 14. Applications. 15. Structural Optimization. Part 3: Interior-Point Methods. 16. The Central Path. 17. A Path-Following Method. 18. The KKT System. 19. Implementation Issues. 20. The Affine-Scaling Method. 21. The Homogeneous Self-Dual Method. Part 4: Extensions. 22. Integer Programming. 23. Quadratic Programming. 24. Convex Programming. Appendix A: Source Listings. Answers to Selected Exercises. Bibliography. Index."
            },
            "slug": "Linear-Programming:-Foundations-and-Extensions-Vanderbei",
            "title": {
                "fragments": [],
                "text": "Linear Programming: Foundations and Extensions"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The Simplex Method in Matrix Notation and Duality Theory, and Applications: Foundations of Convex Programming."
            },
            "venue": {
                "fragments": [],
                "text": "Kluwer international series in operations research and management service"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35074400"
                        ],
                        "name": "Jean Charles Gilbert",
                        "slug": "Jean-Charles-Gilbert",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Gilbert",
                            "middleNames": [
                                "Charles"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean Charles Gilbert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2784955"
                        ],
                        "name": "J. Nocedal",
                        "slug": "J.-Nocedal",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Nocedal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nocedal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 23
                            }
                        ],
                        "text": "For example, the paper [123] describes a problem with n 100 in which cos \u03b8k is of order 10\u22122 for hundreds of iterations and the steps \u2016xk \u2212 xk\u22121\u2016 are of order 10\u22122."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 37
                            }
                        ],
                        "text": "1 are taken from Gilbert and Nocedal [123]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 6
                            }
                        ],
                        "text": "Jorge Nocedal Stephen J. Wright\nNumerical Optimization\nWith 85 Illustrations\n, Springer\nContents\nPreface vii\n1 Introduction, 1 Mathematical Formulation 2 Example: A Transportation Problem 4 Continuous versus Discrete Optimization 4 Constrained and Unconstrained Optimization 6 Global and Local Optimization 6 Stochastic and Deterministic Optimization 7 Optimization Algorithms 7 Convexity 8\nNotes and References 9\n2 Fundamentals of Unconstrained Optimization 10 2.1 What Is a Solution?"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 133
                            }
                        ],
                        "text": "1 Iterations and function/gradient evaluations required by three nonlinear conjugate gradient methods on a set of test problems; see [123] Alg FR Alg PR Alg PR+ Problem n it/f-g it/f-g it/f-g mod CALCVAR3 200 2808/5617 2631/5263 2631/5263 0 GENROS 500 \u2217 1068/2151 1067/2149 1 XPOWSING 1000 533/1102 212/473 97/229 3 TRIDIA1 1000 264/531 262/527 262/527 0 MSQRT1 1000 422/849 113/231 113/231 0 XPOWELL 1000 568/1175 212/473 97/229 3 TRIGON 1000 231/467 40/92 40/92 0"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 142
                            }
                        ],
                        "text": "The global convergence of nonlinear conjugate gradient methods has received much attention; see for example Al-Baali [3], Gilbert and Nocedal [123], Dai and Yuan [85], and Hager and Zhang [161]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15235695,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "08b20241e0805724f3a5f46b4fe3b54581be9449",
            "isKey": false,
            "numCitedBy": 926,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper explores the convergence of nonlinear conjugate gradient methods without restarts, and with practical line searches. The analysis covers two classes of methods that are globally convergent on smooth, nonconvex functions. Some properties of the Fletcher\u2013Reeves method play an important role in the first family, whereas the second family shares an important property with the Polak\u2013Ribiere method. Numerical experiments are presented."
            },
            "slug": "Global-Convergence-Properties-of-Conjugate-Gradient-Gilbert-Nocedal",
            "title": {
                "fragments": [],
                "text": "Global Convergence Properties of Conjugate Gradient Methods for Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "This paper explores the convergence of nonlinear conjugate gradient methods without restarts, and with practical line searches, covering two classes of methods that are globally convergent on smooth, nonconvex functions."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Optim."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10419577"
                        ],
                        "name": "D. Gabay",
                        "slug": "D.-Gabay",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Gabay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gabay"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 116733473,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "73f91db12ac9e28dc298e077dc82384e6a641bf5",
            "isKey": false,
            "numCitedBy": 97,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a globally and superlinearly converging method for equality-constrained optimization requiring the updating of a reduced size matrix approximating a restriction of the Hessian of the Lagrangian. Each iterate is obtained by a search along a simple curve defined by a quasi-Newton direction and a feasibility improving direction; an exact penalty function is used to determine the stepsize. The method can be viewed as an efficient approximation to the quasi-Newton along geodesics of [1] where feasibility was enforced at each step. Its relation with multiplier methods and recursive quadratic programming methods is also investigated."
            },
            "slug": "Reduced-quasi-Newton-methods-with-feasibility-for-Gabay",
            "title": {
                "fragments": [],
                "text": "Reduced quasi-Newton methods with feasibility improvement for nonlinearly constrained optimization"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744288"
                        ],
                        "name": "P. Gill",
                        "slug": "P.-Gill",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Gill",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Gill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143873253"
                        ],
                        "name": "W. Murray",
                        "slug": "W.-Murray",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Murray",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Murray"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685738"
                        ],
                        "name": "M. H. Wright",
                        "slug": "M.-H.-Wright",
                        "structuredName": {
                            "firstName": "Margaret",
                            "lastName": "Wright",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. H. Wright"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 25
                            }
                        ],
                        "text": "Jorge Nocedal Stephen J. Wright\nNumerical Optimization\nWith 85 Illustrations\n, Springer\nContents\nPreface vii\n1 Introduction, 1 Mathematical Formulation 2 Example: A Transportation Problem 4 Continuous versus Discrete Optimization 4 Constrained and Unconstrained Optimization 6 Global and Local Optimization 6 Stochastic and Deterministic Optimization 7 Optimization Algorithms 7 Convexity 8\nNotes and References 9\n2 Fundamentals of Unconstrained Optimization 10 2.1 What Is a Solution?"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 116
                            }
                        ],
                        "text": "These observations are the basis of the modified Cholesky algorithm described in detail in Gill, Murray, and Wright [130], which introduces symmetric interchanges of rows and columns to try to reduce the size of the modification."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 98
                            }
                        ],
                        "text": "For a more thorough treatment of the modified Cholesky factorization see Gill, Murray, and Wright [130] or Dennis and Schnabel [92]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 20611582,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b8d9abd1c078573188b13d36c1b1efb7cb2fa865",
            "isKey": true,
            "numCitedBy": 7627,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Practical Optimization MethodsFree eBook: Practical Aspects of Structural Optimization [1701.01450] Practical optimization for hybrid quantum Practical Optimization | 4e70c9cf5faf796993a61adc9f46f2c5Acces PDF Practical OptimizationPractical Bayesian Optimization of Machine Learning Particle Swarm Optimization (PSO) An Overview Practical Issues Optimization Algorithms in Physics Practical Mathematical Optimization Universit T BremenA Practical Price Optimization Approach for Omnichannel A Gentle Introduction to Stochastic Optimization AlgorithmsApplied Sciences | Free Full-Text | Evolutionary 0387986316 Practical Optimization Methods: with A Lecture on Model Predictive ControlPractical Optimization : Algorithms and Engineering Wiley Series in Discrete Mathematics and Optimization Ser PRACTICAL OPTIMIZATION uCozEvolutionary practical optimization | DeepDyveA Practical Guide To Hyperparameter Optimization.Blood platelet production: a novel approach for practical [PDF] Practical Bilevel Optimization Download and Read Stability and Sample-based Approximations of Composite Practical portfolio optimization in Python (2/3) machine (PDF) Practical Financial Optimization. Decision making A Multiobjective Optimization Model for Prevention and Particle swarm optimization WikipediaPractical Methods Of Optimization|RPractical Portfolio Optimization London Business SchoolBao: Making Learned Query Optimization PracticalApache Spark Core Practical Optimization DatabricksPractical Methods of Optimization by R. FletcherChapter 11 Nonlinear Optimization Examples4.7 Applied Optimization Problems \u2013 Calculus Volume 1Practical bayesian optimization using Goptuna | by Masashi Practical Optimization Methods For 4th Generation Cellular Facility location problems \u2014 Mathematical Optimization Practical optimization (2004 edition) | Open Library[J726.Ebook] PDF Download Practical Optimization of Multi-objective Exploration for Practical Optimization Practical Optimization: a Gentle Introduction has moved!?Practical Rod Pumping Optimization on Apple Books(PDF) Practical Optimization with MATLAB The Free StudyPractical portfolio optimization in Python (3/3) code (PDF) Practical, Fast and Robust Point Cloud Registration Numerical Optimization Stanford UniversityPractical Optimization Methods with Mathematica ApplicationsPractical Optimization | 4e70c9cf5faf796993a61adc9f46f2c5Search Engine Optimization: Practical Marketing TechniquesLagout.orgMeter Placement in Active Distribution System using Manual: Practical guide to optimization for mobiles Unity"
            },
            "slug": "Practical-optimization-Gill-Murray",
            "title": {
                "fragments": [],
                "text": "Practical optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This ebook Practical Optimization by Philip E. Gill is presented in pdf format and the full version of this ebook in DjVu, ePub, doc, txt, PDF forms is presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2737945"
                        ],
                        "name": "H. Akaike",
                        "slug": "H.-Akaike",
                        "structuredName": {
                            "firstName": "Hirotugu",
                            "lastName": "Akaike",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Akaike"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 111
                            }
                        ],
                        "text": "It is easy to show that (i) and (ii) both hold for \u03c4 \u2208 [0, 1], so we restrict our attention to the case of \u03c4 \u2208 [1, 2]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 7
                            }
                        ],
                        "text": "Akaike [2] presents a probabilistic analysis of the steepest descent method with exact line searches on quadratic functions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 189767421,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "044853dffabdd9cba56521c33eb2f23ea1273314",
            "isKey": false,
            "numCitedBy": 149,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we define a type of transformation of probability distribution and analyze the limiting behavior of the result of successive applications of the transformation to some initial probability distribution. By using the results of this analysis we can get a fairly general insight into the so-called optimum-gradient method in numerical analysis. We can prove the conjecture which was stated by Forsythe and Motzkin [7] and was used as the logical basis of an acceleration procedure for the optimum gradient method [4][5][6]. It was stated by Forsythe [4] that this conjecture seems to be hard to prove as the related transformation is rather complicated. But our present proof is rather simple. Further, we can see the relation between the condition-number of the related matrix and the convergence rate of the optimum gradient method. By using the relation which according to [5] is first proved by Kantrovich [8], we can say that when the matrix is ill-conditioned the convergence rate tends near to its worst possible value. Using the same data as those treated by Forsythes in paper [5], we give some numerical examples."
            },
            "slug": "On-a-successive-transformation-of-probability-and-Akaike",
            "title": {
                "fragments": [],
                "text": "On a successive transformation of probability distribution and its application to the analysis of the optimum gradient method"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The conjecture which was stated by Forsythe and Motzkin and was used as the logical basis of an acceleration procedure for the optimum gradient method is proved and it is said that when the matrix is ill-conditioned the convergence rate tends near to its worst possible value."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1959
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2129662"
                        ],
                        "name": "F. Rendl",
                        "slug": "F.-Rendl",
                        "structuredName": {
                            "firstName": "Franz",
                            "lastName": "Rendl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Rendl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717322"
                        ],
                        "name": "H. Wolkowicz",
                        "slug": "H.-Wolkowicz",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Wolkowicz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Wolkowicz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 129
                            }
                        ],
                        "text": "Other iterative methods for the solution of a trust-region problem have been proposed by Hager [160], and by Rendl and Wolkowicz [263]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206819668,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dfb80778025db49d252b1617db3c7839aa85e934",
            "isKey": false,
            "numCitedBy": 225,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": "Primal-dual pairs of semidefinite programs provide a general framework for the theory and algorithms for the trust region subproblem (TRS). This latter problem consists in minimizing a general quadratic function subject to a convex quadratic constraint and, therefore, it is a generalization of the minimum eigenvalue problem. The importance of (TRS) is due to the fact that it provides the step in trust region minimization algorithms. The semidefinite framework is studied as an interesting instance of semidefinite programming as well as a tool for viewing known algorithms and deriving new algorithms for (TRS). In particular, a dual simplex type method is studied that solves (TRS) as a parametric eigenvalue problem. This method uses the Lanczos algorithm for the smallest eigenvalue as a black box. Therefore, the essential cost of the algorithm is the matrix-vector multiplication and, thus, sparsity can be exploited. A primal simplex type method provides steps for the so-called hard case. Extensive numerical tests for large sparse problems are discussed. These tests show that the cost of the algorithm is 1 +\u03b1(n) times the cost of finding a minimum eigenvalue using the Lanczos algorithm, where 0"
            },
            "slug": "A-semidefinite-framework-for-trust-region-with-to-Rendl-Wolkowicz",
            "title": {
                "fragments": [],
                "text": "A semidefinite framework for trust region subproblems with applications to large scale minimization"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A dual simplex type method is studied that solves (TRS) as a parametric eigenvalue problem and the essential cost of the algorithm is the matrix-vector multiplication and, thus, sparsity can be exploited."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1757948"
                        ],
                        "name": "W. Davidon",
                        "slug": "W.-Davidon",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Davidon",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Davidon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 56
                            }
                        ],
                        "text": "An interesting historical irony is that Davidon\u2019s paper [87] was not accepted for publication; it remained as a technical report for more than thirty years until it appeared in the first issue of the SIAM Journal on Optimization in 1991 [88]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1819475,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "04a230c976e22278f01e024310bbfc8627cde139",
            "isKey": false,
            "numCitedBy": 802,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "This is a method for determining numerically local minima of differentiable functions of several variables. In the process of locating each minimum, a matrix which characterizes the behavior of the function about the minimum is determined. For a region in which the function depends quadratically on the variables, no more than N iterations are required, where N is the number of variables. By suitable choice of starting values, and without modification of the procedure, linear constraints can be imposed upon the variables."
            },
            "slug": "Variable-Metric-Method-for-Minimization-Davidon",
            "title": {
                "fragments": [],
                "text": "Variable Metric Method for Minimization"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This is a method for determining numerically local minima of differentiable functions of several variables by suitable choice of starting values, and without modification of the procedure, linear constraints can be imposed upon the variables."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Optim."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800149"
                        ],
                        "name": "J. Dennis",
                        "slug": "J.-Dennis",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Dennis",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dennis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144241870"
                        ],
                        "name": "J. J. Mor\u00e9",
                        "slug": "J.-J.-Mor\u00e9",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Mor\u00e9",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. J. Mor\u00e9"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 122193620,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b7a080b5039b50d347d3492a056be96099294067",
            "isKey": false,
            "numCitedBy": 1355,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper is an attempt to motivate and justify quasi-Newton methods as useful modifications of Newton''s method for general and gradient nonlinear systems of equations. References are given to ample numerical justification; here we give an overview of many of the important theoretical results and each is accompanied by sufficient discussion to make the results and hence the methods plausible. Key Words and Phrases: unconstrained minimization, nonlinear simultaneous equations, update methods, quasi-Newton methods."
            },
            "slug": "Quasi-Newton-Methods,-Motivation-and-Theory-Dennis-Mor\u00e9",
            "title": {
                "fragments": [],
                "text": "Quasi-Newton Methods, Motivation and Theory"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153516415"
                        ],
                        "name": "C. T. Kelley",
                        "slug": "C.-T.-Kelley",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Kelley",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. T. Kelley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14643628,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "93ffe3a7f1f182df48e4bd680c945d4c8a4c4570",
            "isKey": false,
            "numCitedBy": 223,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "The Nelder--Mead algorithm can stagnate and converge to a nonoptimal point, even for very simple problems. In this note we propose a test for sufficient decrease which, if passed for all iterations, will guarantee convergence of the Nelder--Mead iteration to a stationary point if the objective function is smooth and the diameters of the Nelder--Mead simplices converge to zero. Failure of this condition is an indicator of potential stagnation. As a remedy we propose a new step, which we call an oriented restart, that reinitializes the simplex to a smaller one with orthogonal edges whose orientation is determined by an approximate descent direction from the current best point. We also give results that apply when the objective function is a low-amplitude perturbation of a smooth function. We illustrate our results with some numerical examples."
            },
            "slug": "Detection-and-Remediation-of-Stagnation-in-the-a-Kelley",
            "title": {
                "fragments": [],
                "text": "Detection and Remediation of Stagnation in the Nelder--Mead Algorithm Using a Sufficient Decrease Condition"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A test for sufficient decrease is proposed which, if passed for all iterations, will guarantee convergence of the Nelder--Mead iteration to a stationary point if the objective function is smooth and the diameters of theNelder-Mead simplices converge to zero."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Optim."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2080288"
                        ],
                        "name": "C. Paige",
                        "slug": "C.-Paige",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Paige",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Paige"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145621255"
                        ],
                        "name": "M. Saunders",
                        "slug": "M.-Saunders",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Saunders",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Saunders"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 21774,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "30cadff20998ea7bea6da42fa0eed48334fdde1e",
            "isKey": false,
            "numCitedBy": 3906,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "An iterative method is given for solving Ax ~ffi b and minU Ax b 112, where the matrix A is large and sparse. The method is based on the bidiagonalization procedure of Golub and Kahan. It is analytically equivalent to the standard method of conjugate gradients, but possesses more favorable numerical properties. Reliable stopping criteria are derived, along with estimates of standard errors for x and the condition number of A. These are used in the FORTRAN implementation of the method, subroutine LSQR. Numerical tests are described comparing I~QR with several other conjugate-gradient algorithms, indicating that I~QR is the most reliable algorithm when A is ill-conditioned."
            },
            "slug": "LSQR:-An-Algorithm-for-Sparse-Linear-Equations-and-Paige-Saunders",
            "title": {
                "fragments": [],
                "text": "LSQR: An Algorithm for Sparse Linear Equations and Sparse Least Squares"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Numerical tests are described comparing I~QR with several other conjugate-gradient algorithms, indicating that I ~QR is the most reliable algorithm when A is ill-conditioned."
            },
            "venue": {
                "fragments": [],
                "text": "TOMS"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144448267"
                        ],
                        "name": "R. Byrd",
                        "slug": "R.-Byrd",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Byrd",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Byrd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2784955"
                        ],
                        "name": "J. Nocedal",
                        "slug": "J.-Nocedal",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Nocedal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nocedal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790701"
                        ],
                        "name": "Ya-Xiang Yuan",
                        "slug": "Ya-Xiang-Yuan",
                        "structuredName": {
                            "firstName": "Ya-Xiang",
                            "lastName": "Yuan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ya-Xiang Yuan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 101
                            }
                        ],
                        "text": "This result was extended to the restricted Broyden class, except for DFP, by Byrd, Nocedal, and Yuan [53]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18652781,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8e89c10d859dcb900c27fb1ab1f15dfb4658c45b",
            "isKey": false,
            "numCitedBy": 308,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the global convergence properties of the restricted Broyden class of quasi-Newton methods, when applied to a convex objective function. We assume that the line search satisfies a standard sufficient decrease condition and that the initial Hessian approximation is any positive definite matrix. We show global and superlinear convergence for this class of methods, except for DFP. This generalizes Powell\u2019s well-known result for the BFGS method. The analysis gives us insight into the properties of these algorithms; in particular it shows that DFP lacks a very desirable self-correcting property possessed by BFGS."
            },
            "slug": "Global-Convergence-of-a-Cass-of-Quasi-Newton-on-Byrd-Nocedal",
            "title": {
                "fragments": [],
                "text": "Global Convergence of a Cass of Quasi-Newton Methods on Convex Problems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725965"
                        ],
                        "name": "L. Luksan",
                        "slug": "L.-Luksan",
                        "structuredName": {
                            "firstName": "Ladislav",
                            "lastName": "Luksan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Luksan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31201691"
                        ],
                        "name": "J. Vl\u010dek",
                        "slug": "J.-Vl\u010dek",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Vl\u010dek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Vl\u010dek"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 27513209,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "26b08aaa96fd91572f302ef250198b28142c5fa0",
            "isKey": false,
            "numCitedBy": 103,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "An inexact Newton algorithm for large sparse equality constrained non-linear programming problems is proposed. This algorithm is based on an indefinitely preconditioned smoothed conjugate gradient method applied to the linear KKT system and uses a simple augmented Lagrangian merit function for Armijo type stepsize selection. Most attention is devoted to the termination of the CG method, guaranteeing sufficient descent in every iteration and decreasing the number of required CG iterations, and especially, to the choice of a suitable preconditioner. We investigate four preconditioners, which have 2 \u00d7 2 block structure, and prove theoretically their good properties. The efficiency of the inexact Newton algorithm, together with a comparison of various preconditioners and strategies, is demonstrated by using a large collection of test problems. \u00a9 1998 John Wiley & Sons, Ltd."
            },
            "slug": "Indefinitely-preconditioned-inexact-Newton-method-Luksan-Vl\u010dek",
            "title": {
                "fragments": [],
                "text": "Indefinitely preconditioned inexact Newton method for large sparse equality constrained non-linear programming problems"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "An inexact Newton algorithm for large sparse equality constrained non-linear programming problems is proposed, based on an indefinitely preconditioned smoothed conjugate gradient method applied to the linear KKT system and uses a simple augmented Lagrangian merit function for Armijo type stepsize selection."
            },
            "venue": {
                "fragments": [],
                "text": "Numer. Linear Algebra Appl."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144241870"
                        ],
                        "name": "J. J. Mor\u00e9",
                        "slug": "J.-J.-Mor\u00e9",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Mor\u00e9",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. J. Mor\u00e9"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2150172"
                        ],
                        "name": "D. Sorensen",
                        "slug": "D.-Sorensen",
                        "structuredName": {
                            "firstName": "Danny",
                            "lastName": "Sorensen",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Sorensen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 222
                            }
                        ],
                        "text": "Another strategy for implementing a line search Newton method when the Hessian contains negative eigenvalues is to compute a direction of negative curvature and use it to define the search direction (see Mor\u00e9 and Sorensen [213] and Goldfarb [132])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 67
                            }
                        ],
                        "text": "Some line search methods (see Goldfarb [132] and Mor\u00e9 and Sorensen [213]) compute a direction of negative curvature, whenever it exists, to prevent the iteration from converging to nonminimizing stationary points."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 31045880,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "71ad4eb5160fc183c3d8d488ab6c84959cb1a400",
            "isKey": false,
            "numCitedBy": 170,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a modified Newton method for the unconstrained minimization problem. The modification occurs in non-convex regions where the information contained in the negative eigenvalues of the Hessian is taken into account by performing a line search along a path which is initially tangent to a direction of negative curvature. We give termination criteria for the line search and prove that the resulting iterates are guaranteed to converge, under reasonable conditions, to a critical point at which the Hessian is positive semidefinite. We also show how the Bunch and Parlett decomposition of a symmetric indefinite matrix can be used to give entirely adequate directions of negative curvature."
            },
            "slug": "On-the-use-of-directions-of-negative-curvature-in-a-Mor\u00e9-Sorensen",
            "title": {
                "fragments": [],
                "text": "On the use of directions of negative curvature in a modified newton method"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "A modified Newton method for the unconstrained minimization problem is presented and it is shown how the Bunch and Parlett decomposition of a symmetric indefinite matrix can be used to give entirely adequate directions of negative curvature."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145323121"
                        ],
                        "name": "J. Nelder",
                        "slug": "J.-Nelder",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Nelder",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nelder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189443"
                        ],
                        "name": "R. Mead",
                        "slug": "R.-Mead",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Mead",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mead"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2208295,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "017ddb7e815236defd0566bc46f6ed8401cc6ba6",
            "isKey": false,
            "numCitedBy": 25605,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "A method is described for the minimization of a function of n variables, which depends on the comparison of function values at the (n 41) vertices of a general simplex, followed by the replacement of the vertex with the highest value by another point. The simplex adapts itself to the local landscape, and contracts on to the final minimum. The method is shown to be effective and computationally compact. A procedure is given for the estimation of the Hessian matrix in the neighbourhood of the minimum, needed in statistical estimation problems."
            },
            "slug": "A-Simplex-Method-for-Function-Minimization-Nelder-Mead",
            "title": {
                "fragments": [],
                "text": "A Simplex Method for Function Minimization"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A method is described for the minimization of a function of n variables, which depends on the comparison of function values at the (n 41) vertices of a general simplex, followed by the replacement of the vertex with the highest value by another point."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. J."
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678834"
                        ],
                        "name": "G. Newsam",
                        "slug": "G.-Newsam",
                        "structuredName": {
                            "firstName": "Garry",
                            "lastName": "Newsam",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Newsam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14445233"
                        ],
                        "name": "John D. Ramsdell",
                        "slug": "John-D.-Ramsdell",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Ramsdell",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John D. Ramsdell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 25
                            }
                        ],
                        "text": "Curtis, Powell, and Reid [83] and Coleman and Mor\u00e9 [68] provide descriptions of some methods and performance comparisons."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 121310834,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "ddbcc017d98c0f2fcaf4ce167d8ed280526f9412",
            "isKey": false,
            "numCitedBy": 176,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "When finding a numerical solution to a system of nonlinear equations, one often estimates the Jacobian by finite differences. Curtis, Powell and Reid [J. Inst. Math. Applics.,13 (1974), pp. 117\u2013119] presented an algorithm that reduces the number of function evaluations required to estimate the Jacobian by taking advantage of sparsity. We show that the problem of finding the best of the Curtis, Powell and Reid type algorithms is NP-complete, and then propose two procedures for estimating the Jacobian that may use fewer function evaluations."
            },
            "slug": "Estimation-of-Sparse-Jacobian-Matrices-Newsam-Ramsdell",
            "title": {
                "fragments": [],
                "text": "Estimation of Sparse Jacobian Matrices"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that the problem of finding the best of the Curtis, Powell and Reid type algorithms is NP-complete, and then two procedures for estimating the Jacobian that may use fewer function evaluations are proposed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145983059"
                        ],
                        "name": "A. Griewank",
                        "slug": "A.-Griewank",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Griewank",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Griewank"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731876"
                        ],
                        "name": "D. Juedes",
                        "slug": "D.-Juedes",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Juedes",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Juedes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2711744"
                        ],
                        "name": "J. Utke",
                        "slug": "J.-Utke",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Utke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Utke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7339428,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2b6adbad7d8f4c5f1ade291264df221e1b7e22eb",
            "isKey": false,
            "numCitedBy": 803,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "The C++ package ADOL-C described here facilitates the evaluation of first and higher derivatives of vector functions that are defined by computer programs written in C or C++. The resulting derivative evaluation routines may be called from C/C++, Fortran, or any other language that can be linked with C. The numerical values of derivative vectors are obtained free of truncation errors at a small multiple of the run-time and randomly accessed memory of the given function evaluation program. Derivative matrices are obtained by columns or rows. For solution curves defined by ordinary differential equations, special routines are provided that evaluate the Taylor coefficient vectors and their Jacobians with respect to the current state vector. The derivative calculations involve a possibly substantial (but always predictable) amount of data that are accessed strictly sequentially and are therefore automatically paged out to external files."
            },
            "slug": "Algorithm-755:-ADOL-C:-a-package-for-the-automatic-Griewank-Juedes",
            "title": {
                "fragments": [],
                "text": "Algorithm 755: ADOL-C: a package for the automatic differentiation of algorithms written in C/C++"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The C++ package ADOL-C described here facilitates the evaluation of first and higher derivatives of vector functions that are defined by computer programs written in C or C++."
            },
            "venue": {
                "fragments": [],
                "text": "TOMS"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2760537"
                        ],
                        "name": "J. Stoer",
                        "slug": "J.-Stoer",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Stoer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Stoer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 131
                            }
                        ],
                        "text": "He also summarizes rate-of-convergence results for asymptotically exact line searches, such as those obtained by Baptist and Stoer [11] and Stoer [282]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 115696066,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f458ad59f5add61416de82db204caff7098abc62",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "SummaryMany algorithms for solving minimization problems of the form\n$$\\mathop {\\min }\\limits_{x \\in R^n } f(x) = f(\\bar x),f:R^n \\to R,$$\n are devised such that they terminate with the optimal solution\n$$\\bar x$$\n within at mostn steps, when applied to the minimization of strictly convex quadratic functionsf onRn. In this paper general conditions are given, which together with the quadratic termination property, will ensure that the algorithm locally converges at leastn-step quadratically to a local minimum\n$$\\bar x$$\n for sufficiently smooth nonquadratic functionsf. These conditions apply to most algorithms with the quadratic termination property."
            },
            "slug": "On-the-relation-between-quadratic-termination-and-Stoer",
            "title": {
                "fragments": [],
                "text": "On the relation between quadratic termination and convergence properties of minimization algorithms"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33308287"
                        ],
                        "name": "John J. H. Forrest",
                        "slug": "John-J.-H.-Forrest",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Forrest",
                            "middleNames": [
                                "J.",
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John J. H. Forrest"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4745449"
                        ],
                        "name": "D. Goldfarb",
                        "slug": "D.-Goldfarb",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Goldfarb",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Goldfarb"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 25000105,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4bc20f5000bb471455924b176887326f2cba015e",
            "isKey": false,
            "numCitedBy": 216,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We present several new steepest-edge simplex algorithms for solving linear programming problems, including variants of both the primal and the dual simplex method. These algorithms differ depending upon the space in which the problem is viewed as residing, and include variants in which this space varies dynamically. We present computational results comparing steepest-edge simplex algorithms and approximate versions of them against simplex algorithms that use standard pivoting rules on truly large-scale realworld linear programs with as many as tens of thousands of rows and columns. These results demonstrate unambiguously the superiority of steepest-edge pivot selection criteria to other pivot selection criteria in the simplex method."
            },
            "slug": "Steepest-edge-simplex-algorithms-for-linear-Forrest-Goldfarb",
            "title": {
                "fragments": [],
                "text": "Steepest-edge simplex algorithms for linear programming"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "This work presents several new steepest-edge simplex algorithms for solving linear programming problems, including variants of both the primal and the dual simplex method."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144731788"
                        ],
                        "name": "Stephen J. Wright",
                        "slug": "Stephen-J.-Wright",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Wright",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen J. Wright"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2947306,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0eca0a4af617b33c2665bf9582894478dc98ad96",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate a modified Cholesky algorithm typical of those used in most interior-point codes for linear programming. Cholesky-based interior-point codes are popular for three reasons: their implementation requires only minimal changes to standard sparse Cholesky algorithms (allowing us to take full advantage of software written by specialists in that area); they tend to be more efficient than competing approaches that use alternative factorizations; and they perform robustly on most practical problems, yielding good interior-point steps even when the coefficient matrix of the main linear system to be solved for the step components is ill conditioned. We investigate this surprisingly robust performance by using analytical tools from matrix perturbation theory and error analysis, illustrating our results with computational experiments. Finally, we point out the potential limitations of this approach."
            },
            "slug": "Modified-Cholesky-Factorizations-in-Interior-Point-Wright",
            "title": {
                "fragments": [],
                "text": "Modified Cholesky Factorizations in Interior-Point Algorithms for Linear Programming"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "A modified Cholesky algorithm typical of those used in most interior-point codes for linear programming is investigated, yielding good interior- point steps even when the coefficient matrix of the main linear system to be solved for the step components is ill conditioned."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Optim."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2294095"
                        ],
                        "name": "M. Powell",
                        "slug": "M.-Powell",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Powell",
                            "middleNames": [
                                "J.",
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Powell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 124553534,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "b9a0842bc07fd6a89ccbf5e25847b73dbe8e6355",
            "isKey": false,
            "numCitedBy": 373,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the global convergence of conjugate gradient methods without restarts, assuming exact arithmetic and exact line searches, when the objective function is twice continuously differentiable and has bounded level sets. Most of our attention is given to the Polak-Ribiere algorithm, and unfortunately we find examples that show that the calculated gradients can remain bounded away from zero. The examples that have only two variables show also that some variable metric algorithms for unconstrained optimization need not converge. However, a global convergence theorem is proved for the Fletcher-Reeves version of the conjugate gradient method."
            },
            "slug": "Nonconvex-minimization-calculations-and-the-method-Powell",
            "title": {
                "fragments": [],
                "text": "Nonconvex minimization calculations and the conjugate gradient method"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "This work considers the global convergence of conjugate gradient methods without restarts, assuming exact arithmetic and exact line searches, when the objective function is twice continuously differentiable and has bounded level sets."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39515755"
                        ],
                        "name": "R. Chamberlain",
                        "slug": "R.-Chamberlain",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Chamberlain",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Chamberlain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2294095"
                        ],
                        "name": "M. Powell",
                        "slug": "M.-Powell",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Powell",
                            "middleNames": [
                                "J.",
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Powell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8773884"
                        ],
                        "name": "C. Lemar\u00e9chal",
                        "slug": "C.-Lemar\u00e9chal",
                        "structuredName": {
                            "firstName": "Claude",
                            "lastName": "Lemar\u00e9chal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lemar\u00e9chal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065948711"
                        ],
                        "name": "H. C. Pedersen",
                        "slug": "H.-C.-Pedersen",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Pedersen",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. C. Pedersen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 116646131,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b83f804c04800bdc10cb958b349b59115b20e523",
            "isKey": false,
            "numCitedBy": 318,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "The watchdog technique is an extension to iterative optimization algorithms that use line searches. The purpose is to allow some iterations to choose step-lengths that are much longer than those that would be allowed normally by the line search objective function. Reasons for using the technique are that it can give large gains in efficiency when a sequence of steps has to follow a curved constraint boundary, and it provides some highly useful algorithms with a Q-superlinear rate of convergence. The watchdog technique is described and discussed, and some global and Q-superlinear convergence properties are proved."
            },
            "slug": "The-watchdog-technique-for-forcing-convergence-in-Chamberlain-Powell",
            "title": {
                "fragments": [],
                "text": "The watchdog technique for forcing convergence in algorithms for constrained optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "The watchdog technique is described and discussed, and some global and Q-superlinear convergence properties are proved."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50390637"
                        ],
                        "name": "S. M. Robinson",
                        "slug": "S.-M.-Robinson",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Robinson",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. M. Robinson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 206799556,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "aa5adcda72d23d182da7bf63903129f273035090",
            "isKey": false,
            "numCitedBy": 206,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an algorithm for solving nonlinearly constrained nonlinear programming problems. The algorithm reduces the original problem to a sequence of linearly-constrained minimization problems, for which efficient algorithms are available. A convergence theorem is given which states that if the process is started sufficiently close to a strict second-order Kuhn\u2014Tucker point, then the sequence produced by the algorithm exists and convergesR-quadratically to that point."
            },
            "slug": "A-quadratically-convergent-algorithm-for-general-Robinson",
            "title": {
                "fragments": [],
                "text": "A quadratically-convergent algorithm for general nonlinear programming problems"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "The algorithm reduces the original problem to a sequence of linearly-constrained minimization problems, for which efficient algorithms are available and a convergence theorem is given which states that if the process is started sufficiently close to a strict second-order Kuhn\u2014Tucker point, then the sequence produced by the algorithm exists and convergesR-quadratically to that point."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32806689"
                        ],
                        "name": "S. Sinha",
                        "slug": "S.-Sinha",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Sinha",
                            "middleNames": [
                                "M."
                            ],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sinha"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121547267,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "dd47c5517c80155d14620c2001a330528ba7d1bc",
            "isKey": false,
            "numCitedBy": 377,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "One reasonable formulation of stochastic linear programming problem leads to a deterministic nonlinear programming problem where the non-linearity occurs in the objective function as the sum of square roots of positive semi-definite quadratic forms. It may, however, be difficult to solve this problem directly because of the nondifferentiability of the terms in the objective function. The present paper establishes a dual to the nonlinear programming problem of which a solution may be easily obtained. A solution of the dual then helps to obtain a solution of the original problem."
            },
            "slug": "A-Duality-Theorem-for-Nonlinear-Programming-Sinha",
            "title": {
                "fragments": [],
                "text": "A Duality Theorem for Nonlinear Programming"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1966
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50390637"
                        ],
                        "name": "S. M. Robinson",
                        "slug": "S.-M.-Robinson",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Robinson",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. M. Robinson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118996919,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b6e72e85e6bbb6984c6f23d1e6a39189c0b8df8c",
            "isKey": false,
            "numCitedBy": 342,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We prove that if the second-order sufficient condition and constraint regularity hold at a local minimizer of a nonlinear programming problem, then for sufficiently smooth perturbations of the constraints and objective function the set of local stationary points is nonempty and continuous; further, if certain polyhedrality assumptions hold (as is usually the case in applications), then the local minimizers, the stationary points and the multipliers all obey a type of Lipschitz condition."
            },
            "slug": "Generalized-equations-and-their-solutions,-part-II:-Robinson",
            "title": {
                "fragments": [],
                "text": "Generalized equations and their solutions, part II: Applications to nonlinear programming"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40639678"
                        ],
                        "name": "Yuhong Dai",
                        "slug": "Yuhong-Dai",
                        "structuredName": {
                            "firstName": "Yuhong",
                            "lastName": "Dai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuhong Dai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790701"
                        ],
                        "name": "Ya-Xiang Yuan",
                        "slug": "Ya-Xiang-Yuan",
                        "structuredName": {
                            "firstName": "Ya-Xiang",
                            "lastName": "Yuan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ya-Xiang Yuan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 162
                            }
                        ],
                        "text": "The global convergence of nonlinear conjugate gradient methods has received much attention; see for example Al-Baali [3], Gilbert and Nocedal [123], Dai and Yuan [85], and Hager and Zhang [161]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8313029,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "cd0a4bfa312a8f7e091d431d37197e5a7a7f20ad",
            "isKey": false,
            "numCitedBy": 934,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Conjugate gradient methods are widely used for unconstrained optimization, especially large scale problems. The strong Wolfe conditions are usually used in the analyses and implementations of conjugate gradient methods. This paper presents a new version of the conjugate gradient method, which converges globally, provided the line search satisfies the standard Wolfe conditions. The conditions on the objective function are also weak, being similar to those required by the Zoutendijk condition."
            },
            "slug": "A-Nonlinear-Conjugate-Gradient-Method-with-a-Strong-Dai-Yuan",
            "title": {
                "fragments": [],
                "text": "A Nonlinear Conjugate Gradient Method with a Strong Global Convergence Property"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This paper presents a new version of the conjugate gradient method, which converges globally, provided the line search satisfies the standard Wolfe conditions."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Optim."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1932759"
                        ],
                        "name": "M. Todd",
                        "slug": "M.-Todd",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Todd",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Todd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145185457"
                        ],
                        "name": "Y. Ye",
                        "slug": "Y.-Ye",
                        "structuredName": {
                            "firstName": "Yinyu",
                            "lastName": "Ye",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Ye"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 42238438,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2353031cdd7f7fc85282c208175a7c94e5eb6039",
            "isKey": false,
            "numCitedBy": 161,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a projective algorithm for linear programming that shares features with Karmarkar's projective algorithm and its variants and with the path-following methods of Gonzaga, Kojima-Mizuno-Yoshise, Monteiro-Adler, Renegar, Vaidya and Ye. It operates in a primal-dual setting, stays close to the central trajectories, and converges in O\u00e2\u0088\u009an L iterations like the latter methods. Here n is the number of variables and L the input size of the problem. However, it is motivated by seeking reductions in a suitable potential function as in projective algorithms, and the approximate centering is an automatic byproduct of our choice of potential function."
            },
            "slug": "A-Centered-Projective-Algorithm-for-Linear-Todd-Ye",
            "title": {
                "fragments": [],
                "text": "A Centered Projective Algorithm for Linear Programming"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "A projective algorithms for linear programming that shares features with Karmarkar's projective algorithm and its variants and with the path-following methods of Gonzaga, Kojima-Mizuno-Yoshise, Monteiro-Adler, Renegar, Vaidya and Ye is described."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Oper. Res."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9611228"
                        ],
                        "name": "T. Schlick",
                        "slug": "T.-Schlick",
                        "structuredName": {
                            "firstName": "Tamar",
                            "lastName": "Schlick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Schlick"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 59
                            }
                        ],
                        "text": "Freely available packages include TN/TNBC [220] and TNPACK [275]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 21869499,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fb96f3b9db845d837f365c86a0b6d88504e09efc",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "In large-scale unconstrained optimization problems, preconditioned conjugate gradient techniques are often used to solve the Newton equations approximately. In certain applications, \u201cnatural\u201d sparse preconditioners can be derived from the structure of the problem and can accelerate convergence significantly. Since these preconditioners are not necessarily positive definite, modified Cholesky (MC) factorizations can be applied to construct related positive definite preconditioners. This paper describes such an adaptation of two MC techniques\u2013GMW (Gill\u2013Murray\u2013Wright) and SE (Schnabel\u2013Eskow)\u2013in a truncated Newton minimization method. This paper then analyzes their effects on three interesting test problems of moderate size. The preliminary results suggest that the two MC algorithms perform quite differently in practice. Trends can be noted for sparse problems that differ from the dense case. Differences in the size of the modifications and in their variances throughout the minimization are observed and relat..."
            },
            "slug": "Modified-Cholesky-Factorizations-for-Sparse-Schlick",
            "title": {
                "fragments": [],
                "text": "Modified Cholesky Factorizations for Sparse Preconditioners"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper describes an adaptation of two MC techniques\u2013GMW (Gill\u2013Murray\u2013Wright) and SE (Schnabel\u2013Eskow)\u2013in a truncated Newton minimization method and analyzes their effects on three interesting test problems of moderate size."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Sci. Comput."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144241870"
                        ],
                        "name": "J. J. Mor\u00e9",
                        "slug": "J.-J.-Mor\u00e9",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Mor\u00e9",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. J. Mor\u00e9"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120487022,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "02037cd93634aa33343119217613e2dc4a7ae603",
            "isKey": false,
            "numCitedBy": 393,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Trust region methods are an important class of iterative methods for the solution of nonlinear optimization problems. Algorithms in this class have been proposed for the solution of systems of nonlinear equations, nonlinear estimation problems, unconstrained and constrained optimization, nondifferentiable optimization, and large scale optimization. Interest in trust region methods derives, in part, from the availability of strong convergence results and from the development of software for these methods which is reliable, efficient, and amazingly free of ad-hoc decisions. In this paper we survey the theoretical and practical results available for trust region methods and discuss the relevance of these results to the implementation of trust region methods."
            },
            "slug": "Recent-Developments-in-Algorithms-and-Software-for-Mor\u00e9",
            "title": {
                "fragments": [],
                "text": "Recent Developments in Algorithms and Software for Trust Region Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The theoretical and practical results available for trust region methods are surveyed and the relevance of these results to the implementation oftrust region methods is discussed."
            },
            "venue": {
                "fragments": [],
                "text": "ISMP"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32827550"
                        ],
                        "name": "R. Bartels",
                        "slug": "R.-Bartels",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Bartels",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bartels"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712211"
                        ],
                        "name": "G. Golub",
                        "slug": "G.-Golub",
                        "structuredName": {
                            "firstName": "Gene",
                            "lastName": "Golub",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Golub"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4398660,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d1af3a8087b0d280229a69ee06d82eee7722b021",
            "isKey": false,
            "numCitedBy": 249,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Standard computer implementations of Dantzig's simplex method for linear programming are based upon forming the inverse of the basic matrix and updating the inverse after every step of the method. These implementations have bad round-off error properties. This paper gives the theoretical background for an implementation which is based upon the LU decomposition, computed with row interchanges, of the basic matrix. The implementation is slow, but has good round-off error behavior. The implementation appears as CACM Algorithm 350."
            },
            "slug": "The-simplex-method-of-linear-programming-using-LU-Bartels-Golub",
            "title": {
                "fragments": [],
                "text": "The simplex method of linear programming using LU decomposition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The theoretical background for an implementation which is based upon the LU decomposition, computed with row interchanges, of the basic matrix of the simplex method, which is slow, but has good round-off error behavior."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1969
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143819746"
                        ],
                        "name": "J. Restrepo",
                        "slug": "J.-Restrepo",
                        "structuredName": {
                            "firstName": "Juan",
                            "lastName": "Restrepo",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Restrepo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35188185"
                        ],
                        "name": "G. Leaf",
                        "slug": "G.-Leaf",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Leaf",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Leaf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145983059"
                        ],
                        "name": "A. Griewank",
                        "slug": "A.-Griewank",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Griewank",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Griewank"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 128
                            }
                        ],
                        "text": "An implementation of checkpointing in the context of variational data assimilation can be found in Restrepo, Leaf, and Griewank [264] ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2735133,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "0c3a8a8595b569f5b840969903157099d0f167b9",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "The aim of data assimilation is to infer the state of a system from a geophysical model and possibly incomplete or nonuniformly distributed spatiotemporal observational data. Used extensively in engineering control theory applications, data assimilation has relatively recently been introduced into meteorological forecasting, natural-resource recovery modeling, and climate dynamics. Variations data assimilation is a promising assimilation technique in which it is assumed that the state of the system is an extrema of a carefully chosen objective function. Provided that an adjoint model is available, the required model gradients can be computed by integrating the model forward and its adjoint backward. the gradients are then used to extremize the cost function with a suitable iterative or conjugate gradient solver. The problem addressed in this study is the explosive growth in both on-line computer memory and remote storage requirements of large-scale assimilation studies. This imposes a severe physical limitation on the size of assimilation studies, even on the largest computers. By using a recursive strategy, a schedule can be constructed that enables the forward/adjoint model runs to be performed in such a way that storage requirements can be traded for longer computational times. This generally applicable strategy enables data assimilation studies on significantly larger domains that would otherwise be possible given particular hardware constraints. The authors show that this tradeoff is indeed viable and that when the schedule is optimized, the storage and computational times grow at most logarithmically."
            },
            "slug": "Circumventing-Storage-Limitations-in-Variational-Restrepo-Leaf",
            "title": {
                "fragments": [],
                "text": "Circumventing Storage Limitations in Variational Data Assimilation Studies"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "The problem addressed in this study is the explosive growth in both on-line computer memory and remote storage requirements of computing the gradient by the forward/adjoint technique which characterizes large-scale assimilation studies."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Sci. Comput."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34913208"
                        ],
                        "name": "M. R. Osborne",
                        "slug": "M.-R.-Osborne",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Osborne",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. R. Osborne"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 123525132,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c510d848d7a287668a1d5f7977ed1940302e60a4",
            "isKey": false,
            "numCitedBy": 149,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract One of the most succesful algorithims for nonlinear least squares calculations is that associated with the names of Levenberg, Marquardt, and Morrison. This algorithim gives a method which depends nonlinearly on a parameter \u03b3 for computing the correction to the current point. In this paper an attempt is made to give a rule for choosing \u03b3 which (a) permits a satisfactory convergence theorem to be proved, and (b) is capable of satisfactory computer implementation. It is beleieved that the stated aims have been met with reasonable success. The convergence theorem is both simple and global in character, and a computer code is given which appears to be at least competitive with existing alternatives."
            },
            "slug": "Nonlinear-least-squares-\u2014-the-Levenberg-algorithm-Osborne",
            "title": {
                "fragments": [],
                "text": "Nonlinear least squares \u2014 the Levenberg algorithm revisited"
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of the Australian Mathematical Society. Series B. Applied Mathematics"
            },
            "year": 1976
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145625464"
                        ],
                        "name": "P. Hough",
                        "slug": "P.-Hough",
                        "structuredName": {
                            "firstName": "Patricia",
                            "lastName": "Hough",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hough"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "150320851"
                        ],
                        "name": "T. Kolda",
                        "slug": "T.-Kolda",
                        "structuredName": {
                            "firstName": "Tamara",
                            "lastName": "Kolda",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kolda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2776399"
                        ],
                        "name": "V. Torczon",
                        "slug": "V.-Torczon",
                        "structuredName": {
                            "firstName": "Virginia",
                            "lastName": "Torczon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Torczon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4667493,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "20a7446870ca474262bd0bafdff0d8cd2abf8fac",
            "isKey": false,
            "numCitedBy": 140,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new asynchronous parallel pattern search (APPS). Parallel pattern search can be quite useful for engineering optimization problems characterized by a small number of variables (say, fifty or less) and by objective functions that are expensive to evaluate, such as those defined by complex simulations that can take anywhere from a few seconds to many hours to run. The target platforms for APPS are the loosely coupled parallel systems now widely available. We exploit the algorithmic characteristics of pattern search to design variants that dynamically initiate actions solely in response to messages, rather than routinely cycling through a fixed set of steps. This gives a versatile concurrent strategy that allows us to effectively balance the computational load across all available processors. Further, it allows us to incorporate a high degree of fault tolerance with almost no additional overhead. We demonstrate the effectiveness of a preliminary implementation of APPS on both standard test problems as well as some engineering optimization problems."
            },
            "slug": "Asynchronous-Parallel-Pattern-Search-for-Nonlinear-Hough-Kolda",
            "title": {
                "fragments": [],
                "text": "Asynchronous Parallel Pattern Search for Nonlinear Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work exploits the algorithmic characteristics of pattern search to design variants that dynamically initiate actions solely in response to messages, rather than routinely cycling through a fixed set of steps, which gives a versatile concurrent strategy that allows us to effectively balance the computational load across all available processors."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Sci. Comput."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34728746"
                        ],
                        "name": "Ciyou Zhu",
                        "slug": "Ciyou-Zhu",
                        "structuredName": {
                            "firstName": "Ciyou",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ciyou Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144448267"
                        ],
                        "name": "R. Byrd",
                        "slug": "R.-Byrd",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Byrd",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Byrd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2337678"
                        ],
                        "name": "P. Lu",
                        "slug": "P.-Lu",
                        "structuredName": {
                            "firstName": "Peihuang",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2784955"
                        ],
                        "name": "J. Nocedal",
                        "slug": "J.-Nocedal",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Nocedal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nocedal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207228122,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1076a9b8181a7f9eb069d38ca10876a3202d2e89",
            "isKey": false,
            "numCitedBy": 2503,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "L-BFGS-B is a limited-memory algorithm for solving large nonlinear optimization problems subject to simple bounds on the variables. It is intended for problems in which information on the Hessian matrix is difficult to obtain, or for large dense problems. L-BFGS-B can also be used for unconstrained problems and in this case performs similarly to its predessor, algorithm L-BFGS (Harwell routine VA15). The algorithm is implemented in Fortran 77."
            },
            "slug": "Algorithm-778:-L-BFGS-B:-Fortran-subroutines-for-Zhu-Byrd",
            "title": {
                "fragments": [],
                "text": "Algorithm 778: L-BFGS-B: Fortran subroutines for large-scale bound-constrained optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "L-BFGS-B is a limited-memory algorithm for solving large nonlinear optimization problems subject to simple bounds on the variables, intended for problems in which information on the Hessian matrix is difficult to obtain, or for large dense problems."
            },
            "venue": {
                "fragments": [],
                "text": "TOMS"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144448267"
                        ],
                        "name": "R. Byrd",
                        "slug": "R.-Byrd",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Byrd",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Byrd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795397"
                        ],
                        "name": "Bobby Schnabel",
                        "slug": "Bobby-Schnabel",
                        "structuredName": {
                            "firstName": "Bobby",
                            "lastName": "Schnabel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bobby Schnabel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1894635"
                        ],
                        "name": "Gerald A. Shultz",
                        "slug": "Gerald-A.-Shultz",
                        "structuredName": {
                            "firstName": "Gerald",
                            "lastName": "Shultz",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gerald A. Shultz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": "Byrd, Schnabel, and Schultz [279], [54] provide a general theory for inexact trustregion methods; they introduce the idea of two-dimensional subspace minimization and also focus on proper handling of the case of indefinite B to ensure stronger local convergence results than Theorems 4."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 88
                            }
                        ],
                        "text": "indefiniteness here, and refer the reader to papers by Byrd, Schnabel, and Schultz (see [54] and [279]) for details."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 30118876,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "86fb9d412336382bd475674b67f2b88be3d77029",
            "isKey": false,
            "numCitedBy": 373,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "The trust region problem, minimization of a quadratic function subject to a spherical trust region constraint, occurs in many optimization algorithms. In a previous paper, the authors introduced an inexpensive approximate solution technique for this problem that involves the solution of a two-dimensional trust region problem. They showed that using this approximation in an unconstrained optimization algorithm leads to the same theoretical global and local convergence properties as are obtained using the exact solution to the trust region problem. This paper reports computational results showing that the two-dimensional minimization approach gives nearly optimal reductions in then-dimension quadratic model over a wide range of test cases. We also show that there is very little difference, in efficiency and reliability, between using the approximate or exact trust region step in solving standard test problems for unconstrained optimization. These results may encourage the application of similar approximate trust region techniques in other contexts."
            },
            "slug": "Approximate-solution-of-the-trust-region-problem-by-Byrd-Schnabel",
            "title": {
                "fragments": [],
                "text": "Approximate solution of the trust region problem by minimization over two-dimensional subspaces"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Computational results are reported showing that the two-dimensional minimization approach gives nearly optimal reductions in then-dimension quadratic model over a wide range of test cases."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144448267"
                        ],
                        "name": "R. Byrd",
                        "slug": "R.-Byrd",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Byrd",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Byrd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35074400"
                        ],
                        "name": "Jean Charles Gilbert",
                        "slug": "Jean-Charles-Gilbert",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Gilbert",
                            "middleNames": [
                                "Charles"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean Charles Gilbert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2784955"
                        ],
                        "name": "J. Nocedal",
                        "slug": "J.-Nocedal",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Nocedal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nocedal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 236211,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "826385c6e255e8d8965dbddcde3a812397d8bab4",
            "isKey": false,
            "numCitedBy": 1316,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract.An algorithm for minimizing a nonlinear function subject to nonlinear inequality constraints is described. It applies sequential quadratic programming techniques to a sequence of barrier problems, and uses trust regions to ensure the robustness of the iteration and to allow the direct use of second order derivatives. This framework permits primal and primal-dual steps, but the paper focuses on the primal version of the new algorithm. An analysis of the convergence properties of this method is presented."
            },
            "slug": "A-trust-region-method-based-on-interior-point-for-Byrd-Gilbert",
            "title": {
                "fragments": [],
                "text": "A trust region method based on interior point techniques for nonlinear programming"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "This paper focuses on the primal version of the new algorithm, an algorithm for minimizing a nonlinear function subject to nonlinear inequality constraints, which applies sequential quadratic programming techniques to a sequence of barrier problems."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143631704"
                        ],
                        "name": "L. Watson",
                        "slug": "L.-Watson",
                        "structuredName": {
                            "firstName": "Layne",
                            "lastName": "Watson",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Watson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 122010139,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "45642521742259f9706b8b87ed90d5c20623e9a1",
            "isKey": false,
            "numCitedBy": 150,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Probability one homotopy algorithms are a class of methods for solving nonlinear systems of equations that are globally convergent with probability one. These methods are theoretically powerful, and if constructed and implemented properly, are robust, numerically stable, accurate, and practical. The concomitant numerical linear algebra problems deal with rectangular matrices, and good algorithms require a delicate balance (not always achieved) of accuracy, robustness, and efficiency in both space and time. The author's experience with globally convergent homotopy algorithms is surveyed here, and some of the linear algebra difficulties for dense and sparse problems are discussed."
            },
            "slug": "Numerical-linear-algebra-aspects-of-globally-Watson",
            "title": {
                "fragments": [],
                "text": "Numerical linear algebra aspects of globally convergent homotopy methods"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735269"
                        ],
                        "name": "T. Coleman",
                        "slug": "T.-Coleman",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Coleman",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Coleman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745867"
                        ],
                        "name": "B. S. Garbow",
                        "slug": "B.-S.-Garbow",
                        "structuredName": {
                            "firstName": "Burton",
                            "lastName": "Garbow",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. S. Garbow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144241870"
                        ],
                        "name": "J. J. Mor\u00e9",
                        "slug": "J.-J.-Mor\u00e9",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Mor\u00e9",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. J. Mor\u00e9"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16997585,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "a4cb0d4cfa7df4d6425b461af7ffee91a28d12e2",
            "isKey": false,
            "numCitedBy": 129,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "In many nonlinear problems it is necessary to estimate the Jacobian matrix of a nonlinear mapping F. In large scale problems the Jacobian of F is usually sparse, and then estimation by differences is attractive because the number of differences can be small compared to the dimension of the problem. For example, if the Jacobian matrix is banded then the number of differences needed to estimate the Jacobian matrix is, at most, the width of the band. In this paper we describe a set of subroutines whose purpose is to estimate the Jacobian matrix of a mapping F with the least possible number of function evaluations."
            },
            "slug": "Software-for-estimating-sparse-Jacobian-matrices-Coleman-Garbow",
            "title": {
                "fragments": [],
                "text": "Software for estimating sparse Jacobian matrices"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "This paper describes a set of subroutines whose purpose is to estimate the Jacobian matrix of a mapping F with the least possible number of function evaluations."
            },
            "venue": {
                "fragments": [],
                "text": "TOMS"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2776399"
                        ],
                        "name": "V. Torczon",
                        "slug": "V.-Torczon",
                        "structuredName": {
                            "firstName": "Virginia",
                            "lastName": "Torczon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Torczon"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 18232246,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "8dc3f1f0a04353f17551b25a460f83d3eaf5da0f",
            "isKey": false,
            "numCitedBy": 868,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce an abstract definition of pattern search methods for solving nonlinear unconstrained optimization problems. Our definition unifies an important collection of optimization methods that neither compute nor explicitly approximate derivatives. We exploit our characterization of pattern search methods to establish a global convergence theory that does not enforce a notion of sufficient decrease. Our analysis is possible because the iterates of a pattern search method lie on a scaled, translated integer lattice. This allows us to relax the classical requirements on the acceptance of the step, at the expense of stronger conditions on the form of the step, and still guarantee global convergence."
            },
            "slug": "On-the-Convergence-of-Pattern-Search-Algorithms-Torczon",
            "title": {
                "fragments": [],
                "text": "On the Convergence of Pattern Search Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The characterization of pattern search methods is exploited to establish a global convergence theory that does not enforce a notion of sufficient decrease, and is possible because the iterates of a pattern search method lie on a scaled, translated integer lattice."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Optim."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145983059"
                        ],
                        "name": "A. Griewank",
                        "slug": "A.-Griewank",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Griewank",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Griewank"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117475403,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ff9c1dc70d456fce6f047609a1879b8775742f9f",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider continuous functions that are defined by programs for their evaluation. The basic arithmetic operations and univariate special functions are real analytic in the interior of their domains. However, another frequent ingredient, the absolute value function, has a kink at the origin, which occurs similarly for max and min. A slightly more serious complication arises with the introduction of Euclidean vector norms. It is shown here that the resulting class of composite functions is still directionally real analytic and we develop formulas for propagating the corresponding directional Taylor-coefficients in the forward mode of automatic differentiation. Finally, we discuss possibilities for using the reverse mode to compute generalized gradients."
            },
            "slug": "Automatic-Directional-Differentiation-of-Nonsmooth-Griewank",
            "title": {
                "fragments": [],
                "text": "Automatic Directional Differentiation of Nonsmooth Composite Functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145983059"
                        ],
                        "name": "A. Griewank",
                        "slug": "A.-Griewank",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Griewank",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Griewank"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145758858"
                        ],
                        "name": "P. Toint",
                        "slug": "P.-Toint",
                        "structuredName": {
                            "firstName": "Philippe",
                            "lastName": "Toint",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Toint"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 73
                            }
                        ],
                        "text": "The concept of partial separability was introduced by Griewank and Toint [156, 155]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 121166840,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "7a619a74f575e9dbc43fc5c86c12f39fed465ba3",
            "isKey": false,
            "numCitedBy": 145,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "SummaryThis paper considers local convergence properties of inexact partitioned quasi-Newton algorithms for the solution of certain non-linear equations and, in particular, the optimization of partially separable objective functions. Using the bounded deterioration principle, one obtains local and linear convergence, which impliesQ-superlinear convergence under the usual conditions on the quasi-Newton updates. For the optimization case, these conditions are shown to be satisfied by any sequence of updates within the convex Broyden class, even if some Hessians are singular at the minimizer. Finally, local andQ-superlinear convergence is established for an inexact partitioned variable metric method under mild assumptions on the initial Hessian approximations."
            },
            "slug": "Local-convergence-analysis-for-partitioned-updates-Griewank-Toint",
            "title": {
                "fragments": [],
                "text": "Local convergence analysis for partitioned quasi-Newton updates"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2294095"
                        ],
                        "name": "M. Powell",
                        "slug": "M.-Powell",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Powell",
                            "middleNames": [
                                "J.",
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Powell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 41901934,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "9515ae10094b673d0f64574f07bcd17f2f92c001",
            "isKey": false,
            "numCitedBy": 102,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "It has been conjectured that the conjugate gradient method for minimizing functions of several variables has a superlinear rate of convergence, but Crowder and Wolfe show by example that the conjecture is false. Now the stronger result is given that, if the objective function is a convex quadratic and if the initial search direction is an arbitrary downhill direction, then either termination occurs or the rate of convergence is only linear, the second possibility being more usual. Relations between the starting point and the initial search direction that are necessary and sufficient for termination in the quadratic case are studied."
            },
            "slug": "Some-convergence-properties-of-the-conjugate-method-Powell",
            "title": {
                "fragments": [],
                "text": "Some convergence properties of the conjugate gradient method"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "It has been conjectured that the conjugate gradient method for minimizing functions of several variables has a superlinear rate of convergence, but Crowder and Wolfe show by example that the conjecture is false."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 1976
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795397"
                        ],
                        "name": "Bobby Schnabel",
                        "slug": "Bobby-Schnabel",
                        "structuredName": {
                            "firstName": "Bobby",
                            "lastName": "Schnabel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bobby Schnabel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2006865"
                        ],
                        "name": "E. Eskow",
                        "slug": "E.-Eskow",
                        "structuredName": {
                            "firstName": "Elizabeth",
                            "lastName": "Eskow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Eskow"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 104
                            }
                        ],
                        "text": "A modified Cholesky factorization based on Gershgorin disk estimates is described in Schnabel and Eskow [276]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 37892119,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce909ed0908c6a8c167e83fc290faf3166404cd4",
            "isKey": false,
            "numCitedBy": 164,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "The modified Cholesky factorization of Gill and Murray plays an important role in optimization algorithms. Given a symmetric but not necessarily positive-definite matrix A, it computes a Cholesky factorization of $A + E$, where $E = 0$ if A is safely positive-definite, and E is a diagonal matrix chosen to make $A + E$ positive-definite otherwise. The factorization costs only a small multiple of $n^2 $ operations more than the standard Cholesky factorization. A new algorithm that has these same properties, but for which the theoretical bound on $||E||_\\infty $ is substantially smaller, is presented. It is based upon two new techniques, the use of Gerschgorin bounds in selecting the elements of E, and a new way of monitoring positive definiteness. In extensive computational tests on indefinite matrices, the new factorization virtually always produces smaller values of $||E||_\\infty $ than the existing method, without impairing the conditioning of $A + E$. In some cases the improvements are substantial. The new factorization has already been useful in optimization algorithms."
            },
            "slug": "A-New-Modified-Cholesky-Factorization-Schnabel-Eskow",
            "title": {
                "fragments": [],
                "text": "A New Modified Cholesky Factorization"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "In extensive computational tests on indefinite matrices, the new factorization virtually always produces smaller values of $||E||_\\infty $ than the existing method, without impairing the conditioning of $A + E$."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Sci. Comput."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054899401"
                        ],
                        "name": "Carsten Keller",
                        "slug": "Carsten-Keller",
                        "structuredName": {
                            "firstName": "Carsten",
                            "lastName": "Keller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carsten Keller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52067790"
                        ],
                        "name": "N. Gould",
                        "slug": "N.-Gould",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Gould",
                            "middleNames": [
                                "I.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Gould"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2029275"
                        ],
                        "name": "A. Wathen",
                        "slug": "A.-Wathen",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Wathen",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Wathen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 34087888,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a702df1b482d1049cfdc556b051d7e37119eb590",
            "isKey": false,
            "numCitedBy": 368,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of finding good preconditioners for the numerical solution of indefinite linear systems is considered. Special emphasis is put on preconditioners that have a 2 \u00d7 2 block structure and that incorporate the (1,2) and (2,1) blocks of the original matrix. Results concerning the spectrum and form of the eigenvectors of the preconditioned matrix and its minimum polynomial are given. The consequences of these results are considered for a variety of Krylov subspace methods. Numerical experiments validate these conclusions."
            },
            "slug": "Constraint-Preconditioning-for-Indefinite-Linear-Keller-Gould",
            "title": {
                "fragments": [],
                "text": "Constraint Preconditioning for Indefinite Linear Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "The problem of finding good preconditioners for the numerical solution of indefinite linear systems is considered and special emphasis is put on preconditionsers that have a 2 \u00d7 2 block structure and that incorporate the (1,2 and (2,1) blocks of the original matrix."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Matrix Anal. Appl."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710850"
                        ],
                        "name": "C. Bischof",
                        "slug": "C.-Bischof",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Bischof",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bischof"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2363532"
                        ],
                        "name": "P. Khademi",
                        "slug": "P.-Khademi",
                        "structuredName": {
                            "firstName": "Peyvand",
                            "lastName": "Khademi",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Khademi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47070391"
                        ],
                        "name": "A. Mauer",
                        "slug": "A.-Mauer",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Mauer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Mauer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14413600"
                        ],
                        "name": "A. Carle",
                        "slug": "A.-Carle",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Carle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Carle"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61408106,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6f16170cbd884cbfd3de41e6cb8e06658ed7948c",
            "isKey": false,
            "numCitedBy": 400,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Numerical codes that calculate not only a result, but also the derivatives of the variables with respect to each other, facilitate sensitivity analysis, inverse problem solving, and optimization. The paper considers how Adifor 2.0, which won the 1995 Wilkinson Prize for Numerical Software, can automatically differentiate complicated Fortran code much faster than a programmer can do it by hand. The Adifor system has three main components: the AdiFor preprocessor, the ADIntrinsics exception-handling system, and the SparsLinC library."
            },
            "slug": "Adifor-2.0:-automatic-differentiation-of-Fortran-77-Bischof-Khademi",
            "title": {
                "fragments": [],
                "text": "Adifor 2.0: automatic differentiation of Fortran 77 programs"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "This paper considers how Adifor 2.0, which won the 1995 Wilkinson Prize for Numerical Software, can automatically differentiate complicated Fortran code much faster than a programmer can do it by hand."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3158732"
                        ],
                        "name": "P. Boggs",
                        "slug": "P.-Boggs",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Boggs",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Boggs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1858423"
                        ],
                        "name": "J. Tolle",
                        "slug": "J.-Tolle",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "Tolle",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tolle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108814734"
                        ],
                        "name": "Pyng Wang",
                        "slug": "Pyng-Wang",
                        "structuredName": {
                            "firstName": "Pyng",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pyng Wang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123557918,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "3503ef8dc8f3907cb023ab4426615e6f30c1e956",
            "isKey": false,
            "numCitedBy": 159,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the application of a general class of quasi-Newton methods to the solution of the classical equality constrained nonlinear optimization problem. Specifically, we develop necessary and sufficient conditions for the Q-superlinear convergence of such methods and present a companion linear convergence theorem. The essential conditions relate to the manner in which the Hessian of the Lagrangian function is approximated."
            },
            "slug": "On-the-Local-Convergence-of-Quasi-Newton-Methods-Boggs-Tolle",
            "title": {
                "fragments": [],
                "text": "On the Local Convergence of Quasi-Newton Methods for Constrained Optimization"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735269"
                        ],
                        "name": "T. Coleman",
                        "slug": "T.-Coleman",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Coleman",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Coleman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144241870"
                        ],
                        "name": "J. J. Mor\u00e9",
                        "slug": "J.-J.-Mor\u00e9",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Mor\u00e9",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. J. Mor\u00e9"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 32698790,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f6d553006ff2f18761b9014cd26d1526fffaefd4",
            "isKey": false,
            "numCitedBy": 158,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Large scale optimization problems often require an approximation to the Hessian matrix. If the Hessian matrix is sparse then estimation by differences of gradients is attractive because the number of required differences is usually small compared to the dimension of the problem. The problem of estimating Hessian matrices by differences can be phrased as follows: Given the sparsity structure of a symmetric matrixA, obtain vectorsd1,d2, \u2026dp such thatAd1,Ad2, \u2026Adp determineA uniquely withp as small as possible. We approach this problem from a graph theoretic point of view and show that both direct and indirect approaches to this problem have a natural graph coloring interpretation. The complexity of the problem is analyzed and efficient practical heuristic procedures are developed. Numerical results illustrate the differences between the various approaches."
            },
            "slug": "Estimation-of-sparse-hessian-matrices-and-graph-Coleman-Mor\u00e9",
            "title": {
                "fragments": [],
                "text": "Estimation of sparse hessian matrices and graph coloring problems"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work approaches the problem of estimating Hessian matrices by differences from a graph theoretic point of view and shows that both direct and indirect approaches have a natural graph coloring interpretation."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32356980"
                        ],
                        "name": "A. Vardi",
                        "slug": "A.-Vardi",
                        "structuredName": {
                            "firstName": "Avi",
                            "lastName": "Vardi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vardi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120964666,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e66fe9d92b2e1badde6e5966f2409eb8876adc9",
            "isKey": false,
            "numCitedBy": 152,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "In unconstrained minimization, trust region algorithms use directions that are a combination of the quasi-Newton direction and the steepest descent direction, depending on the fit between the quadratic approximation of the function and the function itself.Algorithms for nonlinear constrained minimization problems usually determine a quasi-Newton direction and use a line search technique to determine the step. Since trust region strategies have proved to be successful in unconstrained minimization, we develop a new trust region strategy for equality constrained minimization. This algorithm is analyzed and global as well as local superlinear convergence theorems are proved for various versions.We demonstrate how to implement this algorithm in a numerically stable way. A computer program based on this algorithm has performed very satisfactorily on test problems; numerical results are provided."
            },
            "slug": "A-Trust-Region-Algorithm-for-Equality-Constrained-Vardi",
            "title": {
                "fragments": [],
                "text": "A Trust Region Algorithm for Equality Constrained Minimization: Convergence Properties and Implementation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A new trust region strategy for equality constrained minimization is developed and global as well as local superlinear convergence theorems are proved for various versions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33308287"
                        ],
                        "name": "John J. H. Forrest",
                        "slug": "John-J.-H.-Forrest",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Forrest",
                            "middleNames": [
                                "J.",
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John J. H. Forrest"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717630"
                        ],
                        "name": "J. Tomlin",
                        "slug": "J.-Tomlin",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Tomlin",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tomlin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 31525497,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0c6fa32b5753ec46dc934b525f0dc35a9ca4fcd9",
            "isKey": false,
            "numCitedBy": 213,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "In recent years triangular factorization of the basis has greatly enhanced the efficiency of linear programming inversion routines, leading to greater speed, accuracy and a sparser representation. This paper describes a new product form method for updating the triangular factors at each iteration of the simplex method which has proved extremely effective in reducing the rate of growth of the transformation (eta) files, thus reducing the amount of work per iteration and the frequency of re-inversion. We indicate some of the programming measures required to implement the method and give computational experience on real problems of up to 3500 rows."
            },
            "slug": "Updated-triangular-factors-of-the-basis-to-maintain-Forrest-Tomlin",
            "title": {
                "fragments": [],
                "text": "Updated triangular factors of the basis to maintain sparsity in the product form simplex method"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A new product form method for updating the triangular factors at each iteration of the simplex method which has proved extremely effective in reducing the rate of growth of the transformation (eta) files, thus reducing the amount of work per iteration and the frequency of re-inversion."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3181431"
                        ],
                        "name": "E. Birgin",
                        "slug": "E.-Birgin",
                        "structuredName": {
                            "firstName": "Ernesto",
                            "lastName": "Birgin",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Birgin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109795518"
                        ],
                        "name": "J. M. Mart\u00ednez",
                        "slug": "J.-M.-Mart\u00ednez",
                        "structuredName": {
                            "firstName": "Jos\u00e9",
                            "lastName": "Mart\u00ednez",
                            "middleNames": [
                                "Mario"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. M. Mart\u00ednez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1807685"
                        ],
                        "name": "M. Raydan",
                        "slug": "M.-Raydan",
                        "structuredName": {
                            "firstName": "Marcos",
                            "lastName": "Raydan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Raydan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15557940,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c4e955b690a4c5e52e5187782d1bec2e07c3e39d",
            "isKey": false,
            "numCitedBy": 228,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Fortran 77 software implementing the SPG method is introduced. SPG is a nonmonotone projected gradient algorithm for solving large-scale convex-constrained optimization problems. It combines the classical projected gradient method with the spectral gradient choice of steplength and a nonmonotone line-search strategy. The user provides objective function and gradient values, and projections onto the feasible set. Some recent numerical tests are reported on very large location problems, indicating that SPG is substantially more efficient than existing general-purpose software on problems for which projections can be computed efficiently."
            },
            "slug": "Algorithm-813:-SPG\u2014Software-for-Convex-Constrained-Birgin-Mart\u00ednez",
            "title": {
                "fragments": [],
                "text": "Algorithm 813: SPG\u2014Software for Convex-Constrained Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "Fortran 77 software implementing the SPG method, a nonmonotone projected gradient algorithm for solving large-scale convex-constrained optimization problems, which is substantially more efficient than existing general-purpose software on problems for which projections can be computed efficiently."
            },
            "venue": {
                "fragments": [],
                "text": "TOMS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143898851"
                        ],
                        "name": "S. Wolfram",
                        "slug": "S.-Wolfram",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Wolfram",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Wolfram"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 127
                            }
                        ],
                        "text": "Jorge Nocedal Stephen J. Wright\nNumerical Optimization\nWith 85 Illustrations\n, Springer\nContents\nPreface vii\n1 Introduction, 1 Mathematical Formulation 2 Example: A Transportation Problem 4 Continuous versus Discrete Optimization 4 Constrained and Unconstrained Optimization 6 Global and Local Optimization 6 Stochastic and Deterministic Optimization 7 Optimization Algorithms 7 Convexity 8\nNotes and References 9\n2 Fundamentals of Unconstrained Optimization 10 2.1 What Is a Solution?"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 83
                            }
                        ],
                        "text": "Commonly used symbolic manipulation tools can be found in the packages Mathematica [311], Maple [304], and Macsyma [197]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60959654,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "861ab174d39041eaa016b5f7c6ce78dfa62dc41a",
            "isKey": true,
            "numCitedBy": 3940,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nMathematica has defined the state of the art in technical computing for over a decade, and has become a standard in many of the world's leading companies and universities. From simple calculator operations to large-scale programming and the preparation of interactive documents, Mathematica is the tool of choice."
            },
            "slug": "The-Mathematica-Book-Wolfram",
            "title": {
                "fragments": [],
                "text": "The Mathematica Book"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "From the Publisher:  Mathematica has defined the state of the art in technical computing for over a decade, and has become a standard in many of the world's leading companies and universities."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2294095"
                        ],
                        "name": "M. Powell",
                        "slug": "M.-Powell",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Powell",
                            "middleNames": [
                                "J.",
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Powell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 45909653,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "58fa6233beec95154e51e9142a11efa0ed779670",
            "isKey": false,
            "numCitedBy": 298,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Some examples are given of differentiable functions of three variables, having the property that if they are treated by the minimization algorithm that searches along the coordinate directions in sequence, then the search path tends to a closed loop. On this loop the gradient of the objective function is bounded away from zero. We discuss the relevance of these examples to the problem of proving general convergence theorems for minimization algorithms that use search directions."
            },
            "slug": "On-search-directions-for-minimization-algorithms-Powell",
            "title": {
                "fragments": [],
                "text": "On search directions for minimization algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "The relevance of differentiable functions of three variables, having the property that if they are treated by the minimization algorithm that searches along the coordinate directions in sequence, then the search path tends to a closed loop, is discussed."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144733293"
                        ],
                        "name": "R. Fletcher",
                        "slug": "R.-Fletcher",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Fletcher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fletcher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 123487779,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b84b383ad59f79e607ad0f08a8a10876631a0cd",
            "isKey": false,
            "numCitedBy": 9912,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface Table of Notation Part 1: Unconstrained Optimization Introduction Structure of Methods Newton-like Methods Conjugate Direction Methods Restricted Step Methods Sums of Squares and Nonlinear Equations Part 2: Constrained Optimization Introduction Linear Programming The Theory of Constrained Optimization Quadratic Programming General Linearly Constrained Optimization Nonlinear Programming Other Optimization Problems Non-Smooth Optimization References Subject Index."
            },
            "slug": "Practical-Methods-of-Optimization-Fletcher",
            "title": {
                "fragments": [],
                "text": "Practical Methods of Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The aim of this book is to provide a Discussion of Constrained Optimization and its Applications to Linear Programming and Other Optimization Problems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705564"
                        ],
                        "name": "G. Nemhauser",
                        "slug": "G.-Nemhauser",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Nemhauser",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Nemhauser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736128"
                        ],
                        "name": "L. Wolsey",
                        "slug": "L.-Wolsey",
                        "structuredName": {
                            "firstName": "Laurence",
                            "lastName": "Wolsey",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Wolsey"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 231
                            }
                        ],
                        "text": "Network and integer optimization are described in some excellent texts: for instance, Ahuja, Magnanti, and Orlin [1] in the case of network optimization and Nemhauser and Wolsey [224], Papadimitriou and Steiglitz [235], and Wolsey [312] in the case of integer programming."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 17
                            }
                        ],
                        "text": "[77], and Wolsey [312] for comprehensive treatments of this subject."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17594683,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "c2f462695a0e61d113634e6fb4ec72dad643c543",
            "isKey": false,
            "numCitedBy": 4162,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The principles of integer programming are directed toward finding solutions to problems from the fields of economic planning, engineering design, and combinatorial optimization. This highly respected and much-cited text, a standard of graduate-level courses since 1972, presents a comprehensive treatment of the first two decades of research on integer programming."
            },
            "slug": "Integer-Programming-Nemhauser-Wolsey",
            "title": {
                "fragments": [],
                "text": "Integer Programming"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144448267"
                        ],
                        "name": "R. Byrd",
                        "slug": "R.-Byrd",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Byrd",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Byrd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30175095"
                        ],
                        "name": "H. Khalfan",
                        "slug": "H.-Khalfan",
                        "structuredName": {
                            "firstName": "Humaid",
                            "lastName": "Khalfan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Khalfan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795397"
                        ],
                        "name": "Bobby Schnabel",
                        "slug": "Bobby-Schnabel",
                        "structuredName": {
                            "firstName": "Bobby",
                            "lastName": "Schnabel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bobby Schnabel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "7 is given in Byrd, Khalfan, and Schnabel [51]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 959515,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "55356d56504a6bce058c8183e8a26ad580322afb",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Several recent computational studies have shown that the symmetric rank-one (SR1) update is a very competitive quasi-Newton update in optimization algorithms. This paper gives a new analysis of a trust region SR1 method for unconstrained optimization and shows that the method has an $n+1$ step $q$-superlinear rate of convergence. The analysis makes neither of the assumptions of uniform linear independence of the iterates nor positive definiteness of the Hessian approximations that have been made in other recent analyses of SR1 methods. The trust region method that is analyzed is fairly standard, except that it includes the feature that the Hessian approximation is updated after all steps, including rejected steps. We also present computational results that show that this feature, safeguarded in a way that is consistent with the convergence analysis, does not harm the efficiency of the SR1 trust region method."
            },
            "slug": "Analysis-of-a-Symmetric-Rank-One-Trust-Region-Byrd-Khalfan",
            "title": {
                "fragments": [],
                "text": "Analysis of a Symmetric Rank-One Trust Region Method"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A new analysis of a trust region SR1 method for unconstrained optimization and it is shown that the method has an $n+1$ step $q$-superlinear rate of convergence."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Optim."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144241870"
                        ],
                        "name": "J. J. Mor\u00e9",
                        "slug": "J.-J.-Mor\u00e9",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Mor\u00e9",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. J. Mor\u00e9"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2150172"
                        ],
                        "name": "D. Sorensen",
                        "slug": "D.-Sorensen",
                        "structuredName": {
                            "firstName": "Danny",
                            "lastName": "Sorensen",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Sorensen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 56112357,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "2acdaca503cc0a5c8a5be1eddc33068db274d3f5",
            "isKey": false,
            "numCitedBy": 1340,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose an algorithm for the problem of minimizing a quadratic function subject to an ellipsoidal constraint and show that this algorithm is guaranteed to produce a nearly optimal solution in a finite number of iterations. We also consider the use of this algorithm in a trust region Newton's method. In particular, we prove that under reasonable assumptions the sequence generated by Newton's method has a limit point which satisfies the first and second order necessary conditions for a minimizer of the objective function. Numerical results for GQTPAR, which is a Fortran implementaton of our algorithm, show that GQTPAR is quite successful in a trust region method. In our tests a call to GQTPAR only required 1.6 iterations on the average."
            },
            "slug": "Computing-a-Trust-Region-Step-Mor\u00e9-Sorensen",
            "title": {
                "fragments": [],
                "text": "Computing a Trust Region Step"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "An algorithm for the problem of minimizing a quadratic function subject to an ellipsoidal constraint is proposed and it is shown that this algorithm is guaranteed to produce a nearly optimal solution in a finite number of iterations."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2825013"
                        ],
                        "name": "K. G. Murty",
                        "slug": "K.-G.-Murty",
                        "structuredName": {
                            "firstName": "Katta",
                            "lastName": "Murty",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. G. Murty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2017544"
                        ],
                        "name": "S. N. Kabadi",
                        "slug": "S.-N.-Kabadi",
                        "structuredName": {
                            "firstName": "Santosh",
                            "lastName": "Kabadi",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. N. Kabadi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 30500771,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "36a9ce868cfe12fbf0a765ab97a41c3ef34fa8ea",
            "isKey": false,
            "numCitedBy": 945,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "AbstractIn continuous variable, smooth, nonconvex nonlinear programming, we analyze the complexity of checking whether(a)a given feasible solution is not a local minimum, and(b)the objective function is not bounded below on the set of feasible solutions.\nWe construct a special class of indefinite quadratic programs, with simple constraints and integer data, and show that checking (a) or (b) on this class is NP-complete. As a corollary, we show that checking whether a given integer square matrix is not copositive, is NP-complete."
            },
            "slug": "Some-NP-complete-problems-in-quadratic-and-Murty-Kabadi",
            "title": {
                "fragments": [],
                "text": "Some NP-complete problems in quadratic and nonlinear programming"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A special class of indefinite quadratic programs is constructed, with simple constraints and integer data, and it is shown that checking (a) or (b) on this class is NP-complete."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745824"
                        ],
                        "name": "A. W\u00e4chter",
                        "slug": "A.-W\u00e4chter",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "W\u00e4chter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. W\u00e4chter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2818509"
                        ],
                        "name": "L. Biegler",
                        "slug": "L.-Biegler",
                        "structuredName": {
                            "firstName": "Lorenz",
                            "lastName": "Biegler",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Biegler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17831632,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a4cf8b2de2014018ec40226e405a790935727a04",
            "isKey": false,
            "numCitedBy": 102,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract.Using a simple analytical example, we demonstrate that a class of interior point methods for general nonlinear programming, including some current methods, is not globally convergent. It is shown that those algorithms produce limit points that are neither feasible nor stationary points of some measure of the constraint violation, when applied to a well-posed problem."
            },
            "slug": "Failure-of-global-convergence-for-a-class-of-point-W\u00e4chter-Biegler",
            "title": {
                "fragments": [],
                "text": "Failure of global convergence for a class of interior point methods for nonlinear programming"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "It is shown that a class of interior point methods for general nonlinear programming, including some current methods, is not globally convergent, and those algorithms produce limit points that are neither feasible nor stationary points of some measure of the constraint violation, when applied to a well-posed problem."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144241870"
                        ],
                        "name": "J. J. Mor\u00e9",
                        "slug": "J.-J.-Mor\u00e9",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Mor\u00e9",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. J. Mor\u00e9"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144731788"
                        ],
                        "name": "Stephen J. Wright",
                        "slug": "Stephen-J.-Wright",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Wright",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen J. Wright"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 25
                            }
                        ],
                        "text": "Jorge Nocedal Stephen J. Wright\nNumerical Optimization\nWith 85 Illustrations\n, Springer\nContents\nPreface vii\n1 Introduction, 1 Mathematical Formulation 2 Example: A Transportation Problem 4 Continuous versus Discrete Optimization 4 Constrained and Unconstrained Optimization 6 Global and Local Optimization 6 Stochastic and Deterministic Optimization 7 Optimization Algorithms 7 Convexity 8\nNotes and References 9\n2 Fundamentals of Unconstrained Optimization 10 2.1 What Is a Solution?"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 127
                            }
                        ],
                        "text": "For the most part, we have omitted detailed discussions of specific software packages, and refer the reader to Mor\u00e9 and Wright [217] or to the Software Guide section of the NEOS Guide, which can be found at http://www."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122875046,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d5430dd2238f36ae999864bc9913ba247ba0b8f",
            "isKey": false,
            "numCitedBy": 275,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface Part I. Overview of Algorithms. 1. Optimization Problems and Software 2. Unconstrained Optimization 3.Nonlinear Least Squares 4. Nonlinear Equations 5. Linear Programming 6. Quadratic Programming 7. Bound-Constrained Optimization 8. Constrained Optimization 9. Network Optimization 10. Integer Programming Chapter 11. Miscellaneous Optimization Problems Part II: Software Packages. AMPL BQPD BT BTN CNM CONOPT CONSOL-OPTCAD CPLEX C-WHIZ DFNLP DOC DOT FortLP FSQP GAMS GAUSS GENESIS GENOS GINO GRG2 HOMPACK IMSL Fortran and C Library LAMPS LANCELOT LBFGS LINDO LINGO LNOS LPsolver LSGRG2 LSNNO LSSOL M1QN2 and M1QN3 MATLAB MINOS MINPACK-1 MIPIII MODULOPT NAG C library NAG Fortran Library NETFLOW NETSOLVE NITSOL NLPE NLPQL NLPQLB NLSFIT NLSSOL NLPSPR NPSOL OB1 ODRPACK OPSYC OptiA OPTIMA Library OPTPACK OSL PC- PROG PITCON PORT 3 PROC NLP Q01SUBS QAPP QPOPT SPEAKEASY SQP TENMIN TENSOLVE TN/TNBC TNPACK UNCMIN VE08 VE10 VIG and VIMDA What's Best! Appendix: Internet Software References."
            },
            "slug": "Optimization-Software-Guide-Mor\u00e9-Wright",
            "title": {
                "fragments": [],
                "text": "Optimization Software Guide"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "The purpose of this chapter is to discuss optimization problems and software, as well as some of the techniques used in bound-Constrained Optimization, which have been applied to Integer Programming."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1858955"
                        ],
                        "name": "S. H. Cheng",
                        "slug": "S.-H.-Cheng",
                        "structuredName": {
                            "firstName": "Sheung",
                            "lastName": "Cheng",
                            "middleNames": [
                                "Hun"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. H. Cheng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699285"
                        ],
                        "name": "N. Higham",
                        "slug": "N.-Higham",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Higham",
                            "middleNames": [
                                "John"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Higham"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 63
                            }
                        ],
                        "text": "The modified indefinite factorization is from Cheng and Higham [58]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1772727,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c1f4698e3326071d27ffd1f7239b3ea555e9ab4",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Given a symmetric and not necessarily positive definite matrix A, a modified Cholesky algorithm computes a Cholesky factorization P(A+E)PT = RT R, where P is a permutation matrix and E is a perturbation chosen to make A+E positive definite. The aims include producing a small-normed E and making A+E reasonably well conditioned. Modified Cholesky factorizations are widely used in optimization. We propose a new modified Cholesky algorithm based on a symmetric indefinite factorization computed using a new pivoting strategy of Ashcraft, Grimes, and Lewis. We analyze the effectiveness of the algorithm, both in theory and practice, showing that the algorithm is competitive with the existing algorithms of Gill, Murray, and Wright and Schnabel and Eskow. Attractive features of the new algorithm include easy-to-interpret inequalities that explain the extent to which it satisfies its design goals, and the fact that it can be implemented in terms of existing software."
            },
            "slug": "A-Modified-Cholesky-Algorithm-Based-on-a-Symmetric-Cheng-Higham",
            "title": {
                "fragments": [],
                "text": "A Modified Cholesky Algorithm Based on a Symmetric Indefinite Factorization"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A new modified Cholesky algorithm based on a symmetric indefinite factorization computed using a new pivoting strategy of Ashcraft, Grimes, and Lewis is proposed, showing that the algorithm is competitive with the existing algorithms of Gill, Murray, and Wright and Schnabel and Eskow."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741320"
                        ],
                        "name": "V. Klee",
                        "slug": "V.-Klee",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Klee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Klee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145569496"
                        ],
                        "name": "G. Minty",
                        "slug": "G.-Minty",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Minty",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Minty"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117965841,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6f26997d3f1c65fcdaea691b8a2c13b7ec69b3aa",
            "isKey": false,
            "numCitedBy": 1005,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : By constructing long 'increasing' paths on appropriate convex polytopes, It is shown that the simplex algorithm for linear programs (at least with its most commonly used pivot rule) is not a 'good algorithm' in the sense of J. Edmonds. That is, the number of pivots or iterations that may be required is not majorized by any polynomial function of the two parameters that specify the size of the program. (Author)"
            },
            "slug": "HOW-GOOD-IS-THE-SIMPLEX-ALGORITHM-Klee-Minty",
            "title": {
                "fragments": [],
                "text": "HOW GOOD IS THE SIMPLEX ALGORITHM"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "By constructing long 'increasing' paths on appropriate convex polytopes, it is shown that the simplex algorithm for linear programs is not a 'good algorithm' in the sense of J. Edmonds."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145221576"
                        ],
                        "name": "H. Walker",
                        "slug": "H.-Walker",
                        "structuredName": {
                            "firstName": "Homer",
                            "lastName": "Walker",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Walker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14203357,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8432c9fbe845e8c1a5225a20aab37d1d1868eef4",
            "isKey": false,
            "numCitedBy": 315,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The standard implementation of the GMRES method for solving large nonsymmetric linear systems involves a Gram-Schmidt process which is a potential source of significant numerical error. An alternative implementation is outlined here in which orthogonalization by Householder transformations replaces the Gram-Schmidt process. This implementation requires slightly less storage but somewhat more arithmetic than the standard one; however, numerical experiments suggest that it is more stable, especially as the limits of residual reduction are reached. The extra arithmetic required may be less significant when products of the coefficient matrix with vectors are expensive or on vector and, in particular, parallel machines."
            },
            "slug": "Implementation-of-the-GMRES-method-using-Walker",
            "title": {
                "fragments": [],
                "text": "Implementation of the GMRES method using householder transformations"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "An alternative implementation of the GMRES method is outlined here in which orthogonalization by Householder transformations replaces the Gram-Schmidt process, which requires slightly less storage but somewhat more arithmetic."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48306096"
                        ],
                        "name": "M. Hestenes",
                        "slug": "M.-Hestenes",
                        "structuredName": {
                            "firstName": "Magnus",
                            "lastName": "Hestenes",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hestenes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50079822"
                        ],
                        "name": "E. Stiefel",
                        "slug": "E.-Stiefel",
                        "structuredName": {
                            "firstName": "Eduard",
                            "lastName": "Stiefel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Stiefel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 81
                            }
                        ],
                        "text": "The conjugate gradient method was developed in the 1950s by Hestenes and Stiefel [168] as an alternative to factorization methods for finding solutions of symmetric positive definite systems."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2207234,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "466daddfb6340c28cb8da548007028c8cc5df687",
            "isKey": false,
            "numCitedBy": 7184,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "An iterative algorithm is given for solving a system Ax=k of n linear equations in n unknowns. The solution is given in n steps. It is shown that this method is a special case of a very general method which also includes Gaussian elimination. These general algorithms are essentially algorithms for finding an n dimensional ellipsoid. Connections are made with the theory of orthogonal polynomials and continued fractions."
            },
            "slug": "Methods-of-conjugate-gradients-for-solving-linear-Hestenes-Stiefel",
            "title": {
                "fragments": [],
                "text": "Methods of conjugate gradients for solving linear systems"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "An iterative algorithm is given for solving a system Ax=k of n linear equations in n unknowns and it is shown that this method is a special case of a very general method which also includes Gaussian elimination."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1952
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735269"
                        ],
                        "name": "T. Coleman",
                        "slug": "T.-Coleman",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Coleman",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Coleman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745867"
                        ],
                        "name": "B. S. Garbow",
                        "slug": "B.-S.-Garbow",
                        "structuredName": {
                            "firstName": "Burton",
                            "lastName": "Garbow",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. S. Garbow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144241870"
                        ],
                        "name": "J. J. Mor\u00e9",
                        "slug": "J.-J.-Mor\u00e9",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Mor\u00e9",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. J. Mor\u00e9"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6658335,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3df5784804b9349464d2495ab7b06616e68ffc9a",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "The solution of a nonlinear optimization problem often requires an estimate of the Hessian matrix for a function f. In large scale problems, the Hessian matrix is usually sparse, and then estimation by differences of gradients is attractive because the number of differences can be small compared to the dimension of the problem. In this paper we describe a set of subroutines whose purpose is to estimate the Hessian matrix with the least possible number of gradient evaluations."
            },
            "slug": "Software-for-estimating-sparse-Hessian-matrices-Coleman-Garbow",
            "title": {
                "fragments": [],
                "text": "Software for estimating sparse Hessian matrices"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A set of subroutines whose purpose is to estimate the Hessian matrix with the least possible number of gradient evaluations is described."
            },
            "venue": {
                "fragments": [],
                "text": "TOMS"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723559"
                        ],
                        "name": "A. Conn",
                        "slug": "A.-Conn",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Conn",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Conn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144874680"
                        ],
                        "name": "N. Gould",
                        "slug": "N.-Gould",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Gould",
                            "middleNames": [
                                "I.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Gould"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145758858"
                        ],
                        "name": "P. Toint",
                        "slug": "P.-Toint",
                        "structuredName": {
                            "firstName": "Philippe",
                            "lastName": "Toint",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Toint"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 28028770,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "c25fa912843e381cda10f339c9498d86a64b5989",
            "isKey": false,
            "numCitedBy": 192,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Quasi-Newton algorithms for unconstrained nonlinear minimization generate a sequence of matrices that can be considered as approximations of the objective function second derivatives. This paper gives conditions under which these approximations can be proved to converge globally to the true Hessian matrix, in the case where the Symmetric Rank One update formula is used. The rate of convergence is also examined and proven to be improving with the rate of convergence of the underlying iterates. The theory is confirmed by some numerical experiments that also show the convergence of the Hessian approximations to be substantially slower for other known quasi-Newton formulae."
            },
            "slug": "Convergence-of-quasi-Newton-matrices-generated-by-Conn-Gould",
            "title": {
                "fragments": [],
                "text": "Convergence of quasi-Newton matrices generated by the symmetric rank one update"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Conditions under which these approximations can be proved to converge globally to the true Hessian matrix are given, in the case where the Symmetric Rank One update formula is used."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3158732"
                        ],
                        "name": "P. Boggs",
                        "slug": "P.-Boggs",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Boggs",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Boggs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144448267"
                        ],
                        "name": "R. Byrd",
                        "slug": "R.-Byrd",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Byrd",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Byrd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2062162480"
                        ],
                        "name": "J. R. Donaldson",
                        "slug": "J.-R.-Donaldson",
                        "structuredName": {
                            "firstName": "Janet",
                            "lastName": "Donaldson",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. R. Donaldson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46190125"
                        ],
                        "name": "R. Schnabel",
                        "slug": "R.-Schnabel",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schnabel",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schnabel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 116154431,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "da54a144002a763e9385181c6b3fb1d438d661f0",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : This paper describes ORDPACK, a software package for the Orthogonal Distance Regression (ODR) problem. This software implements the algorithm for finding the set of parameters that minimize the sum of the squared orthogonal distances from a set of observations to a curve determined by the parameters. It can also be used to solve the ordinary nonlinear least squares problem. The ODR procedure has application to curve fitting, and to the errors in variables problem in statistics. The algorithm implemented is an efficient and stable trust region (Levenberg-Marquardt) procedure that exploits the structure of the problem so that the computational cost per iteration is equal to that for the same type of algorithm applied to the ordinary nonlinear least squares problem. The package allows a general weighting scheme, provides for finite differences derivatives, and contains extensive error checking and report generating facilities."
            },
            "slug": "ODRPACK-Software-for-Weighted-Orthogonal-Distance-Boggs-Byrd",
            "title": {
                "fragments": [],
                "text": "ODRPACK Software for Weighted Orthogonal Distance Regression."
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The algorithm implemented is an efficient and stable trust region (Levenberg-Marquardt) procedure that exploits the structure of the problem so that the computational cost per iteration is equal to that for the same type of algorithm applied to the ordinary nonlinear least squares problem."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804540"
                        ],
                        "name": "I. Duff",
                        "slug": "I.-Duff",
                        "structuredName": {
                            "firstName": "Iain",
                            "lastName": "Duff",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Duff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144942152"
                        ],
                        "name": "J. Reid",
                        "slug": "J.-Reid",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Reid",
                            "middleNames": [
                                "Ker"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Reid"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2908490,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0c152c85192ebf3fa028a49c9662b133113184c5",
            "isKey": false,
            "numCitedBy": 102,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe the design of a new code for the direct solution of sparse unsymmetric linear systems of equations. The new code utilizes a novel restructuring of the symbolic and numerical phases, which increases speed and saves storage without sacrifice of numerical stability. Other features include switching to full-matrix processing in all phases of the computation enabling the use of all three levels of BLAS, treatment of rectangular or rank-deficient matrices, partial factorization, and integrated facilities for iterative refinement and error estimation."
            },
            "slug": "The-design-of-MA48:-a-code-for-the-direct-solution-Duff-Reid",
            "title": {
                "fragments": [],
                "text": "The design of MA48: a code for the direct solution of sparse unsymmetric linear systems of equations"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "The design of a new code for the direct solution of sparse unsymmetric linear systems of equations is described, which increases speed and saves storage without sacrifice of numerical stability."
            },
            "venue": {
                "fragments": [],
                "text": "TOMS"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711460"
                        ],
                        "name": "Chih-Jen Lin",
                        "slug": "Chih-Jen-Lin",
                        "structuredName": {
                            "firstName": "Chih-Jen",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Jen Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144241870"
                        ],
                        "name": "J. J. Mor\u00e9",
                        "slug": "J.-J.-Mor\u00e9",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Mor\u00e9",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. J. Mor\u00e9"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 84
                            }
                        ],
                        "text": "Software for more general problems, such as LANCELOT [72], KNITRO/CG [50], and TRON [192], employ Newton\u2013CG methods when applied to unconstrained problems."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 103
                            }
                        ],
                        "text": "A trust-region Newton\u2013CG method using this preconditioner is implemented in the LANCELOT [72] and TRON [192] codes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9798402,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "c54952751ddcb0e6488085616d56ee81293ea0c6",
            "isKey": false,
            "numCitedBy": 308,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "We analyze a trust region version of Newton's method for bound-constrained problems. Our approach relies on the geometry of the feasible set, not on the particular representation in terms of constraints. The convergence theory holds for linearly constrained problems and yields global and superlinear convergence without assuming either strict complementarity or linear independence of the active constraints. We also show that the convergence theory leads to an efficient implementation for large bound-constrained problems."
            },
            "slug": "Newton's-Method-for-Large-Bound-Constrained-Lin-Mor\u00e9",
            "title": {
                "fragments": [],
                "text": "Newton's Method for Large Bound-Constrained Optimization Problems"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "A trust region version of Newton's method for bound-constrained problems that holds for linearly constrained problems and yields global and superlinear convergence without assuming either strict complementarity or linear independence of the active constraints."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Optim."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34566790"
                        ],
                        "name": "Shih-Ping Han",
                        "slug": "Shih-Ping-Han",
                        "structuredName": {
                            "firstName": "Shih-Ping",
                            "lastName": "Han",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shih-Ping Han"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 27009396,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "84afabba27330a66943cad96ce13e8eb8f64741c",
            "isKey": false,
            "numCitedBy": 447,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper sufficient conditions for local and superlinear convergence to a Kuhn\u2014Tucker point are established for a class of algorithms which may be broadly defined and comprise a quadratic programming algorithm for repeated solution of a subproblem and a variable metric update to develop the Hessian in the subproblem. In particular the DFP update and an update attributed to Powell are shown to provide a superlinear convergent subclass of algorithms provided a start is made sufficiently close to the solution and the initial Hessian in the subproblem is sufficiently close to the Hessian of the Lagrangian at this point."
            },
            "slug": "Superlinearly-convergent-variable-metric-algorithms-Han",
            "title": {
                "fragments": [],
                "text": "Superlinearly convergent variable metric algorithms for general nonlinear programming problems"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "In this paper sufficient conditions for local and superlinear convergence to a Kuhn\u2014Tucker point are established for a class of algorithms which may be broadly defined and comprise a quadratic programming algorithm for repeated solution of a subproblem and a variable metric update to develop the Hessian in the subproblem."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 1976
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144733293"
                        ],
                        "name": "R. Fletcher",
                        "slug": "R.-Fletcher",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Fletcher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fletcher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34729114"
                        ],
                        "name": "A. Grothey",
                        "slug": "A.-Grothey",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Grothey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Grothey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678370"
                        ],
                        "name": "S. Leyffer",
                        "slug": "S.-Leyffer",
                        "structuredName": {
                            "firstName": "Sven",
                            "lastName": "Leyffer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Leyffer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14885867,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "06049d95137c2ab21055f46b9bf1c9050650155a",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "In nonlinear optimization it is often important to estimate large sparse Hessian or Jacobian matrices, to be used for example in a trust region method. We propose an algorithm for computing a matrix B with a given sparsity pattern from a bundle of the m most recent difference vectors \n \n$$\\Delta = \\left[ {{{\\delta }^{{k - m + 1}}} \\ldots {{\\delta }^{k}}} \\right],\\Gamma = \\left[ {{{\\gamma }^{{k - m + 1}}} \\ldots {{\\gamma }^{k}}} \\right] $$ \n \nwhere B should approximately map \u25b3 into \u0413. In this paper B is chosen such that it satisfies m quasi\u2014Newton conditions B\u25b3 = \u0413 in the least squares sense."
            },
            "slug": "Computing-Sparse-Hessian-and-Jacobian-with-Optimal-Fletcher-Grothey",
            "title": {
                "fragments": [],
                "text": "Computing Sparse Hessian and Jacobian Approximations with Optimal Hereditary Properties"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper proposes an algorithm for computing a matrix B with a given sparsity pattern from a bundle of the m most recent difference vectors that satisfies m quasi\u2014Newton conditions B\u25b3 = \u0413 in the least squares sense."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2870938"
                        ],
                        "name": "R. Dembo",
                        "slug": "R.-Dembo",
                        "structuredName": {
                            "firstName": "Ron",
                            "lastName": "Dembo",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Dembo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777744"
                        ],
                        "name": "S. Eisenstat",
                        "slug": "S.-Eisenstat",
                        "structuredName": {
                            "firstName": "Stanley",
                            "lastName": "Eisenstat",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Eisenstat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2572541"
                        ],
                        "name": "T. Steihaug",
                        "slug": "T.-Steihaug",
                        "structuredName": {
                            "firstName": "Trond",
                            "lastName": "Steihaug",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Steihaug"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 94
                            }
                        ],
                        "text": "All the results presented in this section, which are proved by Dembo, Eisenstat, and Steihaug [89], are local in nature: They assume that the sequence {xk} eventually enters the near vicinity of the solution x\u2217."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 851,
                                "start": 843
                            }
                        ],
                        "text": "21 Models for Trust-Region Methods 26 Scaling 27 Rates of Convergence 28 R-Rates of Convergence 29 Notes and References 30 Exercises 30\n3 Line Search Methods 34 3.1 Step Length 36\nThe Wolfe Conditions 37 The Goldstein Conditions 41 Sufficient Decrease and Backtracking 41\n3.2 Convergence of Line Search Methods 43 3.3 Rate of Convergence 46\nConvergence Rate of Steepest Descent 47 Quasi-Newton Methods 49 Newton's Method 51 Coordinate Descent Methods 53\n3.4 Step-Length Selection Algorithms 55 Interpolation 56 The Initial Step Length 58 A Line Search Algorithm for the Wolfe Conditions 58 Notes and References 61 Exercises 62\n4 Trust-Region Methods 64 Outline of the Algorithm 67\n4.1 The Cauchy Point and Related Algorithms 69 The Cauchy Point 69 Improving on the Cauchy Point 70 The Dogleg Method 71 Two-Dimensional Subspace Minimization 74 Steihaug's Approach 75 4.2 Using Nearly Exact Solutions to the Subproblem 77 Characterizing Exact Solutions 77 Calculating Nearly Exact Solutions 78 The Hard Case 82 Proof of Theorem 4.3 84 4.3 Global Convergence 87\nCONTENTS xiii\nReduction Obtained by the Cauchy Point 87 Convergence to Stationary Points 89 Convergence of Algorithms Based on Nearly Exact Solutions 93\n4.4 Other Enhancements 94 Scaling 94 Non-Euclidean Trust Regions 96 Notes and References 97 Exercises 97\nConjugate Gradient Methods 100 5.1 The Linear Conjugate Gradient Method 102\nConjugate Direction Methods 102 Basic Properties of the Conjugate Gradient Method 107 A Practical Form of the Conjugate Gradient Method I l l Rate of Convergence 112 Preconditioning 118 Practical Preconditioners 119\n5.2 Nonlinear Conjugate Gradient Methods 120 The Fletcher-Reeves Method 120 The Polak-Ribiere Method 121 Quadratic Termination and Restarts 122 Numerical Performance 124 Behavior of the Fletcher-Reeves Method 124 Global Convergence 127 Notes and References 131 Exercises 132\nPractical Newton'Methods 134 6.1 Inexact Newton Steps 136 6.2 Line Search Newton Methods 139\nLine Search Newton-CG Method 139 Modified Newton's Method 141\n6.3 Hessian Modifications ."
                    },
                    "intents": []
                }
            ],
            "corpusId": 122513309,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8e9ef67f33ee5d027cd29b317cb0ee96df2b6f7a",
            "isKey": true,
            "numCitedBy": 1509,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "A classical algorithm for solving the system of nonlinear equations $F(x) = 0$ is Newton\u2019s method \\[ x_{k + 1} = x_k + s_k ,\\quad {\\text{where }}F'(x_k )s_k = - F(x_k ),\\quad x_0 {\\text{ given}}.\\]..."
            },
            "slug": "INEXACT-NEWTON-METHODS-Dembo-Eisenstat",
            "title": {
                "fragments": [],
                "text": "INEXACT NEWTON METHODS"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145204338"
                        ],
                        "name": "C. Kelley",
                        "slug": "C.-Kelley",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Kelley",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Kelley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122790803,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "708f2e12fa6e063d9b68e20ab3892639f7d8107f",
            "isKey": false,
            "numCitedBy": 2547,
            "numCiting": 99,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface How to Get the Software Part I. Linear Equations. 1. Basic Concepts and Stationary Iterative Methods 2. Conjugate Gradient Iteration 3. GMRES Iteration Part II. Nonlinear Equations. 4. Basic Concepts and Fixed Point Iteration 5. Newton's Method 6. Inexact Newton Methods 7. Broyden's Method 8. Global Convergence Bibliography Index."
            },
            "slug": "Iterative-Methods-for-Linear-and-Nonlinear-Kelley",
            "title": {
                "fragments": [],
                "text": "Iterative Methods for Linear and Nonlinear Equations"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "Preface How to Get the Software How to get the Software Part I."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52067790"
                        ],
                        "name": "N. Gould",
                        "slug": "N.-Gould",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Gould",
                            "middleNames": [
                                "I.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Gould"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 123137294,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "539336be3a47b2e0b8ff2f6edcaedd6399f3610c",
            "isKey": false,
            "numCitedBy": 69,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "The convergence behaviour of a class of iterative methods for solving the constrained minimization problem is analysed. The methods are based on the sequential minimization of a simple differentiable penalty function. They are sufficiently general to ensure global convergence of the iterates to the solution of the problem at an asymptotic (two-step Q-) superlinear rate."
            },
            "slug": "On-the-convegence-of-a-sequential-penalty-function-Gould",
            "title": {
                "fragments": [],
                "text": "On the convegence of a sequential penalty function method for constrained minimization"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "The convergence behaviour of a class of iterative methods for solving the constrained minimization problem is analysed and it is found that they are sufficiently general to ensure global convergence of the iterates to the solution of the problem at an asymptotic (two-step Q-) superlinear rate."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107206555"
                        ],
                        "name": "A. I. Cohen",
                        "slug": "A.-I.-Cohen",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Cohen",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. I. Cohen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 6
                            }
                        ],
                        "text": "Cohen [63] and Burmeister [45] prove n-step"
                    },
                    "intents": []
                }
            ],
            "corpusId": 120071880,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "dd2d1a22bbbb3e21fe5405b542b86bd3104f5579",
            "isKey": false,
            "numCitedBy": 92,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Conjugate gradient algorithms are used to minimize nonlinear, nonquadratic, real-valued functions on $\\mathbb{R}^n $. Rates of convergence are found for several of these algorithms where the conjugate variable is reinitialized every r steps (where r is greater than or equal to n). It is shown in a neighborhood of the minimum that the error, when starting from a point of reinitialization, decreases by order 2 after n steps."
            },
            "slug": "Rate-of-convergence-of-several-conjugate-gradient-Cohen",
            "title": {
                "fragments": [],
                "text": "Rate of convergence of several conjugate gradient algorithms."
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown in a neighborhood of the minimum that the error, when starting from a point of reinitialization, decreases by order 2 after n steps."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145983059"
                        ],
                        "name": "A. Griewank",
                        "slug": "A.-Griewank",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Griewank",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Griewank"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145758858"
                        ],
                        "name": "P. Toint",
                        "slug": "P.-Toint",
                        "structuredName": {
                            "firstName": "Philippe",
                            "lastName": "Toint",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Toint"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121362190,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d10c40235eea4c44929ce30c7414f7416dcbcf94",
            "isKey": false,
            "numCitedBy": 164,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "SummaryThis paper presents a minimization method based on the idea of partitioned updating of the Hessian matrix in the case where the objective function can be decomposed in a sum of convex \u201celement\u201d functions. This situation occurs in a large class of practical problems including nonlinear finite elements calculations. Some theoretical and algorithmic properties of the update are discussed and encouraging numerical results are presented."
            },
            "slug": "Partitioned-variable-metric-updates-for-large-Griewank-Toint",
            "title": {
                "fragments": [],
                "text": "Partitioned variable metric updates for large structured optimization problems"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "A minimization method based on the idea of partitioned updating of the Hessian matrix in the case where the objective function can be decomposed in a sum of convex \u201celement\u201d functions is presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52356551"
                        ],
                        "name": "T. Choi",
                        "slug": "T.-Choi",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Choi",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Choi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145204338"
                        ],
                        "name": "C. Kelley",
                        "slug": "C.-Kelley",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Kelley",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Kelley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 115353,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "00e34b54dff233a4225fb2924d984edfc0404aeb",
            "isKey": false,
            "numCitedBy": 72,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we show how the implicit filtering algorithm can be coupled with the BFGS quasi-Newton update to obtain a superlinearly convergent iteration if the noise in the objective function decays sufficiently rapidly as the optimal point is approached. In this way we give insight into the observations of good performance in practice of quasi-Newton methods when they are coupled with implicit filtering. We also report on numerical experiments that show how an implementation of implicit filtering that exploits these new results can improve the performance of the algorithm."
            },
            "slug": "Superlinear-Convergence-and-Implicit-Filtering-Choi-Kelley",
            "title": {
                "fragments": [],
                "text": "Superlinear Convergence and Implicit Filtering"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "This paper shows how the implicit filtering algorithm can be coupled with the BFGS quasi-Newton update to obtain a superlinearly convergent iteration if the noise in the objective function decays sufficiently rapidly as the optimal point is approached."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Optim."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48221642"
                        ],
                        "name": "J. Grimm",
                        "slug": "J.-Grimm",
                        "structuredName": {
                            "firstName": "Jos\u00e9",
                            "lastName": "Grimm",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Grimm"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34635073"
                        ],
                        "name": "L. Pottier",
                        "slug": "L.-Pottier",
                        "structuredName": {
                            "firstName": "Lo\u00efc",
                            "lastName": "Pottier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Pottier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403842437"
                        ],
                        "name": "N. Rostaing-Schmidt",
                        "slug": "N.-Rostaing-Schmidt",
                        "structuredName": {
                            "firstName": "Nicole",
                            "lastName": "Rostaing-Schmidt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Rostaing-Schmidt"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 137
                            }
                        ],
                        "text": "Descriptions of this approach, sometimes known as checkpointing, can be found in Griewank [150] and Grimm, Pottier, and Rostaing-Schmidt [157]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15487824,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "87c8dc434a38f191c617ca869fddacc0f22c51f7",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "This report concerns time/space trade-offs for the reverse mode of automatic differentiation on the straight-line programs with nested loops class of programs. In the first part we consider the problem of reversing a finite sequence given by $u_{n+1}=3Df(u_n)$, which can mode lize a certain class of finite loops. We show an optimal time strategy for this problem, the number of available registers being fixed, and a lower bound on the time-space product equal to $\\frac{p (\\ln p)^2}{(\\ln 4)^2}$. We then present an optimal strategy on nested loops with the objective of taking care of the program structure. Finally we consider an application of this storage/recomputation strategy to compute in reverse mode the derivatives of a function represented as a {\\sc fortran} program."
            },
            "slug": "Optimal-Time-and-Minimum-Space-Time-Product-for-a-Grimm-Pottier",
            "title": {
                "fragments": [],
                "text": "Optimal Time and Minimum Space-Time Product for Reversing a Certain Class of Programs"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "This report concerns time/space trade-offs for the reverse mode of automatic differentiation on the straight-line programs with nested loops class of programs and presents an optimal strategy on nested loops with the objective of taking care of the program structure."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34566790"
                        ],
                        "name": "Shih-Ping Han",
                        "slug": "Shih-Ping-Han",
                        "structuredName": {
                            "firstName": "Shih-Ping",
                            "lastName": "Han",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shih-Ping Han"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 123023247,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "df49f31377cadf5b09cb3eef3c0bbbf124176e47",
            "isKey": false,
            "numCitedBy": 1047,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently developed Newton and quasi-Newton methods for nonlinear programming possess only local convergence properties. Adopting the concept of the damped Newton method in unconstrained optimization, we propose a stepsize procedure to maintain the monotone decrease of an exact penalty function. In so doing, the convergence of the method is globalized."
            },
            "slug": "A-globally-convergent-method-for-nonlinear-Han",
            "title": {
                "fragments": [],
                "text": "A globally convergent method for nonlinear programming"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102835246"
                        ],
                        "name": "J. Hiriart-Urruty",
                        "slug": "J.-Hiriart-Urruty",
                        "structuredName": {
                            "firstName": "Jean-Baptiste",
                            "lastName": "Hiriart-Urruty",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hiriart-Urruty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8773884"
                        ],
                        "name": "C. Lemar\u00e9chal",
                        "slug": "C.-Lemar\u00e9chal",
                        "structuredName": {
                            "firstName": "Claude",
                            "lastName": "Lemar\u00e9chal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lemar\u00e9chal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 106
                            }
                        ],
                        "text": "Nonsmooth optimization is beyond the scope of this book; we refer instead to HiriartUrruty and Lemar\u00e9chal [170] for an extensive discussion of theory."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 96
                            }
                        ],
                        "text": "A more analytical treatment of nonsmooth optimization is given by Hiriart-Urruty and Lemar\u00e9chal [170]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 118755909,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "065c019bb27e23ed10492ca61b186eb9375dd5de",
            "isKey": false,
            "numCitedBy": 2940,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "IX. Inner Construction of the Subdifferential.- X. Conjugacy in Convex Analysis.- XI. Approximate Subdifferentials of Convex Functions.- XII. Abstract Duality for Practitioners.- XIII. Methods of ?-Descent.- XIV. Dynamic Construction of Approximate Subdifferentials: Dual Form of Bundle Methods.- XV. Acceleration of the Cutting-Plane Algorithm: Primal Forms of Bundle Methods.- Bibliographical Comments.- References."
            },
            "slug": "Convex-analysis-and-minimization-algorithms-Hiriart-Urruty-Lemar\u00e9chal",
            "title": {
                "fragments": [],
                "text": "Convex analysis and minimization algorithms"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51043392"
                        ],
                        "name": "J. Gondzio",
                        "slug": "J.-Gondzio",
                        "structuredName": {
                            "firstName": "Jacek",
                            "lastName": "Gondzio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gondzio"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14928865,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "555af317d0ac25faa7e5b9514249790f6c616aef",
            "isKey": false,
            "numCitedBy": 119,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "HOPDM-(version-2.12)-\u2014-A-fast-LP-solver-based-on-a-Gondzio",
            "title": {
                "fragments": [],
                "text": "HOPDM (version 2.12) \u2014 A fast LP solver based on a primal-dual interior point method"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145555137"
                        ],
                        "name": "G. Schuller",
                        "slug": "G.-Schuller",
                        "structuredName": {
                            "firstName": "Gunther",
                            "lastName": "Schuller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Schuller"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 66
                            }
                        ],
                        "text": "Even faster rates of convergence can be established (see Schuller [278], Ritter [265]), under the assumption that the search directions are uniformly linearly independent, but this assumption is hard to verify and does not often occur in practice."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 122779359,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "baad78a37464c59c0a305f83c4bfe17aa918681e",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "SummaryThis paper deals with the problem of finding the order of convergence of certain Quasi-Newton-methods for function minimizations. It is shown that these methods have the same order of convergence \u03c4>1, \u03c4n+1\u2212\u03c4n\u22121=0, as (n+1)-point secant methods, if comparable assumptions are made."
            },
            "slug": "On-the-order-of-convergence-of-certain-Schuller",
            "title": {
                "fragments": [],
                "text": "On the order of convergence of certain Quasi-Newton-methods"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723232"
                        ],
                        "name": "M. Overton",
                        "slug": "M.-Overton",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Overton",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Overton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61075121,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fc7a9cf7b10b4ec21924c2d3b046daedc5f76a88",
            "isKey": false,
            "numCitedBy": 293,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A broad overview of numerical computing, in a historical context, with a special focus on the IEEE standard for binary floating point arithmetic. Key ideas are developed step by step, taking the reader from floating point representation, correctly rounded arithmetic, and the IEEE philosophy on exceptions, to an understanding of the crucial concepts of conditioning and stability, explained in a simple yet rigorous context. There are technical details and challenging exercises that go beyond the subjects covered in the text."
            },
            "slug": "Numerical-Computing-with-IEEE-Floating-Point-Overton",
            "title": {
                "fragments": [],
                "text": "Numerical Computing with IEEE Floating Point Arithmetic"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "A broad overview of numerical computing, in a historical context, with a special focus on the IEEE standard for binary floating point arithmetic, explained in a simple yet rigorous context."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145064923"
                        ],
                        "name": "C. Lawson",
                        "slug": "C.-Lawson",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Lawson",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lawson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144308260"
                        ],
                        "name": "R. Hanson",
                        "slug": "R.-Hanson",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Hanson",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hanson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 122862057,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b35e036e0226d923d03c06ae393084b40c9dec7e",
            "isKey": false,
            "numCitedBy": 4447,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Since the lm function provides a lot of features it is rather complicated. So we are going to instead use the function lsfit as a model. It computes only the coefficient estimates and the residuals. Now would be a good time to read the help file for lsfit. Note that lsfit supports the fitting of multiple least squares models and weighted least squares. Our function will not, hence we can omit the arguments wt, weights and yname. Also, changing tolerances is a little advanced so we will trust the default values and omit the argument tolerance as well."
            },
            "slug": "Solving-least-squares-problems-Lawson-Hanson",
            "title": {
                "fragments": [],
                "text": "Solving least squares problems"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "Since the lm function provides a lot of features it is rather complicated so it is going to instead use the function lsfit as a model, which computes only the coefficient estimates and the residuals."
            },
            "venue": {
                "fragments": [],
                "text": "Classics in applied mathematics"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144733293"
                        ],
                        "name": "R. Fletcher",
                        "slug": "R.-Fletcher",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Fletcher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fletcher"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14347873,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e471426fffc20e84f680ec330283f4364ba8fd91",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract.Stable techniques are considered for updating the reduced Hessian matrix that arises in a null-space active set method for quadratic programming when the Hessian matrix itself may be indefinite. A scheme for defining and updating the null-space basis matrix is described which is adequately stable and allows advantage to be taken of sparsity in the constraint matrix. A new canonical form for the reduced Hessian matrix is proposed that can be updated in a numerically stable way. Some consequences for the choice of minor iteration search direction are described."
            },
            "slug": "Stable-reduced-Hessian-updates-for-indefinite-Fletcher",
            "title": {
                "fragments": [],
                "text": "Stable reduced Hessian updates for indefinite quadratic programming"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A new canonical form for the reduced Hessian matrix is proposed that can be updated in a numerically stable way and allows advantage to be taken of sparsity in the constraint matrix."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144873562"
                        ],
                        "name": "R. Bellman",
                        "slug": "R.-Bellman",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Bellman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bellman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145718274"
                        ],
                        "name": "L. Zadeh",
                        "slug": "L.-Zadeh",
                        "structuredName": {
                            "firstName": "Lotfi",
                            "lastName": "Zadeh",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Zadeh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 61922378,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "64c1064a50a10f651a02d17faa714baf12dba488",
            "isKey": false,
            "numCitedBy": 7053,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "By decision-making in a fuzzy environment is meant a decision process in which the goals and/or the constraints, but not necessarily the system under control, are fuzzy in nature. This means that the goals and/or the constraints constitute classes of alternatives whose boundaries are not sharply defined. \n \nAn example of a fuzzy constraint is: \u201cThe cost of A should not be substantially higher than \u03b1,\u201d where \u03b1 is a specified constant. Similarly, an example of a fuzzy goal is: \u201cx should be in the vicinity of x0,\u201d where x0 is a constant. The italicized words are the sources of fuzziness in these examples. \n \nFuzzy goals and fuzzy constraints can be defined precisely as fuzzy sets in the space of alternatives. A fuzzy decision, then, may be viewed as an intersection of the given goals and constraints. A maximizing decision is defined as a point in the space of alternatives at which the membership function of a fuzzy decision attains its maximum value. \n \nThe use of these concepts is illustrated by examples involving multistage decision processes in which the system under control is either deterministic or stochastic. By using dynamic programming, the determination of a maximizing decision is reduced to the solution of a system of functional equations. A reverse-flow technique is described for the solution of a functional equation arising in connection with a decision process in which the termination time is defined implicitly by the condition that the process stops when the system under control enters a specified set of states in its state space."
            },
            "slug": "Decision-making-in-a-fuzzy-environment-Bellman-Zadeh",
            "title": {
                "fragments": [],
                "text": "Decision-making in a fuzzy environment"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A reverse-flow technique is described for the solution of a functional equation arising in connection with a decision process in which the termination time is defined implicitly by the condition that the process stops when the system under control enters a specified set of states in its state space."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145983059"
                        ],
                        "name": "A. Griewank",
                        "slug": "A.-Griewank",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Griewank",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Griewank"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 90
                            }
                        ],
                        "text": "Descriptions of this approach, sometimes known as checkpointing, can be found in Griewank [150] and Grimm, Pottier, and Rostaing-Schmidt [157]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15952812,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fad51de16c93a9803caeecd413fcba9962109712",
            "isKey": false,
            "numCitedBy": 397,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "In its basic form the reverse mode of automatic differentiation yields gradient vectors at a small multiple of the computational work needed to evaluate the underlying scalar function. The practical applicability of this temporal complexity result, due originally to Linnainmaa, seemed to be severely limited by the fact that the memory requirement of the basic implementation is proportional to the run timeT, of the original evaluation program. It is shown here that, by a recursive scheme related to the multilevel differentiation approach of Volin and Ostrovskii, the growth in both temporal and spatial complexity can be limited to a fixed multiple of log(T). Other compromises between the run time and memory requirement are possible, so that the reverse mode becomes applicable to computational problems of virtually any size."
            },
            "slug": "Achieving-logarithmic-growth-of-temporal-and-in-Griewank",
            "title": {
                "fragments": [],
                "text": "Achieving logarithmic growth of temporal and spatial complexity in reverse automatic differentiation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown here that, by a recursive scheme related to the multilevel differentiation approach of Volin and Ostrovskii, the growth in both temporal and spatial complexity can be limited to a fixed multiple of log(T)."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145758858"
                        ],
                        "name": "P. Toint",
                        "slug": "P.-Toint",
                        "structuredName": {
                            "firstName": "Philippe",
                            "lastName": "Toint",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Toint"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 55
                            }
                        ],
                        "text": "Sparse quasi-Newton updates have been studied by Toint [288, 289] and Fletcher et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 43207355,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "996d9d6d2af598497ef9cf818cf8895dfd642fdf",
            "isKey": false,
            "numCitedBy": 130,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "A procedure for symmetric matrix updating subject to a linear equation and retaining any sparsity present in the original matrix is derived. The main feature of this procedure is the reduction of the problem to the solution of an n dimensional sparse system of linear equations. The matrix of this system is shown to be symmetric and positive definite. The method depends on the Frobenius matrix norm. Comments are made on the difficulties of extending the technique so that it uses more general norms, the main points being shown by a numerical example."
            },
            "slug": "On-sparse-and-symmetric-matrix-updating-subject-to-Toint",
            "title": {
                "fragments": [],
                "text": "On sparse and symmetric matrix updating subject to a linear equation"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "A procedure for symmetric matrix updating subject to a linear equation and retaining any sparsity present in the original matrix is derived, with the main points being shown by a numerical example."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735269"
                        ],
                        "name": "T. Coleman",
                        "slug": "T.-Coleman",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Coleman",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Coleman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144241870"
                        ],
                        "name": "J. J. Mor\u00e9",
                        "slug": "J.-J.-Mor\u00e9",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Mor\u00e9",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. J. Mor\u00e9"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 51
                            }
                        ],
                        "text": "Curtis, Powell, and Reid [83] and Coleman and Mor\u00e9 [68] provide descriptions of some methods and performance comparisons."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 55566735,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "856ace8239bdd6b9cf19c4b13f8f7bd88ce3cb1f",
            "isKey": false,
            "numCitedBy": 468,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Given a mapping with a sparse Jacobian matrix, the problem of minimizing the number of function evaluations needed to estimate the Jacobian matrix by differences is investigated. This problem can be attacked as a graph coloring problem and this approach leads to very efficient algorithms. The behavior of these algorithms is studied and, in particular, it is proved that two of the algorithms are optimal for band graphs. Numerical evidence is presented which indicates that these two algorithms are nearly optimal on practical problems."
            },
            "slug": "Estimation-of-sparse-jacobian-matrices-and-graph-Coleman-Mor\u00e9",
            "title": {
                "fragments": [],
                "text": "Estimation of sparse jacobian matrices and graph coloring problems"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is proved that two of the algorithms investigated are optimal for band graphs, andumerical evidence indicates that these two algorithms are nearly optimal on practical problems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38145224"
                        ],
                        "name": "H. Scheel",
                        "slug": "H.-Scheel",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Scheel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Scheel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055390"
                        ],
                        "name": "S. Scholtes",
                        "slug": "S.-Scholtes",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Scholtes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Scholtes"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 39592080,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c1174bd9f50f0bb96f1cba1b2d98b10d3bf61fe2",
            "isKey": false,
            "numCitedBy": 633,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We study mathematical programs with complementarity constraints. Several stationarity concepts, based on a piecewise smooth formulation, are presented and compared. The concepts are related to stationarity conditions for certain smooth programs as well as to stationarity concepts for a nonsmooth exact penalty function. Further, we present Fiacco-McCormick type second order optimality conditions and an extension of the stability results of Robinson and Kojima to mathematical programs with complementarity constraints."
            },
            "slug": "Mathematical-Programs-with-Complementarity-and-Scheel-Scholtes",
            "title": {
                "fragments": [],
                "text": "Mathematical Programs with Complementarity Constraints: Stationarity, Optimality, and Sensitivity"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "Several stationarity concepts, based on a piecewise smooth formulation, are presented and compared and Fiacco-McCormick type second order optimality conditions and an extension of the stability results of Robinson and Kojima are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Oper. Res."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790701"
                        ],
                        "name": "Ya-Xiang Yuan",
                        "slug": "Ya-Xiang-Yuan",
                        "structuredName": {
                            "firstName": "Ya-Xiang",
                            "lastName": "Yuan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ya-Xiang Yuan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 58
                            }
                        ],
                        "text": "11) that is at least half as good as the optimal decrease [320]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 44344572,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "29d3ce09da58aac64298078f50c77b76bdfbabdf",
            "isKey": false,
            "numCitedBy": 82,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract.In this paper, we consider the truncated conjugate gradient method for minimizing a convex quadratic function subject to a ball trust region constraint. It is shown that the reduction in the objective function by the solution obtained by the truncated CG method is at least half of the reduction by the global minimizer in the trust region."
            },
            "slug": "On-the-truncated-conjugate-gradient-method-Yuan",
            "title": {
                "fragments": [],
                "text": "On the truncated conjugate gradient method"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown that the reduction in the objective function by the solution obtained by the truncated CG method is at least half of the reduction by the global minimizer in the trust region."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3231797"
                        ],
                        "name": "R. Ge",
                        "slug": "R.-Ge",
                        "structuredName": {
                            "firstName": "Renpu",
                            "lastName": "Ge",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ge"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2294095"
                        ],
                        "name": "M. Powell",
                        "slug": "M.-Powell",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Powell",
                            "middleNames": [
                                "J.",
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Powell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 97
                            }
                        ],
                        "text": "A study of the convergence of BFGS matrices for nonlinear problems can be found in Ge and Powell [119] and Boggs and Tolle [32]; however, the results are not as satisfactory as for SR1 updating."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 8113073,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ef2fa408d7751de25bc2fb274b884ed3a8c569da",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "It is proved that, if the DFP or BFGS algorithm with step-lengths of one is applied to a functionF(x) that has a Lipschitz continuous second derivative, and if the calculated vectors of variables converge to a point at which \u2207F is zero and \u22072F is positive definite, then the sequence of variable metric matrices also converges. The limit of this sequence is identified in the case whenF(x) is a strictly convex quadratic function."
            },
            "slug": "The-convergence-of-variable-metric-matrices-in-Ge-Powell",
            "title": {
                "fragments": [],
                "text": "The convergence of variable metric matrices in unconstrained optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "It is proved that, if the DFP or BFGS algorithm with step-lengths of one is applied to a functionF(x) that has a Lipschitz continuous second derivative, and if the calculated vectors of variables converge to a point at which \u2207F is zero and \u22072F is positive definite, then the sequence of variable metric matrices also converges."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1409732724"
                        ],
                        "name": "M. El-Sayed",
                        "slug": "M.-El-Sayed",
                        "structuredName": {
                            "firstName": "Mohamed",
                            "lastName": "El-Sayed",
                            "middleNames": [
                                "A.",
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. El-Sayed"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5247243,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "9d91c8c9e6332652e2c1055fec5a114a3aaeda43",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Artificial-neural-network-for-reactive-power-El-Sayed",
            "title": {
                "fragments": [],
                "text": "Artificial neural network for reactive power optimization"
            },
            "venue": {
                "fragments": [],
                "text": "Neurocomputing"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744288"
                        ],
                        "name": "P. Gill",
                        "slug": "P.-Gill",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Gill",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Gill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143873253"
                        ],
                        "name": "W. Murray",
                        "slug": "W.-Murray",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Murray",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Murray"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 31396642,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "d8347b095592bc466665fbb44eb2fec3a7678334",
            "isKey": false,
            "numCitedBy": 169,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Numerically stable algorithms for quadratic programming are discussed. A new algorithm is described for indefinite quadratic programming which utilizes methods for updating positivedefinite factorizations only. Consequently all the updating procedures required are common to algorithms for linearly-constrained optimization. The new algorithm can be used for the positive-definite case without loss of efficiency."
            },
            "slug": "Numerically-stable-methods-for-quadratic-Gill-Murray",
            "title": {
                "fragments": [],
                "text": "Numerically stable methods for quadratic programming"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "A new algorithm is described for indefinite quadratic programming which utilizes methods for updating positivedefinite factorizations only and can be used for the positive-definite case without loss of efficiency."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1692336"
                        ],
                        "name": "S. Vavasis",
                        "slug": "S.-Vavasis",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Vavasis",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Vavasis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 33101755,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "b140b699a3d5566ae9d5b7477305a3b8eb69aa94",
            "isKey": false,
            "numCitedBy": 131,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Quadratic-Programming-is-in-NP-Vavasis",
            "title": {
                "fragments": [],
                "text": "Quadratic Programming is in NP"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Process. Lett."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742664"
                        ],
                        "name": "R. Poli",
                        "slug": "R.-Poli",
                        "structuredName": {
                            "firstName": "Riccardo",
                            "lastName": "Poli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Poli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143953756"
                        ],
                        "name": "J. Kennedy",
                        "slug": "J.-Kennedy",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Kennedy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kennedy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1787135"
                        ],
                        "name": "T. Blackwell",
                        "slug": "T.-Blackwell",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Blackwell",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Blackwell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3114196,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b7919bcfa38aa97514187501a23c983e8eb5482b",
            "isKey": false,
            "numCitedBy": 30416,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract\nParticle swarm optimization (PSO) has undergone many changes since its introduction in 1995. As researchers have learned about the technique, they have derived new versions, developed new applications, and published theoretical studies of the effects of the various parameters and aspects of the algorithm. This paper comprises a snapshot of particle swarming from the authors\u2019 perspective, including variations in the algorithm, current and ongoing research, applications and open problems.\n"
            },
            "slug": "Particle-swarm-optimization-Poli-Kennedy",
            "title": {
                "fragments": [],
                "text": "Particle swarm optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A snapshot of particle swarming from the authors\u2019 perspective, including variations in the algorithm, current and ongoing research, applications and open problems, is included."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of ICNN'95 - International Conference on Neural Networks"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50390637"
                        ],
                        "name": "S. M. Robinson",
                        "slug": "S.-M.-Robinson",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Robinson",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. M. Robinson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 29415632,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "54915d57ff1e3526c8c2fc57055ac4bdb162d1d1",
            "isKey": false,
            "numCitedBy": 252,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper establishes quantitative bounds for the variation of an isolated local minimizer for a general nonlinear program under perturbations in the objective function and constraints. These bounds are then applied to establish rates of convergence for a class of recursive nonlinear-programming algorithms."
            },
            "slug": "Perturbed-Kuhn-Tucker-points-and-rates-of-for-a-of-Robinson",
            "title": {
                "fragments": [],
                "text": "Perturbed Kuhn-Tucker points and rates of convergence for a class of nonlinear-programming algorithms"
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735269"
                        ],
                        "name": "T. Coleman",
                        "slug": "T.-Coleman",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Coleman",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Coleman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723559"
                        ],
                        "name": "A. Conn",
                        "slug": "A.-Conn",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Conn",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Conn"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 29772643,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "7e0aec70caad6e26633da9b47e0504fef90bfda0",
            "isKey": false,
            "numCitedBy": 126,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we consider the final stage of a \u2018global\u2019 method to solve the nonlinear programming problem. We prove 2-step superlinear convergence. In the process of analyzing this asymptotic behavior, we compare our method (theoretically) to the popular successive quadratic programming approach."
            },
            "slug": "Nonlinear-programming-via-an-exact-penalty-analysis-Coleman-Conn",
            "title": {
                "fragments": [],
                "text": "Nonlinear programming via an exact penalty function: Asymptotic analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "2-step superlinear convergence is proved to be the final stage of a \u2018global\u2019 method to solve the nonlinear programming problem and is compared (theoretically) to the popular successive quadratic programming approach."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4316949"
                        ],
                        "name": "S. Granville",
                        "slug": "S.-Granville",
                        "structuredName": {
                            "firstName": "S\u00e9rgio",
                            "lastName": "Granville",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Granville"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122120865,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "c6e88a8d3631df56db39ab628d33bbecc9387116",
            "isKey": false,
            "numCitedBy": 844,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "An implementation of an interior point method to the optimal reactive dispatch problem is described. The interior point method used is based on the primal-dual algorithm and the numerical results in large scale networks (1832 and 3467 bus systems) have shown that this technique can be very effective to some optimal power flow applications. >"
            },
            "slug": "Optimal-reactive-dispatch-through-interior-point-Granville",
            "title": {
                "fragments": [],
                "text": "Optimal reactive dispatch through interior point methods"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1894635"
                        ],
                        "name": "Gerald A. Shultz",
                        "slug": "Gerald-A.-Shultz",
                        "structuredName": {
                            "firstName": "Gerald",
                            "lastName": "Shultz",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gerald A. Shultz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46190125"
                        ],
                        "name": "R. Schnabel",
                        "slug": "R.-Schnabel",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schnabel",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schnabel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144448267"
                        ],
                        "name": "R. Byrd",
                        "slug": "R.-Byrd",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Byrd",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Byrd"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 28
                            }
                        ],
                        "text": "Byrd, Schnabel, and Schultz [279], [54] provide a general theory for inexact trustregion methods; they introduce the idea of two-dimensional subspace minimization and also focus on proper handling of the case of indefinite B to ensure stronger local convergence results than Theorems 4."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 97
                            }
                        ],
                        "text": "indefiniteness here, and refer the reader to papers by Byrd, Schnabel, and Schultz (see [54] and [279]) for details."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 62
                            }
                        ],
                        "text": "Our approach here follows that of Schultz, Schnabel, and Byrd [279]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 8101050,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "836faee39b90616b3555f0cedd6e1b7ec3b723ac",
            "isKey": false,
            "numCitedBy": 248,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper has two aims: to exhibit very general conditions under which members of a broad class of unconstrained minimization algorithms are globally convergent in a strong sense, and to propose several new algorithms that use second derivative information and achieve such convergence. In the first part of the paper we present a general trust-region-based algorithm schema that includes an undefined step selection strategy. We give general conditions on this step selection strategy under which limit points of the algorithm will satisfy first and second order necessary conditions for unconstrained minimization. Our algorithm schema is sufficiently broad to include line search algorithms as well. Next, we show that a wide range of step selection strategies satisfy the requirements of our convergence theory. This leads us to propose several new algorithms that use second derivative information and achieve strong global convergence, including an indefinite line search algorithm, several indefinite dogleg algo..."
            },
            "slug": "A-Family-of-Trust-Region-Based-Algorithms-for-with-Shultz-Schnabel",
            "title": {
                "fragments": [],
                "text": "A Family of Trust Region Based Algorithms for Unconstrained Minimization with Strong Global Convergence Properties."
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper presents a general trust-region-based algorithm schema that includes an undefined step selection strategy, and gives general conditions under which limit points of the algorithm will satisfy first and second order necessary conditions for unconstrained minimization."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "118530872"
                        ],
                        "name": "S. Mizuno",
                        "slug": "S.-Mizuno",
                        "structuredName": {
                            "firstName": "Shinji",
                            "lastName": "Mizuno",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mizuno"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1932759"
                        ],
                        "name": "M. Todd",
                        "slug": "M.-Todd",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Todd",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Todd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145185457"
                        ],
                        "name": "Y. Ye",
                        "slug": "Y.-Ye",
                        "structuredName": {
                            "firstName": "Yinyu",
                            "lastName": "Ye",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Ye"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 41886303,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "79a1342e2bc96a2fc2ed0341c9e3f31e6edace3d",
            "isKey": false,
            "numCitedBy": 413,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe several adaptive-step primal-dual interior point algorithms for linear programming. All have polynomial time complexity while some allow very long steps in favorable circumstances. We provide heuristic reasoning for expecting that the algorithms will perform much better in practice than guaranteed by the worst-case estimates, based on an analysis using a nonrigorous probabilistic assumption."
            },
            "slug": "On-Adaptive-Step-Primal-Dual-Interior-Point-for-Mizuno-Todd",
            "title": {
                "fragments": [],
                "text": "On Adaptive-Step Primal-Dual Interior-Point Algorithms for Linear Programming"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Heuristic reasoning is provided for expecting that the algorithms will perform much better in practice than guaranteed by the worst-case estimates, based on an analysis using a nonrigorous probabilistic assumption."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Oper. Res."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800149"
                        ],
                        "name": "J. Dennis",
                        "slug": "J.-Dennis",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Dennis",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dennis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33225083"
                        ],
                        "name": "D. Gay",
                        "slug": "D.-Gay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Gay",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gay"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2242562"
                        ],
                        "name": "R. Welsch",
                        "slug": "R.-Welsch",
                        "structuredName": {
                            "firstName": "Roy",
                            "lastName": "Welsch",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Welsch"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 207651652,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c9ac7ab30f9de30b1e01b64352554b4fc8b00e65",
            "isKey": false,
            "numCitedBy": 388,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Reference [ 1] explains the algorithm realized by NL2SOL in detail. The algorithm amounts to a variation on Newton's method in which part of the Hessian matrix is computed exactly and part is approximated by a secant (quasi-Newton) updating method. Once the iterates come sufficiently close to a local solution, they usually converge quite rapidly. To promote convergence from poor starting guesses, NL2SOL uses a model/trust-region technique along with an adaptive"
            },
            "slug": "Algorithm-573:-NL2SOL\u2014An-Adaptive-Nonlinear-[E4]-Dennis-Gay",
            "title": {
                "fragments": [],
                "text": "Algorithm 573: NL2SOL\u2014An Adaptive Nonlinear Least-Squares Algorithm [E4]"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "The algorithm amounts to a variation on Newton's method in which part of the Hessian matrix is computed exactly and part is approximated by a secant (quasi-Newton) updating method to promote convergence from poor starting guesses."
            },
            "venue": {
                "fragments": [],
                "text": "TOMS"
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152620991"
                        ],
                        "name": "James P. Evans",
                        "slug": "James-P.-Evans",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Evans",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James P. Evans"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34593026"
                        ],
                        "name": "F. J. Gould",
                        "slug": "F.-J.-Gould",
                        "structuredName": {
                            "firstName": "Floyd",
                            "lastName": "Gould",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. J. Gould"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1858423"
                        ],
                        "name": "J. Tolle",
                        "slug": "J.-Tolle",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "Tolle",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tolle"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 28938920,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f43ab92f7a8797ee817c800523452ae8c6577c57",
            "isKey": false,
            "numCitedBy": 216,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper some new theoretic results on piecewise differentiable exact penalty functions are presented. Sufficient conditions are given for the existence of exact penalty functions for inequality constrained problems more general than concave and several classes of such functions are presented."
            },
            "slug": "Exact-penalty-functions-in-nonlinear-programming-Evans-Gould",
            "title": {
                "fragments": [],
                "text": "Exact penalty functions in nonlinear programming"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "Sufficient conditions are given for the existence of exact penalty functions for inequality constrained problems more general than concave and several classes of such functions are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "116307686"
                        ],
                        "name": "Richard J. Coppins",
                        "slug": "Richard-J.-Coppins",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Coppins",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Richard J. Coppins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69560838"
                        ],
                        "name": "Nesa L'abbe Wu",
                        "slug": "Nesa-L'abbe-Wu",
                        "structuredName": {
                            "firstName": "Nesa",
                            "lastName": "Wu",
                            "middleNames": [
                                "L'abbe"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nesa L'abbe Wu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 31
                            }
                        ],
                        "text": "We refer the reader to Dantzig [86] and Fourer, Gay, and Kernighan [112] for more comprehensive discussion of this issue."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 92
                            }
                        ],
                        "text": "Information about modeling techniques for various application areas can be found in Dantzig [86], Ahuja, Magnanti, and Orlin [1], Fourer, Gay, and Kernighan [112], Winston [308], and Rardin [262]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 60017879,
            "fieldsOfStudy": [
                "Business",
                "Mathematics",
                "Computer Science"
            ],
            "id": "24a183b16c5c9ee803e8af3daa1ac5164c6e19e3",
            "isKey": false,
            "numCitedBy": 1894,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "\uf0b7 Formulation skills for problems as a deterministic linear mathematical model (linear programming, goal (multi-objective) programming, integer programming, transportation, transshipment) \uf0b7 Travel Sales Person (TSP) problems \uf0b7 Simplex method for solving linear programming (LP) \uf0b7 Dual of an LP and its application \uf0b7 Extensive sensitivity analysis to answer \u201cwhat if\u201d questions (for all models) \uf0b7 Application of software to solve the problems"
            },
            "slug": "Linear-programming-and-extensions-Coppins-Wu",
            "title": {
                "fragments": [],
                "text": "Linear programming and extensions"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "Formulation skills for problems as a deterministic linear mathematical model, application of software to solve the problems and extensive sensitivity analysis to answer \u201cwhat if\u201d questions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751023"
                        ],
                        "name": "D. Kleinman",
                        "slug": "D.-Kleinman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Kleinman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kleinman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145603802"
                        ],
                        "name": "S. Baron",
                        "slug": "S.-Baron",
                        "structuredName": {
                            "firstName": "Sheldon",
                            "lastName": "Baron",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Baron"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1891530"
                        ],
                        "name": "W. Levison",
                        "slug": "W.-Levison",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Levison",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Levison"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61571365,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "41e85c7c30b1de6400e0a4c31f38f1c806d295a1",
            "isKey": false,
            "numCitedBy": 439,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-optimal-control-model-of-human-response-part-I:-Kleinman-Baron",
            "title": {
                "fragments": [],
                "text": "An optimal control model of human response part I: Theory and validation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706816"
                        ],
                        "name": "N. Megiddo",
                        "slug": "N.-Megiddo",
                        "structuredName": {
                            "firstName": "Nimrod",
                            "lastName": "Megiddo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Megiddo"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117016605,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a8d01944bb26f1a5786c4ad70dcea07b51a75419",
            "isKey": false,
            "numCitedBy": 631,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter presents continuous paths leading to the set of optimal solutions of a linear programming problem. These paths are derived from the weighted logarithmic barrier function. The defining equations are bilinear and have some nice primal-dual symmetry properties. Extensions to the general linear complementarity problem are indicated."
            },
            "slug": "Pathways-to-the-optimal-set-in-linear-programming-Megiddo",
            "title": {
                "fragments": [],
                "text": "Pathways to the optimal set in linear programming"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145732246"
                        ],
                        "name": "E. Polak",
                        "slug": "E.-Polak",
                        "structuredName": {
                            "firstName": "Elijah",
                            "lastName": "Polak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Polak"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117576894,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2ddf3d30fbefc1e7d0fdbd26f8b6f5a48fa4df21",
            "isKey": false,
            "numCitedBy": 1198,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1 Unconstrained Optimization.- 2 Finite Min-Max and Constrained Optimization.- 3 Semi-Infinite Optimization.- 4 Optimal Control.- 5 Mathematical Background."
            },
            "slug": "Optimization:-Algorithms-and-Consistent-Polak",
            "title": {
                "fragments": [],
                "text": "Optimization: Algorithms and Consistent Approximations"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper presents a meta-anatomy of semi-Infinite Optimization, a branch of optimization that combines the efforts of biologists and mathematicians to study the response of the immune system to injury."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33395252"
                        ],
                        "name": "R. Eberhart",
                        "slug": "R.-Eberhart",
                        "structuredName": {
                            "firstName": "Russell",
                            "lastName": "Eberhart",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Eberhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143953756"
                        ],
                        "name": "J. Kennedy",
                        "slug": "J.-Kennedy",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Kennedy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kennedy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14086189,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8263efcbc5e6b3c7c022b1131038b888babc8548",
            "isKey": false,
            "numCitedBy": 13770,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The optimization of nonlinear functions using particle swarm methodology is described. Implementations of two paradigms are discussed and compared, including a recently developed locally oriented paradigm. Benchmark testing of both paradigms is described, and applications, including neural network training and robot task learning, are proposed. Relationships between particle swarm optimization and both artificial life and evolutionary computation are reviewed."
            },
            "slug": "A-new-optimizer-using-particle-swarm-theory-Eberhart-Kennedy",
            "title": {
                "fragments": [],
                "text": "A new optimizer using particle swarm theory"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "The optimization of nonlinear functions using particle swarm methodology is described and implementations of two paradigms are discussed and compared, including a recently developed locally oriented paradigm."
            },
            "venue": {
                "fragments": [],
                "text": "MHS'95. Proceedings of the Sixth International Symposium on Micro Machine and Human Science"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40770452"
                        ],
                        "name": "L. Goddard",
                        "slug": "L.-Goddard",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Goddard",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Goddard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 172
                            }
                        ],
                        "text": "Information about modeling techniques for various application areas can be found in Dantzig [86], Ahuja, Magnanti, and Orlin [1], Fourer, Gay, and Kernighan [112], Winston [308], and Rardin [262]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 4250374,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "60ec5ddae05190537c72974b8f93beb46b8db857",
            "isKey": false,
            "numCitedBy": 1105,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Introduction to Operations ResearchBy A. Kaufmann, and R. Faure. Translated by Henry C. Sneyd. (Mathematics in Science and Engineering, Vol. 47.) Pp. xi + 300. (Academic Press: New York and London, September 1968.) 135s 4d."
            },
            "slug": "Operations-Research-Goddard",
            "title": {
                "fragments": [],
                "text": "Operations Research"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1969
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781621"
                        ],
                        "name": "A. Neumaier",
                        "slug": "A.-Neumaier",
                        "structuredName": {
                            "firstName": "Arnold",
                            "lastName": "Neumaier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Neumaier"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 37196096,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ecb39c5f5de1cd2102dadc92d11dcb6050161dec",
            "isKey": false,
            "numCitedBy": 4360,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1. The numerical evaluation of expressions 2. Linear systems of equations 3. Interpolation and numerical differentiation 4. Numerical integration 5. Univariate nonlinear equations 6. Systems of nonlinear equations."
            },
            "slug": "Introduction-to-Numerical-Analysis-Neumaier",
            "title": {
                "fragments": [],
                "text": "Introduction to Numerical Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "1. The numerical evaluation of expressions 2. Linear systems of equations 3. Interpolation and numerical differentiation 4. Numerical integration 5. Univariate non linear equations 6. Systems of nonlinear equations."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144241870"
                        ],
                        "name": "J. J. Mor\u00e9",
                        "slug": "J.-J.-Mor\u00e9",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Mor\u00e9",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. J. Mor\u00e9"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 59917169,
            "fieldsOfStudy": [
                "Physics",
                "Engineering"
            ],
            "id": "d4bd8b2ac8604d3bd2657463110acab2270368e4",
            "isKey": false,
            "numCitedBy": 2815,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "A conduit arrangement for a tilt cylinder of a bulldozer comprises a trunnion having a hole to be connected to the hole in a truck frame and a conduit adapted to be connected through the frame to the tilt cylinder. The conduit is arranged to extend through the hole in the truck frame into the latter, the end of said conduit being connected by means of a coupling to a stationary conduit on the side of a hydraulic oil tank. A cover is secured to the base of the frame, whereby connection and disconnection between the conduit and the stationary conduit can be easily effected."
            },
            "slug": "The-Levenberg-Marquardt-algo-rithm:-Implementation-Mor\u00e9",
            "title": {
                "fragments": [],
                "text": "The Levenberg-Marquardt algo-rithm: Implementation and theory"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A conduit arrangement for a tilt cylinder of a bulldozer comprises a trunnion having a hole to beconnected to the hole in a truck frame and a conduit adapted to be connected through the frame to the tilt cylinder."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144256170"
                        ],
                        "name": "L. Grippo",
                        "slug": "L.-Grippo",
                        "structuredName": {
                            "firstName": "Luigi",
                            "lastName": "Grippo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Grippo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3261920"
                        ],
                        "name": "F. Lampariello",
                        "slug": "F.-Lampariello",
                        "structuredName": {
                            "firstName": "Francesco",
                            "lastName": "Lampariello",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Lampariello"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2593984"
                        ],
                        "name": "S. Lucidi",
                        "slug": "S.-Lucidi",
                        "structuredName": {
                            "firstName": "Stefano",
                            "lastName": "Lucidi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lucidi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122212110,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "4f2262711d2a9ded266f1a17166576b7eae067a3",
            "isKey": false,
            "numCitedBy": 1056,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper a nonmonotone steplength selection rule for Newton\u2019s method is proposed, which can be viewed as a generalization of Armijo\u2019s rule. Numerical results are reported which indicate that the proposed technique may allow a considerable saving both in the number of line searches and in the number of function evaluations."
            },
            "slug": "A-nonmonotone-line-search-technique-for-Newton's-Grippo-Lampariello",
            "title": {
                "fragments": [],
                "text": "A nonmonotone line search technique for Newton's method"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737295"
                        ],
                        "name": "W. Hager",
                        "slug": "W.-Hager",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Hager",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Hager"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 95
                            }
                        ],
                        "text": "Other iterative methods for the solution of a trust-region problem have been proposed by Hager [160], and by Rendl and Wolkowicz [263]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14495484,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5cb09bb055f391ba95154c0b4bbc60a40ab1651a",
            "isKey": false,
            "numCitedBy": 136,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "A new method, the sequential subspace method (SSM), is developed for the problem of minimizing a quadratic over a sphere. In our scheme, the quadratic is minimized over a subspace which is adjusted in successive iterations to ensure convergence to an optimum. When a sequential quadratic programming iterate is included in the subspace, convergence is locally quadratic. Numerical comparisons with other recent methods are given."
            },
            "slug": "Minimizing-a-Quadratic-Over-a-Sphere-Hager",
            "title": {
                "fragments": [],
                "text": "Minimizing a Quadratic Over a Sphere"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "A new method, the sequential subspace method (SSM), is developed for the problem of minimizing a quadratic over a sphere by minimizing a subspace which is adjusted in successive iterations to ensure convergence to an optimum."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Optim."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34548558"
                        ],
                        "name": "J. Czyzyk",
                        "slug": "J.-Czyzyk",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Czyzyk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Czyzyk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35282765"
                        ],
                        "name": "Sanjay Mehrotra",
                        "slug": "Sanjay-Mehrotra",
                        "structuredName": {
                            "firstName": "Sanjay",
                            "lastName": "Mehrotra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sanjay Mehrotra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114342917"
                        ],
                        "name": "M. Wagner",
                        "slug": "M.-Wagner",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Wagner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Wagner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144731788"
                        ],
                        "name": "Stephen J. Wright",
                        "slug": "Stephen-J.-Wright",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Wright",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen J. Wright"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120994015,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dae41d61aab86b02251b214eed99ade368b52fe2",
            "isKey": false,
            "numCitedBy": 78,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe the code PCx, a primal-dual interior-point code for linear programming. Information is given about problem formulation and the underlying algorithm, along with instructions for installing, invoking, and using the code. Computational results on standard test problems are reported."
            },
            "slug": "PCx:-an-interior-point-code-for-linear-programming-Czyzyk-Mehrotra",
            "title": {
                "fragments": [],
                "text": "PCx: an interior-point code for linear programming"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "The code PCx is described, a primal-dual interior-point code for linear programming, along with instructions for installing, invoking, and using the code."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4745449"
                        ],
                        "name": "D. Goldfarb",
                        "slug": "D.-Goldfarb",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Goldfarb",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Goldfarb"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 246,
                                "start": 241
                            }
                        ],
                        "text": "Another strategy for implementing a line search Newton method when the Hessian contains negative eigenvalues is to compute a direction of negative curvature and use it to define the search direction (see Mor\u00e9 and Sorensen [213] and Goldfarb [132])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 39
                            }
                        ],
                        "text": "Some line search methods (see Goldfarb [132] and Mor\u00e9 and Sorensen [213]) compute a direction of negative curvature, whenever it exists, to prevent the iteration from converging to nonminimizing stationary points."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16760403,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "21950e0839e930c67d8b7b9eba858c3307375494",
            "isKey": false,
            "numCitedBy": 81,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "The Armijo and Goldstein step-size rules are modified to allow steps along a curvilinear path of the formx(\u03b1) + x + \u03b1s + \u03b12d, wherex is the current estimate of the minimum,s is a descent direction andd is a nonascent direction of negative curvature. By using directions of negative curvature when they exist, we are able to prove, under fairly mild assumptions, that the sequences of iterates produced by these algorithms converge to stationary points at which the Hessian matrix of the objective function is positive semidefinite."
            },
            "slug": "Curvilinear-path-steplength-algorithms-for-which-of-Goldfarb",
            "title": {
                "fragments": [],
                "text": "Curvilinear path steplength algorithms for minimization which use directions of negative curvature"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "By using directions of negative curvature when they exist, it is proved that the sequences of iterates produced by these algorithms converge to stationary points at which the Hessian matrix of the objective function is positive semidefinite."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711460"
                        ],
                        "name": "Chih-Jen Lin",
                        "slug": "Chih-Jen-Lin",
                        "structuredName": {
                            "firstName": "Chih-Jen",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Jen Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144241870"
                        ],
                        "name": "J. J. Mor\u00e9",
                        "slug": "J.-J.-Mor\u00e9",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Mor\u00e9",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. J. Mor\u00e9"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 77
                            }
                        ],
                        "text": "Software for computing incomplete Cholesky preconditioners includes the ICFS [193] and MA57 [166] packages."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6824105,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bcedb242ff1d233e572620a55f6f608e5e4f10c6",
            "isKey": false,
            "numCitedBy": 158,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose an incomplete Cholesky factorization for the solution of large-scale trust region subproblems and positive definite systems of linear equations. This factorization depends on a parameter p that specifies the amount of additional memory (in multiples of n, the dimension of the problem) that is available; there is no need to specify a drop tolerance. Our numerical results show that the number of conjugate gradient iterations and the computing time are reduced dramatically for small values of p. We also show that in contrast with drop tolerance strategies, the new approach is more stable in terms of number of iterations and memory requirements."
            },
            "slug": "Incomplete-Cholesky-Factorizations-with-Limited-Lin-Mor\u00e9",
            "title": {
                "fragments": [],
                "text": "Incomplete Cholesky Factorizations with Limited Memory"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "An incomplete Cholesky factorization for the solution of large-scale trust region subproblems and positive definite systems of linear equations depends on a parameter p that specifies the amount of additional memory that is available; there is no need to specify a drop tolerance."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Sci. Comput."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144448267"
                        ],
                        "name": "R. Byrd",
                        "slug": "R.-Byrd",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Byrd",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Byrd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2784955"
                        ],
                        "name": "J. Nocedal",
                        "slug": "J.-Nocedal",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Nocedal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nocedal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1795397"
                        ],
                        "name": "Bobby Schnabel",
                        "slug": "Bobby-Schnabel",
                        "structuredName": {
                            "firstName": "Bobby",
                            "lastName": "Schnabel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bobby Schnabel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 87
                            }
                        ],
                        "text": "24), that provides a compact representation of the inverse BFGS approximation Hk ; see [52] for details."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 109
                            }
                        ],
                        "text": "Our discussion of compact representations of limited-memory matrices is based on Byrd, Nocedal, and Schnabel [52]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5581219,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "dff7bb898da45b502608c3603b4673315540d4fd",
            "isKey": false,
            "numCitedBy": 673,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We derive compact representations of BFGS and symmetric rank-one matrices for optimization. These representations allow us to efficiently implement limited memory methods for large constrained optimization problems. In particular, we discuss how to compute projections of limited memory matrices onto subspaces. We also present a compact representation of the matrices generated by Broyden's update for solving systems of nonlinear equations."
            },
            "slug": "Representations-of-quasi-Newton-matrices-and-their-Byrd-Nocedal",
            "title": {
                "fragments": [],
                "text": "Representations of quasi-Newton matrices and their use in limited memory methods"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "This work derives compact representations of BFGS and symmetric rank-one matrices for optimization and presents a compact representation of the matrices generated by Broyden's update for solving systems of nonlinear equations."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50984882"
                        ],
                        "name": "Y. Saad",
                        "slug": "Y.-Saad",
                        "structuredName": {
                            "firstName": "Youcef",
                            "lastName": "Saad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Saad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143892596"
                        ],
                        "name": "M. Schultz",
                        "slug": "M.-Schultz",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Schultz",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Schultz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18390597,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "7bb9bab74df4d2939bbdf41fc33027b59e0f229e",
            "isKey": false,
            "numCitedBy": 10423,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an iterative method for solving linear systems, which has the property of minimizing at every step the norm of the residual vector over a Krylov subspace. The algorithm is derived from t..."
            },
            "slug": "GMRES:-a-generalized-minimal-residual-algorithm-for-Saad-Schultz",
            "title": {
                "fragments": [],
                "text": "GMRES: a generalized minimal residual algorithm for solving nonsymmetric linear systems"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "An iterative method for solving linear systems, which has the property of minimizing at every step the norm of the residual vector over a Krylov subspace."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47447717"
                        ],
                        "name": "W. Burmeister",
                        "slug": "W.-Burmeister",
                        "structuredName": {
                            "firstName": "Walther",
                            "lastName": "Burmeister",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Burmeister"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 26
                            }
                        ],
                        "text": "Cohen [63] and Burmeister [45] prove n-step"
                    },
                    "intents": []
                }
            ],
            "corpusId": 123550163,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d789ec3ee9382b150858875ee5d45683519799ac",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Das betrachtete Verfahren gehort zu der Klasse von Minimterungsverfahren, die das Minimum einer quadratischen Funktion von n Argumenten in hochstens n Schritten liefern. Fur nichtquadratische konvexe Zielfunktionen wird gezeigt, das das Verfahren mindestens die Konvergenzordnung n\u221a2 besitzt. \n \n \n \nThe method under consideration belongs to the class of minimization methods finding the minimum of a quadratic function of n variables in at most n steps. For non-quadratic convex objective functions it is proved that the rate of convergence of this method is at least n\u221a2."
            },
            "slug": "Die-Konvergenzordnung-des-Burmeister",
            "title": {
                "fragments": [],
                "text": "Die Konvergenzordnung des Fletcher\u2010Powell\u2010Algorithmus"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33133899"
                        ],
                        "name": "E. Andersen",
                        "slug": "E.-Andersen",
                        "structuredName": {
                            "firstName": "Erling",
                            "lastName": "Andersen",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Andersen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51043392"
                        ],
                        "name": "J. Gondzio",
                        "slug": "J.-Gondzio",
                        "structuredName": {
                            "firstName": "Jacek",
                            "lastName": "Gondzio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gondzio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "13774326"
                        ],
                        "name": "C. M\u00e9sz\u00e1ros",
                        "slug": "C.-M\u00e9sz\u00e1ros",
                        "structuredName": {
                            "firstName": "Csaba",
                            "lastName": "M\u00e9sz\u00e1ros",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. M\u00e9sz\u00e1ros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2134833354"
                        ],
                        "name": "Xiaojie Xu",
                        "slug": "Xiaojie-Xu",
                        "structuredName": {
                            "firstName": "Xiaojie",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaojie Xu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 9525587,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "0db3e4abb2a51bcc219bfb3a9325ee5f1e6619f2",
            "isKey": false,
            "numCitedBy": 299,
            "numCiting": 109,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we give an overview of the mostimportant characteristics of advanced implementations of interior point methods."
            },
            "slug": "Implementation-of-Interior-Point-Methods-for-Large-Andersen-Gondzio",
            "title": {
                "fragments": [],
                "text": "Implementation of Interior Point Methods for Large Scale Linear Programming"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "An overview of the most important characteristics of advanced implementations of interior point methods is given."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34641136"
                        ],
                        "name": "J. Bunch",
                        "slug": "J.-Bunch",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Bunch",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bunch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2002058"
                        ],
                        "name": "B. Parlett",
                        "slug": "B.-Parlett",
                        "structuredName": {
                            "firstName": "Beresford",
                            "lastName": "Parlett",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Parlett"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120373754,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "0d5b7c186b7342959c7715a1a676f026f243d21e",
            "isKey": false,
            "numCitedBy": 412,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Methods for solving symmetric indefinite systems are surveyed including a new one which is stable and almost as fast as the Cholesky method."
            },
            "slug": "Direct-Methods-for-Solving-Symmetric-Indefinite-of-Bunch-Parlett",
            "title": {
                "fragments": [],
                "text": "Direct Methods for Solving Symmetric Indefinite Systems of Linear Equations"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Methods for solving symmetric indefinite systems are surveyed including a new one which is stable and almost as fast as the Cholesky method."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2063343"
                        ],
                        "name": "F. Clarke",
                        "slug": "F.-Clarke",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Clarke",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Clarke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 148
                            }
                        ],
                        "text": "Our treatment of optimality conditions is thorough but not exhaustive; some concepts are discussed more extensively in Mangasarian [198] and Clarke [62]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122470312,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "849a2f9fff249cb5c7356aa6b8f4a7c43ce74746",
            "isKey": false,
            "numCitedBy": 9147,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Introduction and Preview 2. Generalized Gradients 3. Differential Inclusions 4. The Calculus of Variations 5. Optimal Control 6. Mathematical Programming 7. Topics in Analysis."
            },
            "slug": "Optimization-And-Nonsmooth-Analysis-Clarke",
            "title": {
                "fragments": [],
                "text": "Optimization And Nonsmooth Analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107390599"
                        ],
                        "name": "D. Anderson",
                        "slug": "D.-Anderson",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Anderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Anderson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 112
                            }
                        ],
                        "text": "Golden section and Fibonacci differ in the way in which the trial step lengths are generated; see, for example, [79, 39]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 169
                            }
                        ],
                        "text": "Practical line search algorithms also make use of the properties of the interpolating polynomials to make educated guesses of where the next step length should lie; see [39, 216]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62598143,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "83615ab0b900c7f6179b6ffdf5b771b05560970f",
            "isKey": false,
            "numCitedBy": 1887,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "This monograph describes and analyzes some practical methods for finding approximate zeros and minima of functions."
            },
            "slug": "Algorithms-for-minimization-without-derivatives-Anderson",
            "title": {
                "fragments": [],
                "text": "Algorithms for minimization without derivatives"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This monograph describes and analyzes some practical methods for finding approximate zeros and minima of functions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48199632"
                        ],
                        "name": "P. Williamson",
                        "slug": "P.-Williamson",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Williamson",
                            "middleNames": [
                                "L"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Williamson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39684597"
                        ],
                        "name": "R. Schlegel",
                        "slug": "R.-Schlegel",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schlegel",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schlegel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 41483427,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "60b142043008d7ebbea0cb40ae2dc935bf277bdf",
            "isKey": false,
            "numCitedBy": 249,
            "numCiting": 191,
            "paperAbstract": {
                "fragments": [],
                "text": "That some membranes restrict certain lipid species to one side of the bilayer and others to the opposite side has been known for two decades. However, how this asymmetric transbilayer distribution is generated and controlled, how many and what type of membranes are so structured, and even the reason for its existence is just now beginning to be understood. It has been a decade since the discovery of an activity which transports in an ATP-dependent manner only the aminophospholipids from the outer to the inner leaflet of the plasma membrane. This aminophospholipid translocase has yet to be isolated, reconstituted, and identified molecularly. Elevating intracellular Ca2+ allows all the major classes of phospholipids to move freely across the bilayer, scrambling lipids and dissipating asymmetry. The nature of this pathway and its mode of activation by Ca2+ remain to be determined. Though loss of transbilayer asymmetry by blood cells clearly produces a procoagulant surface and increases interactions with the reticuloendothelial system, it remains to be elucidated whether maintenance of blood homeostasis is just one expression of a more general raison d'\u00eatre for lipid asymmetry. It is these persisting uncertainties and gaps in our knowledge which make the field such an interesting and exciting challenge at the present time."
            },
            "slug": "Back-and-forth:-the-regulation-and-function-of-in-Williamson-Schlegel",
            "title": {
                "fragments": [],
                "text": "Back and forth: the regulation and function of transbilayer phospholipid movement in eukaryotic cells."
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Though loss of transbilayer asymmetry by blood cells clearly produces a procoagulant surface and increases interactions with the reticuloendothelial system, it remains to be elucidated whether maintenance of blood homeostasis is just one expression of a more general raison d'\u00eatre for lipid asymmetry."
            },
            "venue": {
                "fragments": [],
                "text": "Molecular membrane biology"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145369815"
                        ],
                        "name": "K. Ritter",
                        "slug": "K.-Ritter",
                        "structuredName": {
                            "firstName": "Klaus",
                            "lastName": "Ritter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Ritter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 80
                            }
                        ],
                        "text": "Even faster rates of convergence can be established (see Schuller [278], Ritter [265]), under the assumption that the search directions are uniformly linearly independent, but this assumption is hard to verify and does not often occur in practice."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 7
                            }
                        ],
                        "text": "Ritter [265] shows that in fact, the rate is superquadratic, that is,"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 122950094,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "3de06f605e11c0e68bbb20dd2c57bfc909ec6aed",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "SummaryThis paper considers a class of variable metric methods for unconstrained minimization. Without requiring exact line searches each algorithm in this class converges globally and superlinearly on convex functions. Various results on the rate of the superlinear convergence are obtained."
            },
            "slug": "On-the-rate-of-superlinear-convergence-of-a-class-Ritter",
            "title": {
                "fragments": [],
                "text": "On the rate of superlinear convergence of a class of variable metric methods"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52067790"
                        ],
                        "name": "N. Gould",
                        "slug": "N.-Gould",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Gould",
                            "middleNames": [
                                "I.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Gould"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 36930925,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b3cb395e7bf56362ddf48f107fc15541c05c5c42",
            "isKey": false,
            "numCitedBy": 85,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "On presente des methodes fiables de calcul d'une direction de recherche utilisee dans les methodes sequentielles de resolution des problemes de programmation non lineaire"
            },
            "slug": "On-the-Accurate-Determination-of-Search-Directions-Gould",
            "title": {
                "fragments": [],
                "text": "On the Accurate Determination of Search Directions for Simple Differentiable Penalty Functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "137425771"
                        ],
                        "name": "E. Polak",
                        "slug": "E.-Polak",
                        "structuredName": {
                            "firstName": "E.",
                            "lastName": "Polak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Polak"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123306444"
                        ],
                        "name": "G. Ribi\u00e8re",
                        "slug": "G.-Ribi\u00e8re",
                        "structuredName": {
                            "firstName": "G\u00e9rard",
                            "lastName": "Ribi\u00e8re",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Ribi\u00e8re"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 43
                            }
                        ],
                        "text": "The Polak\u2013Ribi\u00e8re method was introduced in [237], and the example showing that it may fail to converge on nonconvex problems is given by Powell [253]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 116971894,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "95e5fe49a0d14ccc40b3cd1726d8f3843f7a3c5d",
            "isKey": false,
            "numCitedBy": 997,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "L\u2019acc\u00e8s aux archives de la revue \u00ab Revue fran\u00e7aise d\u2019informatique et de recherche op\u00e9rationnelle, s\u00e9rie rouge \u00bb implique l\u2019accord avec les conditions g\u00e9n\u00e9rales d\u2019utilisation (http://www.numdam.org/legal.php). Toute utilisation commerciale ou impression syst\u00e9matique est constitutive d\u2019une infraction p\u00e9nale. Toute copie ou impression de ce fichier doit contenir la pr\u00e9sente mention de copyright."
            },
            "slug": "Note-sur-la-convergence-de-m\u00e9thodes-de-directions-Polak-Ribi\u00e8re",
            "title": {
                "fragments": [],
                "text": "Note sur la convergence de m\u00e9thodes de directions conjugu\u00e9es"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "L\u2019acc\u00e8s aux archives de la revue \u00ab Revue fran\u00e7aise d\u2019informatique and de recherche op\u00e9rationnelle, s\u00e9rie rouge \u00bb implique l\u2019 Accord avec les conditions g\u00e9n\u00e9rales d\u201dutilisation (http://www.numdam.org/legal)."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1969
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404043228"
                        ],
                        "name": "M. Al-Baali",
                        "slug": "M.-Al-Baali",
                        "structuredName": {
                            "firstName": "Mehiddin",
                            "lastName": "Al-Baali",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Al-Baali"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 117
                            }
                        ],
                        "text": "The global convergence of nonlinear conjugate gradient methods has received much attention; see for example Al-Baali [3], Gilbert and Nocedal [123], Dai and Yuan [85], and Hager and Zhang [161]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122736598,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "91f71b22971b09d0fe84e22626a1dcd8846ecaaf",
            "isKey": false,
            "numCitedBy": 459,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Si on utilise une recherche de ligne inexacte qui satisfait des conditions standards on peut demontrer que la methode de Fletcher-Reeves a une propriete de descente et est globalement convergente en un certain sens"
            },
            "slug": "Descent-Property-and-Global-Convergence-of-the-with-Al-Baali",
            "title": {
                "fragments": [],
                "text": "Descent Property and Global Convergence of the Fletcher\u2014Reeves Method with Inexact Line Search"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144448267"
                        ],
                        "name": "R. Byrd",
                        "slug": "R.-Byrd",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Byrd",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Byrd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46190125"
                        ],
                        "name": "R. Schnabel",
                        "slug": "R.-Schnabel",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schnabel",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schnabel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1894635"
                        ],
                        "name": "Gerald A. Shultz",
                        "slug": "Gerald-A.-Shultz",
                        "structuredName": {
                            "firstName": "Gerald",
                            "lastName": "Shultz",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gerald A. Shultz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122088377,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fba93653aef131718cb3216d635d8e47281dd51e",
            "isKey": false,
            "numCitedBy": 356,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a trust region-based method for the general nonlinearly equality constrained optimization problem. The method works by iteratively minimizing a quadratic model of the Lagrangian subject ..."
            },
            "slug": "A-Trust-Region-Algorithm-for-Nonlinearly-Byrd-Schnabel",
            "title": {
                "fragments": [],
                "text": "A Trust Region Algorithm for Nonlinearly Constrained Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "A trust region-based method for the general nonlinearly equality constrained optimization problem by iteratively minimizing a quadratic model of the Lagrangian subject subject."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34641136"
                        ],
                        "name": "J. Bunch",
                        "slug": "J.-Bunch",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Bunch",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bunch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10706913"
                        ],
                        "name": "L. Kaufman",
                        "slug": "L.-Kaufman",
                        "structuredName": {
                            "firstName": "Linda",
                            "lastName": "Kaufman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Kaufman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11913688,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "53bcb63d1b02d795bc170d5a6b690aa1807ce403",
            "isKey": false,
            "numCitedBy": 363,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Several decompositions ofsymmetric matrices for calculating inertia and solving systems of linear equations are discussed. New partial pivoting strategies for decomposing symmetric matrices are introduced and analyzed."
            },
            "slug": "Some-stable-methods-for-calculating-inertia-and-Bunch-Kaufman",
            "title": {
                "fragments": [],
                "text": "Some stable methods for calculating inertia and solving symmetric linear systems"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "Several decompositions of symmetry matrices for calculating inertia and solving systems of linear equations are discussed and new partial pivoting strategies for decomposing symmetric matrices are introduced and analyzed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804540"
                        ],
                        "name": "I. Duff",
                        "slug": "I.-Duff",
                        "structuredName": {
                            "firstName": "Iain",
                            "lastName": "Duff",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Duff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144942152"
                        ],
                        "name": "J. Reid",
                        "slug": "J.-Reid",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Reid",
                            "middleNames": [
                                "Ker"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Reid"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 6248281,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "8c6e313b3418e42afe4a852116e18d0f24284f35",
            "isKey": false,
            "numCitedBy": 942,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "On etend la methode frontale pour resoudre des systemes lineaires d'equations en permettant a plus d'un front d'apparaitre en meme temps"
            },
            "slug": "The-Multifrontal-Solution-of-Indefinite-Sparse-Duff-Reid",
            "title": {
                "fragments": [],
                "text": "The Multifrontal Solution of Indefinite Sparse Symmetric Linear"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "On etend la methode frontale pour resoudre des systemes lineaires d'equations en permettant a plus d'un front d'apparaitre en meme temps."
            },
            "venue": {
                "fragments": [],
                "text": "TOMS"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "83156422"
                        ],
                        "name": "M. R. Celis",
                        "slug": "M.-R.-Celis",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Celis",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. R. Celis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800149"
                        ],
                        "name": "J. Dennis",
                        "slug": "J.-Dennis",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Dennis",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dennis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719462"
                        ],
                        "name": "R. Tapia",
                        "slug": "R.-Tapia",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Tapia",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tapia"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 208868951,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "acbae0627509f3dfba898c1bce3e452e25e10e1e",
            "isKey": false,
            "numCitedBy": 199,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This work was also published as a Rice University thesis/dissertation: http://hdl.handle.net/1911/15885"
            },
            "slug": "A-trust-region-strategy-for-nonlinear-equality-Celis-Dennis",
            "title": {
                "fragments": [],
                "text": "A trust region strategy for nonlinear equality constrained op-timization"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This work was also published as a Rice University thesis/dissertation: http://hdl.handle.net/1911/15885."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51203938"
                        ],
                        "name": "Y. Chen",
                        "slug": "Y.-Chen",
                        "structuredName": {
                            "firstName": "Y",
                            "lastName": "Chen",
                            "middleNames": [
                                "Q"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Chen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 20255225,
            "fieldsOfStudy": [
                "Medicine",
                "Biology"
            ],
            "id": "23e555aed76504f76762b8a186c851e69d05a333",
            "isKey": false,
            "numCitedBy": 1210,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The serum level of alpha 1-antitrypsin (alpha 1-AT) were observed in 32 cases with spontaneous pneumothorax. The results showed the level of alpha 1-AT in patients of spontaneous pneumothorax is higher than normal control. The degree and duration of elevated alpha 1-AT level was related with the recovery of spontaneous pneumothorax and its reoccurrence."
            },
            "slug": "[The-change-of-serum-alpha-1-antitrypsin-level-in-Chen",
            "title": {
                "fragments": [],
                "text": "[The change of serum alpha 1-antitrypsin level in patients with spontaneous pneumothorax]."
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "The serum level of alpha 1-antitrypsin (alpha 1-AT) was observed in 32 cases with spontaneous pneumothorax and the degree and duration of elevated alpha 2-AT level was related with the recovery of spontaneous pneumonia and its reoccurrence."
            },
            "venue": {
                "fragments": [],
                "text": "Zhonghua jie he he hu xi za zhi = Zhonghua jiehe he huxi zazhi = Chinese journal of tuberculosis and respiratory diseases"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145732246"
                        ],
                        "name": "E. Polak",
                        "slug": "E.-Polak",
                        "structuredName": {
                            "firstName": "Elijah",
                            "lastName": "Polak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Polak"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 87
                            }
                        ],
                        "text": "It is related to PARTAN, the method of parallel tangents (see, for example, Luenberger [195])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 84
                            }
                        ],
                        "text": "Our presentation of the linear conjugate gradient method follows that of Luenberger [195]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 39
                            }
                        ],
                        "text": "By using similar reasoning, Luenberger [195] establishes the following estimate, which gives a useful characterization of the behavior of the CG method."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 48
                            }
                        ],
                        "text": "The proof of this result is given by Luenberger [195]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 122931192,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "95c23bb25a03d296a7eedb7c8dffe1748bb614c6",
            "isKey": true,
            "numCitedBy": 2291,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Introduction-to-linear-and-nonlinear-programming-Polak",
            "title": {
                "fragments": [],
                "text": "Introduction to linear and nonlinear programming"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2061638057"
                        ],
                        "name": "R. Fletcher",
                        "slug": "R.-Fletcher",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Fletcher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fletcher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8722060"
                        ],
                        "name": "C. M. Reeves",
                        "slug": "C.-M.-Reeves",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Reeves",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. M. Reeves"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 20
                            }
                        ],
                        "text": "Fletcher and Reeves [107] showed how to extend the conjugate gradient method to nonlinear functions by making two simple changes in Algorithm 5."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 85
                            }
                        ],
                        "text": "Interestingly enough, the nonlinear conjugate gradient method of Fletcher and Reeves [107] was proposed after the linear conjugate gradient method had fallen out of favor, but several years before it was rediscovered as an iterative method for linear systems."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1673,
                                "start": 1667
                            }
                        ],
                        "text": "21 Models for Trust-Region Methods 26 Scaling 27 Rates of Convergence 28 R-Rates of Convergence 29 Notes and References 30 Exercises 30\n3 Line Search Methods 34 3.1 Step Length 36\nThe Wolfe Conditions 37 The Goldstein Conditions 41 Sufficient Decrease and Backtracking 41\n3.2 Convergence of Line Search Methods 43 3.3 Rate of Convergence 46\nConvergence Rate of Steepest Descent 47 Quasi-Newton Methods 49 Newton's Method 51 Coordinate Descent Methods 53\n3.4 Step-Length Selection Algorithms 55 Interpolation 56 The Initial Step Length 58 A Line Search Algorithm for the Wolfe Conditions 58 Notes and References 61 Exercises 62\n4 Trust-Region Methods 64 Outline of the Algorithm 67\n4.1 The Cauchy Point and Related Algorithms 69 The Cauchy Point 69 Improving on the Cauchy Point 70 The Dogleg Method 71 Two-Dimensional Subspace Minimization 74 Steihaug's Approach 75 4.2 Using Nearly Exact Solutions to the Subproblem 77 Characterizing Exact Solutions 77 Calculating Nearly Exact Solutions 78 The Hard Case 82 Proof of Theorem 4.3 84 4.3 Global Convergence 87\nCONTENTS xiii\nReduction Obtained by the Cauchy Point 87 Convergence to Stationary Points 89 Convergence of Algorithms Based on Nearly Exact Solutions 93\n4.4 Other Enhancements 94 Scaling 94 Non-Euclidean Trust Regions 96 Notes and References 97 Exercises 97\nConjugate Gradient Methods 100 5.1 The Linear Conjugate Gradient Method 102\nConjugate Direction Methods 102 Basic Properties of the Conjugate Gradient Method 107 A Practical Form of the Conjugate Gradient Method I l l Rate of Convergence 112 Preconditioning 118 Practical Preconditioners 119\n5.2 Nonlinear Conjugate Gradient Methods 120 The Fletcher-Reeves Method 120 The Polak-Ribiere Method 121 Quadratic Termination and Restarts 122 Numerical Performance 124 Behavior of the Fletcher-Reeves Method 124 Global Convergence 127 Notes and References 131 Exercises 132\nPractical Newton'Methods 134 6.1 Inexact Newton Steps 136 6.2 Line Search Newton Methods 139\nLine Search Newton-CG Method 139 Modified Newton's Method 141\n6.3 Hessian Modifications ."
                    },
                    "intents": []
                }
            ],
            "corpusId": 62179557,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6ce051b85e7c21287de9dd53f81b4e72925e1a9",
            "isKey": true,
            "numCitedBy": 4254,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Function-minimization-by-conjugate-gradients-Fletcher-Reeves",
            "title": {
                "fragments": [],
                "text": "Function minimization by conjugate gradients"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. J."
            },
            "year": 1964
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2294095"
                        ],
                        "name": "M. Powell",
                        "slug": "M.-Powell",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Powell",
                            "middleNames": [
                                "J.",
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Powell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 115810962,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "192818e804f5b014dcf4d678795856594fb969b8",
            "isKey": false,
            "numCitedBy": 1411,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-method-for-nonlinear-constraints-in-minimization-Powell",
            "title": {
                "fragments": [],
                "text": "A method for nonlinear constraints in minimization problems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1969
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72599094"
                        ],
                        "name": "A. Edrei",
                        "slug": "A.-Edrei",
                        "structuredName": {
                            "firstName": "Albert",
                            "lastName": "Edrei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Edrei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729514"
                        ],
                        "name": "E. Saff",
                        "slug": "E.-Saff",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Saff",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Saff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145119134"
                        ],
                        "name": "R. Varga",
                        "slug": "R.-Varga",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Varga",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Varga"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123684581,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "6031b705b1afdd0522fe23f8edded525fcdea1e3",
            "isKey": false,
            "numCitedBy": 526,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Proof-of-Theorem-4-Edrei-Saff",
            "title": {
                "fragments": [],
                "text": "Proof of Theorem 4"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21456228"
                        ],
                        "name": "Chandler Davis",
                        "slug": "Chandler-Davis",
                        "structuredName": {
                            "firstName": "Chandler",
                            "lastName": "Davis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chandler Davis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 125070227,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "3b24a74e268d56d58b6f1f6aac5781b1b77dbed4",
            "isKey": false,
            "numCitedBy": 272,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "THEORY-OF-POSITIVE-LINEAR-DEPENDENCE.-Davis",
            "title": {
                "fragments": [],
                "text": "THEORY OF POSITIVE LINEAR DEPENDENCE."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1954
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144733293"
                        ],
                        "name": "R. Fletcher",
                        "slug": "R.-Fletcher",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Fletcher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fletcher"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122301549,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3bac440be61213ab93632dec63dd040da00d730f",
            "isKey": false,
            "numCitedBy": 202,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-General-Quadratic-Programming-Algorithm-Fletcher",
            "title": {
                "fragments": [],
                "text": "A General Quadratic Programming Algorithm"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71225891"
                        ],
                        "name": "D. Marquardt",
                        "slug": "D.-Marquardt",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Marquardt",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Marquardt"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122360030,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "288f41a655a178bf28d5883f68aa95807edbc950",
            "isKey": false,
            "numCitedBy": 27200,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-Algorithm-for-Least-Squares-Estimation-of-Marquardt",
            "title": {
                "fragments": [],
                "text": "An Algorithm for Least-Squares Estimation of Nonlinear Parameters"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1963
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144638062"
                        ],
                        "name": "D. Pierre",
                        "slug": "D.-Pierre",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Pierre",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pierre"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120989923,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "c54d42090dbabfa62ce50c605dba185f4eb78c65",
            "isKey": false,
            "numCitedBy": 406,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Optimization-Theory-with-Applications-Pierre",
            "title": {
                "fragments": [],
                "text": "Optimization Theory with Applications"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1843103"
                        ],
                        "name": "Stephen P. Boyd",
                        "slug": "Stephen-P.-Boyd",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Boyd",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen P. Boyd"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 203667344,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "46a16d3648aa719abaf319b7ca474cfde62f76e4",
            "isKey": false,
            "numCitedBy": 11234,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Linear-Matrix-Inequalities-in-Systems-and-Control-Boyd",
            "title": {
                "fragments": [],
                "text": "Linear Matrix Inequalities in Systems and Control Theory"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804540"
                        ],
                        "name": "I. Duff",
                        "slug": "I.-Duff",
                        "structuredName": {
                            "firstName": "Iain",
                            "lastName": "Duff",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Duff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144942152"
                        ],
                        "name": "J. Reid",
                        "slug": "J.-Reid",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Reid",
                            "middleNames": [
                                "Ker"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Reid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50256246"
                        ],
                        "name": "N. Munksgaard",
                        "slug": "N.-Munksgaard",
                        "structuredName": {
                            "firstName": "N.",
                            "lastName": "Munksgaard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Munksgaard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2597782"
                        ],
                        "name": "H. B. Nielsen",
                        "slug": "H.-B.-Nielsen",
                        "structuredName": {
                            "firstName": "Hans",
                            "lastName": "Nielsen",
                            "middleNames": [
                                "Bruun"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. B. Nielsen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121935080,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dae46f18033f36241d2a783a3db507724a928ee4",
            "isKey": false,
            "numCitedBy": 71,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Direct-Solution-of-Sets-of-Linear-Equations-whose-Duff-Reid",
            "title": {
                "fragments": [],
                "text": "Direct Solution of Sets of Linear Equations whose Matrix is Sparse, Symmetric and Indefinite"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145820374"
                        ],
                        "name": "P. Wolfe",
                        "slug": "P.-Wolfe",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Wolfe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Wolfe"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120254229,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c889027e37d0ccc30d0ad3fd6b62fcb25b8361cf",
            "isKey": false,
            "numCitedBy": 80,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Composite-Simplex-Algorithm-Wolfe",
            "title": {
                "fragments": [],
                "text": "The Composite Simplex Algorithm"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3026940"
                        ],
                        "name": "P. Ciarlet",
                        "slug": "P.-Ciarlet",
                        "structuredName": {
                            "firstName": "Philippe",
                            "lastName": "Ciarlet",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Ciarlet"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120712164,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e21e86ed0ee3573efaa722074c4f648530736545",
            "isKey": false,
            "numCitedBy": 391,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Introduction-a-l'analyse-numerique-matricielle-et-a-Ciarlet",
            "title": {
                "fragments": [],
                "text": "Introduction a l'analyse numerique matricielle et a l'optimisation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786249"
                        ],
                        "name": "D. Bertsekas",
                        "slug": "D.-Bertsekas",
                        "structuredName": {
                            "firstName": "Dimitri",
                            "lastName": "Bertsekas",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Bertsekas"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118821639,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cb89a25141febad0c14d080e2791c506ae6e4a76",
            "isKey": false,
            "numCitedBy": 3895,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Constrained-Optimization-and-Lagrange-Multiplier-Bertsekas",
            "title": {
                "fragments": [],
                "text": "Constrained Optimization and Lagrange Multiplier Methods"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1405251937"
                        ],
                        "name": "J. Navarro-Pedre\u00f1o",
                        "slug": "J.-Navarro-Pedre\u00f1o",
                        "structuredName": {
                            "firstName": "Jos\u00e9",
                            "lastName": "Navarro-Pedre\u00f1o",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Navarro-Pedre\u00f1o"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118836352,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2388e6f48c3316cecc0146ecd40b54d997947c97",
            "isKey": false,
            "numCitedBy": 2780,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Numerical-Methods-for-Least-Squares-Problems-Navarro-Pedre\u00f1o",
            "title": {
                "fragments": [],
                "text": "Numerical Methods for Least Squares Problems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723559"
                        ],
                        "name": "A. Conn",
                        "slug": "A.-Conn",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Conn",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Conn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2005127"
                        ],
                        "name": "K. Scheinberg",
                        "slug": "K.-Scheinberg",
                        "structuredName": {
                            "firstName": "Katya",
                            "lastName": "Scheinberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Scheinberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145758858"
                        ],
                        "name": "P. Toint",
                        "slug": "P.-Toint",
                        "structuredName": {
                            "firstName": "Philippe",
                            "lastName": "Toint",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Toint"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117394249,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "cfcfb1641aa655a133c9c3e5b0d4306a54cfc733",
            "isKey": false,
            "numCitedBy": 187,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-the-convergence-of-derivative-free-methods-for-Conn-Scheinberg",
            "title": {
                "fragments": [],
                "text": "On the convergence of derivative-free methods for unconstrained optimization"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8773884"
                        ],
                        "name": "C. Lemar\u00e9chal",
                        "slug": "C.-Lemar\u00e9chal",
                        "structuredName": {
                            "firstName": "Claude",
                            "lastName": "Lemar\u00e9chal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lemar\u00e9chal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 41
                            }
                        ],
                        "text": "See Dennis and Schnabel [92], Lemar\u00e9chal [189], Fletcher [101], Mor\u00e9 and Thuente [216] (in particular), and Hager and Zhang [161]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 117630670,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d44769c3b250771625987289d66a29c0fa22f2b",
            "isKey": false,
            "numCitedBy": 75,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-view-of-line-searches-Lemar\u00e9chal",
            "title": {
                "fragments": [],
                "text": "A view of line-searches"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2294095"
                        ],
                        "name": "M. Powell",
                        "slug": "M.-Powell",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Powell",
                            "middleNames": [
                                "J.",
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Powell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 115696750,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "67bd083b395dc37cfe58f09b168aafa6a76aa5b4",
            "isKey": false,
            "numCitedBy": 1688,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-fast-algorithm-for-nonlinearly-constrained-Powell",
            "title": {
                "fragments": [],
                "text": "A fast algorithm for nonlinearly constrained optimization calculations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145758858"
                        ],
                        "name": "P. Toint",
                        "slug": "P.-Toint",
                        "structuredName": {
                            "firstName": "Philippe",
                            "lastName": "Toint",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Toint"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 115391681,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "38f5e8f9c56bf86e6b2794d28b0b99f33ce29cbe",
            "isKey": false,
            "numCitedBy": 227,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Towards-an-efficient-sparsity-exploiting-newton-for-Toint",
            "title": {
                "fragments": [],
                "text": "Towards an efficient sparsity exploiting newton method for minimization"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144733293"
                        ],
                        "name": "R. Fletcher",
                        "slug": "R.-Fletcher",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Fletcher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fletcher"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 115468827,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e2707e39f39c2d27cb8bcba0242b2e29ad339e8b",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Second-order-corrections-for-non-differentiable-Fletcher",
            "title": {
                "fragments": [],
                "text": "Second order corrections for non-differentiable optimization"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786249"
                        ],
                        "name": "D. Bertsekas",
                        "slug": "D.-Bertsekas",
                        "structuredName": {
                            "firstName": "Dimitri",
                            "lastName": "Bertsekas",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Bertsekas"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 64649729,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "11106aadd1c133477b163f08de6c9436cd5468fe",
            "isKey": false,
            "numCitedBy": 9680,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Nonlinear-Programming-Bertsekas",
            "title": {
                "fragments": [],
                "text": "Nonlinear Programming"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2784955"
                        ],
                        "name": "J. Nocedal",
                        "slug": "J.-Nocedal",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Nocedal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nocedal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144731788"
                        ],
                        "name": "Stephen J. Wright",
                        "slug": "Stephen-J.-Wright",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Wright",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen J. Wright"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 64170906,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "74aa79f8c1a8e1759e858658bfc1e53a0c90f8df",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Fundamentals-of-Unconstrained-Optimization-Nocedal-Wright",
            "title": {
                "fragments": [],
                "text": "Fundamentals of Unconstrained Optimization"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2294095"
                        ],
                        "name": "M. Powell",
                        "slug": "M.-Powell",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Powell",
                            "middleNames": [
                                "J.",
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Powell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 61484018,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0cba55f550d18630e6c4d4f3d78f203d4c238599",
            "isKey": false,
            "numCitedBy": 740,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Nonlinear-Programming\u2014Sequential-Unconstrained-Powell",
            "title": {
                "fragments": [],
                "text": "Nonlinear Programming\u2014Sequential Unconstrained Minimization Techniques"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1969
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2294095"
                        ],
                        "name": "M. Powell",
                        "slug": "M.-Powell",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Powell",
                            "middleNames": [
                                "J.",
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Powell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62756844,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "027e3321f34c9b2af721f9d102b9d9eb85147dcf",
            "isKey": false,
            "numCitedBy": 3945,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-efficient-method-for-finding-the-minimum-of-a-of-Powell",
            "title": {
                "fragments": [],
                "text": "An efficient method for finding the minimum of a function of several variables without calculating derivatives"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. J."
            },
            "year": 1964
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143756356"
                        ],
                        "name": "H. Rosenbrock",
                        "slug": "H.-Rosenbrock",
                        "structuredName": {
                            "firstName": "Howard",
                            "lastName": "Rosenbrock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Rosenbrock"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 62755334,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "755c4d6ca13e371e2f684c06caa348162190004b",
            "isKey": false,
            "numCitedBy": 2683,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-Automatic-Method-for-Finding-the-Greatest-or-of-Rosenbrock",
            "title": {
                "fragments": [],
                "text": "An Automatic Method for Finding the Greatest or Least Value of a Function"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. J."
            },
            "year": 1960
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143794449"
                        ],
                        "name": "S. French",
                        "slug": "S.-French",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "French",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. French"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62719608,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "865f7cd3391e4494baac014f56cb5c9768ff2c25",
            "isKey": false,
            "numCitedBy": 72,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Finite-Algorithms-in-Optimization-and-Data-Analysis-French",
            "title": {
                "fragments": [],
                "text": "Finite Algorithms in Optimization and Data Analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103033816"
                        ],
                        "name": "L. G. H. Cijan",
                        "slug": "L.-G.-H.-Cijan",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Cijan",
                            "middleNames": [
                                "G.",
                                "Ha"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. G. H. Cijan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118963004,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4a997e2e9f8a66b932993ca39d903737fa00393a",
            "isKey": false,
            "numCitedBy": 1449,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-polynomial-algorithm-in-linear-programming-Cijan",
            "title": {
                "fragments": [],
                "text": "A polynomial algorithm in linear programming"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40186251"
                        ],
                        "name": "N. Meyers",
                        "slug": "N.-Meyers",
                        "structuredName": {
                            "firstName": "Norman",
                            "lastName": "Meyers",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Meyers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2221470"
                        ],
                        "name": "J. Serrin",
                        "slug": "J.-Serrin",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Serrin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Serrin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 23913163,
            "fieldsOfStudy": [
                "Mathematics",
                "Medicine"
            ],
            "id": "e7a5f41b547afe5b8f38860436446a123c495738",
            "isKey": false,
            "numCitedBy": 3419,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "H-=-W.-Meyers-Serrin",
            "title": {
                "fragments": [],
                "text": "H = W."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 1964
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737104"
                        ],
                        "name": "J. Burke",
                        "slug": "J.-Burke",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Burke",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Burke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144241870"
                        ],
                        "name": "J. J. Mor\u00e9",
                        "slug": "J.-J.-Mor\u00e9",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Mor\u00e9",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. J. Mor\u00e9"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 41231734,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f557aa8c2d747f3b565d52d6642f27a018651f11",
            "isKey": false,
            "numCitedBy": 50,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Exposing-Constraints-Burke-Mor\u00e9",
            "title": {
                "fragments": [],
                "text": "Exposing Constraints"
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Optim."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3267844"
                        ],
                        "name": "K. Schittkowski",
                        "slug": "K.-Schittkowski",
                        "structuredName": {
                            "firstName": "Klaus",
                            "lastName": "Schittkowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Schittkowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2907488"
                        ],
                        "name": "Christian Zillober",
                        "slug": "Christian-Zillober",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Zillober",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Zillober"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 77
                            }
                        ],
                        "text": "The case n 2 can be studied in closed form; see Bazaraa, Sherali, and Shetty [14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 44060508,
            "fieldsOfStudy": [],
            "id": "d4143c46910f249bedbdc37caf88e4c292124c08",
            "isKey": false,
            "numCitedBy": 6359,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "NONLINEAR-PROGRAMMING-Schittkowski-Zillober",
            "title": {
                "fragments": [],
                "text": "NONLINEAR PROGRAMMING"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2271961"
                        ],
                        "name": "V. Chandru",
                        "slug": "V.-Chandru",
                        "structuredName": {
                            "firstName": "Vijay",
                            "lastName": "Chandru",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Chandru"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144635404"
                        ],
                        "name": "M. Rao",
                        "slug": "M.-Rao",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Rao",
                            "middleNames": [
                                "Rammohan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Rao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[77], and Wolsey [312] for comprehensive treatments of this subject."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1558541,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "df65c38f24b8010ddb030d4d88ec4d7bbe8d36cd",
            "isKey": false,
            "numCitedBy": 1688,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Combinatorial-Optimization-Chandru-Rao",
            "title": {
                "fragments": [],
                "text": "Combinatorial Optimization"
            },
            "venue": {
                "fragments": [],
                "text": "The Computer Science and Engineering Handbook"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721020"
                        ],
                        "name": "R. Wets",
                        "slug": "R.-Wets",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Wets",
                            "middleNames": [
                                "J.-B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Wets"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 118
                            }
                        ],
                        "text": "For further information on stochastic optimization, consult the books of Birge and Louveaux [22] and Kall and Wallace [174]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 94
                            }
                        ],
                        "text": "Books on stochastic optimization are only now appearing; we mention those of Kall and Wallace [174], Birge and Louveaux [22]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18025293,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b2c91b913b50546e28b9bdad4bcc70d66de5979e",
            "isKey": false,
            "numCitedBy": 1649,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Stochastic-Programming-Wets",
            "title": {
                "fragments": [],
                "text": "Stochastic Programming"
            },
            "venue": {
                "fragments": [],
                "text": "Advanced Optimization for Process Systems Engineering"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710850"
                        ],
                        "name": "C. Bischof",
                        "slug": "C.-Bischof",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Bischof",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bischof"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2454215"
                        ],
                        "name": "G. Corliss",
                        "slug": "G.-Corliss",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Corliss",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Corliss"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145983059"
                        ],
                        "name": "A. Griewank",
                        "slug": "A.-Griewank",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Griewank",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Griewank"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 48
                            }
                        ],
                        "text": "For details, see Bischof, Corliss, and Griewank [26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 122041338,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5f60aa68924e2d8aa3495be0e4f819acc71a0cda",
            "isKey": false,
            "numCitedBy": 64,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Second- and higher-order derivatives are required by applications in scientific computation, especially for optimization algorithms. The two complementary ideas of interpolating partial derivatives from univariate Taylor series and preaccumulating of \u201clocal\u201d derivatives form the mathematical foundations for accurate, efficient computation of second-and higher-order partial derivatives for large codes. We compute derivatives in a fashion that parallelizes well, exploits sparsity or other structure frequently found in Hessian matrices, can compute only selected elements of a Hessian matrix, and computes Hessian \u00d7 vector products."
            },
            "slug": "Structured-second-and-higher-order-derivatives-Bischof-Corliss",
            "title": {
                "fragments": [],
                "text": "Structured second-and higher-order derivatives through univariate Taylor series"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work computes derivatives in a fashion that parallelizes well, exploits sparsity or other structure frequently found in Hessian matrices, can compute only selected elements of a Hessian matrix, and computes Hessian \u00d7 vector products."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065563135"
                        ],
                        "name": "Dafydd Gibbon",
                        "slug": "Dafydd-Gibbon",
                        "structuredName": {
                            "firstName": "Dafydd",
                            "lastName": "Gibbon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dafydd Gibbon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109855796"
                        ],
                        "name": "Roger K. Moore",
                        "slug": "Roger-K.-Moore",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Moore",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roger K. Moore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2096877955"
                        ],
                        "name": "Richard Winski",
                        "slug": "Richard-Winski",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Winski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Richard Winski"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 148399948,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "29a4c9f69b51bb01cc314ccd48cd0ae59c4fa60f",
            "isKey": false,
            "numCitedBy": 1589,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "1-User\u2019s-guide-Gibbon-Moore",
            "title": {
                "fragments": [],
                "text": "1 User\u2019s guide"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685738"
                        ],
                        "name": "M. H. Wright",
                        "slug": "M.-H.-Wright",
                        "structuredName": {
                            "firstName": "Margaret",
                            "lastName": "Wright",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. H. Wright"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 64988980,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "9e0b30065d83c7a95bd7a8cd2c0a59f06fb8cfd0",
            "isKey": false,
            "numCitedBy": 331,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Direct-search-methods:-Once-scorned,-now-Wright",
            "title": {
                "fragments": [],
                "text": "Direct search methods: Once scorned, now respectable"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Finite Element Method: Linear Static and Dynamic Finite Element Analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 22
                            }
                        ],
                        "text": "2 are used in LBFGS-B [322], IPOPT [301], and KNITRO."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 21
                            }
                        ],
                        "text": "The program L-BFGS-B [322] makes extensive use of compact limited-memory approximations to solve large nonlinear optimization problems with bound constraints."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Algorithm 778: L-BFGS-B"
            },
            "venue": {
                "fragments": [],
                "text": "FORTRAN subroutines for large scale bound constrained optimization, ACM Transactions on Mathematical Software, 23 "
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Some global convergence properties of a variable metric algorithm for minimization without exact line searches"
            },
            "venue": {
                "fragments": [],
                "text": "Nonlinear Programming"
            },
            "year": 1976
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Convergence properties of the Nelder-Mead simplex algorithm in low dimensions"
            },
            "venue": {
                "fragments": [],
                "text": "SIAM Journal on Optimization, 9 "
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 62
                            }
                        ],
                        "text": "One of the earliest works on trust-region methods is Winfield [307]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Function and functional optimization by interpolation in data tables"
            },
            "venue": {
                "fragments": [],
                "text": "PhD thesis, Harvard University, Cambridge, USA"
            },
            "year": 1969
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the solution of equality constrained quadratic problems arising in optimization"
            },
            "venue": {
                "fragments": [],
                "text": "SIAM Journal on Scientific Computing, 23 "
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An inexact Levenberg-Marquardt method for large sparse nonlinear least squares problems"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of the Australian Mathematical Society, Series B, 26 "
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 21
                            }
                        ],
                        "text": "Nemirovsky and Yudin [225] devote some attention to the global efficiency of the Fletcher\u2013Reeves and Polak\u2013Ribi\u00e8re methods with exact line searches."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 23
                            }
                        ],
                        "text": "Subsequently, Nesterov [225] presented an algorithm that attains this optimal bound."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Problem complexity and method efficiency"
            },
            "venue": {
                "fragments": [],
                "text": "John Wiley & Sons, New York"
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Linearly constrained optimization and projected preconditioned conjugate gradients"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Fifth SIAM Conference on Applied Linear Algebra, J. Lewis, ed., Philadelphia, USA"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 52
                            }
                        ],
                        "text": "Computational experiments by Conn, Gould, and Toint [70, 73] and Khalfan, Byrd, and Schnabel [181], using both line search and trust-region approaches, indicate that the SR1 method appears to be competitive with the BFGS method."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Testing a class of algorithms for solving minimization problems with simple bounds on the variables"
            },
            "venue": {
                "fragments": [],
                "text": "Mathematics of Computation, 50 "
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A collection of nonlinear model problems, in Computational Solution of Nonlinear Systems of Equations"
            },
            "venue": {
                "fragments": [],
                "text": "Lectures in Applied Mathematics"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The logarithmic potential method of convex programming"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report, University Institute of Economics, Oslo, Norway"
            },
            "year": 1955
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Interior Point Polynomial Methods in Convex Programming"
            },
            "venue": {
                "fragments": [],
                "text": "SIAM Publications, Philadelphia"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A simplicial algorithm for concave programming"
            },
            "venue": {
                "fragments": [],
                "text": "PhD thesis, Graduate School of Business Administration, Harvard University"
            },
            "year": 1963
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 163
                            }
                        ],
                        "text": "The process of forming and writing the graph can be implemented as a straightforward extension to the elementary operations via operator overloading (as in ADOL-C [154])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 28
                            }
                        ],
                        "text": "Other tools (such as ADOL-C [154]) keep a record of the elementary computations that take place while the function evaluation code for a given point x is executing on the computer."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "ADOL-C"
            },
            "venue": {
                "fragments": [],
                "text": "A package for the automatic differentiation of algorithms written in C/C++, ACM Transactions on Mathematical Software, 22 "
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Algorithm 573 \u2014 NL2SOL"
            },
            "venue": {
                "fragments": [],
                "text": "An adaptive nonlinear least-squares algorithm, ACM Transactions on Mathematical Software, 7 "
            },
            "year": 1981
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Superlinearly convergent quasi-Newton methods for nonlinearly constrained optimization problems"
            },
            "venue": {
                "fragments": [],
                "text": "Mathematical Programming, 11 "
            },
            "year": 1976
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "User\u2019s guide for SOL/QPSOL"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report SOL84\u20136, Department of Operations Research, Stanford University, Stanford, California"
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An interior-point nonlinear programming algorithm for large scale optimization"
            },
            "venue": {
                "fragments": [],
                "text": "Technical report MCT TECH-003, Mathematics and Computing Technology, The Boeing Company, P.O. Box 3707, Seattle, WA 98124-2207"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Steering exact penalty methods"
            },
            "venue": {
                "fragments": [],
                "text": "Optimization Technology Center"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Interior Point Polynomial Methods in Convex Programming"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "AND M"
            },
            "venue": {
                "fragments": [],
                "text": "A. SAUNDERS, User\u2019s guide for SNOPT (Version 5.3): A FOR- TRAN package for large-scale nonlinear programming, Technical Report NA 97-4, Department of Mathematics, University of California, San Diego"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Some history of the conjugate gradient methods and the Lanczos algorithms"
            },
            "venue": {
                "fragments": [],
                "text": "SIAM Review"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A gradient projection algorithm for nonlinear constraints"
            },
            "venue": {
                "fragments": [],
                "text": "Numerical Methods for Non-Linear Optimization, F. A. Lootsma, ed., Academic Press, London and New York"
            },
            "year": 1972
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 92
                            }
                        ],
                        "text": "Software for computing incomplete Cholesky preconditioners includes the ICFS [193] and MA57 [166] packages."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A catalogue of subroutines (release 13)"
            },
            "venue": {
                "fragments": [],
                "text": "AERE Harwell Laboratory, Harwell, Oxfordshire, England"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A view of line searches, in Optimization and Optimal Control"
            },
            "venue": {
                "fragments": [],
                "text": "Lecture Notes in Control and Information Science"
            },
            "year": 1981
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A gradient projection algorithm for nonlinear constraints"
            },
            "venue": {
                "fragments": [],
                "text": "Numerical Methods for Non-Linear Optimization, F. A. Lootsma"
            },
            "year": 1972
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Superlinearly convergent quasi-Newton methods for nonlinearly constrained optimization problems"
            },
            "venue": {
                "fragments": [],
                "text": "Mathematical Programming"
            },
            "year": 1976
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "LAPACK User\u2019s Guide"
            },
            "venue": {
                "fragments": [],
                "text": "SIAM, Philadelphia"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A hybrid method for nonlinear equations"
            },
            "venue": {
                "fragments": [],
                "text": "Numerical Methods for Nonlinear Algebraic Equations, P. Rabinowitz"
            },
            "year": 1970
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 83
                            }
                        ],
                        "text": "For a discussion of such issues and an approach to dealing with them, see Griewank [151, 152]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Evaluating Derivatives: Principles and Techniques of Automatic Differentiation"
            },
            "venue": {
                "fragments": [],
                "text": "vol. 19 of Frontiers in Applied Mathematics, SIAM"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A limited-memory variable-metric algorithm for bound constrained minimization"
            },
            "venue": {
                "fragments": [],
                "text": "Numerical Analysis Report P909-0901, ANL, Argonne, IL, USA"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "of Handbooks in Operations Research and Management"
            },
            "venue": {
                "fragments": [],
                "text": "Optimization"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Iterative solution of problems of linear and quadratic programming"
            },
            "venue": {
                "fragments": [],
                "text": "Soviet Mathematics-Doklady, 8 "
            },
            "year": 1967
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The performance of two subroutines for constrained optimization on some difficult test problems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 101
                            }
                        ],
                        "text": "For a history of the development of the conjugate gradient and Lanczos methods see Golub and O\u2019Leary [135]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "O\u2019LEARY, Some history of the conjugate gradient methods and the Lanczos algorithms: 1948\u20131976"
            },
            "venue": {
                "fragments": [],
                "text": "SIAM Review,"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Problems related to unconstrained optimization, in Numerical Methods for Unconstrained Optimization"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Note sur la convergence de m\u00e9thodes de directions conjugu\u00e9es, Revue Fran\u00e7aise d'Informatique et de Recherche Op\u00e9rationnelle"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1969
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 33,
            "methodology": 51,
            "result": 5
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 270,
        "totalPages": 27
    },
    "page_url": "https://www.semanticscholar.org/paper/Numerical-Optimization-Nocedal-Wright/43c3bfffdcd313c549b2045980855ea001d6f13b?sort=total-citations"
}