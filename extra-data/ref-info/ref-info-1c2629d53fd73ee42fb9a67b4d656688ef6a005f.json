{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763086"
                        ],
                        "name": "Alexei A. Efros",
                        "slug": "Alexei-A.-Efros",
                        "structuredName": {
                            "firstName": "Alexei",
                            "lastName": "Efros",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexei A. Efros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39668247"
                        ],
                        "name": "A. Berg",
                        "slug": "A.-Berg",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Berg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10771328"
                        ],
                        "name": "Greg Mori",
                        "slug": "Greg-Mori",
                        "structuredName": {
                            "firstName": "Greg",
                            "lastName": "Mori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Greg Mori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[7] proposed an approach to recognizing human actions at low resolutions which consisted of a motion descriptor based on smoothed and aggregated optical flow"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1350374,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "804d86dd7ab3498266922244e73a88c1add5a6ab",
            "isKey": false,
            "numCitedBy": 1470,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Our goal is to recognize human action at a distance, at resolutions where a whole person may be, say, 30 pixels tall. We introduce a novel motion descriptor based on optical flow measurements in a spatiotemporal volume for each stabilized human figure, and an associated similarity measure to be used in a nearest-neighbor framework. Making use of noisy optical flow measurements is the key challenge, which is addressed by treating optical flow not as precise pixel displacements, but rather as a spatial pattern of noisy measurements which are carefully smoothed and aggregated to form our spatiotemporal motion descriptor. To classify the action being performed by a human figure in a query sequence, we retrieve nearest neighbor(s) from a database of stored, annotated video sequences. We can also use these retrieved exemplars to transfer 2D/3D skeletons onto the figures in the query sequence, as well as two forms of data-based action synthesis \"do as I do\" and \"do as I say\". Results are demonstrated on ballet, tennis as well as football datasets."
            },
            "slug": "Recognizing-action-at-a-distance-Efros-Berg",
            "title": {
                "fragments": [],
                "text": "Recognizing action at a distance"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A novel motion descriptor based on optical flow measurements in a spatiotemporal volume for each stabilized human figure is introduced, and an associated similarity measure to be used in a nearest-neighbor framework is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Ninth IEEE International Conference on Computer Vision"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50170517"
                        ],
                        "name": "M. Blank",
                        "slug": "M.-Blank",
                        "structuredName": {
                            "firstName": "Moshe",
                            "lastName": "Blank",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Blank"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3089071"
                        ],
                        "name": "Lena Gorelick",
                        "slug": "Lena-Gorelick",
                        "structuredName": {
                            "firstName": "Lena",
                            "lastName": "Gorelick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lena Gorelick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2177801"
                        ],
                        "name": "E. Shechtman",
                        "slug": "E.-Shechtman",
                        "structuredName": {
                            "firstName": "Eli",
                            "lastName": "Shechtman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Shechtman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144611617"
                        ],
                        "name": "M. Irani",
                        "slug": "M.-Irani",
                        "structuredName": {
                            "firstName": "Michal",
                            "lastName": "Irani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Irani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760994"
                        ],
                        "name": "R. Basri",
                        "slug": "R.-Basri",
                        "structuredName": {
                            "firstName": "Ronen",
                            "lastName": "Basri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Basri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 8
                            }
                        ],
                        "text": "Whereas [3] reports a runtime of 30 minutes on the same architecture for this dataset, our results represent a considerable increase in performance over existing template-based methods."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 61
                            }
                        ],
                        "text": "We tested the proposed method on the Weizmann action dataset [3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 175905,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1a9eb04b9b07d4a58aa78eb9f68a77ade0199fab",
            "isKey": false,
            "numCitedBy": 1704,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "Human action in video sequences can be seen as silhouettes of a moving torso and protruding limbs undergoing articulated motion. We regard human actions as three-dimensional shapes induced by the silhouettes in the space-time volume. We adopt a recent approach by Gorelick et al. (2004) for analyzing 2D shapes and generalize it to deal with volumetric space-time action shapes. Our method utilizes properties of the solution to the Poisson equation to extract space-time features such as local space-time saliency, action dynamics, shape structure and orientation. We show that these features are useful for action recognition, detection and clustering. The method is fast, does not require video alignment and is applicable in (but not limited to) many scenarios where the background is known. Moreover, we demonstrate the robustness of our method to partial occlusions, non-rigid deformations, significant changes in scale and viewpoint, high irregularities in the performance of an action and low quality video"
            },
            "slug": "Actions-as-space-time-shapes-Blank-Gorelick",
            "title": {
                "fragments": [],
                "text": "Actions as space-time shapes"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The method is fast, does not require video alignment and is applicable in many scenarios where the background is known, and the robustness of the method is demonstrated to partial occlusions, non-rigid deformations, significant changes in scale and viewpoint, high irregularities in the performance of an action and low quality video."
            },
            "venue": {
                "fragments": [],
                "text": "Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688328"
                        ],
                        "name": "A. Bobick",
                        "slug": "A.-Bobick",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Bobick",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bobick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144429686"
                        ],
                        "name": "James W. Davis",
                        "slug": "James-W.-Davis",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Davis",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James W. Davis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 13
                            }
                        ],
                        "text": "Bobick et al [4] computed Hu moments of motion energy images and motion-history images to create action templates based on a set of training examples which were represented by the mean and covariance matrix of the moments."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2006961,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "886431a362bfdbcc6dd518f844eb374950b9de86",
            "isKey": false,
            "numCitedBy": 2877,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "A view-based approach to the representation and recognition of human movement is presented. The basis of the representation is a temporal template-a static vector-image where the vector value at each point is a function of the motion properties at the corresponding spatial location in an image sequence. Using aerobics exercises as a test domain, we explore the representational power of a simple, two component version of the templates: The first value is a binary value indicating the presence of motion and the second value is a function of the recency of motion in a sequence. We then develop a recognition method matching temporal templates against stored instances of views of known actions. The method automatically performs temporal segmentation, is invariant to linear changes in speed, and runs in real-time on standard platforms."
            },
            "slug": "The-Recognition-of-Human-Movement-Using-Temporal-Bobick-Davis",
            "title": {
                "fragments": [],
                "text": "The Recognition of Human Movement Using Temporal Templates"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A view-based approach to the representation and recognition of human movement is presented, and a recognition method matching temporal templates against stored instances of views of known actions is developed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40150389"
                        ],
                        "name": "O. Alatas",
                        "slug": "O.-Alatas",
                        "structuredName": {
                            "firstName": "Orkun",
                            "lastName": "Alatas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Alatas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144855557"
                        ],
                        "name": "Pingkun Yan",
                        "slug": "Pingkun-Yan",
                        "structuredName": {
                            "firstName": "Pingkun",
                            "lastName": "Yan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pingkun Yan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145103012"
                        ],
                        "name": "M. Shah",
                        "slug": "M.-Shah",
                        "structuredName": {
                            "firstName": "Mubarak",
                            "lastName": "Shah",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Shah"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11548344,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c5756f1ae20eb343c78d79eef721effcacfc9e0",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Feature selection and extraction is a key operation in video analysis for achieving a higher level of abstraction. In this paper, we introduce a general framework to extract a new spatio-temporal feature that represents the directions in which a video is regular, i.e., the pixel appearances change the least. We propose to model the directions of regular variations with a 3-D vector field, which is referred to as spatio-temporal regularity flow (SPREF). SPREF vectors are designed to have three cross-sectional parallel components Fx, Fy , and Ft for convenient use in different applications. They are estimated using all the frames simultaneously by minimizing an energy functional formulated according to its definition. In this paper, we first introduce translational SPREF (T-SPREF) and then extend our framework to affine SPREF (A-SPREF). The successful use of SPREF in a few applications, including object removal, video inpainting, and video compression, is also demonstrated"
            },
            "slug": "Spatio\u2013Temporal-Regularity-Flow-(SPREF):-Its-and-Alatas-Yan",
            "title": {
                "fragments": [],
                "text": "Spatio\u2013Temporal Regularity Flow (SPREF): Its Estimation and Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A general framework to extract a new spatio-temporal feature that represents the directions in which a video is regular, i.e., the pixel appearances change the least is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Circuits and Systems for Video Technology"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2652428"
                        ],
                        "name": "R. Polana",
                        "slug": "R.-Polana",
                        "structuredName": {
                            "firstName": "Ramprasad",
                            "lastName": "Polana",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Polana"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113399896"
                        ],
                        "name": "R. Nelson",
                        "slug": "R.-Nelson",
                        "structuredName": {
                            "firstName": "Randall",
                            "lastName": "Nelson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Nelson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6353138,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "1ee01bf96b5dbd441eabda533fa89da3fa4d916a",
            "isKey": false,
            "numCitedBy": 371,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The recognition of human movements such as walking, running or climbing has been approached previously by tracking a number of feature points and either classifying the trajectories directly or matching them with a high-level model of the movement. A major difficulty with these methods is acquiring and trading the requisite feature points, which are generally specific joints such as knees or angles. This requires previous recognition and/or part segmentation of the actor. We show that the recognition of walking or any repetitive motion activity can be accomplished on the basis of bottom up processing, which does not require the prior identification of specific parts, or classification of the actor. In particular, we demonstrate that repetitive motion is such a strong cue, that the moving actor can be segmented, normalized spatially and temporally, and recognized by matching against a spatiotemporal template of motion features. We have implemented a real-time system that can recognize and classify repetitive motion activities in normal gray-scale image sequences.<<ETX>>"
            },
            "slug": "Low-level-recognition-of-human-motion-(or-how-to-Polana-Nelson",
            "title": {
                "fragments": [],
                "text": "Low level recognition of human motion (or how to get your man without finding his body parts)"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is demonstrated that repetitive motion is such a strong cue, that the moving actor can be segmented, normalized spatially and temporally, and recognized by matching against a spatiotemporal template of motion features."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1994 IEEE Workshop on Motion of Non-rigid and Articulated Objects"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2177801"
                        ],
                        "name": "E. Shechtman",
                        "slug": "E.-Shechtman",
                        "structuredName": {
                            "firstName": "Eli",
                            "lastName": "Shechtman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Shechtman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144611617"
                        ],
                        "name": "M. Irani",
                        "slug": "M.-Irani",
                        "structuredName": {
                            "firstName": "Michal",
                            "lastName": "Irani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Irani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 20
                            }
                        ],
                        "text": "Shechtman and Irani [18] avoid explicit flow computations by employing a rank-based constraint directly on the intensity information of spatio-temporal cuboids to enforce consistency between a template and a target."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6891864,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4811a11937448ea5d40a0af36c974e08ab12c08c",
            "isKey": false,
            "numCitedBy": 386,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a behavior-based similarity measure which tells us whether two different space-time intensity patterns of two different video segments could have resulted from a similar underlying motion field. This is done directly from the intensity information, without explicitly computing the underlying motions. Such a measure allows us to detect similarity between video segments of differently dressed people performing the same type of activity. It requires no foreground/background segmentation, no prior learning of activities, and no motion estimation or tracking. Using this behavior-based similarity measure, we extend the notion of 2-dimensional image correlation into the 3-dimensional space-time volume, thus allowing to correlate dynamic behaviors and actions. Small space-time video segments (small video clips) are \"correlated\" against entire video sequences in all three dimensions (x,y, and t). Peak correlation values correspond to video locations with similar dynamic behaviors. Our approach can detect very complex behaviors in video sequences (e.g., ballet movements, pool dives, running water), even when multiple complex activities occur simultaneously within the field-of-view of the camera."
            },
            "slug": "Space-time-behavior-based-correlation-Shechtman-Irani",
            "title": {
                "fragments": [],
                "text": "Space-time behavior based correlation"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "A behavior-based similarity measure is introduced which tells us whether two different space-time intensity patterns of two different video segments could have resulted from a similar underlying motion field, thus allowing to correlate dynamic behaviors and actions."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2189118"
                        ],
                        "name": "Ashish Kapoor",
                        "slug": "Ashish-Kapoor",
                        "structuredName": {
                            "firstName": "Ashish",
                            "lastName": "Kapoor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ashish Kapoor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114755206"
                        ],
                        "name": "Yuan Qi",
                        "slug": "Yuan-Qi",
                        "structuredName": {
                            "firstName": "Yuan",
                            "lastName": "Qi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuan Qi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719389"
                        ],
                        "name": "Rosalind W. Picard",
                        "slug": "Rosalind-W.-Picard",
                        "structuredName": {
                            "firstName": "Rosalind",
                            "lastName": "Picard",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rosalind W. Picard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Whereas [3] reports a runtime of 30 minutes on the same architecture for this dataset, our results represent a considerable increase in performance over existing template-based methods."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 11828808,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4580b1220dfdb88b3d878673724a8c19e178e6ea",
            "isKey": false,
            "numCitedBy": 144,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "We provide a new fully automatic framework to analyze facial action units, the fundamental building blocks of facial expression enumerated in Paul Ekman's facial action coding system (FACS). The action units examined here include upper facial muscle movements such as inner eyebrow raise, eye widening, and so forth, which combine to form facial expressions. Although prior methods have obtained high recognition rates for recognizing facial action units, these methods either use manually preprocessed image sequences or require human specification of facial features; thus, they have exploited substantial human intervention. We present a fully automatic method, requiring no such human specification. The system first robustly detects the pupils using an infrared sensitive camera equipped with infrared LEDs. For each frame, the pupil positions are used to localize and normalize eye and eyebrow regions, which are analyzed using PCA to recover parameters that relate to the shape of the facial features. These parameters are used as input to classifiers based on support vector machines to recognize upper facial action units and all their possible combinations. On a completely natural dataset with lots of head movements, pose changes and occlusions, the new framework achieved a recognition accuracy of 69.3% for each individual AU and an accuracy of 62.5% for all possible AU combinations. This framework achieves a higher recognition accuracy on the Cohn-Kanade AU-coded facial expression database, which has been previously used to evaluate other facial action recognition system."
            },
            "slug": "Fully-automatic-upper-facial-action-recognition-Kapoor-Qi",
            "title": {
                "fragments": [],
                "text": "Fully automatic upper facial action recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A new fully automatic framework to analyze facial action units, the fundamental building blocks of facial expression enumerated in Paul Ekman's facial action coding system (FACS), achieves a higher recognition accuracy on the Cohn-Kanade AU-coded facial expression database, which has been previously used to evaluate other facial action recognition system."
            },
            "venue": {
                "fragments": [],
                "text": "2003 IEEE International SOI Conference. Proceedings (Cat. No.03CH37443)"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49641404"
                        ],
                        "name": "I. Cohen",
                        "slug": "I.-Cohen",
                        "structuredName": {
                            "firstName": "Ira",
                            "lastName": "Cohen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Cohen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703601"
                        ],
                        "name": "N. Sebe",
                        "slug": "N.-Sebe",
                        "structuredName": {
                            "firstName": "N.",
                            "lastName": "Sebe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Sebe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729828"
                        ],
                        "name": "A. Garg",
                        "slug": "A.-Garg",
                        "structuredName": {
                            "firstName": "Ashutosh",
                            "lastName": "Garg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Garg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108579847"
                        ],
                        "name": "Lawrence S. Chen",
                        "slug": "Lawrence-S.-Chen",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Chen",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lawrence S. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652752"
                        ],
                        "name": "Thomas S. Huang",
                        "slug": "Thomas-S.-Huang",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Huang",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas S. Huang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 46
                            }
                        ],
                        "text": "Unlike a significant number of existing works [12, 5, 1], no prior facial model or feature tracking was used in training."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 8693566,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f1970df3155799ae35fc47e71c3047198f532d9",
            "isKey": false,
            "numCitedBy": 858,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Facial-expression-recognition-from-video-sequences:-Cohen-Sebe",
            "title": {
                "fragments": [],
                "text": "Facial expression recognition from video sequences: temporal and static modeling"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Image Underst."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2218905"
                        ],
                        "name": "M. Bartlett",
                        "slug": "M.-Bartlett",
                        "structuredName": {
                            "firstName": "Marian",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "Stewart"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46548046"
                        ],
                        "name": "G. Littlewort",
                        "slug": "G.-Littlewort",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Littlewort",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Littlewort"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145595070"
                        ],
                        "name": "M. Frank",
                        "slug": "M.-Frank",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Frank",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Frank"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2767464"
                        ],
                        "name": "C. Lainscsek",
                        "slug": "C.-Lainscsek",
                        "structuredName": {
                            "firstName": "Claudia",
                            "lastName": "Lainscsek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lainscsek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2039025"
                        ],
                        "name": "I. Fasel",
                        "slug": "I.-Fasel",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Fasel",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Fasel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741200"
                        ],
                        "name": "J. Movellan",
                        "slug": "J.-Movellan",
                        "structuredName": {
                            "firstName": "Javier",
                            "lastName": "Movellan",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Movellan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15823252,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1bcbf2a4500d27d036e0f9d36d7af71c72f8ab61",
            "isKey": false,
            "numCitedBy": 680,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a systematic comparison of machine learning methods applied to the problem of fully automatic recognition of facial expressions. We report results on a series of experiments comparing recognition engines, including AdaBoost, support vector machines, linear discriminant analysis. We also explored feature selection techniques, including the use of AdaBoost for feature selection prior to classification by SVM or LDA. Best results were obtained by selecting a subset of Gabor filters using AdaBoost followed by classification with support vector machines. The system operates in real-time, and obtained 93% correct generalization to novel subjects for a 7-way forced choice on the Cohn-Kanade expression dataset. The outputs of the classifiers change smoothly as a function of time and thus can be used to measure facial expression dynamics. We applied the system to to fully automated recognition of facial actions (FACS). The present system classifies 17 action units, whether they occur singly or in combination with other actions, with a mean accuracy of 94.8%. We present preliminary results for applying this system to spontaneous facial expressions."
            },
            "slug": "Recognizing-facial-expression:-machine-learning-and-Bartlett-Littlewort",
            "title": {
                "fragments": [],
                "text": "Recognizing facial expression: machine learning and application to spontaneous behavior"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The system operates in real-time, and obtained 93% correct generalization to novel subjects for a 7-way forced choice on the Cohn-Kanade expression dataset, and has a mean accuracy of 94.8%."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21472040"
                        ],
                        "name": "Irfan Essa",
                        "slug": "Irfan-Essa",
                        "structuredName": {
                            "firstName": "Irfan",
                            "lastName": "Essa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Irfan Essa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144994682"
                        ],
                        "name": "A. Pentland",
                        "slug": "A.-Pentland",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Pentland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pentland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 18
                            }
                        ],
                        "text": "Essa and Pentland [8] generated spatio-temporal templates based on optical flow energy functions to recognize facial action units."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2117401,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "74839c6ded777733519acaf44684d927c5e625bd",
            "isKey": false,
            "numCitedBy": 1053,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a computer vision system for observing facial motion by using an optimal estimation optical flow method coupled with geometric, physical and motion-based dynamic models describing the facial structure. Our method produces a reliable parametric representation of the face's independent muscle action groups, as well as an accurate estimate of facial motion. Previous efforts at analysis of facial expression have been based on the facial action coding system (FACS), a representation developed in order to allow human psychologists to code expression from static pictures. To avoid use of this heuristic coding scheme, we have used our computer vision system to probabilistically characterize facial motion and muscle activation in an experimental population, thus deriving a new, more accurate, representation of human facial expressions that we call FACS+. Finally, we show how this method can be used for coding, analysis, interpretation, and recognition of facial expressions."
            },
            "slug": "Coding,-Analysis,-Interpretation,-and-Recognition-Essa-Pentland",
            "title": {
                "fragments": [],
                "text": "Coding, Analysis, Interpretation, and Recognition of Facial Expressions"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A computer vision system for observing facial motion by using an optimal estimation optical flow method coupled with geometric, physical and motion-based dynamic models describing the facial structure produces a reliable parametric representation of the face's independent muscle action groups, as well as an accurate estimate of facial motion."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145738190"
                        ],
                        "name": "Yeon-Ho Kim",
                        "slug": "Yeon-Ho-Kim",
                        "structuredName": {
                            "firstName": "Yeon-Ho",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yeon-Ho Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1384255355"
                        ],
                        "name": "Aleix M. Martinez",
                        "slug": "Aleix-M.-Martinez",
                        "structuredName": {
                            "firstName": "Aleix M.",
                            "lastName": "Martinez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aleix M. Martinez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703247"
                        ],
                        "name": "A. Kak",
                        "slug": "A.-Kak",
                        "structuredName": {
                            "firstName": "Avinash",
                            "lastName": "Kak",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kak"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 111
                            }
                        ],
                        "text": "Therefore, numerous flow estimation approaches have been proposed which deal with large motion discontinuities [13], complex illumination variation, iconic changes [2], and three-dimensional scene flow [20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2144750,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "393fbfc705b7c27675a848450c3fa700f3f5fc63",
            "isKey": false,
            "numCitedBy": 112,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Robust-motion-estimation-under-varying-illumination-Kim-Martinez",
            "title": {
                "fragments": [],
                "text": "Robust motion estimation under varying illumination"
            },
            "venue": {
                "fragments": [],
                "text": "Image Vis. Comput."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1401342227"
                        ],
                        "name": "P. Hennings-Yeomans",
                        "slug": "P.-Hennings-Yeomans",
                        "structuredName": {
                            "firstName": "Pablo",
                            "lastName": "Hennings-Yeomans",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hennings-Yeomans"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "133709785"
                        ],
                        "name": "B. Kumar",
                        "slug": "B.-Kumar",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Kumar",
                            "middleNames": [
                                "V.",
                                "K.",
                                "Vijaya"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794486"
                        ],
                        "name": "M. Savvides",
                        "slug": "M.-Savvides",
                        "structuredName": {
                            "firstName": "Marios",
                            "lastName": "Savvides",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Savvides"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 99
                            }
                        ],
                        "text": "Action recognition constitutes one of the most challenging problems in computer vision, yet effective solutions capable of recognizing motion patterns in uncontrolled environments could lend themselves to a host of important application domains, such as video indexing, surveillance, human-computer\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17529499,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "00f0367d2364297a742178eda3944820cfaf9ac9",
            "isKey": false,
            "numCitedBy": 90,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a palmprint classification algorithm with the use of multiple correlation filters per class. Correlation filters are two-class classifiers that produce a sharp peak when filtering a sample of their class and a noisy output otherwise. For every class, we train the filters for a palm at different locations, where the palmprint region has a high degree of line content. With the use of a line detection procedure and a simple line energy measure, any region of the palm can be scored and the top-ranked regions are used to train the filters for each class. Using an enhanced palmprint segmentation algorithm, our proposed classifier achieves an average equal error rate of 1.12 times10-4% on a large database of 385 classes using multiple filters of size 64 times 64 pixels. The average false acceptance rate when the false rejection rate is zero is 2.25 times10-4%."
            },
            "slug": "Palmprint-Classification-Using-Multiple-Advanced-Hennings-Yeomans-Kumar",
            "title": {
                "fragments": [],
                "text": "Palmprint Classification Using Multiple Advanced Correlation Filters and Palm-Specific Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "A palmprint classification algorithm with the use of multiple correlation filters per class using multiple filters of size 64 times 64 pixels to achieve an average equal error rate on a large database of 385 classes."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Information Forensics and Security"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40595326"
                        ],
                        "name": "S. Sims",
                        "slug": "S.-Sims",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Sims",
                            "middleNames": [
                                "Richard",
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sims"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31927072"
                        ],
                        "name": "Abhijit Mahalanobis",
                        "slug": "Abhijit-Mahalanobis",
                        "structuredName": {
                            "firstName": "Abhijit",
                            "lastName": "Mahalanobis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Abhijit Mahalanobis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 142
                            }
                        ],
                        "text": "Traditionally, MACH filters have been employed in object classification, palm print identification [9], and aided target recognition problems [19]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 293,
                                "start": 289
                            }
                        ],
                        "text": "Once the column vectors are obtained for all the examples of the action, the Action MACH filter (which minimizes average correlation energy, average similarity measure, output noise variance; and maximizes average correlation height) can be synthesized in the frequency domain, as follows [19]:"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 119777431,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "8b93717dcbcb460de2b75b8cf43bf8c86bd77456",
            "isKey": false,
            "numCitedBy": 50,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "The detection and discrimination of targets in infrared imagery has been a challenging problem due to the variability of the target and clutter (background) signatures. In this paper we discuss the application of a novel quadratic filtering method using missile seeker infrared closing sequences. Image filtering techniques are well suited for target detection applications since they avoid the disadvantages of typical pixel-based detection schemes (such as segmentation and edge extraction). Another advantage is that the throughput complexity of the filtering approach, in the detection process, also does not vary with scene content. The performance of the proposed approach is assessed on several data sets, and the results are compared with that of previous linear filtering techniques. Since we can obtain the signature of some of the clutter \u201cin-the-field\u201d or during operation, we examine the impact of updating the filters to adapt to the clutter."
            },
            "slug": "Performance-evaluation-of-quadratic-correlation-for-Sims-Mahalanobis",
            "title": {
                "fragments": [],
                "text": "Performance evaluation of quadratic correlation filters for target detection and discrimination in infrared imagery"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The application of a novel quadratic filtering method using missile seeker infrared closing sequences, which can obtain the signature of some of the clutter \"in the field\" or during operation, is discussed."
            },
            "venue": {
                "fragments": [],
                "text": "SPIE Defense + Commercial Sensing"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143814637"
                        ],
                        "name": "Berthold K. P. Horn",
                        "slug": "Berthold-K.-P.-Horn",
                        "structuredName": {
                            "firstName": "Berthold",
                            "lastName": "Horn",
                            "middleNames": [
                                "K.",
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Berthold K. P. Horn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717435"
                        ],
                        "name": "B. G. Schunck",
                        "slug": "B.-G.-Schunck",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Schunck",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. G. Schunck"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 94
                            }
                        ],
                        "text": "formulation of the classical flow estimation methods, such as Horn and Schunck\u2019s optical flow [10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1371968,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a3229dc33ecb80c59a75b906c46b586dd059b781",
            "isKey": false,
            "numCitedBy": 11343,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "Optical flow cannot be computed locally, since only one independent measurement is available from the image sequence at a point, while the flow velocity has two components. A second constraint is needed. A method for finding the optical flow pattern is presented which assumes that the apparent velocity of the brightness pattern varies smoothly almost everywhere in the image. An iterative implementation is shown which successfully computes the optical flow for a number of synthetic image sequences. The algorithm is robust in that it can handle image sequences that are quantized rather coarsely in space and time. It is also insensitive to quantization of brightness levels and additive noise. Examples are included where the assumption of smoothness is violated at singular points or along lines in the image."
            },
            "slug": "Determining-Optical-Flow-Horn-Schunck",
            "title": {
                "fragments": [],
                "text": "Determining Optical Flow"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "An iterative implementation is shown which successfully computes the optical flow for a number of synthetic image sequences and is robust in that it can handle image sequences that are quantified rather coarsely in space and time."
            },
            "venue": {
                "fragments": [],
                "text": "Other Conferences"
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40383812"
                        ],
                        "name": "Ying-li Tian",
                        "slug": "Ying-li-Tian",
                        "structuredName": {
                            "firstName": "Ying-li",
                            "lastName": "Tian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ying-li Tian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737918"
                        ],
                        "name": "J. Cohn",
                        "slug": "J.-Cohn",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Cohn",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cohn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16251989,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "e3f2391513693647e0ea87bfa86cd89e468f51d0",
            "isKey": false,
            "numCitedBy": 2691,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Within the past decade, significant effort has occurred in developing methods of facial expression analysis. Because most investigators have used relatively limited data sets, the generalizability of these various methods remains unknown. We describe the problem space for facial expression analysis, which includes level of description, transitions among expressions, eliciting conditions, reliability and validity of training and test data, individual differences in subjects, head orientation and scene complexity image characteristics, and relation to non-verbal behavior. We then present the CMU-Pittsburgh AU-Coded Face Expression Image Database, which currently includes 2105 digitized image sequences from 182 adult subjects of varying ethnicity, performing multiple tokens of most primary FACS action units. This database is the most comprehensive testbed to date for comparative studies of facial expression analysis."
            },
            "slug": "Comprehensive-database-for-facial-expression-Kanade-Tian",
            "title": {
                "fragments": [],
                "text": "Comprehensive database for facial expression analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The problem space for facial expression analysis is described, which includes level of description, transitions among expressions, eliciting conditions, reliability and validity of training and test data, individual differences in subjects, head orientation and scene complexity image characteristics, and relation to non-verbal behavior."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3285540"
                        ],
                        "name": "J. Ebling",
                        "slug": "J.-Ebling",
                        "structuredName": {
                            "firstName": "Julia",
                            "lastName": "Ebling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ebling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1810101"
                        ],
                        "name": "G. Scheuermann",
                        "slug": "G.-Scheuermann",
                        "structuredName": {
                            "firstName": "Gerik",
                            "lastName": "Scheuermann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Scheuermann"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 54
                            }
                        ],
                        "text": "For this purpose, we follow the framework proposed in [6], which consists of applying an algebraic extension to the degrees of freedom of a multi-dimensional Fourier transform by embedding the spectral domain into a domain of Clifford numbers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10330348,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "41ce67428ee60748a4142dee0eea28ed997855e6",
            "isKey": false,
            "numCitedBy": 149,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Image processing and computer vision have robust methods for feature extraction and the computation of derivatives of scalar fields. Furthermore, interpolation and the effects of applying a filter can be analyzed in detail and can be advantages when applying these methods to vector fields to obtain a solid theoretical basis for feature extraction. We recently introduced the Clifford convolution, which is an extension of the classical convolution on scalar fields and provides a unified notation for the convolution of scalar and vector fields. It has attractive geometric properties that allow pattern matching on vector fields. In image processing, the convolution and the Fourier transform operators are closely related by the convolution theorem and, in this paper, we extend the Fourier transform to include general elements of Clifford Algebra, called multivectors, including scalars and vectors. The resulting convolution and derivative theorems are extensions of those for convolution and the Fourier transform on scalar fields. The Clifford Fourier transform allows a frequency analysis of vector fields and the behavior of vector-valued filters. In frequency space, vectors are transformed into general multivectors of the Clifford Algebra. Many basic vector-valued patterns, such as source, sink, saddle points, and potential vortices, can be described by a few multivectors in frequency space."
            },
            "slug": "Clifford-Fourier-transform-on-vector-fields-Ebling-Scheuermann",
            "title": {
                "fragments": [],
                "text": "Clifford Fourier transform on vector fields"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The Clifford Fourier transform is extended to include general elements of Clifford Algebra, called multivectors, including scalars and vectors, and the resulting convolution and derivative theorems are extensions of those for Convolution and the Fouriertransform on scalar fields."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Visualization and Computer Graphics"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7678301"
                        ],
                        "name": "Hanying Zhou",
                        "slug": "Hanying-Zhou",
                        "structuredName": {
                            "firstName": "Hanying",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hanying Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2223765"
                        ],
                        "name": "T. Chao",
                        "slug": "T.-Chao",
                        "structuredName": {
                            "firstName": "Tien-Hsin",
                            "lastName": "Chao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Chao"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Traditionally, MACH filters have been employed in object classification, palm print identification [10], and aided target recognition problems [22, 20]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 53487005,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "660b1dced0571d2b6ee5fc16cd7a4474a9820bba",
            "isKey": false,
            "numCitedBy": 74,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We have recently demonstrated a compact, high speed, gray- scale optical correlator for target detection. The capability of the direct gray-scale scene input and the gray-scale (real- valued) filter modulation enables us to implement a near- theoretical optimal filter on the optical correlator. This paper describes filter synthesizing algorithm for detecting targets in cluttered background input scene and the projection from the complex filter version to the real version for implementation on the gray-scale optical correlator. It is based on optimal-tradeoff MACH filter. It was found that using an appropriate simulated noise image to substitute the commonly used white noise in the filter design procedure is a very effective way to suppress clutter noise while maintain high tolerance for distortion. Both simulation and experimental results are provided."
            },
            "slug": "MACH-filter-synthesizing-for-detecting-targets-in-Zhou-Chao",
            "title": {
                "fragments": [],
                "text": "MACH filter synthesizing for detecting targets in cluttered environment for grayscale optical correlator"
            },
            "venue": {
                "fragments": [],
                "text": "Defense, Security, and Sensing"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105795"
                        ],
                        "name": "Michael J. Black",
                        "slug": "Michael-J.-Black",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Black",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Black"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793739"
                        ],
                        "name": "David J. Fleet",
                        "slug": "David-J.-Fleet",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Fleet",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Fleet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1964574"
                        ],
                        "name": "Y. Yacoob",
                        "slug": "Y.-Yacoob",
                        "structuredName": {
                            "firstName": "Yaser",
                            "lastName": "Yacoob",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Yacoob"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 164
                            }
                        ],
                        "text": "Therefore, numerous flow estimation approaches have been proposed which deal with large motion discontinuities [13], complex illumination variation, iconic changes [2], and three-dimensional scene flow [20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2077571,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff1bdcc77c6298c3a12facddf0960ca5a4d242e5",
            "isKey": false,
            "numCitedBy": 171,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a generalized model of image \u201cappearance change\u201d in which brightness variation over time is represented as a probabilistic mixture of different causes. We define four generative models of appearance change due to (1) object or camera motion; (2) illumination phenomena; (3) specular reflections; and (4) \u201ciconic changes\u201d which are specific to the objects being viewed. These iconic changes include complex occlusion events and changes in the material properties of the objects. We develop a robust statistical framework for recovering these appearance changes in image sequences. This approach generalizes previous work on optical flow to provide a richer description of image events and more reliable estimates of image motion in the presense of shadows and specular reflections."
            },
            "slug": "Robustly-Estimating-Changes-in-Image-Appearance-Black-Fleet",
            "title": {
                "fragments": [],
                "text": "Robustly Estimating Changes in Image Appearance"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A generalized model of image \u201cappearance change\u201d in which brightness variation over time is represented as a probabilistic mixture of different causes is proposed to provide a richer description of image events and more reliable estimates of image motion in the presense of shadows and specular reflections."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Image Underst."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39335615"
                        ],
                        "name": "S. Vedula",
                        "slug": "S.-Vedula",
                        "structuredName": {
                            "firstName": "Sundar",
                            "lastName": "Vedula",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Vedula"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145347688"
                        ],
                        "name": "S. Baker",
                        "slug": "S.-Baker",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Baker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Baker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1885955"
                        ],
                        "name": "P. Rander",
                        "slug": "P.-Rander",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Rander",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Rander"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143980462"
                        ],
                        "name": "R. Collins",
                        "slug": "R.-Collins",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Collins",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Collins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733113"
                        ],
                        "name": "T. Kanade",
                        "slug": "T.-Kanade",
                        "structuredName": {
                            "firstName": "Takeo",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanade"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 202
                            }
                        ],
                        "text": "Therefore, numerous flow estimation approaches have been proposed which deal with large motion discontinuities [13], complex illumination variation, iconic changes [2], and three-dimensional scene flow [20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1241772,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f572bfbf7835203bd39af9cfab0bb9e376ff693f",
            "isKey": false,
            "numCitedBy": 516,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Just as optical flow is the two-dimensional motion of points in an image, scene flow is the three-dimensional motion of points in the world. The fundamental difficulty with optical flow is that only the normal flow can be computed directly from the image measurements, without some form of smoothing or regularization. In this paper, we begin by showing that the same fundamental limitation applies to scene flow; however, many cameras are used to image the scene. There are then two choices when computing scene flow: 1) perform the regularization in the images or 2) perform the regularization on the surface of the object in the scene. In this paper, we choose to compute scene flow using regularization in the images. We describe three algorithms, the first two for computing scene flow from optical flows and the third for constraining scene structure from the inconsistencies in multiple optical flows."
            },
            "slug": "Three-dimensional-scene-flow-Vedula-Baker",
            "title": {
                "fragments": [],
                "text": "Three-dimensional scene flow"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "Three algorithms are described, the first two for computing scene flow from optical flows and the third for constraining scene structure from the inconsistencies in multiple optical flows."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726733"
                        ],
                        "name": "Ron Kohavi",
                        "slug": "Ron-Kohavi",
                        "structuredName": {
                            "firstName": "Ron",
                            "lastName": "Kohavi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ron Kohavi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "We used the 5-fold crossvalidation framework [14] to partition the dataset into K subsamples."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2702042,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8c70a0a39a686bf80b76cb1b77f9eef156f6432d",
            "isKey": false,
            "numCitedBy": 11152,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "We review accuracy estimation methods and compare the two most common methods crossvalidation and bootstrap. Recent experimental results on artificial data and theoretical re cults in restricted settings have shown that for selecting a good classifier from a set of classifiers (model selection), ten-fold cross-validation may be better than the more expensive leaveone-out cross-validation. We report on a largescale experiment--over half a million runs of C4.5 and a Naive-Bayes algorithm--to estimate the effects of different parameters on these algrithms on real-world datasets. For crossvalidation we vary the number of folds and whether the folds are stratified or not, for bootstrap, we vary the number of bootstrap samples. Our results indicate that for real-word datasets similar to ours, The best method to use for model selection is ten fold stratified cross validation even if computation power allows using more folds."
            },
            "slug": "A-Study-of-Cross-Validation-and-Bootstrap-for-and-Kohavi",
            "title": {
                "fragments": [],
                "text": "A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The results indicate that for real-word datasets similar to the authors', the best method to use for model selection is ten fold stratified cross validation even if computation power allows using more folds."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 61
                            }
                        ],
                        "text": "We have collected a set of actions from various sports featured on broadcast television channels such as the BBC and ESPN."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 42
                            }
                        ],
                        "text": "The dataset contains over 200 video sequences at a resolution of 720\u00d7480."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Actions as Space-Time Shapes. ICCV"
            },
            "venue": {
                "fragments": [],
                "text": "Actions as Space-Time Shapes. ICCV"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In this section we extend our approach to include vector data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Three-dimensional scene flow. PAMI"
            },
            "venue": {
                "fragments": [],
                "text": "Three-dimensional scene flow. PAMI"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Alatas and Shah . Spatio - Temporal Regularity Flow ( SPREF ) : Its Estimation and Applications"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 233,
                                "start": 229
                            }
                        ],
                        "text": "Finally, we perform an extensive set of experiments and compare our method with some of the most recent approaches in the field by using publicly available datasets, and two new annotated human action datasets which include actions performed in classic feature films and sports broadcast television."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Low level recognition of human motion ( or how to get your manwithout finding his body parts ) . Motion of Non - Rigid and Articulated Objects ,"
            },
            "venue": {
                "fragments": [],
                "text": "Circuits and Systems for Video Technology , IEEE Transactions on"
            },
            "year": 2007
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 8,
            "methodology": 8,
            "result": 3
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 24,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Action-MACH-a-spatio-temporal-Maximum-Average-for-Rodriguez-Ahmed/1c2629d53fd73ee42fb9a67b4d656688ef6a005f?sort=total-citations"
}