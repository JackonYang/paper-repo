{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34789794"
                        ],
                        "name": "H. Ng",
                        "slug": "H.-Ng",
                        "structuredName": {
                            "firstName": "Hwee",
                            "lastName": "Ng",
                            "middleNames": [
                                "Tou"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710450"
                        ],
                        "name": "J. Zelle",
                        "slug": "J.-Zelle",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Zelle",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Zelle"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14767534,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "854607e1032c46a2a80b438ff2f3dc5024ede530",
            "isKey": false,
            "numCitedBy": 57,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "into empirical, corpus-based learning approaches to natural language processing (NLP). Most empirical NLP work to date has focused on relatively low-level language processing such as part-ofspeech tagging, text segmentation, and syntactic parsing. The success of these approaches has stimulated research in using empirical learning techniques in other facets of NLP, including semantic analysis\u2014uncovering the meaning of an utterance. This article is an introduction to some of the emerging research in the application of corpusbased learning techniques to problems in semantic interpretation. In particular, we focus on two important problems in semantic interpretation, namely, word-sense disambiguation and semantic parsing."
            },
            "slug": "Corpus-Based-Approaches-to-Semantic-Interpretation-Ng-Zelle",
            "title": {
                "fragments": [],
                "text": "Corpus-Based Approaches to Semantic Interpretation in Natural Language Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An introduction to some of the emerging research in the application of corpusbased learning techniques to problems in semantic interpretation, namely, word-sense disambiguation and semantic parsing."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749837"
                        ],
                        "name": "Eugene Charniak",
                        "slug": "Eugene-Charniak",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Charniak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eugene Charniak"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5371566,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b84276fe751ca4f1389549281383b151a746107b",
            "isKey": false,
            "numCitedBy": 1140,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nEugene Charniak breaks new ground in artificial intelligence research by presenting statistical language processing from an artificial intelligence point of view in a text for researchers and scientists with a traditional computer science background. \nNew, exacting empirical methods are needed to break the deadlock in such areas of artificial intelligence as robotics, knowledge representation, machine learning, machine translation, and natural language processing (NLP). It is time, Charniak observes, to switch paradigms. This text introduces statistical language processing techniques -- word tagging, parsing with probabilistic context free grammars, grammar induction, syntactic disambiguation, semantic word classes, word-sense disambiguation -- along with the underlying mathematics and chapter exercises. \nCharniak points out that as a method of attacking NLP problems, the statistical approach has several advantages. It is grounded in real text and therefore promises to produce usable results, and it offers an obvious way to approach learning: \"one simply gathers statistics.\" \nLanguage, Speech, and Communication"
            },
            "slug": "Statistical-language-learning-Charniak",
            "title": {
                "fragments": [],
                "text": "Statistical language learning"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Eugene Charniak points out that as a method of attacking NLP problems, the statistical approach has several advantages and is grounded in real text and therefore promises to produce usable results, and it offers an obvious way to approach learning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30663130"
                        ],
                        "name": "E. Dura",
                        "slug": "E.-Dura",
                        "structuredName": {
                            "firstName": "Elzbieta",
                            "lastName": "Dura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Dura"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5598004,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b1251807ffb08f881a6cb67c2ec6eb322206bfaf",
            "isKey": false,
            "numCitedBy": 107,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "It seems the time is ripe for the two to meet: NLP has grown out of prototypes and IR is having hard time trying to improve precision. Two examples of possible approaches are considered below. Lexware is a lexicon-based system for text analysis of Swedish applied in an information retrieval task. NLIR is an information retrieval system using intensive natural language processing to provide index terms on a higher level of abstraction than stems."
            },
            "slug": "Natural-Language-in-Information-Retrieval-Dura",
            "title": {
                "fragments": [],
                "text": "Natural Language in Information Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "The time is ripe for the two to meet: NLP has grown out of prototypes and IR is having hard time trying to improve precision, so two examples of possible approaches are considered."
            },
            "venue": {
                "fragments": [],
                "text": "CICLing"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2580777"
                        ],
                        "name": "David M. Magerman",
                        "slug": "David-M.-Magerman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Magerman",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David M. Magerman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 626195,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9e78155b28b1f4db52a7c9076c89e81ac4b7d8ce",
            "isKey": false,
            "numCitedBy": 284,
            "numCiting": 100,
            "paperAbstract": {
                "fragments": [],
                "text": "Traditional natural language parsers are based on rewrite rule systems developed in an arduous, time-consuming manner by grammarians. A majority of the grammarian's efforts are devoted to the disambiguation process, first hypothesizing rules which dictate constituent categories and relationships among words in ambiguous sentences, and then seeking exceptions and corrections to these rules. \nIn this work, I propose an automatic method for acquiring a statistical parser from a set of parsed sentences which takes advantage of some initial linguistic input, but avoids the pitfalls of the iterative and seemingly endless grammar development process. Based on distributionally-derived and linguistically-based features of language, this parser acquires a set of statistical decision trees which assign a probability distribution on the space of parse trees given the input sentence. These decision trees take advantage of significant amount of contextual information, potentially including all of the lexical information in the sentence, to produce highly accurate statistical models of the disambiguation process. By basing the disambiguation criteria selection on entropy reduction rather than human intuition, this parser development method is able to consider more sentences than a human grammarian can when making individual disambiguation rules. \nIn experiments between a parser, acquired using this statistical framework, and a grammarian's rule-based parser, developed over a ten-year period, both using the same training material and test sentences, the decision tree parser significantly outperformed the grammar-based parser on the accuracy measure which the grammarian was trying to maximize, achieving an accuracy of 78% compared to the grammar-based parser's 69%."
            },
            "slug": "Natural-Language-Parsing-as-Statistical-Pattern-Magerman",
            "title": {
                "fragments": [],
                "text": "Natural Language Parsing as Statistical Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work proposes an automatic method for acquiring a statistical parser from a set of parsed sentences which takes advantage of some initial linguistic input, but avoids the pitfalls of the iterative and seemingly endless grammar development process."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680223"
                        ],
                        "name": "A. Smeaton",
                        "slug": "A.-Smeaton",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Smeaton",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Smeaton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2893534,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3e706cc15637a01a622f6f0bbaf7d200f3bdba3e",
            "isKey": false,
            "numCitedBy": 152,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Techniques of automatic natural language processing have been under development since the earliest computing machines, and in recent years these techniques have proven to be robust, reliable and efficient enough to lead to commercial products in many areas. The applications include machine translation, natural language interfaces and the stylistic analysis of texts but NLP techniques have also been applied to other computing tasks besides these. In this paper we will examine and review recent progress in using the lexical, syntactic, semantic and discourse levels of the language analysis for tasks like automatic and semi-automatic indexing of text, text retrieval, text abstracting and summarisation, thesaurus generation from text corpus and conceptual information retrieval. Our own work on the application of syntactic analysis to the matching and ranking of phrases using structured representations of texts, will be included in the overview. Finally, the prospects for gains in terms of overall retrieval effectiveness or quality will be discussed."
            },
            "slug": "Progress-in-the-Application-of-Natural-Language-to-Smeaton",
            "title": {
                "fragments": [],
                "text": "Progress in the Application of Natural Language Processing to Information Retrieval Tasks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper will examine and review recent progress in using the lexical, syntactic, semantic and discourse levels of the language analysis for tasks like automatic and semi-automatic indexing of text, text retrieval, text abstracting and summarisation, thesaurus generation from text corpus and conceptual information retrieval."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. J."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802969"
                        ],
                        "name": "Ciprian Chelba",
                        "slug": "Ciprian-Chelba",
                        "structuredName": {
                            "firstName": "Ciprian",
                            "lastName": "Chelba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ciprian Chelba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2877845,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "673992da19d9209434615b12d55bdd36be706e9e",
            "isKey": false,
            "numCitedBy": 210,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper presents a language model that develops syntactic structure and uses it to extract meaningful information from the word history, thus enabling the use of long distance dependencies. The model assigns probability to every joint sequence of words-binary-parse-structure with headword annotation and operates in a left-to-right manner --- therefore usable for automatic speech recognition. The model, its probabilistic parameterization, and a set of experiments meant to evaluate its predictive power are presented; an improvement over standard trigram modeling is achieved."
            },
            "slug": "Exploiting-Syntactic-Structure-for-Language-Chelba-Jelinek",
            "title": {
                "fragments": [],
                "text": "Exploiting Syntactic Structure for Language Modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "A language model that develops syntactic structure and uses it to extract meaningful information from the word history, thus enabling the use of long distance dependencies and usable for automatic speech recognition is presented."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802969"
                        ],
                        "name": "Ciprian Chelba",
                        "slug": "Ciprian-Chelba",
                        "structuredName": {
                            "firstName": "Ciprian",
                            "lastName": "Chelba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ciprian Chelba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 219305524,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1654fe181a016298e6fc1f9f3ca10a67837b97a3",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Exploiting-Syntactic-Structure-for-Language-Chelba-Jelinek",
            "title": {
                "fragments": [],
                "text": "Exploiting Syntactic Structure for Language Modeling"
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            }
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110271578"
                        ],
                        "name": "Scott Miller",
                        "slug": "Scott-Miller",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Miller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Scott Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145482266"
                        ],
                        "name": "D. Stallard",
                        "slug": "D.-Stallard",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Stallard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Stallard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2189985"
                        ],
                        "name": "R. Bobrow",
                        "slug": "R.-Bobrow",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Bobrow",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bobrow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35442155"
                        ],
                        "name": "R. Schwartz",
                        "slug": "R.-Schwartz",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Schwartz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schwartz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10983275,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e9c1f510bcf5933d3cf8ec8108a04a9ba601a843",
            "isKey": false,
            "numCitedBy": 158,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a natural language interface system which is based entirely on trained statistical models. The system consists of three stages of processing: parsing, semantic interpretation, and discourse. Each of these stages is modeled as a statistical process. The models are fully integrated, resulting in an end-to-end system that maps input utterances into meaning representation frames."
            },
            "slug": "A-Fully-Statistical-Approach-to-Natural-Language-Miller-Stallard",
            "title": {
                "fragments": [],
                "text": "A Fully Statistical Approach to Natural Language Interfaces"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "This work presents a natural language interface system which is based entirely on trained statistical models, resulting in an end-to-end system that maps input utterances into meaning representation frames."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680292"
                        ],
                        "name": "P. Resnik",
                        "slug": "P.-Resnik",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Resnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Resnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2488776,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "01d88ddd3e7a9c5af2acc91a05734b7f066908dc",
            "isKey": false,
            "numCitedBy": 142,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, I argue for the use of a probabilistic form of tree-adjoining grammar (TAG) in statistical natural language processing. I first discuss two previous statistical approaches --- one that concentrates on the probabilities of structural operations, and another that emphasizes co-occurrence relationships between words. I argue that a purely structural apprach, exemplified by probabilistic context-free grammar, lacks sufficient sensitivity to lexical context, and, conversely, that lexical co-occurence analyses require a richer notion of locality that is best provided by importing some notion of structure.I then propose probabilistic TAG as a framework for statistical language modelling, arguing that it provides an advantageous combination of structure, locality, and lexical sensitivity. Issues in the acquisition of probabilistic TAG and parameter estimation are briefly considered."
            },
            "slug": "Probabilistic-Tree-Adjoining-Grammar-as-a-Framework-Resnik",
            "title": {
                "fragments": [],
                "text": "Probabilistic Tree-Adjoining Grammar as a Framework for Statistical Natural Language Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper argues that a purely structural apprach, exemplified by probabilistic context-free grammar, lacks sufficient sensitivity to lexical context, and that lexical co-occurence analyses require a richer notion of locality that is best provided by importing some notion of structure."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794100"
                        ],
                        "name": "Brian Roark",
                        "slug": "Brian-Roark",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Roark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brian Roark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749837"
                        ],
                        "name": "Eugene Charniak",
                        "slug": "Eugene-Charniak",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Charniak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eugene Charniak"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 219307649,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2e0415088488704d05f2cfacdff3b480129e7f0c",
            "isKey": false,
            "numCitedBy": 98,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "Generating semantic lexicons semi-automatically could be a great time saver, relative to creating them by hand. In this paper, we present an algorithm for extracting potential entries for a category from an on-line corpus, based upon a small set of exemplars. Our algorithm finds more correct terms and fewer incorrect ones than previous work in this area. Additionally, the entries that are generated potentially provide broader coverage of the category than would occur to an individual coding them by hand. Our algorithm finds many terms not included within Wordnet (many more than previous algorithms), and could be viewed as an ``enhancer'' of existing broad-coverage resources."
            },
            "slug": "Noun-phrase-co-occurrence-statistics-for-semantic-Roark-Charniak",
            "title": {
                "fragments": [],
                "text": "Noun-phrase co-occurrence statistics for semi-automatic semantic lexicon construction"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper presents an algorithm for extracting potential entries for a category from an on-line corpus, based upon a small set of exemplars, that could be viewed as an ``enhancer'' of existing broad-coverage resources."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782522"
                        ],
                        "name": "P. Velardi",
                        "slug": "P.-Velardi",
                        "structuredName": {
                            "firstName": "Paola",
                            "lastName": "Velardi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Velardi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802339"
                        ],
                        "name": "M. Pazienza",
                        "slug": "M.-Pazienza",
                        "structuredName": {
                            "firstName": "Maria",
                            "lastName": "Pazienza",
                            "middleNames": [
                                "Teresa"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pazienza"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5626010,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "c6a24aa2b33a2875e4410eb17b44e280cddf4402",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the problem of developing a large semantic lexicon for natural language processing. The increasing availability of machine readable documents offers an opportunity to the field of lexical semantics, by providing experimental evidence of word uses (on-line texts) and word definitions (on-line dictionaries).The system presented hereafter, PETRARCA, detects word cooccurrences from a large sample of press agency releases on finance and economics, and uses these associations to build a case-based semantic lexicon. Syntactically valid cooccurences including a new word W are detected by a high-coverage morphosyntactic analyzer. Syntactic relations are interpreted e.g. replaced by case relations, using a a catalogue of patterns/interpretation pairs, a concept type hierarchy, and a set of selectional restriction rules on semantic interpretation types."
            },
            "slug": "Computer-Aided-Interpretation-of-Lexical-Velardi-Pazienza",
            "title": {
                "fragments": [],
                "text": "Computer Aided Interpretation of Lexical Coocurrences"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The system presented hereafter, PETRARCA, detects word cooccurrences from a large sample of press agency releases on finance and economics, and uses these associations to build a case-based semantic lexicon."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2580777"
                        ],
                        "name": "David M. Magerman",
                        "slug": "David-M.-Magerman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Magerman",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David M. Magerman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 608,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f0a14be7e7f5614b91d0f648ae5f2baafc6d7036",
            "isKey": false,
            "numCitedBy": 717,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Syntactic natural language parsers have shown themselves to be inadequate for processing highly-ambiguous large-vocabulary text, as is evidenced by their poor performance on domains like the Wall Street Journal, and by the movement away from parsing-based approaches to text-processing in general. In this paper, I describe SPATTER, a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result. This work is based on the following premises: (1) grammars are too complex and detailed to develop manually for most interesting domains; (2) parsing models must rely heavily on lexical and contextual information to analyze sentences accurately; and (3) existing n-gram modeling techniques are inadequate for parsing models. In experiments comparing SPATTER with IBM's computer manuals parser, SPATTER significantly outperforms the grammar-based parser. Evaluating SPATTER against the Penn Treebank Wall Street Journal corpus using the PARSEVAL measures, SPATTER achieves 86% precision, 86% recall, and 1.3 crossing brackets per sentence for sentences of 40 words or less, and 91% precision, 90% recall, and 0.5 crossing brackets for sentences between 10 and 20 words in length."
            },
            "slug": "Statistical-Decision-Tree-Models-for-Parsing-Magerman",
            "title": {
                "fragments": [],
                "text": "Statistical Decision-Tree Models for Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "SPATTER is described, a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1798723"
                        ],
                        "name": "Mandar Mitra",
                        "slug": "Mandar-Mitra",
                        "structuredName": {
                            "firstName": "Mandar",
                            "lastName": "Mitra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mandar Mitra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144009691"
                        ],
                        "name": "C. Buckley",
                        "slug": "C.-Buckley",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Buckley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Buckley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145163573"
                        ],
                        "name": "A. Singhal",
                        "slug": "A.-Singhal",
                        "structuredName": {
                            "firstName": "Amit",
                            "lastName": "Singhal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Singhal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748501"
                        ],
                        "name": "Claire Cardie",
                        "slug": "Claire-Cardie",
                        "structuredName": {
                            "firstName": "Claire",
                            "lastName": "Cardie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Claire Cardie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5156714,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3002262c3c696d5caf2ae37ee3d978ee66647f22",
            "isKey": false,
            "numCitedBy": 277,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "As the amount of textual information available through the World Wide Web grows, there is a growing need for high-precision IR systems that enable a user to find useful information from the masses of available textual data. Phrases have traditionally been regarded as precision-enhancing devices and have proved useful as content-identifiers in representing documents. In this study, we compare the usefulness of phrases recognized using linguistic methods and those recognized by statistical techniques. We focus in particular on high-precision retrieval. We discover that once a good basic ranking scheme is being used, the use of phrases does not have a major effect on precision at high ranks. Phrases are more useful at lower ranks where the connection between documents and relevance is more tenuous. Also, we find that the syntactic and statistical methods for recognizing phrases yield comparable performance."
            },
            "slug": "An-Analysis-of-Statistical-and-Syntactic-Phrases-Mitra-Buckley",
            "title": {
                "fragments": [],
                "text": "An Analysis of Statistical and Syntactic Phrases"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is discovered that once a good basic ranking scheme is being used, the use of phrases does not have a major effect on precision at high ranks, and phrases are more useful at lower ranks where the connection between documents and relevance is more tenuous."
            },
            "venue": {
                "fragments": [],
                "text": "RIAO"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145903504"
                        ],
                        "name": "R. Rosenfeld",
                        "slug": "R.-Rosenfeld",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rosenfeld"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1735632,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0b26fa1b848ed808a0511db34bce2426888f0b68",
            "isKey": false,
            "numCitedBy": 417,
            "numCiting": 120,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : Language modeling is the attempt to characterize, capture and exploit regularities in natural language. In statistical language modeling, large amounts of text are used to automatically determine the model's parameters. Language modeling is useful in automatic speech recognition, machine translation, and any other application that processes natural language with incomplete knowledge. In this thesis, I view language as an information source which emits a stream of symbols from a finite alphabet (the vocabulary). The goal of language modeling is then to identify and exploit sources of information in the language stream, so as to minimize its perceived entropy. Most existing statistical language models exploit the immediate past only. To extract information from further back in the document's history, I use trigger pairs as the basic information bearing elements. This allows the model to adapt its expectations to the topic of discourse. Next, statistical evidence from many sources must be combined. Traditionally, linear interpolation and its variants have been used, but these are shown here to be seriously deficient. Instead, I apply the principle of Maximum Entropy (ME). Each information source gives rise to a set of constraints, to be imposed on the combined estimate. The intersection of these constraints is the set of probability functions which are consistent with all the information sources. The function with the highest entropy within that set is the NE solution. Language modeling, Adaptive language modeling, Statistical language modeling, Maximum entropy, Speech recognition."
            },
            "slug": "Adaptive-Statistical-Language-Modeling;-A-Maximum-Rosenfeld",
            "title": {
                "fragments": [],
                "text": "Adaptive Statistical Language Modeling; A Maximum Entropy Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This thesis views language as an information source which emits a stream of symbols from a finite alphabet (the vocabulary), and applies the principle of Maximum Entropy to identify and exploit sources of information in the language stream, so as to minimize its perceived entropy."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707726"
                        ],
                        "name": "J. Pustejovsky",
                        "slug": "J.-Pustejovsky",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Pustejovsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pustejovsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2209533"
                        ],
                        "name": "S. Bergler",
                        "slug": "S.-Bergler",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Bergler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Bergler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723684"
                        ],
                        "name": "Peter G. Anick",
                        "slug": "Peter-G.-Anick",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Anick",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter G. Anick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9416249,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "8bd683d5aef2704207fca32a175f0c79f871e180",
            "isKey": false,
            "numCitedBy": 182,
            "numCiting": 144,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we outline a research program for computational linguistics, making extensive use of text corpora. We demonstrate how a semantic framework for lexical knowledge can suggest richer relationships among words in text beyond that of simple co-occurrence. The work suggests how linguistic phenomena such as metonymy and polysemy might be exploitable for semantic tagging of lexical items. Unlike with purely statistical collocational analyses, the framework of a semantic theory allows the automatic construction of predictions about deeper semantic relationships among words appearing in collocational systems. We illustrate the approach for the acquisition of lexical information for several classes of nominals, and how such techniques can fine-tune the lexical structures acquired from an initial seeding of a machine-readable dictionary. In addition to conventional lexical semantic relations, we show how information concerning lexical presuppositions and preference relations can also be acquired from corpora, when analyzed with the appropriate semantic tools. Finally, we discuss the potential that corpus studies have for enriching the data set for theoretical linguistic research, as well as helping to confirm or disconfirm linguistic hypotheses."
            },
            "slug": "Lexical-Semantic-Techniques-for-Corpus-Analysis-Pustejovsky-Bergler",
            "title": {
                "fragments": [],
                "text": "Lexical Semantic Techniques for Corpus Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "This paper demonstrates how a semantic framework for lexical knowledge can suggest richer relationships among words in text beyond that of simple co-occurrence, and outlines the approach for the acquisition of lexical information for several classes of nominals."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30844359"
                        ],
                        "name": "Ye-Yi Wang",
                        "slug": "Ye-Yi-Wang",
                        "structuredName": {
                            "firstName": "Ye-Yi",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ye-Yi Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11122272,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c447c0cb2673037633f71faf8ccf4f89806ba1b0",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Most statistical machine translation systems employ a word-based alignment model. In this paper we demonstrate that word-based alignment is a major cause of translation errors. We propose a new alignment model based on shallow phrase structures, and the structures can be automatically acquired from parallel corpus. This new model achieved over 10% error reduction for our spoken language translation task."
            },
            "slug": "Modeling-with-Structures-in-Statistical-Machine-Wang-Waibel",
            "title": {
                "fragments": [],
                "text": "Modeling with Structures in Statistical Machine translation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new alignment model based on shallow phrase structures is proposed, and the structures can be automatically acquired from parallel corpus, and this new model achieved over 10% error reduction for the authors' spoken language translation task."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33611948"
                        ],
                        "name": "D. Palmer",
                        "slug": "D.-Palmer",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Palmer",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Palmer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716902"
                        ],
                        "name": "Marti A. Hearst",
                        "slug": "Marti-A.-Hearst",
                        "structuredName": {
                            "firstName": "Marti",
                            "lastName": "Hearst",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marti A. Hearst"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9701154,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3b14be12cecba09db74a901bf25a33a67ac784a6",
            "isKey": false,
            "numCitedBy": 173,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "The sentence is a standard textual unit in natual language processing applications. In many language the punctuation mark that indicates the end-of-sentence boundary is ambiguous; thus the tokenizers of most NLP systems must be equipped with special sentence boundary recognition rules for every new text collection.As an alternative, this article presents an efficient, trainable system for sentence boundary disambiguation. The system, called Satz, makes simple estimates of the parts of speech of the tokens immediately preceding and following each punctuation mark, and uses these estimates as input to a machine learning algorithm that then classifies the punctuation mark. Satz is very fast both in training and sentence analysis, and its combined robustness and accuracy surpass existing techniques. The system needs only a small lexicon and training corpus, and has been shown to transfer quickly and easily from English to other languages, as demonstrated on Franch and German."
            },
            "slug": "Adaptive-Multilingual-Sentence-Boundary-Palmer-Hearst",
            "title": {
                "fragments": [],
                "text": "Adaptive Multilingual Sentence Boundary Disambiguation"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This article presents an efficient, trainable system for sentence boundary disambiguation, called Satz, which makes simple estimates of the parts of speech of the tokens immediately preceding and following each punctuation mark, and uses these estimates as input to a machine learning algorithm that then classifies the punctuated mark."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693517"
                        ],
                        "name": "David Yarowsky",
                        "slug": "David-Yarowsky",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Yarowsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Yarowsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1693468,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d922631a6bf8361d7602e12cafb9e15d421c827",
            "isKey": false,
            "numCitedBy": 836,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a program that disambiguates English word senses in unrestricted text using statistical models of the major Roget's Thesaurus categories. Roget's categories serve as approximations of conceptual classes. The categories listed for a word in Roget's index tend to correspond to sense distinctions; thus selecting the most likely category provides a useful level of sense disambiguation. The selection of categories is accomplished by identifying and weighting words that are indicative of each category when seen in context, using a Bayesian theoretical framework.Other statistical approaches have required special corpora or hand-labeled training examples for much of the lexicon. Our use of class models overcomes this knowledge acquisition bottleneck, enabling training on unrestricted monolingual text without human intervention. Applied to the 10 million word Grolier's Encyclopedia, the system correctly disambiguated 92% of the instances of 12 polysemous words that have been previously studied in the literature."
            },
            "slug": "Word-Sense-Disambiguation-Using-Statistical-Models-Yarowsky",
            "title": {
                "fragments": [],
                "text": "Word-Sense Disambiguation Using Statistical Models of Roget\u2019s Categories Trained on Large Corpora"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "A program that disambiguates English word senses in unrestricted text using statistical models of the major Roget's Thesaurus categories, enabling training on unrestricted monolingual text without human intervention."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732071"
                        ],
                        "name": "R. Weischedel",
                        "slug": "R.-Weischedel",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Weischedel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Weischedel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3227843"
                        ],
                        "name": "M. Meteer",
                        "slug": "M.-Meteer",
                        "structuredName": {
                            "firstName": "Marie",
                            "lastName": "Meteer",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Meteer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35442155"
                        ],
                        "name": "R. Schwartz",
                        "slug": "R.-Schwartz",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Schwartz",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744313"
                        ],
                        "name": "L. Ramshaw",
                        "slug": "L.-Ramshaw",
                        "structuredName": {
                            "firstName": "Lance",
                            "lastName": "Ramshaw",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Ramshaw"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2452408"
                        ],
                        "name": "Jeff Palmucci",
                        "slug": "Jeff-Palmucci",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Palmucci",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Palmucci"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6838726,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6a94da952fb8ffc77881028081e90efb494f1c5d",
            "isKey": false,
            "numCitedBy": 329,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "From spring 1990 through fall 1991, we performed a battery of small experiments to test the effectiveness of supplementing knowledge-based techniques with probabilistic models. This paper reports our experiments in predicting parts of speech of highly ambiguous words, predicting the intended interpretation of an utterance when more than one interpretation satisfies all known syntactic and semantic constraints, and learning caseframe informationfor verbsfrom example uses.From these experiments, we are convinced that probabilistic models based on annotated corpora can effectively reduce the ambiguity in processing text and can be used to acquire lexical informationfrom a corpus, by supplementing knowledge-based techniques.Based on the results of those experiments, we have constructed a new natural language system (PLUM) for extracting data from text, e.g., newswire text."
            },
            "slug": "Coping-with-Ambiguity-and-Unknown-Words-through-Weischedel-Meteer",
            "title": {
                "fragments": [],
                "text": "Coping with Ambiguity and Unknown Words through Probabilistic Models"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "A new natural language system (PLUM) is constructed for extracting data from text, e.g., newswire text, based on results of experiments in predicting parts of speech of highly ambiguous words, predicting the intended interpretation of an utterance when more than one interpretation satisfies all known syntactic and semantic constraints."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734174"
                        ],
                        "name": "M. Marcus",
                        "slug": "M.-Marcus",
                        "structuredName": {
                            "firstName": "Mitchell",
                            "lastName": "Marcus",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marcus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2424234"
                        ],
                        "name": "Beatrice Santorini",
                        "slug": "Beatrice-Santorini",
                        "structuredName": {
                            "firstName": "Beatrice",
                            "lastName": "Santorini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Beatrice Santorini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2063206"
                        ],
                        "name": "Mary Ann Marcinkiewicz",
                        "slug": "Mary-Ann-Marcinkiewicz",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "Marcinkiewicz",
                            "middleNames": [
                                "Ann"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mary Ann Marcinkiewicz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 252796,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0b44fcbeea9415d400c5f5789d6b892b6f98daff",
            "isKey": false,
            "numCitedBy": 8175,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : As a result of this grant, the researchers have now published oil CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, with over 3 million words of that material assigned skeletal grammatical structure. This material now includes a fully hand-parsed version of the classic Brown corpus. About one half of the papers at the ACL Workshop on Using Large Text Corpora this past summer were based on the materials generated by this grant."
            },
            "slug": "Building-a-Large-Annotated-Corpus-of-English:-The-Marcus-Santorini",
            "title": {
                "fragments": [],
                "text": "Building a Large Annotated Corpus of English: The Penn Treebank"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "As a result of this grant, the researchers have now published on CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, which includes a fully hand-parsed version of the classic Brown corpus."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34165212"
                        ],
                        "name": "Jan O. Pedersen",
                        "slug": "Jan-O.-Pedersen",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Pedersen",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jan O. Pedersen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14842061,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2ff0a2bf7bfc077ddea9f91552d8b59e1d386116",
            "isKey": false,
            "numCitedBy": 146,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes an algorithm for word sense disambiguation based on a vector representation of word similarity derived from lexical co-occurrence. It diiers from standard approaches by allowing for as ne grained distinctions as is warranted by the information at hand, rather than supposing a xed number of senses per word, and by allowing for more than one sense to be assigned to a given word occurrence. The algorithm is applied to the standard vector-space information retrieval model and an evaluation is performed over the Category B TREC-1 corpus (WSJ subcollection). Results show that this sense disambiguation algorithm improves performance by between 7% and 14% on average ."
            },
            "slug": "Information-Retrieval-Based-on-Word-Senses-Pedersen",
            "title": {
                "fragments": [],
                "text": "Information Retrieval Based on Word Senses"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "An algorithm for word sense disambiguation based on a vector representation of word similarity derived from lexical co-occurrence is proposed and an evaluation is performed over the Category B TREC-1 corpus (WSJ subcollection)."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793475"
                        ],
                        "name": "A. Ratnaparkhi",
                        "slug": "A.-Ratnaparkhi",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ratnaparkhi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ratnaparkhi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15366907,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c43d3fbca6086a5c096ea69675928d8dbcff5a4b",
            "isKey": false,
            "numCitedBy": 241,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Many problems in natural language processing can be viewed as lin guistic classi cation problems in which linguistic contexts are used to pre dict linguistic classes Maximum entropy models o er a clean way to com bine diverse pieces of contextual evidence in order to estimate the proba bility of a certain linguistic class occurring with a certain linguistic con text This report demonstrates the use of a particular maximum entropy model on an example problem and then proves some relevant mathemat ical facts about the model in a simple and accessible manner This report also describes an existing procedure called Generalized Iterative Scaling which estimates the parameters of this particular model The goal of this report is to provide enough detail to re implement the maximum entropy models described in Ratnaparkhi Reynar and Ratnaparkhi Ratnaparkhi and also to provide a simple explanation of the max imum entropy formalism Introduction Many problems in natural language processing NLP can be re formulated as statistical classi cation problems in which the task is to estimate the probability of class a occurring with context b or p a b Contexts in NLP tasks usually include words and the exact context depends on the nature of the task for some tasks the context b may consist of just a single word while for others b may consist of several words and their associated syntactic labels Large text corpora usually contain some information about the cooccurrence of a s and b s but never enough to completely specify p a b for all possible a b pairs since the words in b are typically sparse The problem is then to nd a method for using the sparse evidence about the a s and b s to reliably estimate a probability model p a b Consider the Principle of Maximum Entropy Jaynes Good which states that the correct distribution p a b is that which maximizes en tropy or uncertainty subject to the constraints which represent evidence i e the facts known to the experimenter Jaynes discusses its advan tages in making inferences on the basis of partial information we must use that probability distribution which has maximum entropy sub ject to whatever is known This is the only unbiased assignment we can make to use any other would amount to arbitrary assumption of information which by hypothesis we do not have More explicitly if A denotes the set of possible classes and B denotes the set of possible contexts p should maximize the entropy H p X"
            },
            "slug": "A-Simple-Introduction-to-Maximum-Entropy-Models-for-Ratnaparkhi",
            "title": {
                "fragments": [],
                "text": "A Simple Introduction to Maximum Entropy Models for Natural Language Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "The goal of this report is to provide enough detail to re implement the maximum entropy models described in Reynar and Ratnaparkhi and also to provide a simple explanation of the max imum entropy formalism."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2001885"
                        ],
                        "name": "Ted Pedersen",
                        "slug": "Ted-Pedersen",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Pedersen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ted Pedersen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2614094"
                        ],
                        "name": "Rebecca F. Bruce",
                        "slug": "Rebecca-F.-Bruce",
                        "structuredName": {
                            "firstName": "Rebecca",
                            "lastName": "Bruce",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rebecca F. Bruce"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 58116,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e50547416475df1716769f569e0c6f6f99293f77",
            "isKey": false,
            "numCitedBy": 183,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes an experimental comparison of three unsupervised learning algorithms that distinguish the sense of an ambiguous word in untagged text. The methods described in this paper, McQuitty's similarity analysis, Ward's minimum-variance method, and the EM algorithm, assign each instance of an ambiguous word to a known sense definition based solely on the values of automatically identifiable features in text. These methods and feature sets are found to be more successful in disambiguating nouns rather than adjectives or verbs. Overall, the most accurate of these procedures is McQuitty's similarity analysis in combination with a high dimensional feature set. 1 I n t r o d u c t i o n Statistical methods for natural language processing are often dependent on the availability of costly knowledge sources such as manually annotated text or semantic networks. This limits the applicability of such approaches to domains where this hard to acquire knowledge is already available. This paper presents three unsupervised learning algorithms that are able to distinguish among the known senses (i.e., as defined in some dictionary) of a word, based only on features that can be automatically extracted from untagged text. The object of unsupervised learning is to determine the class membership of each observation (i.e. each object to be classified), in a sample without using training examples of correct classifications. We discuss three algorithms, McQuitty's similarity analysis (McQuitty, 1966), Ward's minimum-variance method (Ward, 1963) and the EM algorithm (Dempster, Laird, and Rubin, 1977), that can be used to distinguish among the known senses of an ambiguous word without the aid of disambiguated examples. The EM algorithm produces maximum likelihood estimates of the parameters of a probabilistic model, where that model has been specified in advance. Both Ward's and McQuitty's methods are agglomerative clustering algorithms that form classes of unlabeled observations that minimize their respective distance measures between class members. The rest of this paper is organized as follows. First, we present introductions to Ward's and McQuitty 's methods (Section 2) and the EM algorithm (Section 3). We discuss the thirteen words (Section 4) and the three feature sets (Section 5) used in our experiments. We present our experimental results (Section 6) and close with a discussion of related work (Section 7). 2 Agglomerat ive Clustering In general, clustering methods rely on the assumption that classes occupy distinct regions in the feature space. The distance between two points in a multi-dimensional space can be measured using any of a wide variety of metrics (see, e.g. (Devijver and Kittler, 1982)). Observations are grouped in the manner that minimizes the distance between the members of each class. Ward's and McQuitty's method are agglomerative clustering algorithms that differ primarily in how they compute the distance between clusters. All such algorithms begin by placing each observation in a unique cluster, i.e. a cluster of one. The two closest clusters are merged to form a new cluster that replaces the two merged clusters. Merging of the two closest clusters continues until only some specified number of clusters remain. However, our data does not immediately lend itself to a distance-based interpretation. Our features represent part-of-speech (POS) tags, morphological characteristics, and word co-occurrence; such features are nominal and their values do not have scale. Given a POS feature, for example, we could choose noun = 1, verb = 2, adjective = 3, and adverb = 4. That adverb is represented by a larger number than noun is purely coincidental and implies nothing about the relationship between nouns and adverbs. Thus, before we employ either clustering algo-"
            },
            "slug": "Distinguishing-Word-Senses-in-Untagged-Text-Pedersen-Bruce",
            "title": {
                "fragments": [],
                "text": "Distinguishing Word Senses in Untagged Text"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "An experimental comparison of three unsupervised learning algorithms that distinguish the sense of an ambiguous word in untagged text using McQuitty's similarity analysis, Ward's minimum-variance method, and the EM algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2403128"
                        ],
                        "name": "C. Samuelsson",
                        "slug": "C.-Samuelsson",
                        "structuredName": {
                            "firstName": "Christer",
                            "lastName": "Samuelsson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Samuelsson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751188"
                        ],
                        "name": "Atro Voutilainen",
                        "slug": "Atro-Voutilainen",
                        "structuredName": {
                            "firstName": "Atro",
                            "lastName": "Voutilainen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Atro Voutilainen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 665,
                                "start": 8
                            }
                        ],
                        "text": "be viewed as a generalization of Naive Bayes. Instead of treating all features as independent, features are grouped into mutually dependent subsets. Independence is then assumed only between features in different subsets, not for all pairs of features as is the case in the Naive Bayes classifier. Bruce and Wiebe (1994) apply decomposable models to disambiguation with good results. Other disambiguation algorithms that rely on lexical resources are (Karov and Edelman (Guthrie et al. and et al. 1998). Karov and Edelman (1998) present a formalism that takes advantage of evidence both from a corpus and a dictionary, with good tion results. Guthrie et al. (1991) use the subject field codes in (Procter 1978) in a way similar to the thesaurus classes in (Yarowsky 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 321,
                                "start": 8
                            }
                        ],
                        "text": "be viewed as a generalization of Naive Bayes. Instead of treating all features as independent, features are grouped into mutually dependent subsets. Independence is then assumed only between features in different subsets, not for all pairs of features as is the case in the Naive Bayes classifier. Bruce and Wiebe (1994) apply decomposable models to disambiguation with good results."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 529,
                                "start": 8
                            }
                        ],
                        "text": "be viewed as a generalization of Naive Bayes. Instead of treating all features as independent, features are grouped into mutually dependent subsets. Independence is then assumed only between features in different subsets, not for all pairs of features as is the case in the Naive Bayes classifier. Bruce and Wiebe (1994) apply decomposable models to disambiguation with good results. Other disambiguation algorithms that rely on lexical resources are (Karov and Edelman (Guthrie et al. and et al. 1998). Karov and Edelman (1998) present a formalism that takes advantage of evidence both from a corpus and a dictionary, with good tion results."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 787,
                                "start": 8
                            }
                        ],
                        "text": "be viewed as a generalization of Naive Bayes. Instead of treating all features as independent, features are grouped into mutually dependent subsets. Independence is then assumed only between features in different subsets, not for all pairs of features as is the case in the Naive Bayes classifier. Bruce and Wiebe (1994) apply decomposable models to disambiguation with good results. Other disambiguation algorithms that rely on lexical resources are (Karov and Edelman (Guthrie et al. and et al. 1998). Karov and Edelman (1998) present a formalism that takes advantage of evidence both from a corpus and a dictionary, with good tion results. Guthrie et al. (1991) use the subject field codes in (Procter 1978) in a way similar to the thesaurus classes in (Yarowsky 1992). et al. (1998) apply transformation-based learning (see section 10."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3263107,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9dad7b5ba8309259994908baa28660ad41845dc4",
            "isKey": true,
            "numCitedBy": 128,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Concerning different approaches to automatic PoS tagging: EngCG-2, a constraint-based morphological tagger, is compared in a double-blind test with a state-of-the-art statistical tagger on a common disambiguation task using a common tag set. The experiments show that for the same amount of remaining ambiguity, the error rate of the statistical tagger is one order of magnitude greater than that of the rule-based one. The two related issues of priming effects compromising the results and disagreement between human annotators are also addressed."
            },
            "slug": "Comparing-a-Linguistic-and-a-Stochastic-Tagger-Samuelsson-Voutilainen",
            "title": {
                "fragments": [],
                "text": "Comparing a Linguistic and a Stochastic Tagger"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "EngCG-2, a constraint-based morphological tagger, is compared in a double-blind test with a state-of-the-art statistical tagger on a common disambiguation task using a common tag set and the issues of priming effects compromising the results and disagreement between human annotators are addressed."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691993"
                        ],
                        "name": "E. Riloff",
                        "slug": "E.-Riloff",
                        "structuredName": {
                            "firstName": "Ellen",
                            "lastName": "Riloff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Riloff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068932662"
                        ],
                        "name": "J. Shepherd",
                        "slug": "J.-Shepherd",
                        "structuredName": {
                            "firstName": "Jessica",
                            "lastName": "Shepherd",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shepherd"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1437,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "b3e9130ecab419f8267fccadf80c1ee2489be793",
            "isKey": false,
            "numCitedBy": 257,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Semantic knowledge can be a great asset to natural language processing systems, but it is usually hand-coded for each application. Although some semantic information is available in general-purpose knowledge bases such as WordNet and Cyc, many applications require domain-specific lexicons that represent words and categories for a particular topic. In this paper, we present a corpus-based method that can be used to build semantic lexicons for specific categories. The input to the system is a small set of seed words for a category and a representative text corpus. The output is a ranked list of words that are associated with the category. A user then reviews the top-ranked words and decides which ones should be entered in the semantic lexicon. In experiments with five categories, users typically found about 60 words per category in 10-15 minutes to build a core semantic lexicon."
            },
            "slug": "A-Corpus-Based-Approach-for-Building-Semantic-Riloff-Shepherd",
            "title": {
                "fragments": [],
                "text": "A Corpus-Based Approach for Building Semantic Lexicons"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper presents a corpus-based method that can be used to build semantic lexicons for specific categories using a small set of seed words for a category and a representative text corpus."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1801071"
                        ],
                        "name": "Frank Smadja",
                        "slug": "Frank-Smadja",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Smadja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Frank Smadja"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16151922,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a3ff7801bcf72fea30117c88d397403a570c5c68",
            "isKey": false,
            "numCitedBy": 999,
            "numCiting": 86,
            "paperAbstract": {
                "fragments": [],
                "text": "Natural languages are full of collocations, recurrent combinations of words that co-occur more often than expected by chance and that correspond to arbitrary word usages. Recent work in lexicography indicates that collocations are pervasive in English; apparently, they are common in all types of writing, including both technical and nontechnical genres. Several approaches have been proposed to retrieve various types of collocations from the analysis of large samples of textual data. These techniques automatically produce large numbers of collocations along with statistical figures intended to reflect the relevance of the associations. However, none of these techniques provides functional information along with the collocation. Also, the results produced often contained improper word associations reflecting some spurious aspect of the training corpus that did not stand for true collocations.In this paper, we describe a set of techniques based on statistical methods for retrieving and identifying collocations from large textual corpora. These techniques produce a wide range of collocations and are based on some original filtering methods that allow the production of richer and higher-precision output. These techniques have been implemented and resulted in a lexicographic tool, Xtract. The techniques are described and some results are presented on a 10 million-word corpus of stock market news reports. A lexicographic evaluation of Xtract as a collocation retrieval tool has been made, and the estimated precision of Xtract is 80%."
            },
            "slug": "Retrieving-Collocations-from-Text:-Xtract-Smadja",
            "title": {
                "fragments": [],
                "text": "Retrieving Collocations from Text: Xtract"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A set of techniques based on statistical methods for retrieving and identifying collocations from large textual corpora, based on some original filtering methods that allow the production of richer and higher-precision output are described."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1801071"
                        ],
                        "name": "Frank Smadja",
                        "slug": "Frank-Smadja",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Smadja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Frank Smadja"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145590324"
                        ],
                        "name": "K. McKeown",
                        "slug": "K.-McKeown",
                        "structuredName": {
                            "firstName": "Kathleen",
                            "lastName": "McKeown",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. McKeown"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 380794,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "094c0495ebb34c7eb61bad86a96eeebab06dab08",
            "isKey": false,
            "numCitedBy": 187,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Collocational knowledge is necessary for language generation. The problem is that collocations come in a large variety of forms. They can involve two, three or more words, these words can be of different syntactic categories and they can be involved in more or less rigid ways. This leads to two main difficulties: collocational knowledge has to be acquired and it must be represented flexibly so that it can be used for language generation. We address both problems in this paper, focusing on the acquisition problem. We describe a program, Xtract, that automatically acquires a range of collocations from large textual corpora and we describe how they can be represented in a flexible lexicon using a unification based formalism."
            },
            "slug": "Automatically-Extracting-and-Representing-for-Smadja-McKeown",
            "title": {
                "fragments": [],
                "text": "Automatically Extracting and Representing Collocations for Language Generation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A program is described, Xtract, that automatically acquires a range of collocations from large textual corpora and how they can be represented in a flexible lexicon using a unification based formalism is described."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153060797"
                        ],
                        "name": "Mary S. Neff",
                        "slug": "Mary-S.-Neff",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "Neff",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mary S. Neff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2070280240"
                        ],
                        "name": "Jean-Marc Langr",
                        "slug": "Jean-Marc-Langr",
                        "structuredName": {
                            "firstName": "Jean-Marc",
                            "lastName": "Langr",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean-Marc Langr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2085189149"
                        ],
                        "name": "Hubert Lehinann",
                        "slug": "Hubert-Lehinann",
                        "structuredName": {
                            "firstName": "Hubert",
                            "lastName": "Lehinann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hubert Lehinann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2085888780"
                        ],
                        "name": "Isabel",
                        "slug": "Isabel",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Isabel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Isabel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47322464"
                        ],
                        "name": "Z. Dominguez",
                        "slug": "Z.-Dominguez",
                        "structuredName": {
                            "firstName": "Zapata",
                            "lastName": "Dominguez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Dominguez"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12895750,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "28d4e039a476a4f8ba8ca937f149f71b5540a6de",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The acquisition and maintenance of lexical entries for MT can be a costly piece of work. While estimates are in the order of half an hour for creating one lexicon entry, popular wisdom holds that around 40,000 entries is a minimum for a working MT application that is not restricted to a very well defined sublanguage. Automated methods of lexicon acquisition are of interest to any large-scale MT project."
            },
            "slug": "Get-It-Where-You-Can-:-Acquiring-and-Maintaining-Neff-Langr",
            "title": {
                "fragments": [],
                "text": "Get It Where You Can : Acquiring and Maintaining Bilingual Lexicons for Machine Translation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680292"
                        ],
                        "name": "P. Resnik",
                        "slug": "P.-Resnik",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Resnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Resnik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716902"
                        ],
                        "name": "Marti A. Hearst",
                        "slug": "Marti-A.-Hearst",
                        "structuredName": {
                            "firstName": "Marti",
                            "lastName": "Hearst",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marti A. Hearst"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16000644,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "709fca8ad83a64e2d55eb5f57b2a2e3371bb32c1",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Lexical co-occurrence statistics are becoming widely used in the syntactic analysis of unconstrained text. However, analyses based solely on lexical relationships suffer from sparseness of data: it is sometimes necessary to use a less informed model in order to reliably estimate statistical parameters. For example, the \"lexical association\" strategy for resolving ambiguous prepositional phrase attachments [Hindle and Rooth. 1991] takes into account only the attachment site (a verb or its direct object) and the preposition, ignoring the object of the preposition. We investigated an extension of the lexical association strategy to make use of noun class information, thus permitting a disambiguation strategy to take more information into account. Although in preliminary experiments the extended strategy did not yield improved performance over lexical association alone. a qualitative analysis of the results suggests that the problem lies not in the noun class information, but rather in the multiplicity of classes available for each noun in the absence of sense disambiguation. This suggests several possible revisions of our proposal. 1. P r e f e r e n c e S t r a t e g i e s Prepositional phrase attachment is a paradigmatic case of the structural ambiguity problems faced by natural language parsing systems. Most models of grammar will not constrain the analysis of such attachments in examples like (1): the grammar simply specifies that a prepositional phrase such as on computer theft can be attached in several ways, and leaves the problem of selecting the correct choice to some other process. (1) a. Eventually, Mr. Stoll was invited to both the CIA and NSA to brief high-ranking officers on computer theft. b. Eventually, Mr. Stoll was invited to both the ClA and NSA [to brief [high-ranking officers on computer theft]]. c. Eventually, Mr. Stoll was invited to both the CIA and NSA [to brief [high-ranking ollicers] [on computer theft]]. As [Church and Patil, 1982] point out, the number of analyses given combinations of such \"all ways ambiguous\" constructions grows rapidly even for sentences of quite Marti A. Hearst Computer Science Division 465 Evans Hall University of California, Berkeley Berkeley, CA 94720 USA mar t i @ c s . b e r k e l e y . e d u reasonable length, so this other process has an important role to play. Discussions of sentence processing have focused primarily on structurally-based preference strategies such as right association and minimal attachment [Kimball, 1973; Frazier, 1979; Ford et al., 1982]; [Hobbs and Bear, 1990], while acknowledging the importance of semantics and pragmatics in attachment decisions, propose two syntactically-based attachment rules that are meant to be generalizations of those structural strategies. Others, however, have argued that syntactic considerations alone are insumcient for determining prepositional phrase attachments, suggesting instead that preference relationships among lexical items are the crucial factor. For example: [Wilks et aL, 1985] argue that the right attachment rules posited by [Frazier, 1979] are incorrect for phrases in general, and supply counterexarnples. They further argue that lexical preferences alone as suggested by [Ford et al., 1982] are too simplistic, and suggest instad the use of preference semantics. In the preference semantics framework, attachment relations of phrases are determined by comparing the preferences emanating from all the entities involved in the attachment, until the best mutual fit is found. Their CASSEX system represents the various meanings of the preposition in terms of (a) the preferred semantic class of the noun or verb that proceeds the preposition (e.g., move, be, strike), (b) the case of the preposition (e.g., instrument, time, loc.static), and (c) the preferred semantic class of the head noun of the prepositional phrase (e.g., physob, event). The difficult part of this method is the identification of preference relationships and particularly determining the strengths of the preferences and how they should interact. (See also discussion in [Schubert, 19841.) lDahlgren and McDowell, 1986] also suggests using preferences based on hand-built knowledge about the prepositions and their objects, specifying a simpler set of rules than those of [Wilks et al., 1985]."
            },
            "slug": "Structural-Ambiguity-and-Conceptual-Relations-Resnik-Hearst",
            "title": {
                "fragments": [],
                "text": "Structural Ambiguity and Conceptual Relations"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work investigated an extension of the lexical association strategy to make use of noun class information, thus permitting a disambiguation strategy to take more information into account, and found that the problem lies not in the nounclass information, but rather in the multiplicity of classes available for each noun in the absence of sense disambigsuation."
            },
            "venue": {
                "fragments": [],
                "text": "VLC@ACL"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144590225"
                        ],
                        "name": "D. Roth",
                        "slug": "D.-Roth",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Roth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1785,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3ed17a1114e2dc48597ab17cc8d5234006f525c9",
            "isKey": false,
            "numCitedBy": 222,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "We analyze a few of the commonly used statistics based and machine learning algorithms for natural language disambiguation tasks and observe that they can be recast as learning linear separators in the feature space. Each of the methods makes a priori assumptions which it employs, given the data, when searching for its hypothesis. Nevertheless, as we show, it searches a space that is as rich as the space of all linear separators. We use this to build an argument for a data driven approach which merely searches for a good linear separator in the feature space, without further assumptions on the domain or a specific problem.We present such an approach - a sparse network of linear separators, utilizing the Winnow learning algorithm - and show how to use it in a variety of ambiguity resolution problems. The learning approach presented is attribute-efficient and, therefore, appropriate for domains having very large number of attributes.In particular, we present an extensive experimental comparison of our approach with other methods on several well studied lexical disambiguation tasks such as context-sensitive spelling correction, prepositional phrase attachment and part of speech tagging. In all cases we show that our approach either outperforms other methods tried for these tasks or performs comparably to the best."
            },
            "slug": "Learning-to-Resolve-Natural-Language-Ambiguities:-A-Roth",
            "title": {
                "fragments": [],
                "text": "Learning to Resolve Natural Language Ambiguities: A Unified Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "An extensive experimental comparison of the approach with other methods on several well studied lexical disambiguation tasks such as context-sensitive spelling correction, prepositional phrase attachment and part of speech tagging shows that it outperforms other methods tried for these tasks or performs comparably to the best."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3033272"
                        ],
                        "name": "Sayori Shimohata",
                        "slug": "Sayori-Shimohata",
                        "structuredName": {
                            "firstName": "Sayori",
                            "lastName": "Shimohata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sayori Shimohata"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2613764"
                        ],
                        "name": "T. Sugio",
                        "slug": "T.-Sugio",
                        "structuredName": {
                            "firstName": "Toshiyuki",
                            "lastName": "Sugio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sugio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2076604497"
                        ],
                        "name": "Junji Nagata",
                        "slug": "Junji-Nagata",
                        "structuredName": {
                            "firstName": "Junji",
                            "lastName": "Nagata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Junji Nagata"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14182970,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "b0a71a42e5f5643519a48786e44635eaf621882a",
            "isKey": false,
            "numCitedBy": 85,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we describe a method for automatically retrieving collocations from large text corpora. This method retrieve collocations in the following stages: 1) extracting strings of characters as units of collocations 2) extracting recurrent combinations of strings in accordance with their word order in a corpus as collocations. Through the method, various range of collocations, especially domain specific collocations, are retrieved. The method is practical because it uses plain texts without any information dependent on a language such as lexical knowledge and parts of speech."
            },
            "slug": "Retrieving-Collocations-by-Co-Occurrences-and-Word-Shimohata-Sugio",
            "title": {
                "fragments": [],
                "text": "Retrieving Collocations by Co-Occurrences and Word Order Constraints"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "This method retrieve collocations in the following stages: extracting strings of characters as units of collocations, and extracting recurrent combinations of strings in accordance with their word order in a corpus as collocations."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2580777"
                        ],
                        "name": "David M. Magerman",
                        "slug": "David-M.-Magerman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Magerman",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David M. Magerman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734174"
                        ],
                        "name": "M. Marcus",
                        "slug": "M.-Marcus",
                        "structuredName": {
                            "firstName": "Mitchell",
                            "lastName": "Marcus",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marcus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2376935,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "da838db79e7593018894ada44db35eee670941d6",
            "isKey": false,
            "numCitedBy": 114,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a natural language parsing algorithm for unrestricted text which uses a probability-based scoring function to select the \"best\" parse of a sentence. The parser, Pearl, is a time-asynchronous bottom-up chart parser with Earley-type top-down prediction which pursues the highest-scoring theory in the chart, where the score of a theory represents the extent to which the context of the sentence predicts that interpretation. This parser differs from previous attempts at stochastic parsers in that it uses a richer form of conditional probabilities based on context to predict likelihood. Pearl also provides a framework for incorporating the results of previous work in part-of-speech assignment, unknown word models, and other probabilistic models of linguistic features into one parsing tool, interleaving these techniques instead of using the traditional pipeline architecture. In preliminary tests, Pearl has been successful at resolving part-of-speech and word (in speech processing) ambiguity, determining categories for unknown words, and selecting correct parses first using a very loosely fitting covering grammar."
            },
            "slug": "Pearl:-A-Probabilistic-Chart-Parser-Magerman-Marcus",
            "title": {
                "fragments": [],
                "text": "Pearl: A Probabilistic Chart Parser"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A natural language parsing algorithm for unrestricted text which uses a probability-based scoring function to select the \"best\" parse of a sentence and provides a framework for incorporating the results of previous work in part-of-speech assignment, unknown word models, and other probabilistic models of linguistic features into one parsing tool, interleaving these techniques instead of using the traditional pipeline architecture."
            },
            "venue": {
                "fragments": [],
                "text": "EACL"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2390150"
                        ],
                        "name": "Dekai Wu",
                        "slug": "Dekai-Wu",
                        "structuredName": {
                            "firstName": "Dekai",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dekai Wu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 74294,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4711ff01d8eff9b9d10deeb3b68f366f7944c208",
            "isKey": false,
            "numCitedBy": 181,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a polynomial-time algorithm for statistical machine translation. This algorithm can be used in place of the expensive, slow best-first search strategies in current statistical translation architectures. The approach employs the stochastic bracketing transduction grammar (SBTG) model we recently introduced to replace earlier word alignment channel models, while retaining a bigram language model. The new algorithm in our experience yields major speed improvement with no significant loss of accuracy."
            },
            "slug": "A-Polynomial-Time-Algorithm-for-Statistical-Machine-Wu",
            "title": {
                "fragments": [],
                "text": "A Polynomial-Time Algorithm for Statistical Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "A polynomial-time algorithm for statistical machine translation that employs the stochastic bracketing transduction grammar (SBTG) model to replace earlier word alignment channel models, while retaining a bigram language model."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1718399"
                        ],
                        "name": "Wojciech Skut",
                        "slug": "Wojciech-Skut",
                        "structuredName": {
                            "firstName": "Wojciech",
                            "lastName": "Skut",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wojciech Skut"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784037"
                        ],
                        "name": "T. Brants",
                        "slug": "T.-Brants",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Brants",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Brants"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3262500,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f21c600925122bfd7e31135a6dca51f90ee9cfa",
            "isKey": false,
            "numCitedBy": 78,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a partial parser that assigns syntactic structures to sequences of part-of-speech tags. The program uses the maximum entropy parameter estimation method, which allows a flexible combination of different knowledge sources: the hierarchical structure, parts of speech and phrasal categories. In effect, the parser goes beyond simple bracketing and recognises even fairly complex structures. We give accuracy figures for different applications of the parser."
            },
            "slug": "A-Maximum-Entropy-Partial-Parser-for-Unrestricted-Skut-Brants",
            "title": {
                "fragments": [],
                "text": "A Maximum-Entropy Partial Parser for Unrestricted Text"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "A partial parser that assigns syntactic structures to sequences of part-of-speech tags using the maximum entropy parameter estimation method, which allows a flexible combination of different knowledge sources."
            },
            "venue": {
                "fragments": [],
                "text": "VLC@COLING/ACL"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143746600"
                        ],
                        "name": "J. McMahon",
                        "slug": "J.-McMahon",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "McMahon",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. McMahon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145894070"
                        ],
                        "name": "F. Smith",
                        "slug": "F.-Smith",
                        "structuredName": {
                            "firstName": "Francis",
                            "lastName": "Smith",
                            "middleNames": [
                                "Jack"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 248095,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bf32a271a17c9c3376127d287f746e4876779d49",
            "isKey": false,
            "numCitedBy": 64,
            "numCiting": 90,
            "paperAbstract": {
                "fragments": [],
                "text": "An automatic word-classification system has been designed that uses word unigram and bigram frequency statistics to implement a binary top-down form of word clustering and employs an average class mutual information metric. Words are represented as structural tags---n-bit numbers the most significant bit-patterns of which incorporate class information. The classification system has revealed some of the lexical structure of English, as well as some phonemic and semantic structure. The system has been compared---directly and indirectly---with other recent word-classification systems. We see our classification as a means towards the end of constructing multilevel class-based interpolated language models. We have built some of these models and carried out experiments that show a 7% drop in test set perplexity compared to a standard interpolated trigram language model."
            },
            "slug": "Improving-Statistical-Language-Model-Performance-McMahon-Smith",
            "title": {
                "fragments": [],
                "text": "Improving Statistical Language Model Performance with Automatically Generated Word Hierarchies"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "An automatic word-classification system has been designed that uses word unigram and bigram frequency statistics to implement a binary top-down form of word clustering and employs an average class mutual information metric."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680292"
                        ],
                        "name": "P. Resnik",
                        "slug": "P.-Resnik",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Resnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Resnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2281602,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "812c2ad5d26f474d1c499c2665ba1a9e4fd74436",
            "isKey": false,
            "numCitedBy": 154,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "In this position paper, we make several observations about the state of the art in automatic word sense disambiguation. Motivated by these observations, we offer several specific proposals to the community regarding improved evaluation criteria, common training and testing resources, and the definition of sense inventories."
            },
            "slug": "A-Perspective-on-Word-Sense-Disambiguation-Methods-Resnik",
            "title": {
                "fragments": [],
                "text": "A Perspective on Word Sense Disambiguation Methods and Their Evaluation"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "This position paper makes several specific proposals to the community regarding improved evaluation criteria, common training and testing resources, and the definition of sense inventories about the state of the art in automatic word sense disambiguation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066174184"
                        ],
                        "name": "Khalil Simaan",
                        "slug": "Khalil-Simaan",
                        "structuredName": {
                            "firstName": "Khalil",
                            "lastName": "Simaan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Khalil Simaan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1101955,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f0b9b14218c530f7ccea91873ae7a0136506801e",
            "isKey": false,
            "numCitedBy": 131,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper studies the computational complexity of disambiguation under probabilistic tree-grammars as in (Bod, 1992; Schabes and Waters, 1993). It presents a proof that the following problems are NP-hard: computing the Most Probable Parse from a sentence or from a word-graph, and computing the Most Probable Sentence (MPS) from a word-graph. The NP-hardness of computing the MPS from a word-graph also holds for Stochastic Context-Free Grammars (SCFGs)."
            },
            "slug": "Computational-Complexity-of-Probabilistic-by-means-Simaan",
            "title": {
                "fragments": [],
                "text": "Computational Complexity of Probabilistic Disambiguation by means of Tree-Grammars"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The NP-hardness of computing the Most Probable Parse from a sentence or from a word-graph also holds for Stochastic Context-Free Grammars (SCFGs)."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144421407"
                        ],
                        "name": "P. Sheridan",
                        "slug": "P.-Sheridan",
                        "structuredName": {
                            "firstName": "P\u00e1raic",
                            "lastName": "Sheridan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Sheridan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680223"
                        ],
                        "name": "A. Smeaton",
                        "slug": "A.-Smeaton",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Smeaton",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Smeaton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5112568,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3baa3b5aa1c79a7cf90677321bb0234a7c54ff84",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Application-of-Morpho-Syntactic-Language-to-Sheridan-Smeaton",
            "title": {
                "fragments": [],
                "text": "The Application of Morpho-Syntactic Language Processing to Effective Phrase Matching"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Process. Manag."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744313"
                        ],
                        "name": "L. Ramshaw",
                        "slug": "L.-Ramshaw",
                        "structuredName": {
                            "firstName": "Lance",
                            "lastName": "Ramshaw",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Ramshaw"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734174"
                        ],
                        "name": "M. Marcus",
                        "slug": "M.-Marcus",
                        "structuredName": {
                            "firstName": "Mitchell",
                            "lastName": "Marcus",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marcus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8185806,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2acd1e59ef22c88b3a5efaed8f75662531e73853",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Eric Brill has recently proposed a simple and powerful corpus-based language modeling approach that can be applied to various tasks including part-of-speech tagging and building phrase structure trees. The method learns a series of symbolic transformational rules, which can then be applied in sequence to a test corpus to produce predictions. The learning process only requires counting matches for a given set of rule templates, allowing the method to survey a very large space of possible contextual factors. This paper analyses Brill's approach as an interesting variation on existing decision tree methods, based on experiments involving part-of-speech tagging for both English and ancient Greek corpora. In particular, the analysis throws light on why the new mechanism seems surprisingly resistant to overtraining. A fast, incremental implementation and a mechanism for recording the dependencies that underlie the resulting rule sequence are also described."
            },
            "slug": "Exploring-the-Statistical-Derivation-of-Rule-for-Ramshaw-Marcus",
            "title": {
                "fragments": [],
                "text": "Exploring the Statistical Derivation of Transformational Rule Sequences for Part-of-Speech Tagging"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper analyses Brill's approach as an interesting variation on existing decision tree methods, based on experiments involving part-of-speech tagging for both English and ancient Greek corpora, and throws light on why the new mechanism seems surprisingly resistant to overtraining."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693517"
                        ],
                        "name": "David Yarowsky",
                        "slug": "David-Yarowsky",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Yarowsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Yarowsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1487550,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "944cba683d10d8c1a902e05cd68e32a9f47b372e",
            "isKey": false,
            "numCitedBy": 2536,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations. The algorithm is based on two powerful constraints---that words tend to have one sense per discourse and one sense per collocation---exploited in an iterative bootstrapping procedure. Tested accuracy exceeds 96%."
            },
            "slug": "Unsupervised-Word-Sense-Disambiguation-Rivaling-Yarowsky",
            "title": {
                "fragments": [],
                "text": "Unsupervised Word Sense Disambiguation Rivaling Supervised Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "An unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33611948"
                        ],
                        "name": "D. Palmer",
                        "slug": "D.-Palmer",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Palmer",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Palmer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716902"
                        ],
                        "name": "Marti A. Hearst",
                        "slug": "Marti-A.-Hearst",
                        "structuredName": {
                            "firstName": "Marti",
                            "lastName": "Hearst",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marti A. Hearst"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 44278,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2510c7da837cf4ad083a6aa97a857e524cb4f142",
            "isKey": false,
            "numCitedBy": 102,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Labeling of sentence boundaries is a necessary prerequisite for many natural language processing tasks, including part-of-speech tagging and sentence alignment. End-of-sentence punctuation marks are ambiguous; to disambiguate them most systems use brittle, special-purpose regular expression grammars and exception rules. As an alternative, we have developed an efficient, trainable algorithm that uses a lexicon with part-of-speech probabilities and a feed-forward neural network. This work demonstrates the feasibility of using prior probabilities of part-of-speech assignments, as opposed to words or definite part-of-speech assignments, as contextual information. After training for less than one minute, the method correctly labels over 98.5% of sentence boundaries in a corpus of over 27,000 sentence-boundary marks. We show the method to be efficient and easily adaptable to different text genres, including single-case texts."
            },
            "slug": "Adaptive-Sentence-Boundary-Disambiguation-Palmer-Hearst",
            "title": {
                "fragments": [],
                "text": "Adaptive Sentence Boundary Disambiguation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An efficient, trainable algorithm that uses a lexicon with part-of-speech probabilities and a feed-forward neural network to label sentence boundaries and is shown to be efficient and easily adaptable to different text genres, including single-case texts."
            },
            "venue": {
                "fragments": [],
                "text": "ANLP"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145310589"
                        ],
                        "name": "R. L. Trask",
                        "slug": "R.-L.-Trask",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Trask",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. L. Trask"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58405820,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "7df0cc66fe2e1fef6d3c77114ff27f2840d21668",
            "isKey": false,
            "numCitedBy": 410,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This dictionary of grammatical terms covers both current and traditional terminology in syntax and morphology. It includes descriptive terms, the major theoretical concepts of the most influential grammatical frameworks, and the chief terms from mathematical and computational linguistics. It contains over 1500 entries, providing definitions and examples, pronunciations, the earliest sources of terms and suggestions for further reading, and recommendations about competing and conflicting usages. The book focuses on non-theory-boumd descriptive terms, which are likely to remain current for some years. Aimed at students and teachers of linguistics, it allows a reader puzzled by a grammatical term to look it up and locate further reading with ease."
            },
            "slug": "A-Dictionary-of-Grammatical-Terms-in-Linguistics-Trask",
            "title": {
                "fragments": [],
                "text": "A Dictionary of Grammatical Terms in Linguistics"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "This dictionary of grammatical terms covers both current and traditional terminology in syntax and morphology, and includes descriptive terms, the major theoretical concepts of the most influential grammatical frameworks, and the chief terms from mathematical and computational linguistics."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793475"
                        ],
                        "name": "A. Ratnaparkhi",
                        "slug": "A.-Ratnaparkhi",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ratnaparkhi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ratnaparkhi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2046487"
                        ],
                        "name": "Jeffrey C. Reynar",
                        "slug": "Jeffrey-C.-Reynar",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Reynar",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeffrey C. Reynar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781292"
                        ],
                        "name": "S. Roukos",
                        "slug": "S.-Roukos",
                        "structuredName": {
                            "firstName": "Salim",
                            "lastName": "Roukos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roukos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 217,
                                "start": 107
                            }
                        ],
                        "text": "Much other work has used various other features, in particular the identity of the head noun inside the PP (Resnik and Hearst 1993; Brill and Resnik 1994; Ratnaparkhi et al. 1994; Zavrel et al. 1997; Ratnaparkhi 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1197,
                                "start": 155
                            }
                        ],
                        "text": "Much other work has used various other features, in particular the identity of the head noun inside the PP (Resnik and Hearst 1993; Brill and Resnik 1994; Ratnaparkhi et al. 1994; Zavrel et al. 1997; Ratnaparkhi 1998). Franz (1996) is able to include lots of features within a loglinear model approach, but at the cost of reducing the most basic association strength parameters to categorical variables. A second major limitation is that Hindle and Rooth (1993) consider only the most basic case of a PP immediately after an NP object which is modifying either the immediately preceding noun or verb. But there are many more possibilities for PP attachments than this. Gibson and Pearlmutter (1994) argue that psycholinguistic studies have been greatly biased by their overconcentration on this one particular case. A PP separated from an object noun by another PP may modify any of the noun inside the preceding PP, the object noun, or the preceding verb. Figure 8.2 shows a variety of the distant and complex attachment patterns that occur in texts. Additionally, in a complex sentence, a PP might not modify just the immediately preceding verb, but might modify a higher verb. See Franz (1997) for further discussion, and exercise 8."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 699,
                                "start": 155
                            }
                        ],
                        "text": "Much other work has used various other features, in particular the identity of the head noun inside the PP (Resnik and Hearst 1993; Brill and Resnik 1994; Ratnaparkhi et al. 1994; Zavrel et al. 1997; Ratnaparkhi 1998). Franz (1996) is able to include lots of features within a loglinear model approach, but at the cost of reducing the most basic association strength parameters to categorical variables. A second major limitation is that Hindle and Rooth (1993) consider only the most basic case of a PP immediately after an NP object which is modifying either the immediately preceding noun or verb. But there are many more possibilities for PP attachments than this. Gibson and Pearlmutter (1994) argue that psycholinguistic studies have been greatly biased by their overconcentration on this one particular case."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 462,
                                "start": 155
                            }
                        ],
                        "text": "Much other work has used various other features, in particular the identity of the head noun inside the PP (Resnik and Hearst 1993; Brill and Resnik 1994; Ratnaparkhi et al. 1994; Zavrel et al. 1997; Ratnaparkhi 1998). Franz (1996) is able to include lots of features within a loglinear model approach, but at the cost of reducing the most basic association strength parameters to categorical variables. A second major limitation is that Hindle and Rooth (1993) consider only the most basic case of a PP immediately after an NP object which is modifying either the immediately preceding noun or verb."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 232,
                                "start": 155
                            }
                        ],
                        "text": "Much other work has used various other features, in particular the identity of the head noun inside the PP (Resnik and Hearst 1993; Brill and Resnik 1994; Ratnaparkhi et al. 1994; Zavrel et al. 1997; Ratnaparkhi 1998). Franz (1996) is able to include lots of features within a loglinear model approach, but at the cost of reducing the most basic association strength parameters to categorical variables."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 129886,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "00f20179b9087fbf24b6656008a9380c590d9ec9",
            "isKey": true,
            "numCitedBy": 272,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "A parser for natural language must often choose between two or more equally grammatical parses for the same sentence. Often the correct parse can be determined from the lexical properties of certain key words or from the context in which the sentence occurs. For example in the sentence."
            },
            "slug": "A-Maximum-Entropy-Model-for-Prepositional-Phrase-Ratnaparkhi-Reynar",
            "title": {
                "fragments": [],
                "text": "A Maximum Entropy Model for Prepositional Phrase Attachment"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A parser for natural language must often choose between two or more equally grammatical parses for the same sentence."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1801071"
                        ],
                        "name": "Frank Smadja",
                        "slug": "Frank-Smadja",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Smadja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Frank Smadja"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145590324"
                        ],
                        "name": "K. McKeown",
                        "slug": "K.-McKeown",
                        "structuredName": {
                            "firstName": "Kathleen",
                            "lastName": "McKeown",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. McKeown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799688"
                        ],
                        "name": "V. Hatzivassiloglou",
                        "slug": "V.-Hatzivassiloglou",
                        "structuredName": {
                            "firstName": "Vasileios",
                            "lastName": "Hatzivassiloglou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Hatzivassiloglou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 180
                            }
                        ],
                        "text": "What we have said about the value of statistical corpus analysis for monolingual dictionaries applies equally to bilingual dictionaries, at least if an aligned corpus is available (Smadja et al. 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6720757,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0285f18f1642c3684e6abb7d5162348278c41abf",
            "isKey": false,
            "numCitedBy": 576,
            "numCiting": 84,
            "paperAbstract": {
                "fragments": [],
                "text": "Collocations are notoriously difficult for non-native speakers to translate, primarily because they are opaque and cannot be translated on a word-by-word basis. We describe a program named Champollion which, given a pair of parallel corpora in two different languages and a list of collocations in one of them, automatically produces their translations. Our goal is to provide a tool for compiling bilingual lexical information above the word level in multiple languages, for different domains. The algorithm we use is based on statistical methods and produces p-word translations of n-word collocations in which n and p need not be the same. For example, Champollion translates make...decision, employment equity, and stock market into prendre...decision, equite en matiere d'emploi, and bourse respectively. Testing Champollion on three years' worth of the Hansards corpus yielded the French translations of 300 collocations for each year, evaluated at 73% accuracy on average. In this paper, we describe the statistical measures used, the algorithm, and the implementation of Champollion, presenting our results and evaluation."
            },
            "slug": "Translating-Collocations-for-Bilingual-Lexicons:-A-Smadja-McKeown",
            "title": {
                "fragments": [],
                "text": "Translating Collocations for Bilingual Lexicons: A Statistical Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A program named Champollion is described which, given a pair of parallel corpora in two different languages and a list of collocations in one of them, automatically produces their translations, to provide a tool for compiling bilingual lexical information above the word level in multiple languages, for different domains."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693517"
                        ],
                        "name": "David Yarowsky",
                        "slug": "David-Yarowsky",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Yarowsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Yarowsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1580335,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "6903729430e70fa0a564dec6f294424f837781c8",
            "isKey": false,
            "numCitedBy": 484,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a statistical decision procedure for lexical ambiguity resolution. The algorithm exploits both local syntactic patterns and more distant collocational evidence, generating an efficient, effective, and highly perspicuous recipe for resolving a given ambiguity. By identifying and utilizing only the single best disambiguating evidence in a target context, the algorithm avoids the problematic complex modeling of statistical dependencies. Although directly applicable to a wide class of ambiguities, the algorithm is described and evaluated in a realistic case study, the problem of restoring missing accents in Spanish and French text. Current accuracy exceeds 99% on the full task, and typically is over 90% for even the most difficult ambiguities."
            },
            "slug": "Decision-Lists-for-Lexical-Ambiguity-Resolution:-to-Yarowsky",
            "title": {
                "fragments": [],
                "text": "Decision Lists for Lexical Ambiguity Resolution: Application to Accent Restoration in Spanish and French"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "This paper presents a statistical decision procedure for lexical ambiguity resolution that exploits both local syntactic patterns and more distant collocational evidence, generating an efficient, effective, and highly perspicuous recipe for resolving a given ambiguity."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2001885"
                        ],
                        "name": "Ted Pedersen",
                        "slug": "Ted-Pedersen",
                        "structuredName": {
                            "firstName": "Ted",
                            "lastName": "Pedersen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ted Pedersen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3261395,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f8ef4fbd7e807a6eb2ce11d6d2f6520d23c6b4a1",
            "isKey": false,
            "numCitedBy": 146,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical methods for automatically identifying dependent word pairs (i.e. dependent bigrams) in a corpus of natural language text have traditionally been performed using asymptotic tests of significance. This paper suggests that Fisher's exact test is a more appropriate test due to the skewed and sparse data samples typical of this problem. Both theoretical and experimental comparisons between Fisher's exact test and a variety of asymptotic tests (the t-test, Pearson's chi-square test, and Likelihood-ratio chi-square test) are presented. These comparisons show that Fisher's exact test is more reliable in identifying dependent word pairs. The usefulness of Fisher's exact test extends to other problems in statistical natural language processing as skewed and sparse data appears to be the rule in natural language. The experiment presented in this paper was performed using PROC FREQ of the SAS System."
            },
            "slug": "Fishing-for-Exactness-Pedersen",
            "title": {
                "fragments": [],
                "text": "Fishing for Exactness"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "Comparisons show that Fisher's exact test is more reliable in identifying dependent word pairs and extends to other problems in statistical natural language processing as skewed and sparse data appears to be the rule in natural language."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "27119069"
                        ],
                        "name": "David D. McDonald",
                        "slug": "David-D.-McDonald",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "McDonald",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David D. McDonald"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 30258255,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5d625c9b1554d0daf6bdb6ac172cd3a7644ba0e2",
            "isKey": false,
            "numCitedBy": 170,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe the proper name recognition and classification facility (\"PNV') of the SPARSER natural language understanding system. PNF has been used very successfully in the analysis of unrestricted texts in several sublanguages taken from online news sources. It makes its categorizations on the basis of 'external' evidence from the context of the phrases adjacent to the name as well as 'internal' evidence within the sequence of words and characters. A semantic model of each name and its components is maintained and used for subsequent reference. We describe PNF's operations of delimiting, classifying, and semantically recording the structure of a name; we situate PNF with respect to the related parsing mechanisms within Sparser; and finally we work through an extended example that is typical of the sorts of text we have applied PNF to."
            },
            "slug": "Internal-and-External-Evidence-in-the-and-Semantic-McDonald",
            "title": {
                "fragments": [],
                "text": "Internal and External Evidence in the Identification and Semantic Categorization of Proper Names"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "The proper name recognition and classification facility (\"PNV\") of the SPARSER natural language understanding system has been described and has been used very successfully in the analysis of unrestricted texts in several sublanguages taken from online news sources."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2024985"
                        ],
                        "name": "P. Suppes",
                        "slug": "P.-Suppes",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Suppes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Suppes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2155275265"
                        ],
                        "name": "L. Liang",
                        "slug": "L.-Liang",
                        "structuredName": {
                            "firstName": "Lin",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Liang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34612911"
                        ],
                        "name": "Michael B\u00f6ttner",
                        "slug": "Michael-B\u00f6ttner",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "B\u00f6ttner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael B\u00f6ttner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15852664,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "c9e0541c4dda9d3ef6cdd5b9bb098feb1b9af64b",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Comprehension grammars for a sample of ten languages (English, Dutch, German, French, Spanish, Catalan, Russian, Chinese, Korean, and Japanese) were derived by machine learning from corpora of about 400 sentences. Key concepts in our learning theory are: probabilistic association of words and meanings, grammatical and semantical form generalization, grammar computations, congruence of meaning, and dynamical assignment of denotational value to a word."
            },
            "slug": "Machine-Learning-Comprehension-Grammars-for-Ten-Suppes-Liang",
            "title": {
                "fragments": [],
                "text": "Machine Learning Comprehension Grammars for Ten Languages"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Key concepts in the learning theory are: probabilistic association of words and meanings, grammatical and semantical form generalization, grammar computations, congruence of meaning, and dynamical assignment of denotational value to a word."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34789794"
                        ],
                        "name": "H. Ng",
                        "slug": "H.-Ng",
                        "structuredName": {
                            "firstName": "Hwee",
                            "lastName": "Ng",
                            "middleNames": [
                                "Tou"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3161371"
                        ],
                        "name": "H. Lee",
                        "slug": "H.-Lee",
                        "structuredName": {
                            "firstName": "Hian",
                            "lastName": "Lee",
                            "middleNames": [
                                "Beng"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11202365,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fd303a438cef5eb647c63cd1f25bad12a5babba3",
            "isKey": false,
            "numCitedBy": 567,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a new approach for word sense disambiguation (WSD) using an exemplar-based learning algorithm. This approach integrates a diverse set of knowledge sources to disambiguate word sense, including part of speech of neighboring words, morphological form, the unordered set of surrounding words, local collocations, and verb-object syntactic relation. We tested our WSD program, named LEXAS, on both a common data set used in previous work, as well as on a large sense-tagged corpus that we separately constructed. LEXAS achieves a higher accuracy on the common data set, and performs better than the most frequent heuristic on the highly ambiguous words in the large corpus tagged with the refined senses of WORDNET."
            },
            "slug": "Integrating-Multiple-Knowledge-Sources-to-Word-An-Ng-Lee",
            "title": {
                "fragments": [],
                "text": "Integrating Multiple Knowledge Sources to Disambiguate Word Sense: An Exemplar-Based Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "This approach integrates a diverse set of knowledge sources to disambiguate word sense, including part of speech of neighboring words, morphological form, the unordered set of surrounding words, local collocations, and verb-object syntactic relation."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2579894"
                        ],
                        "name": "B. Carpenter",
                        "slug": "B.-Carpenter",
                        "structuredName": {
                            "firstName": "Bob",
                            "lastName": "Carpenter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Carpenter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1716,
                                "start": 160
                            }
                        ],
                        "text": "See Lewis and Jones (1996) and Krovetz (1991) for further discussion of the question of using collocation discovery and NLP in Information Retrieval and Nevill-Manning et al. (1997) for an alternative non-statistical approach to using phrases in IR. Steier and Belew (1993) present an interesting study of how the treatment of phrases (for example, for phrase weighting) should change as we move from a subdomain to a general domain. For example, invasive procedure is completely compositional and a less interesting collocation in the subdomain of medical articles, but becomes interesting and compositional when 'exported' to a general collection that is a mixture of many specialized domains. Two other important applications of collocations, which we will just mention, are natural language generation (Smadja 1993) and language information retrieval (Hull and Grefenstette 1 An important area that we haven't been able to cover is the discovery of proper nouns, which can be regarded as a of collocation. Proper nouns cannot be exhaustively covered in dictionaries since new people, places, and other entities come into existence and are named all the time. Proper nouns also present their own set of challenges: co-reference (How can we tell that IBM and International Business Machines refer to the same entity?), disambiguation (When does refer to the American Exchange, when to American Express?), and classification (Is this new entity that the text refers to the name of a person, a location or a company?). One of the earliest studies on topic is (Coates-Stephens 1993). McDonald (1995) focuses on lexicosemantic patterns that can be used as cues for proper noun detection and classification. and (1995) and Paik et al."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 160
                            }
                        ],
                        "text": "See Lewis and Jones (1996) and Krovetz (1991) for further discussion of the question of using collocation discovery and NLP in Information Retrieval and Nevill-Manning et al. (1997) for an alternative non-statistical approach to using phrases in IR."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1739,
                                "start": 160
                            }
                        ],
                        "text": "See Lewis and Jones (1996) and Krovetz (1991) for further discussion of the question of using collocation discovery and NLP in Information Retrieval and Nevill-Manning et al. (1997) for an alternative non-statistical approach to using phrases in IR. Steier and Belew (1993) present an interesting study of how the treatment of phrases (for example, for phrase weighting) should change as we move from a subdomain to a general domain. For example, invasive procedure is completely compositional and a less interesting collocation in the subdomain of medical articles, but becomes interesting and compositional when 'exported' to a general collection that is a mixture of many specialized domains. Two other important applications of collocations, which we will just mention, are natural language generation (Smadja 1993) and language information retrieval (Hull and Grefenstette 1 An important area that we haven't been able to cover is the discovery of proper nouns, which can be regarded as a of collocation. Proper nouns cannot be exhaustively covered in dictionaries since new people, places, and other entities come into existence and are named all the time. Proper nouns also present their own set of challenges: co-reference (How can we tell that IBM and International Business Machines refer to the same entity?), disambiguation (When does refer to the American Exchange, when to American Express?), and classification (Is this new entity that the text refers to the name of a person, a location or a company?). One of the earliest studies on topic is (Coates-Stephens 1993). McDonald (1995) focuses on lexicosemantic patterns that can be used as cues for proper noun detection and classification. and (1995) and Paik et al. (1995) propose ways of classifying proper nouns according to type."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 274,
                                "start": 160
                            }
                        ],
                        "text": "See Lewis and Jones (1996) and Krovetz (1991) for further discussion of the question of using collocation discovery and NLP in Information Retrieval and Nevill-Manning et al. (1997) for an alternative non-statistical approach to using phrases in IR. Steier and Belew (1993) present an interesting study of how the treatment of phrases (for example, for phrase weighting) should change as we move from a subdomain to a general domain."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1599,
                                "start": 160
                            }
                        ],
                        "text": "See Lewis and Jones (1996) and Krovetz (1991) for further discussion of the question of using collocation discovery and NLP in Information Retrieval and Nevill-Manning et al. (1997) for an alternative non-statistical approach to using phrases in IR. Steier and Belew (1993) present an interesting study of how the treatment of phrases (for example, for phrase weighting) should change as we move from a subdomain to a general domain. For example, invasive procedure is completely compositional and a less interesting collocation in the subdomain of medical articles, but becomes interesting and compositional when 'exported' to a general collection that is a mixture of many specialized domains. Two other important applications of collocations, which we will just mention, are natural language generation (Smadja 1993) and language information retrieval (Hull and Grefenstette 1 An important area that we haven't been able to cover is the discovery of proper nouns, which can be regarded as a of collocation. Proper nouns cannot be exhaustively covered in dictionaries since new people, places, and other entities come into existence and are named all the time. Proper nouns also present their own set of challenges: co-reference (How can we tell that IBM and International Business Machines refer to the same entity?), disambiguation (When does refer to the American Exchange, when to American Express?), and classification (Is this new entity that the text refers to the name of a person, a location or a company?). One of the earliest studies on topic is (Coates-Stephens 1993). McDonald (1995) focuses on lexicosemantic patterns that can be used as cues for proper noun detection and classification."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6847055,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "81926eab7d1eb37b0c72c8aeb04420617568e965",
            "isKey": false,
            "numCitedBy": 64,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a novel parser based on a probabilistic version of a left-corner parser. The left-corner strategy is attractive because rule probabilities can be conditioned on both top-down goals and bottom-up derivations. We develop the underlying theory and explain how a grammar can be induced from analyzed data. We show that the left-corner approach provides an advantage over simple top-down probabilistic context-free grammars in parsing the Wall Street Journal using a grammar induced from the Penn Treebank. We also conclude that the Penn Treebank provides a fairly weak tes bed due to the flatness of its bracketings and to the obvious overgeneration and undergeneration of its induced grammar."
            },
            "slug": "Probabilistic-Parsing-using-Left-Corner-Language-Manning-Carpenter",
            "title": {
                "fragments": [],
                "text": "Probabilistic Parsing using Left Corner Language Models"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that the left-corner approach provides an advantage over simple top-down probabilistic context-free grammars in parsing the Wall Street Journal using a grammar induced from the Penn Treebank."
            },
            "venue": {
                "fragments": [],
                "text": "IWPT"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2403128"
                        ],
                        "name": "C. Samuelsson",
                        "slug": "C.-Samuelsson",
                        "structuredName": {
                            "firstName": "Christer",
                            "lastName": "Samuelsson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Samuelsson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18379498,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "48f30de025d6bfba7249a1f77ed8c4c8d8d18b93",
            "isKey": false,
            "numCitedBy": 65,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "The influence of various information sources on the ability of a statistical tagger to assign lexical categories to unknown words is investigated. The literal word form is found to be very much more important than other information sources such as the local syntactic context. Different ways of combining information sources are discussed. Methods for improving estimates based on scarce data are proposed and examined experimentally."
            },
            "slug": "Morphological-Tagging-Based-Entirely-on-Bayesian-Samuelsson",
            "title": {
                "fragments": [],
                "text": "Morphological Tagging Based Entirely on Bayesian Inference"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "The influence of various information sources on the ability of a statistical tagger to assign lexical categories to unknown words is investigated and methods for improving estimates based on scarce data are proposed and examined experimentally."
            },
            "venue": {
                "fragments": [],
                "text": "NODALIDA"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37491468"
                        ],
                        "name": "Mikio Yamamoto",
                        "slug": "Mikio-Yamamoto",
                        "structuredName": {
                            "firstName": "Mikio",
                            "lastName": "Yamamoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mikio Yamamoto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2244184"
                        ],
                        "name": "Kenneth Ward Church",
                        "slug": "Kenneth-Ward-Church",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Church",
                            "middleNames": [
                                "Ward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth Ward Church"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 563442,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9df3e898f36c8cd7afc82f3cd080dc94fd779aca",
            "isKey": false,
            "numCitedBy": 237,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Bigrams and trigrams are commonly used in statistical natural language processing; this paper will describe techniques for working with much longer n-grams. Suffix arrays (Manber and Myers 1990) were first introduced to compute the frequency and location of a substring (n-gram) in a sequence (corpus) of length N. To compute frequencies over all N(N+1)/2 substrings in a corpus, the substrings are grouped into a manageable number of equivalence classes. In this way, a prohibitive computation over substrings is reduced to a manageable computation over classes. This paper presents both the algorithms and the code that were used to compute term frequency (tf) and document frequency (df) for all n-grams in two large corpora, an English corpus of 50 million words of Wall Street Journal and a Japanese corpus of 216 million characters of Mainichi Shimbun. The second half of the paper uses these frequencies to find interesting substrings. Lexicographers have been interested in n-grams with high mutual information (MI) where the joint term frequency is higher than what would be expected by chance, assuming that the parts of the n-gram combine independently. Residual inverse document frequency (RIDF) compares document frequency to another model of chance where terms with a particular term frequency are distributed randomly throughout the collection. MI tends to pick out phrases with noncompositional semantics (which often violate the independence assumption) whereas RIDF tends to highlight technical terminology, names, and good keywords for information retrieval (which tend to exhibit nonrandom distributions over documents). The combination of both MI and RIDF is better than either by itself in a Japanese word extraction task."
            },
            "slug": "Using-Suffix-Arrays-to-Compute-Term-Frequency-and-a-Yamamoto-Church",
            "title": {
                "fragments": [],
                "text": "Using Suffix Arrays to Compute Term Frequency and Document Frequency for All Substrings in a Corpus"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "The algorithms and the code that were used to compute term frequency (tf) and document frequency (df) for all n-grams in two large corpora, an English corpus of 50 million words of Wall Street Journal and a Japanese corpus of 216 million characters of Mainichi Shimbun are presented."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797655"
                        ],
                        "name": "R. Mooney",
                        "slug": "R.-Mooney",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Mooney",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mooney"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 431099,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a2b5cb0285d593dde7c5a1b844ab0361aebfc85d",
            "isKey": false,
            "numCitedBy": 250,
            "numCiting": 108,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes an experimental comparison of seven different learning algorithms on the problem of learning to disambiguate the meaning of a word from context. The algorithms tested include statistical, neural-network, decision-tree, rule-based, and case-based classification techniques. The specific problem tested involves disambiguating six senses of the word ``line'' using the words in the current and proceeding sentence as context. The statistical and neural-network methods perform the best on this particular problem and we discuss a potential reason for this observed difference. We also discuss the role of bias in machine learning and its importance in explaining performance differences observed on specific problems."
            },
            "slug": "Comparative-Experiments-on-Disambiguating-Word-An-Mooney",
            "title": {
                "fragments": [],
                "text": "Comparative Experiments on Disambiguating Word Senses: An Illustration of the Role of Bias in Machine Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "An experimental comparison of seven different learning algorithms on the problem of learning to disambiguate the meaning of a word from context finds the statistical and neural-network methods perform the best on this particular problem."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2832706"
                        ],
                        "name": "E. Viegas",
                        "slug": "E.-Viegas",
                        "structuredName": {
                            "firstName": "Evelyne",
                            "lastName": "Viegas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Viegas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2008000"
                        ],
                        "name": "B. Onyshkevych",
                        "slug": "B.-Onyshkevych",
                        "structuredName": {
                            "firstName": "Boyan",
                            "lastName": "Onyshkevych",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Onyshkevych"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143934140"
                        ],
                        "name": "V. Raskin",
                        "slug": "V.-Raskin",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Raskin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Raskin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689693"
                        ],
                        "name": "S. Nirenburg",
                        "slug": "S.-Nirenburg",
                        "structuredName": {
                            "firstName": "Sergei",
                            "lastName": "Nirenburg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nirenburg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1562205,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "8e866bb6e52134d26c8afdf4a7004cb618c89a6e",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper deals with the discovery, representation, and use of lexical rules (LRs) during large-scale semi-automatic computational lexicon acquisition. The analysis is based on a set of LRs implemented and tested on the basis of Spanish and English business- and finance-related corpora. We show that, though the use of LRs is justified, they do not come cost-free. Semi-automatic output checking is required, even with blocking and preemtion procedures built in. Nevertheless, large-scope LRs are justified because they facilitate the unavoidable process of large-scale semi-automatic lexical acquisition. We also argue that the place of LRs in the computational process is a complex issue."
            },
            "slug": "From-Submit-to-Submitted-via-Submission:-On-Lexical-Viegas-Onyshkevych",
            "title": {
                "fragments": [],
                "text": "From Submit to Submitted via Submission: On Lexical Rules in Large-Scale Lexicon Acquisition"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is argued that the place of LRs in the computational process is a complex issue and large-scope LRs are justified because they facilitate the unavoidable process of large-scale semi-automatic lexical acquisition."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3316623"
                        ],
                        "name": "Jakub Zavrel",
                        "slug": "Jakub-Zavrel",
                        "structuredName": {
                            "firstName": "Jakub",
                            "lastName": "Zavrel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jakub Zavrel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735272"
                        ],
                        "name": "Walter Daelemans",
                        "slug": "Walter-Daelemans",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Daelemans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Walter Daelemans"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143667674"
                        ],
                        "name": "J. Veenstra",
                        "slug": "J.-Veenstra",
                        "structuredName": {
                            "firstName": "Jorn",
                            "lastName": "Veenstra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Veenstra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1742928,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "200224e8cd06bf822c05ad023e233bbfc6a96b64",
            "isKey": false,
            "numCitedBy": 81,
            "numCiting": 71,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we describe the application of Memory-Based Learning to the problem of Prepositional Phrase attachment disambiguation. We compare Memory-Based Learning, which stores examples in memory and generalizes by using intelligent similarity metrics, with a number of recently proposed statistical methods that are well suited to large numbers of features. We evaluate our methods on a common benchmark dataset and show that our method compares favorably to previous methods, and is well-suited to incorporating various unconventional representations of word patterns such as value difference metrics and Lexical Space."
            },
            "slug": "Resolving-PP-attachment-Ambiguities-with-Learning-Zavrel-Daelemans",
            "title": {
                "fragments": [],
                "text": "Resolving PP attachment Ambiguities with Memory-Based Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "The application of Memory-Based Learning to the problem of Prepositional Phrase attachment disambiguation is described and the method compares favorably to previous methods, and is well-suited to incorporating various unconventional representations of word patterns such as value difference metrics and Lexical Space."
            },
            "venue": {
                "fragments": [],
                "text": "CoNLL"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2165139"
                        ],
                        "name": "H. V. Halteren",
                        "slug": "H.-V.-Halteren",
                        "structuredName": {
                            "firstName": "Hans",
                            "lastName": "Halteren",
                            "middleNames": [
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. V. Halteren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3316623"
                        ],
                        "name": "Jakub Zavrel",
                        "slug": "Jakub-Zavrel",
                        "structuredName": {
                            "firstName": "Jakub",
                            "lastName": "Zavrel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jakub Zavrel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735272"
                        ],
                        "name": "Walter Daelemans",
                        "slug": "Walter-Daelemans",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Daelemans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Walter Daelemans"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 852013,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b863f180d6588222663cd6e2c434f50a7f06fff3",
            "isKey": false,
            "numCitedBy": 153,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we examine how the differences in modelling between different data driven systems performing the same NLP task can be exploited to yield a higher accuracy than the best individual system. We do this by means of an experiment involving the task of morpho-syntactic wordclass tagging. Four well-known tagger generator (Hidden Markov Model, Memory-Based, Transformation Rules and Maximum Entropy) are trained on the same corpus data. After comparison, their outputs are combined using several voting strategies and second stage classifiers. All combination taggers outperform their best component, with the best combination showing a 19.1% lower error rate than the best indvidual tagger."
            },
            "slug": "Improving-Data-Driven-Wordclass-Tagging-by-System-Halteren-Zavrel",
            "title": {
                "fragments": [],
                "text": "Improving Data Driven Wordclass Tagging by System Combination"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "How the differences in modelling between different data driven systems performing the same NLP task can be exploited to yield a higher accuracy than the best individual system is examined."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2390150"
                        ],
                        "name": "Dekai Wu",
                        "slug": "Dekai-Wu",
                        "structuredName": {
                            "firstName": "Dekai",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dekai Wu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2144821,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "b83dbea361cd583f9fdfb134bb55c48f0335d297",
            "isKey": false,
            "numCitedBy": 220,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe our experience with automatic alignment of sentences in parallel English-Chinese texts. Our report concerns three related topics: (1) progress on the HKUST English-Chinese Parallel Bilingual Corpus; (2) experiments addressing the applicability of Gale & Church's (1991) length-based statistical method to the task of alignment involving a non-Indo-European language; and (3) an improved statistical method that also incorporates domain-specific lexical cues."
            },
            "slug": "Aligning-a-Parallel-English-Chinese-Corpus-With-Wu",
            "title": {
                "fragments": [],
                "text": "Aligning a Parallel English-Chinese Corpus Statistically With Lexical Criteria"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "This report concerns three related topics: progress on the HKUST English-Chinese Parallel Bilingual Corpus; experiments addressing the applicability of Gale & Church's length-based statistical method to the task of alignment involving a non-Indo-European language; and an improved statistical method that also incorporates domain-specific lexical cues."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793475"
                        ],
                        "name": "A. Ratnaparkhi",
                        "slug": "A.-Ratnaparkhi",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ratnaparkhi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ratnaparkhi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 330,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "54c846ee00c6132d70429cc279e8577f63ed05e4",
            "isKey": false,
            "numCitedBy": 291,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a statistical parser for natural language that obtains a parsing accuracy---roughly 87% precision and 86% recall---which surpasses the best previously published results on the Wall St. Journal domain. The parser itself requires very little human intervention, since the information it uses to make parsing decisions is specified in a concise and simple manner, and is combined in a fully automatic way under the maximum entropy framework. The observed running time of the parser on a test sentence is linear with respect to the sentence length. Furthermore, the parser returns several scored parses for a sentence, and this paper shows that a scheme to pick the best parse from the 20 highest scoring parses could yield a dramatically higher accuracy of 93% precision and recall."
            },
            "slug": "A-Linear-Observed-Time-Statistical-Parser-Based-on-Ratnaparkhi",
            "title": {
                "fragments": [],
                "text": "A Linear Observed Time Statistical Parser Based on Maximum Entropy Models"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "A statistical parser for natural language that obtains a parsing accuracy that surpasses the best previously published results on the Wall St. Journal domain, and it is shown that a scheme to pick the best parse from the 20 highest scoring parses could yield a dramatically higher accuracy of 93% precision and recall."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144599392"
                        ],
                        "name": "I. D. Melamed",
                        "slug": "I.-D.-Melamed",
                        "structuredName": {
                            "firstName": "I.",
                            "lastName": "Melamed",
                            "middleNames": [
                                "Dan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. D. Melamed"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3074496,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b66fc3bbba9027fd1f0ebf6d1c5c849ef15ca695",
            "isKey": false,
            "numCitedBy": 142,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Many multilingual NLP applications need to translate words between different languages, but cannot afford the computational expenses of inducing or applying a full translation model. For theses applications, we have designed a fast algorithm for estimating a partial translation model, which accounts for translational equivalence only at the word level. The model's precision/recall trade-off can be directly controlled via one threshold parameter. This feature makes the model more suitable for applications that are not fully statistical. The model's hidden parameters can be easily conditioned on information extrinsic to the model, providing an easy way to integrate pre-existing knowledge such as part-of-speech, dictionaries, word order, etc., Our model can link word tokens in parallel texts as well as other translation models in the literature. Unlike other translation models, it can automatically produce dictionary-sized translation lexicons, and it can do so with over 99% accuracy."
            },
            "slug": "A-Word-to-Word-Model-of-Translational-Equivalence-Melamed",
            "title": {
                "fragments": [],
                "text": "A Word-to-Word Model of Translational Equivalence"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A fast algorithm for estimating a partial translation model, which accounts for translational equivalence only at the word level, that can automatically produce dictionary-sized translation lexicons and can do so with over 99% accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145223398"
                        ],
                        "name": "W. Martin",
                        "slug": "W.-Martin",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Martin",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Martin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2244184"
                        ],
                        "name": "Kenneth Ward Church",
                        "slug": "Kenneth-Ward-Church",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Church",
                            "middleNames": [
                                "Ward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth Ward Church"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34886242"
                        ],
                        "name": "R. Patil",
                        "slug": "R.-Patil",
                        "structuredName": {
                            "firstName": "Ramesh",
                            "lastName": "Patil",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Patil"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 14
                            }
                        ],
                        "text": "For instance, Martin et al. (1987) report their system giving 455 parses for the sentence in"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61083499,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "67b4de38535074e6f89b26d053cff5d4af8ce884",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We will trace a brief history of context-free parsing algorithms and then describe some representation issues. The purpose of this paper is to share our philosophy and experience in adapting a well-known context free parsing algorithm (Earley''s algorithm and variations thereof) to the parsing of a difficult and wide ranging corpus of sentences. The sentences were gathered by Malhotra in an experiment which fooled businessmen users into thinking they were interacting with Malhotra in another room. The Malhotra corpus is considerably more difficult than a second collection published by the LADDER Group. Both collections are given in the appendices. Section 4 compares empirical results obtained from these collections against theoretical predictions."
            },
            "slug": "PRELIMINARY-ANALYSIS-OF-A-BREADTH-FIRST-PARSING-AND-Martin-Church",
            "title": {
                "fragments": [],
                "text": "PRELIMINARY ANALYSIS OF A BREADTH-FIRST PARSING ALGORITHM: THEORETICAL AND EXPERIMENTAL RESULTS"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "The purpose of this paper is to share philosophy and experience in adapting a well-known context free parsing algorithm (Earley''s algorithm and variations thereof) to the parsing of a difficult and wide ranging corpus of sentences."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145366908"
                        ],
                        "name": "Fernando C Pereira",
                        "slug": "Fernando-C-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando C Pereira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725500"
                        ],
                        "name": "Yves Schabes",
                        "slug": "Yves-Schabes",
                        "structuredName": {
                            "firstName": "Yves",
                            "lastName": "Schabes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yves Schabes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 36
                            }
                        ],
                        "text": "Papers that use clustering include (Pereira et al. 1993; Zernik 199lb; Dolan 1994; Pedersen and Bruce 1997; Chen and Chang 1998). Pereira et al, (1993) cluster contexts of words in a way similar to Schutze (1998), but based on a different formalization of clustering."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 582,
                                "start": 36
                            }
                        ],
                        "text": "Papers that use clustering include (Pereira et al. 1993; Zernik 199lb; Dolan 1994; Pedersen and Bruce 1997; Chen and Chang 1998). Pereira et al, (1993) cluster contexts of words in a way similar to Schutze (1998), but based on a different formalization of clustering. They do not directly describe a disambiguation algorithm based on the clustering result, but since in this type of unsupervised method assignment to clusters is equivalent to disambiguation, this would be a straightforward extension. See section 14.1.4 for the clustering algorithm they use. Chen and Chang (1998) and Dolan (1994) are concerned with constructing representations for senses by combining several subsenses into one \u2018supersense."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 696805,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0c0eab87d4855c42ae6395bf2e27eefe55003b4a",
            "isKey": true,
            "numCitedBy": 345,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "The inside-outside algorithm for inferring the parameters of a stochastic context-free grammar is extended to take advantage of constituent information in a partially parsed corpus. Experiments on formal and natural language parsed corpora show that the new algorithm can achieve faster convergence and better modelling of hierarchical structure than the original one. In particular, over 90% of the constituents in the most likely analyses of a test set are compatible with test set constituents for a grammar trained on a corpus of 700 hand-parsed part-of-speech strings for ATIS sentences."
            },
            "slug": "Inside-Outside-Reestimation-From-Partially-Corpora-Pereira-Schabes",
            "title": {
                "fragments": [],
                "text": "Inside-Outside Reestimation From Partially Bracketed Corpora"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "The inside-outside algorithm for inferring the parameters of a stochastic context-free grammar is extended to take advantage of constituent information in a partially parsed corpus to achieve faster convergence and better modelling of hierarchical structure than the original one."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145421878"
                        ],
                        "name": "R. Sproat",
                        "slug": "R.-Sproat",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sproat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sproat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145115135"
                        ],
                        "name": "C. Shih",
                        "slug": "C.-Shih",
                        "structuredName": {
                            "firstName": "Chilin",
                            "lastName": "Shih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Shih"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34938639"
                        ],
                        "name": "W. Gale",
                        "slug": "W.-Gale",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Gale",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Gale"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145375772"
                        ],
                        "name": "Nancy Chang",
                        "slug": "Nancy-Chang",
                        "structuredName": {
                            "firstName": "Nancy",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nancy Chang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5651543,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6945192dcb9de0ff618b13510a1593e90c3242cb",
            "isKey": false,
            "numCitedBy": 314,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "The initial stage of text analysis for any NLP task usually involves the tokenization of the input into words. For languages like English one can assume, to a first approximation, that word boundaries are given by whitespace or punctuation. In various Asian languages, including Chinese, on the other hand, whitespace is never used to delimit words, so one must resort to lexical information to \"reconstruct\" the word-boundary information. In this paper we present a stochastic finite-state model wherein the basic workhorse is the weighted finite-state transducer. The model segments Chinese text into dictionary entries and words derived by various productive lexical processes, and---since the primary intended application of this model is to text-to-speech synthesis---provides pronunciations for these words. We evaluate the system's performance by comparing its segmentation \"judgments\" with the judgements of a pool of human segmenters, and the system is shown to perform quite well."
            },
            "slug": "A-Stochastic-Finite-State-Word-Segmentation-for-Sproat-Shih",
            "title": {
                "fragments": [],
                "text": "A Stochastic Finite-State Word-Segmentation Algorithm for Chinese"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper presents a stochastic finite-state model wherein the basic workhorse is the weighted finite- state transducer and the model segments Chinese text into dictionary entries and words derived by various productive lexical processes, and provides pronunciations for these words."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145903504"
                        ],
                        "name": "R. Rosenfeld",
                        "slug": "R.-Rosenfeld",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rosenfeld"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13110923,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "076fa8d095c37c657f2aff39cf90bc2ea883b7cb",
            "isKey": false,
            "numCitedBy": 608,
            "numCiting": 94,
            "paperAbstract": {
                "fragments": [],
                "text": "An adaptive statistical language model is described, which successfully integrates long distance linguistic information with other knowledge sources. Most existing statistical language models exploit only the immediate history of a text. To extract information from further back in the document's history, we propose and usetrigger pairsas the basic information bearing elements. This allows the model to adapt its expectations to the topic of discourse. Next, statistical evidence from multiple sources must be combined. Traditionally, linear interpolation and its variants have been used, but these are shown here to be seriously deficient. Instead, we apply the principle of Maximum Entropy (ME). Each information source gives rise to a set of constraints, to be imposed on the combined estimate. The intersection of these constraints is the set of probability functions which are consistent with all the information sources. The function with the highest entropy within that set is the ME solution. Given consistent statistical evidence, a unique ME solution is guaranteed to exist, and an iterative algorithm exists which is guaranteed to converge to it. The ME framework is extremely general: any phenomenon that can be described in terms of statistics of the text can be readily incorporated. An adaptive language model based on the ME approach was trained on theWall Street Journalcorpus, and showed a 32\u201339% perplexity reduction over the baseline. When interfaced to SPHINX-II, Carnegie Mellon's speech recognizer, it reduced its error rate by 10\u201314%. This thus illustrates the feasibility of incorporating many diverse knowledge sources in a single, unified statistical framework."
            },
            "slug": "A-maximum-entropy-approach-to-adaptive-statistical-Rosenfeld",
            "title": {
                "fragments": [],
                "text": "A maximum entropy approach to adaptive statistical language modelling"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "An adaptive statistical language model is described, which successfully integrates long distance linguistic information with other knowledge sources, and shows the feasibility of incorporating many diverse knowledge sources in a single, unified statistical framework."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Speech Lang."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34780795"
                        ],
                        "name": "D. Walker",
                        "slug": "D.-Walker",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Walker",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Walker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 60169934,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b884c8982258c0c20a86cccdbc30a03b44dcd2e8",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper provides an overview of a research program just being defined at Bellcore. The objective is to develop facilities for working with large document collections that provide more refined access to the information contained in these \"source\" materials than is possible through current information retrieval procedures. The tools being used for this purpose are machine-readable dictionaries, encyclopedias, and related \"resources\" that provide geographical, biographical, and other kinds of specialized knowledge. A major feature of the research program is the exploitation of the reciprocal relationships between sources and resources. These interactions between texts and tools are intended to support experts who organize and use information in a workstation environment. Two systems under development will be described to illustrate the approach: one providing capabilities for full-text subject assessment; the other for concept elaboration while reading text. Progress in the research depends critically on developments in artificial intelligence, computational linguistics, and information science to provide a scientific base, and on software engineering, database management, and distributed systems to provide the technology."
            },
            "slug": "Knowledge-Resource-Tools-for-Accessing-Large-Text-Walker",
            "title": {
                "fragments": [],
                "text": "Knowledge Resource Tools for Accessing Large Text Files"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "An overview of a research program just being defined at Bellcore to develop facilities for working with large document collections that provide more refined access to the information contained in these \"source\" materials than is possible through current information retrieval procedures."
            },
            "venue": {
                "fragments": [],
                "text": "TMI"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797808"
                        ],
                        "name": "G. Salton",
                        "slug": "G.-Salton",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Salton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Salton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144009691"
                        ],
                        "name": "C. Buckley",
                        "slug": "C.-Buckley",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Buckley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Buckley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 39937518,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "28d9f9cb94b1295fb2f03103a0c4d2eb72298445",
            "isKey": false,
            "numCitedBy": 93,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "An approach is outlined for the retrieval of natural language texts in response to available search requests and for the recognition of content similarities between text excerpts. The proposed retrieval process is based on flexible text matching procedures carried out in a number of different text environments and is applicable to large text collections covering unrestricted subject matter. For unrestricted text environments this system appears to outperform other currently available methods."
            },
            "slug": "Global-Text-Matching-for-Information-Retrieval-Salton-Buckley",
            "title": {
                "fragments": [],
                "text": "Global Text Matching for Information Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "An approach is outlined for the retrieval of natural language texts in response to available search requests and for the recognition of content similarities between text excerpts that appears to outperform other currently available methods."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751188"
                        ],
                        "name": "Atro Voutilainen",
                        "slug": "Atro-Voutilainen",
                        "structuredName": {
                            "firstName": "Atro",
                            "lastName": "Voutilainen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Atro Voutilainen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6333924,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "c974530e33a22086e46df3f92d95f648a0800b2b",
            "isKey": false,
            "numCitedBy": 109,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "There are two main methodologies for constructing the knowledge base of a natural language analyser: the linguistic and the data-driven. Recent state-of-the-art part-of-speech taggers are based on the data-driven approach. Because of the known feasibility of the linguistic rule-based approach at related levels of description, the success of the data-driven approach in part-of-speech analysis may appear surprising. In this paper, a case is made for the syntactic nature of part-of-speech tagging. A new tagger of English that uses only linguistic distributional rules is outlined and empirically evaluated. Tested against a benchmark corpus of 38,000 words of previously unseen text, this syntax-based system reaches an accuracy of above 99%. Compared to the 95--97% accuracy of its best competitors, this result suggests the feasibility of the linguistic approach also in part-of-speech analysis."
            },
            "slug": "A-syntax-based-part-of-speech-analyser-Voutilainen",
            "title": {
                "fragments": [],
                "text": "A syntax-based part-of-speech analyser"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A new tagger of English that uses only linguistic distributional rules is outlined and empirically evaluated, and it is suggested that the feasibility of the linguistic approach also in part-of-speech analysis is suggested."
            },
            "venue": {
                "fragments": [],
                "text": "EACL"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2257725"
                        ],
                        "name": "M. Webster",
                        "slug": "M.-Webster",
                        "structuredName": {
                            "firstName": "Mort",
                            "lastName": "Webster",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Webster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734174"
                        ],
                        "name": "M. Marcus",
                        "slug": "M.-Marcus",
                        "structuredName": {
                            "firstName": "Mitchell",
                            "lastName": "Marcus",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marcus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11377623,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "6e986f302a0b0b0fd700c89b94ec54585c5e45a7",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a computational model of verb acquisition which uses what we will call the principle of structured overcommitment to eliminate the need for negative evidence. The learner escapes from the need to be told that certain possibilities cannot occur (i.e., are \"ungrammatical\") by one simple expedient: It assumes that all properties it has observed are either obligatory or forbidden until it sees otherwise, at which point it decides that what it thought was either obligatory or forbidden is merely optional. This model is built upon a classification of verbs based upon a simple three-valued set of features which represents key aspects of a verb's syntactic structure, its predicate/argument structure, and the mapping between them."
            },
            "slug": "Automatic-Acquisition-of-the-Lexical-Semantics-of-Webster-Marcus",
            "title": {
                "fragments": [],
                "text": "Automatic Acquisition of the Lexical Semantics of Verbs from Sentence Frames"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A computational model of verb acquisition which uses the principle of structured overcommitment to eliminate the need for negative evidence and is built upon a classification of verbs based upon a simple three-valued set of features."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2393013"
                        ],
                        "name": "I. Sag",
                        "slug": "I.-Sag",
                        "structuredName": {
                            "firstName": "Ivan",
                            "lastName": "Sag",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Sag"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144741427"
                        ],
                        "name": "C. Pollard",
                        "slug": "C.-Pollard",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Pollard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Pollard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14500645,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "3cee61e41731adb3356233aed2a9b33c2280683b",
            "isKey": false,
            "numCitedBy": 4233,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "This book presents the most complete exposition of the theory of head-driven phrase structure grammar (HPSG), introduced in the authors' \"Information-Based Syntax and Semantics.\" HPSG provides an integration of key ideas from the various disciplines of cognitive science, drawing on results from diverse approaches to syntactic theory, situation semantics, data type theory, and knowledge representation. The result is a conception of grammar as a set of declarative and order-independent constraints, a conception well suited to modelling human language processing. This self-contained volume demonstrates the applicability of the HPSG approach to a wide range of empirical problems, including a number which have occupied center-stage within syntactic theory for well over twenty years: the control of \"understood\" subjects, long-distance dependencies conventionally treated in terms of \"wh\"-movement, and syntactic constraints on the relationship between various kinds of pronouns and their antecedents. The authors make clear how their approach compares with and improves upon approaches undertaken in other frameworks, including in particular the government-binding theory of Noam Chomsky."
            },
            "slug": "Head-driven-phrase-structure-grammar-Sag-Pollard",
            "title": {
                "fragments": [],
                "text": "Head-driven phrase structure grammar"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "This book presents the most complete exposition of the theory of head-driven phrase structure grammar, introduced in the authors' \"Information-Based Syntax and Semantics,\" and demonstrates the applicability of the HPSG approach to a wide range of empirical problems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1644031035"
                        ],
                        "name": "SmadjaFrank",
                        "slug": "SmadjaFrank",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "SmadjaFrank",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "SmadjaFrank"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 215943452,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "a1d430d43aef1e3c292128d197fb547e819428d4",
            "isKey": false,
            "numCitedBy": 254,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Natural languages are full of collocations, recurrent combinations of words that co-occur more often than expected by chance and that correspond to arbitrary word usages. Recent work in lexicograph..."
            },
            "slug": "Retrieving-collocations-from-text-SmadjaFrank",
            "title": {
                "fragments": [],
                "text": "Retrieving collocations from text"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Natural languages are full of collocations, recurrent combinations of words that co-occur more often than expected by chance and that correspond to arbitrary word usages."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "68994357"
                        ],
                        "name": "Hinrich Schfitze",
                        "slug": "Hinrich-Schfitze",
                        "structuredName": {
                            "firstName": "Hinrich",
                            "lastName": "Schfitze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hinrich Schfitze"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 19019672,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "701d176a76d7337d0682c09011ea02bb12b5716b",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Tile representation of documents and queries as vectors in space is a well-known information retrieval paradigm (Salton and McGill, 1983). This paper suggests that the context or topic at a given point in a text can also be represented as a vector. A procedure for computing context vectors is introduced and applied to disambiguating ten English words with success rates between 89% and 95%. The structure of context space is analyzed. The paper argues that vectors with their potential for gradedness may be superior for some purposes to other representational schemes. Representing contexts as vectors Recently, there has been a lot of interest in measures of semantic relatedness such as mutual information (Church and Hanks, 1989). The main reason seems to be that for many tasks in language processing a rough measure of semantic similarity and association is needed. In this paper a new representational scheme is introduced that tries to provide a basis for determining closeness in meaning. The approach is motivated by work oil vector representations in information retrieval. In IR systems such as SMART and SIRE documents and queries are represented as vectors in term space (Salton and McGill, 1983). The assumption that two documents are similar to the extent that they contain the same words. An obvious extension of this methodology to the representation of contexts is to assign to each context the set of words that occur in close proximity, say in a window of fifty words. However, the same content can be expressed with very different words, so that in this simple scheme two contexts could have a similarity measure of 0 although they should be very close. The problem is that the absence or presence of a given word is very little information if we treat words as unanalyzed symbols or indices in term vectors. The lexical representations used for comparing contexts have to be enriched. The approach adopted here is to equate words with their patterns of usage in a large text corpus. Figure 1 shows how this can be done. The terms cash and sport are the dimensions of the space in which similarity is to be measured. The columns of the matrix represent the words bank, interest, and finals. Each entry in the matrix is a cooccurrence count. For instance, acash, bank = 300 encodes the fact that the words cash and bank cooccur 300 times in the corpus. Cooccurrence can be defined with respect to windows of a given size or on the basis of sentence boundaries. In information retrieval, the cosine function is one of the similarity measures used: COS(WORDi, WORDj) = ~=l(ak\u2019iak\u2019J) ~ n a 2 X-,n _2 k=l k,i /--~k=l t*k,j Applied to the three word vectors in Figure 1, we can compute the following correlation measures: cos(bank, interest) = 0.94, cos(interest, finals) = 0.92, cos(bank, finals) 0.74. These numbers can beint erpreted geometrically as shown in Figure 2. Terms are axes, words are vectors whose components on the various dimensions are determined by the cooccurrence counts in the collocation matrix. Similarity between vectors has then a straightforward visual equivalent: Closeness in the multidimensional space corresponding to the collocation matrix. In Figure 2 bank and finals are not very close to each other, but both are close to the vector interest between them. Now we are in a position to compute a representation of context that is more reliable than the bag-of-words method criticized above: The centroid of the vectors in a context can be seen as an approximation of its semantic content. If at least some of the words in the context are frequently used to describe what the current context is about then they will pull the centroid toward the direction of that content. It is possible to describe a content exclusively using words that norJbank interest finals cash 300 210 133 sport 75 140 200 Figure 1: A collocation matrix. 113 From: AAAI Technical Report FS-92-04. Copyright \u00a9 1992, AAAI (www.aaai.org). All rights reserved."
            },
            "slug": "Context-Space-Schfitze",
            "title": {
                "fragments": [],
                "text": "Context Space"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The paper argues that vectors with their potential for gradedness may be superior for some purposes to other representational schemes, and tries to provide a basis for determining closeness in meaning in contexts."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516174"
                        ],
                        "name": "Stuart Pook",
                        "slug": "Stuart-Pook",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Pook",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stuart Pook"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144356748"
                        ],
                        "name": "J. Catlett",
                        "slug": "J.-Catlett",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Catlett",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Catlett"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 63551215,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3bf630a2f48ea36a52c05dd4f97353e1cba8441e",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "The trouble with text retrieva lu sing keywords is that English words are imprecise: there may be man yw ords for the entity the user w ants, and each of these may also have other (unintended) meanings. To get closer to the dream of 'retrie ve what I mean, not what I say,' s ystems need to tak ei nto account the sense in which each word is used in the text stored, and to get from the user a more precise and complete statement of the concept sought. Recent research on machine- readable dictionaries and thesauri may soon mak et his a reality. Published in the Online Information Conference, Sydney, A ustralia, 1988. This version printed 15 July 1998. 1. Intr oduction Large databases of te xt containing information such as scientific papers, lega lc ases, newspaper archive sa nd library catalogues have b een common for man yy ears. Such databases are typically too lar ge to allo wau ser to scan the entire collection of te xt in search of an interesting item. Fo rt his reason more practicable and faster methods of retrieving pieces of text have been devised. One of these methods is called keyword retrie val. In this method a user specifies a keyword (or perhaps a boolean combination of k eywords) to be used in retrie val. F or example a simple query such as bridge would retrieve all articles containing the w ord 'bridge'. A boolean combination such as juice and not (apple or orange) might be used to find articles that mention 'juice' but are not concerned with fruit juices. The standard method of te xt retrie va lb yk eyword is fraught with difficulties caused by the imprecise nature of words. Section 2e xplains and attempts to solv et hese problems using the information contained in machine-readable dictionaries and thesauri. Other methods of retrie va lf rom a lar ge database allo wt he user to retrie ve a rticles that concern topics of interest to the user .F or example a user might want all the articles that are concerned with physiology .P revious implementations of this method of text retrieval have r equired the text to be classified by subject before this process can be used. This classification normally has to be performed manually .S ection 3 give sa method by which text classification can be automated."
            },
            "slug": "Making-sense-out-of-searching-Pook-Catlett",
            "title": {
                "fragments": [],
                "text": "Making sense out of searching"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The standard method of text retrieval is fraught with difficulties caused by the imprecise nature of words, and attempts to solve these problems using the information contained in machine-readable dictionaries and thesauri are attempted."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144418438"
                        ],
                        "name": "Hinrich Sch\u00fctze",
                        "slug": "Hinrich-Sch\u00fctze",
                        "structuredName": {
                            "firstName": "Hinrich",
                            "lastName": "Sch\u00fctze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hinrich Sch\u00fctze"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15829786,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e569d99f3a0fcfa038631dda2b44c73a6e8e97b8",
            "isKey": false,
            "numCitedBy": 454,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The representation of documents and queries as vectors in a high-dimensional space is well-established in information retrieval. The author proposes that the semantics of words and contexts in a text be represented as vectors. The dimensions of the space are words and the initial vectors are determined by the words occurring close to the entity to be represented, which implies that the space has several thousand dimensions (words). This makes the vector representations (which are dense) too cumbersome to use directly. Therefore, dimensionality reduction by means of a singular value decomposition is employed. The author analyzes the structure of the vector representations and applies them to word sense disambiguation and thesaurus induction. >"
            },
            "slug": "Dimensions-of-meaning-Sch\u00fctze",
            "title": {
                "fragments": [],
                "text": "Dimensions of meaning"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The author analyzes the structure of the vector representations and applies them to word sense disambiguation and thesaurus induction and finds that dimensionality reduction by means of a singular value decomposition is employed."
            },
            "venue": {
                "fragments": [],
                "text": "Supercomputing '92"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797808"
                        ],
                        "name": "G. Salton",
                        "slug": "G.-Salton",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Salton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Salton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144890574"
                        ],
                        "name": "James Allan",
                        "slug": "James-Allan",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Allan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Allan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144009691"
                        ],
                        "name": "C. Buckley",
                        "slug": "C.-Buckley",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Buckley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Buckley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145163573"
                        ],
                        "name": "A. Singhal",
                        "slug": "A.-Singhal",
                        "structuredName": {
                            "firstName": "Amit",
                            "lastName": "Singhal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Singhal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 32296317,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f492477752b88be89ab897907b18e68c881e6498",
            "isKey": false,
            "numCitedBy": 327,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Vast amounts of text material are now available in machine-readable form for automatic processing. Here, approaches are outlined for manipulating and accessing texts in arbitrary subject areas in accordance with user needs. In particular, methods are given for determining text themes, traversing texts selectively, and extracting summary statements that reflect text content."
            },
            "slug": "Automatic-Analysis,-Theme-Generation,-and-of-Texts-Salton-Allan",
            "title": {
                "fragments": [],
                "text": "Automatic Analysis, Theme Generation, and Summarization of Machine-Readable Texts"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Methods are given for determining text themes, traversing texts selectively, and extracting summary statements that reflect text content in arbitrary subject areas in accordance with user needs."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2046487"
                        ],
                        "name": "Jeffrey C. Reynar",
                        "slug": "Jeffrey-C.-Reynar",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Reynar",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeffrey C. Reynar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793475"
                        ],
                        "name": "A. Ratnaparkhi",
                        "slug": "A.-Ratnaparkhi",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ratnaparkhi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ratnaparkhi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6204420,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6edceaf0fada3588ee5f036e944c1a00661df77a",
            "isKey": false,
            "numCitedBy": 477,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a trainable model for identifying sentence boundaries in raw text. Given a corpus annotated with sentence boundaries, our model learns to classify each occurrence of., ?, and ! as either a valid or invalid sentence boundary. The training procedure requires no hand-crafted rules, lexica, part-of-speech tags, or domain-specific information. The model can therefore be trained easily on any genre of English, and should be trainable on any other Romanalphabet language. Performance is comparable to or better than the performance of similar systems, but we emphasize the simplicity of retraining for new domains."
            },
            "slug": "A-Maximum-Entropy-Approach-to-Identifying-Sentence-Reynar-Ratnaparkhi",
            "title": {
                "fragments": [],
                "text": "A Maximum Entropy Approach to Identifying Sentence Boundaries"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A trainable model for identifying sentence boundaries in raw text that can be trained easily on any genre of English, and should be trainable on any other Romanalphabet language."
            },
            "venue": {
                "fragments": [],
                "text": "ANLP"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52461176"
                        ],
                        "name": "L.W.M. Bod",
                        "slug": "L.W.M.-Bod",
                        "structuredName": {
                            "firstName": "L.W.M.",
                            "lastName": "Bod",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L.W.M. Bod"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2563725"
                        ],
                        "name": "Steven Krauwer",
                        "slug": "Steven-Krauwer",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Krauwer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven Krauwer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2081727"
                        ],
                        "name": "R. Scha",
                        "slug": "R.-Scha",
                        "structuredName": {
                            "firstName": "Remko",
                            "lastName": "Scha",
                            "middleNames": [
                                "J.",
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Scha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3540477"
                        ],
                        "name": "K. Sima'an",
                        "slug": "K.-Sima'an",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Sima'an",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Sima'an"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 625,
                                "start": 79
                            }
                        ],
                        "text": "For example, people tend to rationalize non-rational economic decisions (Kahneman et al. 1982). The most frequently used methodology is to adopt the sense definitions in a dictionary and then to ask subjects to label instances in a corpus based on these definitions. There are different opinions on how well this technique works. Some researchers have reported high agreement between judges (Gale et al. 1992a) as we discussed above. High average agreement is likely if there are many ambiguous words with a skewed distribution, that is, one sense that is used in most of the occurrences. Sanderson and van Rijsbergen (1998) argue that such skewed distributions are typical of ambiguous words."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 104
                            }
                        ],
                        "text": "A few good ones, listed in approximate order of increasing difficulty are (Moore and McCabe 1989; Freedman et al. 1998; Siegel and Castellan 1988; DeGroot 1975). Krenn and Samuelsson (1997) is particularly recommended as a much more thorough introduction to statistics aimed at a Statistical"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9587615,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "61b0da0cf405ad84ed1fcf2cde7076516084d774",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "In Stochastic Tree Substitution Grammars (STSGs), one parse(tree) of an input sentence can be generated by exponentially many derivations ; the probability of a parse is deened as the sum of the probabilities of its derivations. As a result, some methods of Stochastic Context-Free Grammars (SCFGs), e.g. the Viterbi algorithm for nding the most probable parse (MPP) of an input sentence, are not applicable to STSGs. In this paper we study parsing with STSGs and concentrate on the problem of disambiguation. We present polynomial algorithms for computing both the probability of a parse and the probability of an input sentence and its most probable derivation. In addition, we present an optimization technique of search algorithms for the MPP."
            },
            "slug": "Efficient-Disambiguation-by-means-of-Stochastic-Bod-Krauwer",
            "title": {
                "fragments": [],
                "text": "Efficient Disambiguation by means of Stochastic Tree Substitution Grammars"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper presents polynomial algorithms for computing both the probabilities of a parse and the probability of an input sentence and its most probable derivation and an optimization technique of search algorithms for the MPP."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145421878"
                        ],
                        "name": "R. Sproat",
                        "slug": "R.-Sproat",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sproat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sproat"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 60329865,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "f309464d86ebdc0bd0c6808c8143624acf016df0",
            "isKey": false,
            "numCitedBy": 374,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Part 1 Applications of computational morphology: natural language applications speech applications word processing document retrieval. Part 2 The nature of morphology: functions of morphology what is combined, and how? morphemes, the structure of words, and word-formation rules morphotactics - the order of morphemes phonology psycholinguistic evidence. Part 3 Computational morphology: computational mechanisms an overview of URKIMMO augments to the KIMMO approach the computational complexity of two-level morphology other ways of doing computational morphology a prospectus - what is left to do. Part 4 Some peripheral issues: morphological acquisition compound nominals and related constructions."
            },
            "slug": "Morphology-and-computation-Sproat",
            "title": {
                "fragments": [],
                "text": "Morphology and computation"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "The nature of morphology, the structure of words, and word-formation rules, and the order of morphemes phonology psycholinguistic evidence are discussed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144866028"
                        ],
                        "name": "Michel Simard",
                        "slug": "Michel-Simard",
                        "structuredName": {
                            "firstName": "Michel",
                            "lastName": "Simard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michel Simard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2458308"
                        ],
                        "name": "George F. Foster",
                        "slug": "George-F.-Foster",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Foster",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "George F. Foster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2032503"
                        ],
                        "name": "P. Isabelle",
                        "slug": "P.-Isabelle",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Isabelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Isabelle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9234092,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "34d7f231fe2b9f243a60a4a64c06028ad7ba776b",
            "isKey": false,
            "numCitedBy": 384,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "In a recent paper, Gale and Church describe an inexpensive method for aligning bitext, based exclusively on sentence lengths [3]. While this method produces surprisingly good results (a success rate around 96%), even better results are required to perform such tasks as the computer-assisted revision of translations. In this paper, we examine some of the weaknesses of Gale and Church's program, and explain how just a small amount of linguistic knowledge would help to overcome these weaknesses. We discuss how cognates provide for a cheap and reasonably reliable source of linguistic knowledge. To illustrate this, we describe a modification to the program in which the criterion is cognates rather than sentence lengths. Finally, we show how better and more efficient results may be obtained by combining the two criteria length and \"cogneteness\". Our method can be generalized to accommodate other sources of linguistic knowledge, and experimentation shows that it produces better results than alignments based on length alone, at a minimal cost."
            },
            "slug": "Using-cognates-to-align-sentences-in-bilingual-Simard-Foster",
            "title": {
                "fragments": [],
                "text": "Using cognates to align sentences in bilingual corpora"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It is discussed how cognates provide for a cheap and reasonably reliable source of linguistic knowledge, and how better and more efficient results may be obtained by combining the two criteria length and \"cogneteness\"."
            },
            "venue": {
                "fragments": [],
                "text": "TMI"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2977225"
                        ],
                        "name": "N. Ostler",
                        "slug": "N.-Ostler",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Ostler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Ostler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145545744"
                        ],
                        "name": "B. Atkins",
                        "slug": "B.-Atkins",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Atkins",
                            "middleNames": [
                                "T.",
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Atkins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12921583,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "21bdd1d87d0d63e784321421dfbd44ca02f7e35b",
            "isKey": false,
            "numCitedBy": 141,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Drawing on a growing database of systematic relationships between word-senses, the authors argue that a significant class of these represent Lexical Implication Rules, a set of formal rules within the domain of lexical semantics; these they distinguish from other types of semantic relation more closely dependent on metaphor and world-knowledge. Some formal properties of Lexical Implication Rules are proposed, as evidence of their linguistic, rather than real-world, nature."
            },
            "slug": "Predictable-Meaning-Shift:-Some-Linguistic-of-Rules-Ostler-Atkins",
            "title": {
                "fragments": [],
                "text": "Predictable Meaning Shift: Some Linguistic Properties of Lexical Implication Rules"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "Drawing on a growing database of systematic relationships between word-senses, it is argued that a significant class of these represent Lexical Implication Rules, a set of formal rules within the domain of lexical semantics, which distinguish from other types of semantic relation more closely dependent on metaphor and world-knowledge."
            },
            "venue": {
                "fragments": [],
                "text": "SIGLEX Workshop"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725500"
                        ],
                        "name": "Yves Schabes",
                        "slug": "Yves-Schabes",
                        "structuredName": {
                            "firstName": "Yves",
                            "lastName": "Schabes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yves Schabes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5429505,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d607ed3aa8a1762e06988329aeb0c05b997023db",
            "isKey": false,
            "numCitedBy": 181,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "The notion of stochastic lexicalized tree-adjoining grammar (SLTAG) is formally defined. The parameters of a SLTAG correspond to the probability of combining two structures each one associated with a word. The characteristics of SLTAG are unique and novel since it is lexieally sensitive (as N-gram models or Hidden Markov Models) and yet hierarchical (as stochastic context-free grammars).Then, two basic algorithms for SLTAG arc introduced: an algorithm for computing the probability of a sentence generated by a SLTAG and an inside-outside-like iterative algorithm for estimating the parameters of a SLTAG given a training corpus.Finally, we should how SLTAG enables to define a lexicalized version of stochastic context-free grammars and we report preliminary experiments showing some of the advantages of SLTAG over stochastic context-free grammars."
            },
            "slug": "Stochastic-Lexicalized-Tree-adjoining-Grammars-Schabes",
            "title": {
                "fragments": [],
                "text": "Stochastic Lexicalized Tree-adjoining Grammars"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Preliminary experiments showing some of the advantages of SLTAG over stochastic context-free grammars are reported and an algorithm for computing the probability of a sentence generated by a SLTAG and an inside-outside-like iterative algorithm for estimating the parameters of a SL TAG are reported."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30158778"
                        ],
                        "name": "Tony C. Smith",
                        "slug": "Tony-C.-Smith",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Smith",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tony C. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752317"
                        ],
                        "name": "J. Cleary",
                        "slug": "J.-Cleary",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Cleary",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cleary"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7455133,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5bef94f58409f8c023d45e942f1d38cbf3d2e14a",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent research has shown that unification grammars can be adapted to incorporate statistical information, thus preserving the processing benefits of stochastic context-free grammars while offering an efficient mechanism for handling dependencies. While complexity studies show that a probabilistic unification grammar achieves an appropriately lower entropy estimate than an equivalent PCFG, the problem of parameter estimation prevents results from reflecting the empirical distribution. This paper describes how a PUG can be implemented as a Prolog DCG annotated with weights, and how the weights can be interpretted to give accurate entropy estimates. An algorithm for learning correct weights is provided, along with results from some complexity analyses."
            },
            "slug": "Probabilistic-Unification-Grammars-Smith-Cleary",
            "title": {
                "fragments": [],
                "text": "Probabilistic Unification Grammars"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper describes how a PUG can be implemented as a Prolog DCG annotated with weights, and how the weights can be interpretted to give accurate entropy estimates."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2390150"
                        ],
                        "name": "Dekai Wu",
                        "slug": "Dekai-Wu",
                        "structuredName": {
                            "firstName": "Dekai",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dekai Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2250117"
                        ],
                        "name": "Hongsing Wong",
                        "slug": "Hongsing-Wong",
                        "structuredName": {
                            "firstName": "Hongsing",
                            "lastName": "Wong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hongsing Wong"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8360657,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b2e03d014ac0bf8df47821a2a3e10015c87ceda5",
            "isKey": false,
            "numCitedBy": 103,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a stochastic grammatical channel model for machine translation, that synthesizes several desirable characteristics of both statistical and grammatical machine translation. As with the pure statistical translation model described by Wu (1996) (in which a bracketing transduction grammar models the channel), alternative hypotheses compete probabilistically, exhaustive search of the translation hypothesis space can be performed in polynomial time, and robustness heuristics arise naturally from a language-independent inversion-transduction model. However, unlike pure statistical translation models, the generated output string is guaranteed to conform to a given target grammar. The model employs only (1) a translation lexicon, (2) a context-free grammar for the target language, and (3) a bigram language model. The fact that no explicit bilingual translation rules are used makes the model easily portable to a variety of source languages. Initial experiments show that it also achieves significant speed gains over our earlier model."
            },
            "slug": "Machine-Translation-with-a-Stochastic-Grammatical-Wu-Wong",
            "title": {
                "fragments": [],
                "text": "Machine Translation with a Stochastic Grammatical Channel"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "A stochastic grammatical channel model for machine translation, that synthesizes several desirable characteristics of both statistical and grammatical machine translation and achieves significant speed gains over the earlier model."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804836"
                        ],
                        "name": "G. Towell",
                        "slug": "G.-Towell",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Towell",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Towell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746656"
                        ],
                        "name": "E. Voorhees",
                        "slug": "E.-Voorhees",
                        "structuredName": {
                            "firstName": "Ellen",
                            "lastName": "Voorhees",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Voorhees"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10092362,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ac832c85c54c89a2751c15ef7e02b3f976bcd72d",
            "isKey": false,
            "numCitedBy": 100,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "A word sense disambiguator that is able to distinguish among the many senses of common words that are found in general-purpose, broad-coverage lexicons would be useful. For example, experiments have shown that, given accurate sense disambiguation, the lexical relations encoded in lexicons such as WordNet can be exploited to improve the effectiveness of information retrieval systems. This paper describes a classifier whose accuracy may be sufficient for such a purpose. The classifier combines the output of a neural network that learns topical context with the output of a network that learns local context to distinguish among the senses of highly ambiguous words.The accuracy of the classifier is tested on three words, the noun line, the verb serve, and the adjective hard; the classifier has an average accuracy of 87%, 90%, and 81%, respectively, when forced to choose a sense for all test cases. When the classifier is not forced to choose a sense and is trained on a subset of the available senses, it rejects test cases containing unknown senses as well as test cases it would misclassify if forced to select a sense. Finally, when there are few labeled training examples available, we describe an extension of our training method that uses information extracted from unlabeled examples to improve classification accuracy."
            },
            "slug": "Disambiguating-Highly-Ambiguous-Words-Towell-Voorhees",
            "title": {
                "fragments": [],
                "text": "Disambiguating Highly Ambiguous Words"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A classifier whose accuracy may be sufficient for word sense disambiguation and an extension of the training method that uses information extracted from unlabeled examples to improve classification accuracy when there are few labeled training examples available."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38612830"
                        ],
                        "name": "W. Stolz",
                        "slug": "W.-Stolz",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Stolz",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Stolz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1937895"
                        ],
                        "name": "P. Tannenbaum",
                        "slug": "P.-Tannenbaum",
                        "structuredName": {
                            "firstName": "Percy",
                            "lastName": "Tannenbaum",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Tannenbaum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "19181292"
                        ],
                        "name": "F. Carstensen",
                        "slug": "F.-Carstensen",
                        "structuredName": {
                            "firstName": "Fred",
                            "lastName": "Carstensen",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Carstensen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17615653,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a36fa0d28014453d51092eb251815e5f4089a4d4",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "WALTER S. STOLZ, PERCY H. TANNENBAUM AND t?I(EDERICK V. CARSTENSEN U~ive'rsity of Wisconsin, Madison, Wisconsin words to be encountered during tile text processing. More recently, a straight dietionalT approach has been supplemented througtl the use of computational decision procedures. The present paper reports on one such computational system, WISSYN, in which decisions about how to code certain words are based on conditional probabilities of various form classes occurring in given syntactic environments."
            },
            "slug": "Stochastic-approach-to-the-grammatical-coding-of-Stolz-Tannenbaum",
            "title": {
                "fragments": [],
                "text": "Stochastic approach to the grammatical coding of english"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The present paper reports on one such computational system, WISSYN, in which decisions about how to code certain words are based on conditional probabilities of various form classes occurring in given syntactic environments."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686820"
                        ],
                        "name": "B. M\u00e9rialdo",
                        "slug": "B.-M\u00e9rialdo",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "M\u00e9rialdo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. M\u00e9rialdo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2727455,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4614650c3bb3e835c80612d3bca9586f81db95a3",
            "isKey": false,
            "numCitedBy": 628,
            "numCiting": 95,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present some experiments on the use of a probabilistic model to tag English text, i.e. to assign to each word the correct tag (part of speech) in the context of the sentence. The main novelty of these experiments is the use of untagged text in the training of the model. We have used a simple triclass Markov model and are looking for the best way to estimate the parameters of this model, depending on the kind and amount of training data provided. Two approaches in particular are compared and combined:using text that has been tagged by hand and computing relative frequency counts,using text without tags and training the model as a hidden Markov process, according to a Maximum Likelihood principle.Experminents show that the best training is obtained by using as much tagged text as possible. They also show that Maximum Likelihood training, the procedure that is routinely used to estimate hidden Markov models parameters from training data, will not necessarily improve the tagging accuracy. In fact, it will generally degrade this accuracy, except when only a limited amount of hand-tagged text is available."
            },
            "slug": "Tagging-English-Text-with-a-Probabilistic-Model-M\u00e9rialdo",
            "title": {
                "fragments": [],
                "text": "Tagging English Text with a Probabilistic Model"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Experminents show that the best training is obtained by using as much tagged text as possible, and show that Maximum Likelihood training, the procedure that is routinely used to estimate hidden Markov models parameters from training data, will not necessarily improve the tagging accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3316623"
                        ],
                        "name": "Jakub Zavrel",
                        "slug": "Jakub-Zavrel",
                        "structuredName": {
                            "firstName": "Jakub",
                            "lastName": "Zavrel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jakub Zavrel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735272"
                        ],
                        "name": "Walter Daelemans",
                        "slug": "Walter-Daelemans",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Daelemans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Walter Daelemans"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 217,
                                "start": 107
                            }
                        ],
                        "text": "Much other work has used various other features, in particular the identity of the head noun inside the PP (Resnik and Hearst 1993; Brill and Resnik 1994; Ratnaparkhi et al. 1994; Zavrel et al. 1997; Ratnaparkhi 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1138221,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "41c00f7fbec0ab603836357d020cef7843ef7405",
            "isKey": false,
            "numCitedBy": 88,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper analyses the relation between the use of similarity in Memory-Based Learning and the notion of backed-off smoothing in statistical language modeling. We show that the two approaches are closely related, and we argue that feature weighting methods in the Memory-Based paradigm can offer the advantage of automatically specifying a suitable domain-specific hierarchy between most specific and most general conditioning information without the need for a large number of parameters. We report two applications of this approach: PP-attachment and POS-tagging. Our method achieves state-of-the-art performance in both domains, and allows the easy integration of diverse information sources, such as rich lexical representations."
            },
            "slug": "Memory-Based-Learning:-Using-Similarity-for-Zavrel-Daelemans",
            "title": {
                "fragments": [],
                "text": "Memory-Based Learning: Using Similarity for Smoothing"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is argued that feature weighting methods in the Memory-Based paradigm can offer the advantage of automatically specifying a suitable domain-specific hierarchy between most specific and most general conditioning information without the need for a large number of parameters."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144418438"
                        ],
                        "name": "Hinrich Sch\u00fctze",
                        "slug": "Hinrich-Sch\u00fctze",
                        "structuredName": {
                            "firstName": "Hinrich",
                            "lastName": "Sch\u00fctze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hinrich Sch\u00fctze"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34165212"
                        ],
                        "name": "Jan O. Pedersen",
                        "slug": "Jan-O.-Pedersen",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Pedersen",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jan O. Pedersen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 29249810,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "244ac8d99def8e6238f318e5a4cdcec8023970e1",
            "isKey": false,
            "numCitedBy": 327,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Cooccurrence-Based-Thesaurus-and-Two-Applications-Sch\u00fctze-Pedersen",
            "title": {
                "fragments": [],
                "text": "A Cooccurrence-Based Thesaurus and Two Applications to Information Retrieval"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Process. Manag."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734174"
                        ],
                        "name": "M. Marcus",
                        "slug": "M.-Marcus",
                        "structuredName": {
                            "firstName": "Mitchell",
                            "lastName": "Marcus",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marcus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116073558"
                        ],
                        "name": "Grace Kim",
                        "slug": "Grace-Kim",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Grace Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2063206"
                        ],
                        "name": "Mary Ann Marcinkiewicz",
                        "slug": "Mary-Ann-Marcinkiewicz",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "Marcinkiewicz",
                            "middleNames": [
                                "Ann"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mary Ann Marcinkiewicz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33353168"
                        ],
                        "name": "R. MacIntyre",
                        "slug": "R.-MacIntyre",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "MacIntyre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. MacIntyre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3212973"
                        ],
                        "name": "Ann Bies",
                        "slug": "Ann-Bies",
                        "structuredName": {
                            "firstName": "Ann",
                            "lastName": "Bies",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ann Bies"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054785529"
                        ],
                        "name": "Mark Ferguson",
                        "slug": "Mark-Ferguson",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Ferguson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Ferguson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065702818"
                        ],
                        "name": "Karen Katz",
                        "slug": "Karen-Katz",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Katz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karen Katz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2537676"
                        ],
                        "name": "Britta Schasberger",
                        "slug": "Britta-Schasberger",
                        "structuredName": {
                            "firstName": "Britta",
                            "lastName": "Schasberger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Britta Schasberger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5151364,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "59ce9cdbde13affc05a6c1f48a51ee7b0fcb154b",
            "isKey": false,
            "numCitedBy": 897,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "The Penn Treebank has recently implemented a new syntactic annotation scheme, designed to highlight aspects of predicate-argument structure. This paper discusses the implementation of crucial aspects of this new annotation scheme. It incorporates a more consistent treatment of a wide range of grammatical phenomena, provides a set of coindexed null elements in what can be thought of as \"underlying\" position for phenomena such as wh-movement, passive, and the subjects of infinitival constructions, provides some non-context free annotational mechanism to allow the structure of discontinuous constituents to be easily recovered, and allows for a clear, concise tagging system for some semantic roles."
            },
            "slug": "The-Penn-Treebank:-Annotating-Predicate-Argument-Marcus-Kim",
            "title": {
                "fragments": [],
                "text": "The Penn Treebank: Annotating Predicate Argument Structure"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "The implementation of crucial aspects of this new syntactic annotation scheme incorporates a more consistent treatment of a wide range of grammatical phenomena, provides a set of coindexed null elements in what can be thought of as \"underlying\" position for phenomena such as wh-movement, passive, and the subjects of infinitival constructions."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145366908"
                        ],
                        "name": "Fernando C Pereira",
                        "slug": "Fernando-C-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando C Pereira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777660"
                        ],
                        "name": "Naftali Tishby",
                        "slug": "Naftali-Tishby",
                        "structuredName": {
                            "firstName": "Naftali",
                            "lastName": "Tishby",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Naftali Tishby"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145810617"
                        ],
                        "name": "Lillian Lee",
                        "slug": "Lillian-Lee",
                        "structuredName": {
                            "firstName": "Lillian",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lillian Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 35
                            }
                        ],
                        "text": "Papers that use clustering include (Pereira et al. 1993; Zernik Dolan 1994; Pedersen and Bruce 1997; Chen and Chang 1998)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6713452,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5eb328cf7e94995199e4c82a1f4d0696430a80b5",
            "isKey": false,
            "numCitedBy": 1193,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe and evaluate experimentally a method for clustering words according to their distribution in particular syntactic contexts. Words are represented by the relative frequency distributions of contexts in which they appear, and relative entropy between those distributions is used as the similarity measure for clustering. Clusters are represented by average context distributions derived from the given words according to their probabilities of cluster membership. In many cases, the clusters can be thought of as encoding coarse sense distinctions. Deterministic annealing is used to find lowest distortion sets of clusters: as the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical \"soft\" clustering of the data. Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data."
            },
            "slug": "Distributional-Clustering-of-English-Words-Pereira-Tishby",
            "title": {
                "fragments": [],
                "text": "Distributional Clustering of English Words"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Deterministic annealing is used to find lowest distortion sets of clusters: as the annealed parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical \"soft\" clustering of the data."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144421407"
                        ],
                        "name": "P. Sheridan",
                        "slug": "P.-Sheridan",
                        "structuredName": {
                            "firstName": "P\u00e1raic",
                            "lastName": "Sheridan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Sheridan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1994620"
                        ],
                        "name": "M. Wechsler",
                        "slug": "M.-Wechsler",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Wechsler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Wechsler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2133932"
                        ],
                        "name": "P. Sch\u00e4uble",
                        "slug": "P.-Sch\u00e4uble",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Sch\u00e4uble",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Sch\u00e4uble"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14300080,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bf9d3fb159a852176c922fce5248e72de7ba3752",
            "isKey": false,
            "numCitedBy": 57,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "Pkic Sheridan, Martin Wechsler, Peter Schauble Swiss Federal Institute of Technology (ETH) CH-8092 Zurich, Switzerland We present here the realisation of a cross-language speech retrieval system which retrieves German speech documents in response to user queries specified as Prench text. This has been achieved through the integration of two existing modules of the SPIDER information retrieval system, namely the query pseudo-translation module and the speech retrieval module. Our approach to cross-language retrieval uses an automatically constructed corpus-based information structure called a simihwity thesaurus. A similarity thesaurus can be constructed over any loosely comparable corpus a parallel corpus is not necessary. The similarity thesaurus used here was constructed over a 330 MByte corpus of comparable German and Fkench news stones. Our speech retrieval module is based on a speaker-independent phoneme recognize and it indexes speech documents by N-grams of phonemic features. The speech retrieval module includes an additional probabilistic matching technique designed to aid retrieval from erroneous data such as the phonemic output of the speech recognition process. We have evaluated our cross-language speech retrieval system over a collection of 30 hours (3.4 GBytes) of German speech, comparing the effectiveness of l%nch queries (cross-language) against performance on equivalent German queries (mon~lingual). It must be stressed that this work represents our first step in the direction of cross-language speech retrieval. Our aim here is to establish a bmefine of performance on this task, against which we can then measure the success of our continuing research in this area."
            },
            "slug": "Cross-language-speech-retrieval:-establishing-a-Sheridan-Wechsler",
            "title": {
                "fragments": [],
                "text": "Cross-language speech retrieval: establishing a baseline performance"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The realisation of a cross-language speech retrieval system which retrieves German speech documents in response to user queries specified as Prench text through the integration of two existing modules of the SPIDER information retrieval system."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '97"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3127759"
                        ],
                        "name": "B. Masand",
                        "slug": "B.-Masand",
                        "structuredName": {
                            "firstName": "Brij",
                            "lastName": "Masand",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Masand"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2132345"
                        ],
                        "name": "G. Linoff",
                        "slug": "G.-Linoff",
                        "structuredName": {
                            "firstName": "Gordon",
                            "lastName": "Linoff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Linoff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788375"
                        ],
                        "name": "D. Waltz",
                        "slug": "D.-Waltz",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Waltz",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Waltz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7048166,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1240054ed60e8e42de9683947d21bd76582a281d",
            "isKey": false,
            "numCitedBy": 325,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a method for classifying news stories using Memory Based Reasoning (MBR) a k-nearest neighbor method), that does not require manual topic definitions. Using an already coded training database of about 50,000 stories from the Dow Jones Press Release News Wire, and SEEKER [Stanfill] (a text retrieval system that supports relevance feedback) as the underlying match engine, codes are assigned to new, unseen stories with a recall of about 80% and precision of about 70%. There are about 350 different codes to be assigned. Using a massively parallel supercomputer, we leverage the information already contained in the thousands of coded stories and are able to code a story in about 2 seconds. Given SEEKER, the text retrieval system, we achieved these results in about two person-months. We believe this approach is effective in reducing the development time to implement classification systems involving large number of topics for the purpose of classification, message routing etc."
            },
            "slug": "Classifying-news-stories-using-memory-based-Masand-Linoff",
            "title": {
                "fragments": [],
                "text": "Classifying news stories using memory based reasoning"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "A method for classifying news stories using Memory Based Reasoning (MBR) a k-nearest neighbor method, that does not require manual topic definitions, that is effective in reducing the development time to implement classification systems involving large number of topics for the purpose of classification, message routing etc."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '92"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797808"
                        ],
                        "name": "G. Salton",
                        "slug": "G.-Salton",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Salton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Salton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144912302"
                        ],
                        "name": "R. W. Thorpe",
                        "slug": "R.-W.-Thorpe",
                        "structuredName": {
                            "firstName": "R",
                            "lastName": "Thorpe",
                            "middleNames": [
                                "W"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. W. Thorpe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16754958,
            "fieldsOfStudy": [
                "Linguistics",
                "Computer Science"
            ],
            "id": "4807fd245d5288ed1d516c0e890bcbd648548d2d",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "THE generation of proper word boundaries is an important part of several problems in information processing. Specifically, the speech recognition problem is often described as the production of a phonemic transcript, followed by the assembly of phonemes into complete words.1,2,3,4 The automatic translation of certain natural or artificial languages, such as, for example, Chinese and Japanese to English,5,6,7 or English to Braille8 also requires the generation of words in the output language which may correspond either to several items of input, or to only part of an input item. The segmentation problem is often complicated by the fact that each item of input may be associated with several possible output correspondents, only one of which is acceptable in any given context. Frequently, the reduction of each set of multiple correspondents is at least partly dependent upon the proper recognition of word boundaries. The English phoneme sequence/aban/ might, for example, correspond to the indefinite article \"a\" followed by the noun \"ban\", or it might form a verb or noun prefix as in \"abandon\", or \"abandonment\". Similarly, the Chinese character (dzi), which may be translated as \"self\" when standing alone, may in combination with other characters be translated variously as \"freedom\", \"self-defence\", \"ego\", \"originality\", \"naturally\", \"freely\", \"liberalism\", and so on. The generation of syntactically well-formed sentences in the output language is a common requirement for the set of problems under consideration. Since the material being processed does not, however, consist of complete syntactic units, it is first necessary to generate the appropriate structural information before any method based on syntax can be used. Two principal techniques are therefore proposed for the recognition of"
            },
            "slug": "An-approach-to-the-segmentation-problem-in-speech-Salton-Thorpe",
            "title": {
                "fragments": [],
                "text": "An approach to the segmentation problem in speech analysis and language translation"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Two principal techniques are proposed for the recognition of proper word boundaries, which are first necessary to generate the appropriate structural information before any method based on syntax can be used."
            },
            "venue": {
                "fragments": [],
                "text": "EARLYMT"
            },
            "year": 1961
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2580777"
                        ],
                        "name": "David M. Magerman",
                        "slug": "David-M.-Magerman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Magerman",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David M. Magerman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143757098"
                        ],
                        "name": "C. Weir",
                        "slug": "C.-Weir",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Weir",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Weir"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7358857,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fe3788f2de079b08a6b2b130bba17ec0429c7f04",
            "isKey": false,
            "numCitedBy": 81,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes Picky, a probabilistic agenda-based chart parsing algorithm which uses a technique called probabilistic prediction to predict which grammar rules are likely to lead to an acceptable parse of the input. Using a suboptimal search method, Picky significantly reduces the number of edges produced by CKY-like chart parsing algorithms, while maintaining the robustness of pure bottom-up parsers and the accuracy of existing probabilistic parsers. Experiments using Picky demonstrate how probabilistic modelling can impact upon the efficiency, robustness and accuracy of a parser."
            },
            "slug": "Efficiency,-Robustness-and-Accuracy-in-Picky-Chart-Magerman-Weir",
            "title": {
                "fragments": [],
                "text": "Efficiency, Robustness and Accuracy in Picky Chart Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Plymouth's Picky significantly reduces the number of edges produced by CKY-like chart parsing algorithms, while maintaining the robustness of pure bottom-up parsers and the accuracy of existing probabilistic parsers."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144599392"
                        ],
                        "name": "I. D. Melamed",
                        "slug": "I.-D.-Melamed",
                        "structuredName": {
                            "firstName": "I.",
                            "lastName": "Melamed",
                            "middleNames": [
                                "Dan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. D. Melamed"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9736002,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b713cfc891e2686ae1b71de6537f05ecbdd45d39",
            "isKey": false,
            "numCitedBy": 83,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "The first step in most empirical work in multilingual NLP is to construct maps of the correspondence between texts and their translations (bitext maps). The Smooth Injective Map Recognizer (SIMR) algorithm presented here is a generic pattern recognition algorithm that is particularly well-suited to mapping bitext correspondence. SIMR is faster and significantly more accurate than other algorithms in the literature. The algorithm is robust enough to use on noisy texts, such as those resulting from OCR input, and on translations that are not very literal. SIMR encapsulates its language-specific heuristics, so that it can be ported to any language pair with a minimal effort."
            },
            "slug": "A-Portable-Algorithm-for-Mapping-Bitext-Melamed",
            "title": {
                "fragments": [],
                "text": "A Portable Algorithm for Mapping Bitext Correspondence"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The Smooth Injective Map Recognizer (SIMR) algorithm presented here is a generic pattern recognition algorithm that is particularly well-suited to mapping bitext correspondence."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725500"
                        ],
                        "name": "Yves Schabes",
                        "slug": "Yves-Schabes",
                        "structuredName": {
                            "firstName": "Yves",
                            "lastName": "Schabes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yves Schabes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059594591"
                        ],
                        "name": "Michal Roth",
                        "slug": "Michal-Roth",
                        "structuredName": {
                            "firstName": "Michal",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michal Roth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053387284"
                        ],
                        "name": "Randy B. Osborne",
                        "slug": "Randy-B.-Osborne",
                        "structuredName": {
                            "firstName": "Randy",
                            "lastName": "Osborne",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Randy B. Osborne"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13342424,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f88deef9ad93bdfbcf4c157402121823eb65a359",
            "isKey": false,
            "numCitedBy": 72,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We report grammar inference experiments on partially parsed sentences taken from the Wall Street Journal corpus using the inside-outside algorithm for stochastic context-free grammars. The initial grammar for the inference process makes no assumption of the kinds of structures and their distributions. The inferred grammar is evaluated by its predicting power and by comparing the bracketing of held out sentences imposed by the inferred grammar with the partial bracketings of these sentences given in the corpus. Using part-of-speech tags as the only source of lexical information, high bracketing accuracy is achieved even with a small subset of the available training material (1045 sentences): 94.4% for test sentences shorter than 10 words and 90.2% for sentences shorter than 15 words."
            },
            "slug": "Parsing-the-Wall-Street-Journal-with-the-Algorithm-Schabes-Roth",
            "title": {
                "fragments": [],
                "text": "Parsing the Wall Street Journal with the Inside-Outside Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "Grammar inference experiments on partially parsed sentences taken from the Wall Street Journal corpus using the inside-outside algorithm for stochastic context-free grammars are reported, with high bracketing accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "EACL"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144418438"
                        ],
                        "name": "Hinrich Sch\u00fctze",
                        "slug": "Hinrich-Sch\u00fctze",
                        "structuredName": {
                            "firstName": "Hinrich",
                            "lastName": "Sch\u00fctze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hinrich Sch\u00fctze"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9197677,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7a1ef964ad68f23fb4e22b637e4b27e136876a8f",
            "isKey": false,
            "numCitedBy": 151,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a method for inducing the parts of speech of a language and part-of-speech labels for individual words from a large text corpus. Vector representations for the part-of-speech of a word are formed from entries of its near lexical neighbors. A dimensionality reduction creates a space representing the syntactic categories of unambiguous words. A neural net trained on these spatial representations classifies individual contexts of occurrence of ambiguous words. The method classifies both ambiguous and unambiguous words correctly with high accuracy."
            },
            "slug": "Part-of-Speech-Induction-from-Scratch-Sch\u00fctze",
            "title": {
                "fragments": [],
                "text": "Part-of-Speech Induction from Scratch"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "A method for inducing the parts of speech of a language and part-of-speech labels for individual words from a large text corpus and classifies both ambiguous and unambiguous words correctly with high accuracy is presented."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46278220"
                        ],
                        "name": "Emmanuel Roche",
                        "slug": "Emmanuel-Roche",
                        "structuredName": {
                            "firstName": "Emmanuel",
                            "lastName": "Roche",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Emmanuel Roche"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "116801808"
                        ],
                        "name": "Yves Shabes",
                        "slug": "Yves-Shabes",
                        "structuredName": {
                            "firstName": "Yves",
                            "lastName": "Shabes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yves Shabes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14249124,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d2560a8697013f6428f16238d0ec46a3a0b0f6e7",
            "isKey": false,
            "numCitedBy": 464,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \n\"Emmanuel Roche and Yves Schabes have put together a picture of the state of the art in using finite-state techniques in computational linguistics. The contributing authors comprise an impressive collection -- essentially the originating sources for much of the important recent work in this area.\" \n-- Philip Resnik, Assistant Professor, Department of Linguistics and Institute for Advanced Computer Studies, University of Maryland at College Park Finite-state devices, which include finite-state automata, graphs, and finite-state transducers, are in wide use in many areas of computer science. Recently, there has been a resurgence of the use of finite-state devices in all aspects of computational linguistics, including dictionary encoding, text processing, and speech processing. This book describes the fundamental properties of finite-state devices and illustrates their uses. Many of the contributors pioneered the use of finite-automata for different aspects of natural language processing. The topics, which range from the theoretical to the applied, include finite-state morphology, approximation of phrase-structure grammars, deterministic part-of-speech tagging, application of a finite-state intersection grammar, a finite-state transducer for extracting information from text, and speech recognition using weighted finite automata. The introduction presents the basic theoretical results in finite-state automata and transducers. These results and algorithms are described and illustrated with simple formal language examples as well as natural language examples. \nContributors: Douglas Appelt, John Bear, David Clemenceau, Maurice Gross, Jerry R. Hobbs, David Israel, Megumi Kameyama, Lauri Karttunen, Kimmo Koskenniemi, Mehryar Mohri, Eric Laporte, Fernando C. N. Pereira, Michael D. Riley, Emmanuel Roche, Yves Schabes, Max D. Silberztein, Mark Stickel, Pasi Tapanainen, Mabry Tyson, Atro Voutilainen, Rebecca N. Wright. \nLanguage, Speech, and Communication series"
            },
            "slug": "Finite-State-Language-Processing-Roche-Shabes",
            "title": {
                "fragments": [],
                "text": "Finite-State Language Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Topics include finite-state morphology, approximation of phrase-structure grammars, deterministic part-of-speech tagging, application of a finite- state intersection grammar, a infinite-state transducer for extracting information from text, and speech recognition using weighted finite automata."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1918861"
                        ],
                        "name": "Hinrich Schitze",
                        "slug": "Hinrich-Schitze",
                        "structuredName": {
                            "firstName": "Hinrich",
                            "lastName": "Schitze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hinrich Schitze"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1065088,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aadd4657c4260f2885cd536b1ce54ec7845b7f44",
            "isKey": false,
            "numCitedBy": 158,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an algorithm for tagging words whose part-of-speech properties are unknown. Unlike previous work, the algorithm categorizes word tokens in context instead of word types . The algorithm is evaluated on the Brown Corpus."
            },
            "slug": "Distributional-Part-of-Speech-Tagging-Schitze",
            "title": {
                "fragments": [],
                "text": "Distributional Part-of-Speech Tagging"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "This paper presents an algorithm for tagging words whose part-of-speech properties are unknown, which categorizes word tokens in context instead of word types ."
            },
            "venue": {
                "fragments": [],
                "text": "EACL"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680292"
                        ],
                        "name": "P. Resnik",
                        "slug": "P.-Resnik",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Resnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Resnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 116972934,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f1c25c6d030605df497ac599deaf6c0693e6c80e",
            "isKey": false,
            "numCitedBy": 524,
            "numCiting": 211,
            "paperAbstract": {
                "fragments": [],
                "text": "Selectional constraints are limitations on the applicability of predicates to arguments. For example, the statement \"The number two is blue\" may be syntactically well formed, but at some level it is anomalous-- scBLUE is not a predicate that can be applied to numbers. \nIn this dissertation, I propose a new, information-theoretic account of selectional constraints. Unlike previous approaches, this proposal requires neither the identification of primitive semantic features nor the formalization of complex inferences based on world knowledge. The proposed model assumes instead that lexical items are organized in a conceptual taxonomy according to class membership, where classes are defined simply as sets--that is, extensionally, rather than in terms of explicit features or properties. Selection is formalized in terms of a probabilistic relationship between predicates and concepts: the selectional behavior of a predicate is modeled as its distributional effect on the conceptual classes of its arguments, expressed using the information-theoretic measure of relative entropy. The use of relative entropy leads to an illuminating interpretation of what selectional constraints are: the strength of a predicate's selection for an argument is identified with the quantity of information it carries about that argument. \nIn addition to arguing that the model is empirically adequate, I explore its application to two problems. The first concerns a linguistic question: why some transitive verbs permit implicit direct objects (\"John ate $\\emptyset$\") and others do not (\"*John brought $\\emptyset$\"). It has often been observed informally that the omission of objects is connected to the ease with which the object can be inferred. I have made this observation more formal by positing a relationship between inferability and selectional constraints, and have confirmed the connection between selectional constraints and implicit objects in a set of computational experiments. \nSecond, I have explored the practical applications of the model in resolving syntactic ambiguity. A number of authors have recently begun investigating the use of corpus-based lexical statistics in automatic parsing; the results of computational experiments using the present model suggest that often lexical relationships are better viewed in terms of underlying conceptual relationships such as selectional preference and concept similarity. Thus the information-theoretic measures proposed here can serve not only as components in a theory of selectional constraints, but also as tools for practical natural language processing."
            },
            "slug": "Selection-and-information:-a-class-based-approach-Resnik",
            "title": {
                "fragments": [],
                "text": "Selection and information: a class-based approach to lexical relationships"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A new, information-theoretic account of selectional constraints is proposed, which assumes that lexical items are organized in a conceptual taxonomy according to class membership, where classes are defined simply as sets rather than in terms of explicit features or properties."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762744"
                        ],
                        "name": "A. Stolcke",
                        "slug": "A.-Stolcke",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Stolcke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Stolcke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808760"
                        ],
                        "name": "S. Omohundro",
                        "slug": "S.-Omohundro",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Omohundro",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Omohundro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7324510,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0bf69a49c2baed67fa9a044daa24b9e199e73093",
            "isKey": false,
            "numCitedBy": 291,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a framework for inducing probabilistic grammars from corpora of positive samples. First, samples are incorporated by adding ad-hoc rules to a working grammar; subsequently, elements of the model (such as states or nonterminals) are merged to achieve generalization and a more compact representation. The choice of what to merge and when to stop is governed by the Bayesian posterior probability of the grammar given the data, which formalizes a trade-off between a close fit to the data and a default preference for simpler models (\u2018Occam's Razor\u2019). The general scheme is illustrated using three types of probabilistic grammars: Hidden Markov models, class-based n-grams, and stochastic context-free grammars."
            },
            "slug": "Inducing-Probabilistic-Grammars-by-Bayesian-Model-Stolcke-Omohundro",
            "title": {
                "fragments": [],
                "text": "Inducing Probabilistic Grammars by Bayesian Model Merging"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A framework for inducing probabilistic grammars from corpora of positive samples is described, which formalizes a trade-off between a close fit to the data and a default preference for simpler models (\u2018Occam's Razor\u2019)."
            },
            "venue": {
                "fragments": [],
                "text": "ICGI"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35729970"
                        ],
                        "name": "Yiming Yang",
                        "slug": "Yiming-Yang",
                        "structuredName": {
                            "firstName": "Yiming",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yiming Yang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2237198,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d81d19d270106805389d22b8d54b1f755797d440",
            "isKey": false,
            "numCitedBy": 234,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper studies noise reduction for computational efficiency improvements in a statistical learning method for text categorization, the Linear Least Squares Fit (LLSF) mapping. Multiple noise reduction strategies are proposedand evaluated, including: an aggressive removal of \u201cnon-informative words\u201d from texts before training; the use of a truncated singular value decomposition to cut off noisy \u201clatent semantic structures\u201d during training; the elimination of non-influential components in the LLSF solution (a word-concept association matrix) after training. Text collections in different domains were used for evaluation. Significant improvements in computational efficiency without losing categorization accuracy were evident in the testing results."
            },
            "slug": "Noise-reduction-in-a-statistical-approach-to-text-Yang",
            "title": {
                "fragments": [],
                "text": "Noise reduction in a statistical approach to text categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Noise reduction strategies are proposed and evaluated, including an aggressive removal of \u201cnon-informative words\u201d from texts before training; the use of a truncated singular value decomposition to cut off noisy \u201clatent semantic structures\u201d during training."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '95"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3039533"
                        ],
                        "name": "Hadar Shemtov",
                        "slug": "Hadar-Shemtov",
                        "structuredName": {
                            "firstName": "Hadar",
                            "lastName": "Shemtov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hadar Shemtov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5989400,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a33e325f4871f0f1783b12bffad6df0570ce876f",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Making use of previously translated texts is a very appealing idea that can be of considerable practical and economical benefit as a translation aid. There are different ways to exploit the potential of \"re-translation\" with different degrees of generality, complication and ambition. Example-based machine translation is probably the most ambitious end of the spectrum but there can be other points along it. In this paper I describe a simple tool which deals with a particular special case of the \"re-translation\" problem. It occurs when a new version of a previously translated document needs to be translated. The tool identifies the changes between the two versions of the source language (SL) text and retrieves appropriate sentences from the target language (TL) text. With that, it creates a bilingual draft which consists of sections in the TL text from the existing translation and update materials from the SL text, thereby reducing the effort required from the translator. This tool could substantially increase the productivity of translators which deal with technical documents of frequently modified products (software-based products are the best example of that). If this is true, it suggests that simple solutions can be very effective in addressing \"real-life\" translation problems. The paper is structured as follows. The first section discusses some relevant properties of typical texts which are likely to be (re-)translated with this tool. The second section is about the alignment process I will present a new length-based alignment algorithm, designed for dealing with texts that include additions and deletions. In the following section I will propose a quick procedure to find the differences between two versions of the same document. Then, I will show how the bilingual draft is constructed. The last section will discuss possible continuations of this research which will extend the applicability of the tool to more general translation situations."
            },
            "slug": "Text-Alignment-in-a-Tool-for-Translating-Revised-Shemtov",
            "title": {
                "fragments": [],
                "text": "Text Alignment in a Tool for Translating Revised Documents"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A simple tool which deals with a particular special case of the \"re-translation\" problem, which occurs when a new version of a previously translated document needs to be translated, which could substantially increase the productivity of translators which deal with technical documents of frequently modified products."
            },
            "venue": {
                "fragments": [],
                "text": "EACL"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35729970"
                        ],
                        "name": "Yiming Yang",
                        "slug": "Yiming-Yang",
                        "structuredName": {
                            "firstName": "Yiming",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yiming Yang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 16041292,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bc8e59e4c7c2cbb6695ee5488aa569780449b212",
            "isKey": false,
            "numCitedBy": 485,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Expert Network (ExpNet) is our new approach to automatic categorization and retrieval of natural language texts. We use a training set of texts with expert-assigned categories to construct a network which approximately reflects the conditional probabilities of categories given a text. The input nodes of the network are words in the training texts, the nodes on the intermediate level are the training texts, and the output nodes are categories. The links between nodes are computed based on statistics of the word distribution and the category distribution over the training set. ExpNet is used for relevance ranking of candidate categories of an arbitrary text in the case of text categorization, and for relevance ranking of documents via categories in the case of text retrieval. We have evaluated ExpNet in categorization and retrieval on a document collection of the MEDLINE database, and observed a performance in recall and precision comparable to the Linear Least Squares Fit (LLSF) mapping method, and significantly better than other methods tested. Computationally, ExpNet has an O(N 1og N) time complexity which is much more efficient than the cubic complexity of the LLSF method. The simplicity of the model, the high recall-precision rates, and the efficient computation together make ExpNet preferable as a practical solution for real-world applications."
            },
            "slug": "Expert-network:-effective-and-efficient-learning-in-Yang",
            "title": {
                "fragments": [],
                "text": "Expert network: effective and efficient learning from human decisions in text categorization and retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The simplicity of the model, the high recall-precision rates, and the efficient computation together make ExpNet preferable as a practical solution for real-world applications."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '94"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35729970"
                        ],
                        "name": "Yiming Yang",
                        "slug": "Yiming-Yang",
                        "structuredName": {
                            "firstName": "Yiming",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yiming Yang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 93891,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "890c16ca29a781a7b793c603822ffd57aee9f57f",
            "isKey": false,
            "numCitedBy": 2034,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper focuses on a comparative evaluation of a wide-range of text categorization methods, including previously published results on the Reuters corpus and new results of additional experiments. A controlled study using three classifiers, kNN, LLSF and WORD, was conducted to examine the impact of configuration variations in five versions of Reuters on the observed performance of classifiers. Analysis and empirical evidence suggest that the evaluation results on some versions of Reuters were significantly affected by the inclusion of a large portion of unlabelled documents, mading those results difficult to interpret and leading to considerable confusions in the literature. Using the results evaluated on the other versions of Reuters which exclude the unlabelled documents, the performance of twelve methods are compared directly or indirectly. For indirect compararions, kNN, LLSF and WORD were used as baselines, since they were evaluated on all versions of Reuters that exclude the unlabelled documents. As a global observation, kNN, LLSF and a neural network method had the best performance; except for a Naive Bayes approach, the other learning algorithms also performed relatively well."
            },
            "slug": "An-Evaluation-of-Statistical-Approaches-to-Text-Yang",
            "title": {
                "fragments": [],
                "text": "An Evaluation of Statistical Approaches to Text Categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Analysis and empirical evidence suggest that the evaluation results on some versions of Reuters were significantly affected by the inclusion of a large portion of unlabelled documents, mading those results difficult to interpret and leading to considerable confusions in the literature."
            },
            "venue": {
                "fragments": [],
                "text": "Information Retrieval"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762744"
                        ],
                        "name": "A. Stolcke",
                        "slug": "A.-Stolcke",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Stolcke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Stolcke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11595344,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "79fbfc1dc8846379074aaf4deb7fb0a96722eeed",
            "isKey": false,
            "numCitedBy": 401,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an extension of Earley's parser for stochastic context-free grammars that computes the following quantities given a stochastic context-free grammar and an input string: a) probabilities of successive prefixes being generated by the grammar; b) probabilities of substrings being generated by the nonterminals, including the entire string being generated by the grammar; c) most likely (Viterbi) parse of the string; d) posterior expected number of applications of each grammar production, as required for reestimating rule probabilities. Probabilities (a) and (b) are computed incrementally in a single left-to-right pass over the input. Our algorithm compares favorably to standard bottom-up parsing methods for SCFGs in that it works efficiently on sparse grammars by making use of Earley's top-down control structure. It can process any context-free rule format without conversion to some normal form, and combines computations for (a) through (d) in a single algorithm. Finally, the algorithm has simple extensions for processing partially bracketed inputs, and for finding partial parses and their likelihoods on ungrammatical inputs."
            },
            "slug": "An-Efficient-Probabilistic-Context-Free-Parsing-Stolcke",
            "title": {
                "fragments": [],
                "text": "An Efficient Probabilistic Context-Free Parsing Algorithm that Computes Prefix Probabilities"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "An extension of Earley's parser for stochastic context-free grammars that computes probabilities of successive prefixes being generated by the grammar and an input string and posterior expected number of applications of each grammar production, as required for reestimating rule probabilities."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144418438"
                        ],
                        "name": "Hinrich Sch\u00fctze",
                        "slug": "Hinrich-Sch\u00fctze",
                        "structuredName": {
                            "firstName": "Hinrich",
                            "lastName": "Sch\u00fctze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hinrich Sch\u00fctze"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8754851,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3cd9fd8a36c8feb74bb20ae25817edb9c6a0518c",
            "isKey": false,
            "numCitedBy": 1401,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents context-group discrimination, a disambiguation algorithm based on clustering. Senses are interpreted as groups (or clusters) of similar contexts of the ambiguous word. Words, contexts, and senses are represented in Word Space, a high-dimensional, real-valued space in which closeness corresponds to semantic similarity. Similarity in Word Space is based on second-order co-occurrence: two tokens (or contexts) of the ambiguous word are assigned to the same sense cluster if the words they co-occur with in turn occur with similar words in a training corpus. The algorithm is automatic and unsupervised in both training and application: senses are induced from a corpus without labeled training instances or other external knowledge sources. The paper demonstrates good performance of context-group discrimination for a sample of natural and artificial ambiguous words."
            },
            "slug": "Automatic-Word-Sense-Discrimination-Sch\u00fctze",
            "title": {
                "fragments": [],
                "text": "Automatic Word Sense Discrimination"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "This paper presents context-group discrimination, a disambiguation algorithm based on clustering that demonstrates good performance of context- group discrimination for a sample of natural and artificial ambiguous words."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2024985"
                        ],
                        "name": "P. Suppes",
                        "slug": "P.-Suppes",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Suppes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Suppes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15500572,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "d4086b891b7d6981c9a0a01372ad1bc87f79d720",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Although a fully adequate grammar for a substantial portion of any natural language does not exist, a vigorous and controversial discussion of how to choose among several competing grammars has already developed. On occasion, criteria of simplicity have been suggested as systematic scientific criteria for selection. The absence of such systematic criteria of simplicity in other domains of science inevitably raises doubts about the feasibility of such criteria for the selection of a grammar. Although some informal and intuitive discussion of simplicity is often included in the selection of theories or models in physics or in other branches of science, there is no serious systematic literature on problems of measuring simplicity. Nor is there any systematic literature in which criteria of simplicity are used in a substantive fashion to select from among several theories. There are many reasons for this, but perhaps the most pressing one is that the use of more obviously objective criteria leaves little room for the addition of further criteria of simplicity. The central thesis of this paper is that objective probabilistic criteria of a standard scientific sort may be used to select a grammar."
            },
            "slug": "Probabilistic-grammars-for-natural-languages-Suppes",
            "title": {
                "fragments": [],
                "text": "Probabilistic grammars for natural languages"
            },
            "venue": {
                "fragments": [],
                "text": "Synthese"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2179352"
                        ],
                        "name": "M. MacDonald",
                        "slug": "M.-MacDonald",
                        "structuredName": {
                            "firstName": "Maryellen",
                            "lastName": "MacDonald",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. MacDonald"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3387812"
                        ],
                        "name": "N. Pearlmutter",
                        "slug": "N.-Pearlmutter",
                        "structuredName": {
                            "firstName": "Neal",
                            "lastName": "Pearlmutter",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Pearlmutter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2246097"
                        ],
                        "name": "Mark S. Seidenberg",
                        "slug": "Mark-S.-Seidenberg",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Seidenberg",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark S. Seidenberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 145
                            }
                        ],
                        "text": "Semantic preferences, the generosity of speakers in following communicative maxims, and intonational patterns all usually prevent us from garden (MacDonald et al. 1994; Tanenhaus and Trueswell 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15560738,
            "fieldsOfStudy": [
                "Linguistics",
                "Psychology"
            ],
            "id": "34b51001891b3104be96a8d4c31ef80c76b01b08",
            "isKey": false,
            "numCitedBy": 1948,
            "numCiting": 226,
            "paperAbstract": {
                "fragments": [],
                "text": ": Ambiguity resolution is a central problem in language comprehension. Lexical and syntactic ambiguities are standardly assumed to involve different types of knowledge representations and be resolved by different mechanisms. An alternative account is provided in which both types of ambiguity derive from aspects of lexical representation and are resolved by the same processing mechanisms. Reinterpreting syntactic ambiguity resolution as a form of lexical ambiguity resolution obviates the need for special parsing principles to account for syntactic interpretation preferences, reconciles a number of apparently conflicting results concerning the roles of lexical and contextual information in sentence processing, explains differences among ambiguities in terms of ease of resolution, and provides a more unified account of language comprehension than was previously available."
            },
            "slug": "The-lexical-nature-of-syntactic-ambiguity-MacDonald-Pearlmutter",
            "title": {
                "fragments": [],
                "text": "The lexical nature of syntactic ambiguity resolution"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Reinterpreting syntactic ambiguity resolution as a form of lexical ambiguity resolution obviates the need for special parsing principles to account for syntactic interpretation preferences, and provides a more unified account of language comprehension than was previously available."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057287718"
                        ],
                        "name": "M. Porter",
                        "slug": "M.-Porter",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Porter",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Porter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6093716,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "a651bb7cc7fc68ece0cc66ab921486d163373385",
            "isKey": false,
            "numCitedBy": 6533,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The automatic removal of suffixes from words in English is of particular interest in the field of information retrieval. An algorithm for suffix stripping is described, which has been implemented as a short, fast program in BCPL. Although simple, it performs slightly better than a much more elaborate system with which it has been compared. It effectively works by treating complex suffixes as compounds made up of simple suffixes, and removing the simple suffixes in a number of steps. In each step the removal of the suffix is made to depend upon the form of the remaining stem, which usually involves a measure of its syllable length."
            },
            "slug": "An-algorithm-for-suffix-stripping-Porter",
            "title": {
                "fragments": [],
                "text": "An algorithm for suffix stripping"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "An algorithm for suffix stripping is described, which has been implemented as a short, fast program in BCPL and performs slightly better than a much more elaborate system with which it has been compared."
            },
            "venue": {
                "fragments": [],
                "text": "Program"
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40475640"
                        ],
                        "name": "Woojin Paik",
                        "slug": "Woojin-Paik",
                        "structuredName": {
                            "firstName": "Woojin",
                            "lastName": "Paik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Woojin Paik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2752514"
                        ],
                        "name": "Elizabeth D. Liddy",
                        "slug": "Elizabeth-D.-Liddy",
                        "structuredName": {
                            "firstName": "Elizabeth",
                            "lastName": "Liddy",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Elizabeth D. Liddy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2242807"
                        ],
                        "name": "E. S. Yu",
                        "slug": "E.-S.-Yu",
                        "structuredName": {
                            "firstName": "Edmund",
                            "lastName": "Yu",
                            "middleNames": [
                                "Szu-Li"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. S. Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37636566"
                        ],
                        "name": "M. McKenna",
                        "slug": "M.-McKenna",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "McKenna",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. McKenna"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10354374,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8692d1ff9d90095c59ee8d85067ba8cec0dcd6d1",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we describe the most recent implementation and evaluation of the proper noun categorization and standardization module of the DRLINK document detection system being developed at Syracuse University, under the auspices of ARPA's TIPSTER program. We also discuss the expansion of group common nouns and group proper nouns to enhance retrieval recall. Successful proper noun boundary identification within the part of speech tagger is essential for successful categorization. The proper noun classification module is designed to assign a category code to each proper noun entity, using 30 ca tegor ies genera ted f rom corpus analysis . Standardization of variant proper nouns occurs at three levels of processing. Expansion of group proper nouns and group common nouns is performed on queries. Standardization and categorization is performed on queries and documents. DR-LINK's overall precision for proper noun categorization was 93%, based on 589 proper nouns occurring in the evaluation set."
            },
            "slug": "Categorization-and-Standardizing-Proper-Nouns-for-Paik-Liddy",
            "title": {
                "fragments": [],
                "text": "Categorization and Standardizing Proper Nouns for Efficient Information Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "The most recent implementation and evaluation of the proper noun categorization and standardization module of the DRLINK document detection system being developed at Syracuse University, under the auspices of ARPA's TIPSTER program is described."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "93707734"
                        ],
                        "name": "Helmut Schmidt",
                        "slug": "Helmut-Schmidt",
                        "structuredName": {
                            "firstName": "Helmut",
                            "lastName": "Schmidt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Helmut Schmidt"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17392458,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd0bab6fc8cd43c0ce170ad2f4cb34181b31277d",
            "isKey": false,
            "numCitedBy": 2957,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, a new probabilistic tagging method is presented which avoids problems that Markov Model based taggers face, when they have to estimate transition probabilities from sparse data. In this tagging method, transition probabilities are estimated using a decision tree. Based on this method, a part-of-speech tagger (called TreeTagger) has been implemented which achieves 96.36 % accuracy on Penn-Treebank data which is better than that of a trigram tagger (96.06 %) on the same data."
            },
            "slug": "Probabilistic-part-of-speech-tagging-using-decision-Schmidt",
            "title": {
                "fragments": [],
                "text": "Probabilistic part-of-speech tagging using decision trees"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "A new probabilistic tagging method is presented which avoids problems that Markov Model based taggers face, when they have to estimate transition probabilities from sparse data, using a decision tree."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118077301"
                        ],
                        "name": "Jane Morris",
                        "slug": "Jane-Morris",
                        "structuredName": {
                            "firstName": "Jane",
                            "lastName": "Morris",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jane Morris"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145036961"
                        ],
                        "name": "Graeme Hirst",
                        "slug": "Graeme-Hirst",
                        "structuredName": {
                            "firstName": "Graeme",
                            "lastName": "Hirst",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Graeme Hirst"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10970495,
            "fieldsOfStudy": [
                "Linguistics",
                "Computer Science"
            ],
            "id": "ca40dc1300ab085406455894dd42fd02f9cc36f8",
            "isKey": false,
            "numCitedBy": 1091,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "In text, lexical cohesion is the result of chains of related words that contribute to the continuity of lexical meaning. These lexical chains are a direct result of units of text being \"about the same thing,\" and finding text structure involves finding units of text that are about the same thing. Hence, computing the chains is useful, since they will have a correspondence to the structure of the text. Determining the structure of text is an essential step in determining the deep meaning of the text. In this paper, a thesaurus is used as the major knowledge base for computing lexical chains. Correspondences between lexical chains and structural elements are shown to exist. Since the lexical chains are computable, and exist in non-domain-specific text, they provide a valuable indicator of text structure. The lexical chains also provide a semantic context for interpreting words, concepts, and sentences."
            },
            "slug": "Lexical-Cohesion-Computed-by-Thesaural-Relations-as-Morris-Hirst",
            "title": {
                "fragments": [],
                "text": "Lexical Cohesion Computed by Thesaural Relations as an Indicator of the Structure of Text"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Since the lexical chains are computable, and exist in non-domain-specific text, they provide a valuable indicator of text structure, and provide a semantic context for interpreting words, concepts, and sentences."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797808"
                        ],
                        "name": "G. Salton",
                        "slug": "G.-Salton",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Salton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Salton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144890574"
                        ],
                        "name": "James Allan",
                        "slug": "James-Allan",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Allan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Allan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2361827,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "17f327bbacd24198f728d6fecac0c4fb2169b471",
            "isKey": false,
            "numCitedBy": 68,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Many large collections of full-text documents are currently stored in machine-readable form and processed automatically in various ways. These collections may include different types of documents, such as messages, research articles, and books, and the subject matter may vary widely. To process such collections, robust text analysis methods must be used, capable of handling materials in arbitrary subject areas, and flexible access must be provided to texts and text excerpts of varying size. In this study, global text comparison methods are used to identify similarities between text elements, followed by local context-checking operations that resolve ambiguities and distinguish superficially similar texts from texts that actually cover identical topics. A linked text structure is then created that relates similar texts at various levels of detail. In particular, text links are available for full texts, as well as text sections, paragraphs, and sentence groups. The linked structures are usable to identify important text passages, to traverse texts selectively both within particular documents and between documents, and to provide flexible text access to large text collections in response to various kinds of user needs. An automated 29-volume encyclopedia is used as an example to illustrate the text accessing and traversal operations."
            },
            "slug": "Selective-text-utilization-and-text-traversal-Salton-Allan",
            "title": {
                "fragments": [],
                "text": "Selective text utilization and text traversal"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Global text comparison methods are used to identify similarities between text elements, followed by local context-checking operations that resolve ambiguities and distinguish superficially similar texts from texts that actually cover identical topics."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Hum. Comput. Stud."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51955310"
                        ],
                        "name": "H. V. Riemsdijk",
                        "slug": "H.-V.-Riemsdijk",
                        "structuredName": {
                            "firstName": "Henk",
                            "lastName": "Riemsdijk",
                            "middleNames": [
                                "C.",
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. V. Riemsdijk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40451524"
                        ],
                        "name": "E. Williams",
                        "slug": "E.-Williams",
                        "structuredName": {
                            "firstName": "Edwin",
                            "lastName": "Williams",
                            "middleNames": [
                                "Samuel"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Williams"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120329685,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "e932bbf1f4b6a047e1ec1a9740d648cb637769b9",
            "isKey": false,
            "numCitedBy": 351,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "In the last 30 years, linguists have built a considerable and highly sophisticated body of work on generative grammar. Today the field is more active than ever before. \"Introduction to the Theory of Grammar\" makes available to teachers and students of syntax a comprehensive critical review of the main results of present day grammatical theory and shows how they were achieved. It presents the central questions, shows how and why they were asked, what the answers were, and how these have led to new questions.Part I discusses the way in which the overly rich, descriptive rule systems of the fifties and sixties have gradually been replaced by simpler, more constrained rule systems. Much of the work originally done by stipulations in the rules themselves has been taken over by general, universal principles which govern the form and functioning of these rules and the properties of their inputs and outputs. The establishment of such a theory of principles is the main topic of Part II.Part III addresses the problem of how semantics fits into grammar and elaborates a conception of how the syntactic properties of logical representations can be integrated into the overall theory of grammar. The rules and principles of grammar developed in these parts account for grammatical phenomena in an essentially modular way, and this system of modules, which constitutes the study of grammar today, is established in Part IV.An Epilogue describes such current developments as generalized binding, phrase structure, small clauses, tree geometry and NP-structure.Henk van Riemsdijk is full professor in the Department of Language and Literature, Tilburg University, Holland. Edwin Williams is professor of linguistics, University of Massachusetts, Amherst. \"Introduction to the Theory of Grammar\" is twelfth in the series Current Studies in Linguistics."
            },
            "slug": "Introduction-to-the-Theory-of-Grammar-Riemsdijk-Williams",
            "title": {
                "fragments": [],
                "text": "Introduction to the Theory of Grammar"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2539424"
                        ],
                        "name": "V. Poznanski",
                        "slug": "V.-Poznanski",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Poznanski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Poznanski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143661868"
                        ],
                        "name": "A. Sanfilippo",
                        "slug": "A.-Sanfilippo",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Sanfilippo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sanfilippo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5249611,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "9393c7b39801de70c9731f609d7ba2ca63090619",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "There is a widespread belief among linguists that a predicate's subcategorization frames are largely determined by its lexical-semantic properties [23, 11, 12]. Consider the domain of movement verbs. Following Talmy [23], these can he semantically classified with reference to the meaning components: MOTION, MANNER, CAUSATION, THEME (MOVING ENTITY), PATH AND REFERENCE LOCATIONS (GOAL, SOURCE). Lexicalization patterns which arise from identifying clusters of such meaning components in verb senses can be systematically related to distinct subcategorization frames. 1 For example, the arguments of a verb expressing directed caused motion (e.g. bring, put, give) are normally a causative subject (agent), a theme direct object (moving entity) and a directional argument expressing path and reference location (goal), e.g."
            },
            "slug": "Detecting-Dependencies-between-Semantic-Verb-and-in-Poznanski-Sanfilippo",
            "title": {
                "fragments": [],
                "text": "Detecting Dependencies between Semantic Verb Subclasses and Subcategorization Frames in Text Corpora"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "Lexicalization patterns which arise from identifying clusters of such meaning components in verb senses can be systematically related to distinct subcategorization frames in the domain of movement verbs."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46278220"
                        ],
                        "name": "Emmanuel Roche",
                        "slug": "Emmanuel-Roche",
                        "structuredName": {
                            "firstName": "Emmanuel",
                            "lastName": "Roche",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Emmanuel Roche"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725500"
                        ],
                        "name": "Yves Schabes",
                        "slug": "Yves-Schabes",
                        "structuredName": {
                            "firstName": "Yves",
                            "lastName": "Schabes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yves Schabes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1796,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "602bc7d8a4a145311976cb4c1c3116b25f89c68f",
            "isKey": false,
            "numCitedBy": 185,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "Stochastic approaches to natural language processing have often been preferred to rule-based approaches because of their robustness and their automatic training capabilities. This was the case for part-of-speech tagging until Brill showed how state-of-the-art part-of-speech tagging can be achieved with a rule-based tagger by inferring rules from a training corpus. However, current implementations of the rule-based tagger run more slowly than previous approaches. In this paper, we present a finite-state tagger, inspired by the rule-based tagger, that operates in optimal time in the sense that the time to assign tags to a sentence corresponds to the time required to follow a single path in a deterministic finite-state machine. This result is achieved by encoding the application of the rules found in the tagger as a nondeterministic finite-state transducer and then turning it into a deterministic transducer. The resulting deterministic transducer yields a part-of-speech tagger whose speed is dominated by the access time of mass storage devices. We then generalize the techniques to the class of transformation-based systems."
            },
            "slug": "Deterministic-Part-of-Speech-Tagging-with-Roche-Schabes",
            "title": {
                "fragments": [],
                "text": "Deterministic Part-of-Speech Tagging with Finite-State Transducers"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A finite-state tagger is presented, inspired by the rule-based tagger, that operates in optimal time in the sense that the time to assign tags to a sentence corresponds to the time required to follow a single path in a deterministic finite- state machine."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145247319"
                        ],
                        "name": "S. Vogel",
                        "slug": "S.-Vogel",
                        "structuredName": {
                            "firstName": "Stephan",
                            "lastName": "Vogel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Vogel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145322333"
                        ],
                        "name": "H. Ney",
                        "slug": "H.-Ney",
                        "structuredName": {
                            "firstName": "Hermann",
                            "lastName": "Ney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2324070"
                        ],
                        "name": "C. Tillmann",
                        "slug": "C.-Tillmann",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Tillmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Tillmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11644259,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "59c442932e9fcfcac6df5566c2bcd1ec331548c9",
            "isKey": false,
            "numCitedBy": 982,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we describe a new model for word alignment in statistical translation and present experimental results. The idea of the model is to make the alignment probabilities dependent on the differences in the alignment positions rather than on the absolute positions. To achieve this goal, the approach uses a first-order Hidden Markov model (HMM) for the word alignment problem as they are used successfully in speech recognition for the time alignment problem. The difference to the time alignment HMM is that there is no monotony constraint for the possible word orderings. We describe the details of the model and test the model on several bilingual corpora."
            },
            "slug": "HMM-Based-Word-Alignment-in-Statistical-Translation-Vogel-Ney",
            "title": {
                "fragments": [],
                "text": "HMM-Based Word Alignment in Statistical Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A new model for word alignment in statistical translation using a first-order Hidden Markov model for the word alignment problem as they are used successfully in speech recognition for the time alignment problem."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144430625"
                        ],
                        "name": "S. Robertson",
                        "slug": "S.-Robertson",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Robertson",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Robertson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145848824"
                        ],
                        "name": "Karen Sp\u00e4rck Jones",
                        "slug": "Karen-Sp\u00e4rck-Jones",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Sp\u00e4rck Jones",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karen Sp\u00e4rck Jones"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 45186038,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f6e3e57567e9803718623ec088cd7fea65cfbc9d",
            "isKey": false,
            "numCitedBy": 2372,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper examines statistical techniques for exploiting relevance information to weight search terms. These techniques are presented as a natural extension of weighting methods using information about the distribution of index terms in documents in general. A series of relevance weighting functions is derived and is justified by theoretical considerations. In particular, it is shown that specific weighted search methods are implied by a general probabilistic theory of retrieval. Different applications of relevance weighting are illustrated by experimental results for test collections."
            },
            "slug": "Relevance-weighting-of-search-terms-Robertson-Jones",
            "title": {
                "fragments": [],
                "text": "Relevance weighting of search terms"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "This paper examines statistical techniques for exploiting relevance information to weight search terms using information about the distribution of index terms in documents in general and shows that specific weighted search methods are implied by a general probabilistic theory of retrieval."
            },
            "venue": {
                "fragments": [],
                "text": "J. Am. Soc. Inf. Sci."
            },
            "year": 1976
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793475"
                        ],
                        "name": "A. Ratnaparkhi",
                        "slug": "A.-Ratnaparkhi",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ratnaparkhi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ratnaparkhi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5914287,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a574e320d899e7e82e341eb64baef7dfe8a24642",
            "isKey": false,
            "numCitedBy": 1545,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a statistical model which trains from a corpus annotated with Part Of Speech tags and assigns them to previously unseen text with state of the art accuracy The model can be classi ed as a Maximum Entropy model and simultaneously uses many contextual features to predict the POS tag Furthermore this paper demonstrates the use of specialized fea tures to model di cult tagging decisions discusses the corpus consistency problems discovered during the implementation of these features and proposes a training strategy that mitigates these problems"
            },
            "slug": "A-Maximum-Entropy-Model-for-Part-Of-Speech-Tagging-Ratnaparkhi",
            "title": {
                "fragments": [],
                "text": "A Maximum Entropy Model for Part-Of-Speech Tagging"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A statistical model which trains from a corpus annotated with Part Of Speech tags and assigns them to previously unseen text with state of the art accuracy and discusses the corpus consistency problems discovered during the implementation of these features."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70422141"
                        ],
                        "name": "Elizabeth Shriberg",
                        "slug": "Elizabeth-Shriberg",
                        "structuredName": {
                            "firstName": "Elizabeth",
                            "lastName": "Shriberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Elizabeth Shriberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746807"
                        ],
                        "name": "Dan Jurafsky",
                        "slug": "Dan-Jurafsky",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Jurafsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dan Jurafsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758841"
                        ],
                        "name": "K. Ries",
                        "slug": "K.-Ries",
                        "structuredName": {
                            "firstName": "Klaus",
                            "lastName": "Ries",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Ries"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14972057,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "436c3119d16ce2e3c243ffe7a4a1a5dc40b128aa",
            "isKey": false,
            "numCitedBy": 182,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an integrated approach for statistical modeling of discourse structure for natural conversational speech. Our model is based on 42`dialog acts' which were hand-labeled in 1155 conversations from the Switchboard corpus of spontaneous human-to-human telephone speech. We developed several models and algorithms to automatically detect dialog acts from transcribed or automatically recognized words and from prosodic properties of the speech signal, and by using a statistical discourse grammar. All of these components were probabilistic in nature and estimated from data, employing a variety of techniques (hidden Markov models, N-gram language models, maximum entropy estimation, decision tree classiiers, and neural networks). In preliminary studies, we achieved a dialog act labeling accuracy of 65% based on recognized words and prosody, and an accuracy of 72% based on word transcripts. Since humans achieve 84% on this task (with chance performance at 35%) we nd these results encouraging."
            },
            "slug": "Dialog-Act-Modeling-for-Conversational-Speech-Shriberg-Jurafsky",
            "title": {
                "fragments": [],
                "text": "Dialog Act Modeling for Conversational Speech"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "An integrated approach for statistical modeling of discourse structure for natural conversational speech based on 42`dialog acts' which were hand-labeled in 1155 conversations from the Switchboard corpus of spontaneous human-to-human telephone speech."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144866028"
                        ],
                        "name": "Michel Simard",
                        "slug": "Michel-Simard",
                        "structuredName": {
                            "firstName": "Michel",
                            "lastName": "Simard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michel Simard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "29397516"
                        ],
                        "name": "Pierre Plamondon",
                        "slug": "Pierre-Plamondon",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Plamondon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pierre Plamondon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7126603,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "a13275f615b3263d499f4f542aa68c7712c4a79e",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Sentence alignment is the problem of making explicit the relations that exist between the sentences of two texts that are known to be mutual translations. Automatic sentence-alignment methods typically face two kinds of difficulties. First, there is the question of robustness. In real life, discrepancies between a source text and its translation are quite common: differences in layout, omissions, inversions, etc. Sentence-alignment programs must be ready to deal with such phenomena. Then, there is the question of accuracy. Even when translations are \u201cclean\u201d, alignment is still not a trivial matter: some decisions are hard to make, even for humans. We report here on the current state of our ongoing efforts to produce a sentence-alignment program that is both robust and accurate. The method that we propose relies on two new alignment engines: one that produces highly reliable and robust character-level alignments, and one that relies on statistical lexical knowledge to produce accurate mappings. Experimental results are presented which demonstrate the method's effectiveness, and highlight where problems remain to be solved."
            },
            "slug": "Bilingual-Sentence-Alignment:-Balancing-Robustness-Simard-Plamondon",
            "title": {
                "fragments": [],
                "text": "Bilingual Sentence Alignment: Balancing Robustness and Accuracy"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The method that is proposed relies on two new alignment engines: one that produces highly reliable and robust character-level alignments, and one that relies on statistical lexical knowledge to produce accurate mappings."
            },
            "venue": {
                "fragments": [],
                "text": "AMTA"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737754"
                        ],
                        "name": "J. Siskind",
                        "slug": "J.-Siskind",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Siskind",
                            "middleNames": [
                                "Mark"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Siskind"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14577201,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2854f6721708f1f3d5fe746e11efe0c866f83c19",
            "isKey": false,
            "numCitedBy": 542,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-computational-study-of-cross-situational-for-Siskind",
            "title": {
                "fragments": [],
                "text": "A computational study of cross-situational techniques for learning word-to-meaning mappings"
            },
            "venue": {
                "fragments": [],
                "text": "Cognition"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069194860"
                        ],
                        "name": "Ken Samuel",
                        "slug": "Ken-Samuel",
                        "structuredName": {
                            "firstName": "Ken",
                            "lastName": "Samuel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ken Samuel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3326473"
                        ],
                        "name": "S. Carberry",
                        "slug": "S.-Carberry",
                        "structuredName": {
                            "firstName": "Sandra",
                            "lastName": "Carberry",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Carberry"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1400820110"
                        ],
                        "name": "K. Vijay-Shanker",
                        "slug": "K.-Vijay-Shanker",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Vijay-Shanker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Vijay-Shanker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 643470,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2684e9843e0378e6ca62b1068d87732876637e8c",
            "isKey": false,
            "numCitedBy": 164,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "For the task of recognizing dialogue acts, we are applying the Transformation-Based Learning (TBL) machine learning algorithm. To circumvent a sparse data problem, we extract values of well-motivated features of utterances, such as speaker direction, punctuation marks, and a new feature, called dialogue act cues, which we find to be more effective than cue phrases and word n-grams in practice. We present strategies for constructing a set of dialogue act cues automatically by minimizing the entropy of the distribution of dialogue acts in a training corpus, filtering out irrelevant dialogue act cues, and clustering semantically-related words. In addition, to address limitations of TBL, we introduce a Monte Carlo strategy for training efficiently and a committee method for computing confidence measures. These ideas are combined in our working implementation, which labels held-out data as accurately as any other reported system for the dialogue act tagging task."
            },
            "slug": "Dialogue-Act-Tagging-with-Transformation-Based-Samuel-Carberry",
            "title": {
                "fragments": [],
                "text": "Dialogue Act Tagging with Transformation-Based Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work extracts values of well-motivated features of utterances, such as speaker direction, punctuation marks, and a new feature, called dialogue act cues, which it finds to be more effective than cue phrases and word n-grams in practice."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3280967"
                        ],
                        "name": "Carson T. Sch\u00fctze",
                        "slug": "Carson-T.-Sch\u00fctze",
                        "structuredName": {
                            "firstName": "Carson",
                            "lastName": "Sch\u00fctze",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carson T. Sch\u00fctze"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 141069514,
            "fieldsOfStudy": [
                "Linguistics",
                "Psychology"
            ],
            "id": "9eb31f967e7503280834b5cbe8fbfa0f952d8f6d",
            "isKey": false,
            "numCitedBy": 595,
            "numCiting": 291,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface Acknowledgments 1: Introduction 2: Definitions and Historical Background 3: Judging Grammaticality: The Nature of Metalinguistic Performance 4: Subject-Related Factors in Grammaticality Judgments 5: Task-Related Factors in Grammaticality Judgments 6: Theoretical and Methodological Implications 7: Looking Back and Looking Ahead References Index"
            },
            "slug": "The-empirical-base-of-linguistics:-Grammaticality-Sch\u00fctze",
            "title": {
                "fragments": [],
                "text": "The empirical base of linguistics: Grammaticality judgments and linguistic methodology"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145595269"
                        ],
                        "name": "Satoshi Sato",
                        "slug": "Satoshi-Sato",
                        "structuredName": {
                            "firstName": "Satoshi",
                            "lastName": "Sato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Satoshi Sato"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 671074,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9054a77531111a28fe548c922024dd132a4ffa07",
            "isKey": false,
            "numCitedBy": 71,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a Japanese-English translation aid system, CTM, which has a useful capability for flexible retrieval of texts from bilingual corpora or translation databases. Translation examples (pairs of a text and its translation equivalent) are very helpful for us to translate the similar text. Our character-based best match retrieval method can retrieve translation examples similar to the given input. This method has the following advantages: (1) this method accepts free-style translation examples, i.e., pairs of any text string and its translation equivalent, (2) morphological analysis is unneccessary, (3) this method accepts free-style inputs (i.e., any text strings) for retrieval. We show the retrieval examples with the following characteristic features: phrasal expression, long-distance dependency, idiom, synonym, and semantic ambiguity."
            },
            "slug": "CTM:-An-Example-Based-Translation-Aid-System-Sato",
            "title": {
                "fragments": [],
                "text": "CTM: An Example-Based Translation Aid System"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "A character-based best match retrieval method can retrieve translation examples similar to the given input, which has the following advantages: this method accepts free-style translation examples, i.e., pairs of any text string and its translation equivalent, and morphological analysis is unneccessary."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144461837"
                        ],
                        "name": "C. Shannon",
                        "slug": "C.-Shannon",
                        "structuredName": {
                            "firstName": "Claude",
                            "lastName": "Shannon",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Shannon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9101213,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c1e3f2d537e50e0d5263e4731ab6c7983acd6687",
            "isKey": false,
            "numCitedBy": 2529,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A new method of estimating the entropy and redundancy of a language is described. This method exploits the knowledge of the language statistics possessed by those who speak the language, and depends on experimental results in prediction of the next letter when the preceding text is known. Results of experiments in prediction are given, and some properties of an ideal predictor are developed."
            },
            "slug": "Prediction-and-entropy-of-printed-English-Shannon",
            "title": {
                "fragments": [],
                "text": "Prediction and entropy of printed English"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "A new method of estimating the entropy and redundancy of a language is described, which exploits the knowledge of the language statistics possessed by those who speak the language, and depends on experimental results in prediction of the next letter when the preceding text is known."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1951
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143992833"
                        ],
                        "name": "N. Sloane",
                        "slug": "N.-Sloane",
                        "structuredName": {
                            "firstName": "N.",
                            "lastName": "Sloane",
                            "middleNames": [
                                "J.",
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Sloane"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734614"
                        ],
                        "name": "A. Wyner",
                        "slug": "A.-Wyner",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Wyner",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Wyner"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58500389,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4823c69355cc8feceabde6e6a60fdbcbbfa9be9e",
            "isKey": false,
            "numCitedBy": 234,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A new method of estimating the entropy and redundancy of a language is described. This method exploits the knowledge of the language statistics possessed by those who speak the language, and depends on experimental results in prediction of the next letter when the preceding text is known. Results of experiments in prediction are given, and some properties of an ideal predictor are developed."
            },
            "slug": "Prediction-and-Entropy-of-Printed-English-Sloane-Wyner",
            "title": {
                "fragments": [],
                "text": "Prediction and Entropy of Printed English"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "A new method of estimating the entropy and redundancy of a language is described, which exploits the knowledge of the language statistics possessed by those who speak the language, and depends on experimental results in prediction of the next letter when the preceding text is known."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472759"
                        ],
                        "name": "F. Jelinek",
                        "slug": "F.-Jelinek",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Jelinek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Jelinek"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12495425,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "231f6de83cfa4d641da1681e97a11b689a48e3aa",
            "isKey": false,
            "numCitedBy": 2251,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "The speech recognition problem hidden Markov models the acoustic model basic language modelling the Viterbi search hypothesis search on a tree and the fast match elements of information theory the complexity of tasks - the quality of language models the expectation - maximization algorithm and its consequences decision trees and tree language models phonetics from orthography - spelling-to-base from mappings triphones and allophones maximum entropy probability estimation and language models three applications of maximum entropy estimation to language modelling estimation of probabilities from counts and the Back-Off method."
            },
            "slug": "Statistical-methods-for-speech-recognition-Jelinek",
            "title": {
                "fragments": [],
                "text": "Statistical methods for speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The speech recognition problem hidden Markov models the acoustic model basic language modelling the Viterbi search hypothesis search on a tree and the fast match elements of information theory."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2086339"
                        ],
                        "name": "C. T. Meadow",
                        "slug": "C.-T.-Meadow",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Meadow",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. T. Meadow"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 29306929,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bae5dd2c5c6366deeb34891ca4763f94fe1c06a1",
            "isKey": false,
            "numCitedBy": 359,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nThis book's purpose is to teach people who will be searching or designing text retrieval systems how the systems work. For designers, it covers problems they will face and reviews currently available solutions to provide a basis for more advanced study. For the searcher its purpose is to describe why such systems work as they do. The book is primarily about computer-based retrieval systems, but the principles apply to nonmechanized ones as well.. \"The book covers the nature of information, how it is organized for use by a computer, how search functions are carried out, and some of the theory underlying these functions. As well, it discusses the interaction between user and system and how retrieved items, users, and complete systems are evaluated. A limited knowledge of mathematics and of computing is assumed."
            },
            "slug": "Text-information-retrieval-systems-Meadow",
            "title": {
                "fragments": [],
                "text": "Text information retrieval systems"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This book covers the nature of information, how it is organized for use by a computer, how search functions are carried out, and some of the theory underlying these functions, and how retrieved items, users, and complete systems are evaluated."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145903504"
                        ],
                        "name": "R. Rosenfeld",
                        "slug": "R.-Rosenfeld",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rosenfeld"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144531812"
                        ],
                        "name": "Xuedong Huang",
                        "slug": "Xuedong-Huang",
                        "structuredName": {
                            "firstName": "Xuedong",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xuedong Huang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8058984,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "914b06746d7305bcb5a38b6b4234e1b08f30a94b",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe two attempt to improve our stochastic language models. In the first, we identify a systematic overestimation in the traditional backoff model, and use statistical reasoning to correct it. Our modification results in up to 6% reduction in the perplexity of various tasks. Although the improvement is modest, it is achieved with hardly any increase in the complexity of the model. Both analysis and empirical data suggest that the modification is most suitable when training data is sparse.In the second attempt, we propose a new type of adaptive language model. Existing adaptive models use a dynamic cache, based on the history of the document seen up to that point. But another source of information in the history, within-document word sequence correlations, has not yet been tapped. We describe a model that attempts to capture this information, using a framework where one word sequence triggers another, causing its estimated probability to be raised. We discuss various issues in the design of such a model, and describe our first attempt at building one. Our preliminary results include a perplexity reduction of between 10% and 32%, depending on the test set."
            },
            "slug": "Improvements-in-Stochastic-Language-Modeling-Rosenfeld-Huang",
            "title": {
                "fragments": [],
                "text": "Improvements in Stochastic Language Modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "Two attempt to improve stochastic language models are described, and a new type of adaptive language model is proposed, using a framework where one word sequence triggers another, causing its estimated probability to be raised."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144120136"
                        ],
                        "name": "M. Stubbs",
                        "slug": "M.-Stubbs",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Stubbs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Stubbs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 60403874,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "c1b9fc3a3585a15299f3bc06c76edd52c83fe2e3",
            "isKey": false,
            "numCitedBy": 499,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "List of Figures, Concordances and Tables. Acknowledgements. Data Conventions and Terminology. Notes on Corpus Data and Software. Part I: Concepts and History:. 1. Texts and Text Types. 2. British Traditions in Text Analysis: Firth, Halliday and Sinclair. 3. Institutional Linguistics: Firth, Hill and Giddens. Part II: Text and Corpus Analysis:. 4. Baden--Powell: A Comparative Analysis of Two Short Texts. 5. Judging the Facts: An Analysis of One Text in its Institutional Context. 6. Human and Inhuman Geography: A Comparative Analysis of Two Long Texts and a Corpus. 7. Keywords, Collocations and Culture: The Analysis of Word Meanings across Corpora. 8. Towards a Modal Grammar of English: A Matter of Prolonged Fieldwork. 9. The Classic Questions. Notes. References. Name Index. Subject Index."
            },
            "slug": "Text-and-Corpus-Analysis:-Computer-Assisted-Studies-Stubbs",
            "title": {
                "fragments": [],
                "text": "Text and Corpus Analysis: Computer-Assisted Studies of Language and Culture"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145002066"
                        ],
                        "name": "D. Younger",
                        "slug": "D.-Younger",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Younger",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Younger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 40504606,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "30da8ecce8b3ebc3e9344a79e5c2f8dc4c423bd2",
            "isKey": false,
            "numCitedBy": 1035,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Recognition-and-Parsing-of-Context-Free-Languages-Younger",
            "title": {
                "fragments": [],
                "text": "Recognition and Parsing of Context-Free Languages in Time n^3"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Control."
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680292"
                        ],
                        "name": "P. Resnik",
                        "slug": "P.-Resnik",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Resnik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Resnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17857497,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "7fd71b88b0ec248336410f58a4e37ed3fbe84698",
            "isKey": false,
            "numCitedBy": 297,
            "numCiting": 73,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Selectional-constraints:-an-information-theoretic-Resnik",
            "title": {
                "fragments": [],
                "text": "Selectional constraints: an information-theoretic model and its computational realization"
            },
            "venue": {
                "fragments": [],
                "text": "Cognition"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30844359"
                        ],
                        "name": "Ye-Yi Wang",
                        "slug": "Ye-Yi-Wang",
                        "structuredName": {
                            "firstName": "Ye-Yi",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ye-Yi Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10282937,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "03b513b30b9d95e39285df1dc93be63e25f2744e",
            "isKey": false,
            "numCitedBy": 136,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Decoding algorithm is a crucial part in statistical machine translation. We describe a stack decoding algorithm in this paper. We present the hypothesis scoring method and the heuristics used in our algorithm. We report several techniques deployed to improve the performance of the decoder. We also introduce a simplified model to moderate the sparse data problem and to speed up the decoding process. We evaluate and compare these techniques/models in our statistical machine translation system."
            },
            "slug": "Decoding-Algorithm-in-Statistical-Machine-Wang-Waibel",
            "title": {
                "fragments": [],
                "text": "Decoding Algorithm in Statistical Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "A stack decoding algorithm is described and the hypothesis scoring method and the heuristics used in the algorithm are presented, and a simplified model to moderate the sparse data problem and to speed up the decoding process is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3299116"
                        ],
                        "name": "F. B. Thompson",
                        "slug": "F.-B.-Thompson",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Thompson",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. B. Thompson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16173809,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "548ec2852d5d5240f18e91abd723db2e83e2f7d7",
            "isKey": false,
            "numCitedBy": 117,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "What about English as a programming language? Few would question that this is a desirable goal. On the other hand, I dare say every one of us has rather deep reservations both about its feasibility and about a number of problems that it entails. This paper presents a point of view which gives some clarity to the relationship between English and programming languages. This point of view has found substance in an experimental system called DEACON. The second paper in this session will describe the specific DEACON system and its capabilities."
            },
            "slug": "English-for-the-computer-Thompson",
            "title": {
                "fragments": [],
                "text": "English for the computer"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper presents a point of view which gives some clarity to the relationship between English and programming languages and has found substance in an experimental system called DEACON."
            },
            "venue": {
                "fragments": [],
                "text": "AFIPS '66 (Fall)"
            },
            "year": 1966
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760530"
                        ],
                        "name": "M. Walker",
                        "slug": "M.-Walker",
                        "structuredName": {
                            "firstName": "Marilyn",
                            "lastName": "Walker",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Walker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47147237"
                        ],
                        "name": "Johanna D. Moore",
                        "slug": "Johanna-D.-Moore",
                        "structuredName": {
                            "firstName": "Johanna",
                            "lastName": "Moore",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Johanna D. Moore"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1363323,
            "fieldsOfStudy": [
                "Sociology"
            ],
            "id": "13b6415216e9b9105bbd317d420e6af7bc2fd997",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 101,
            "paperAbstract": {
                "fragments": [],
                "text": "Computational theories of discourse are concerned with the context-based interpretation or generation of discourse phenomena in text and dialogue. In the past, research in this area focused on specifying the mechanisms underlying particular discourse phenomena; the models proposed were often motivated by a few constructed examples. While this approach led to many theoretical advances, models developed in this manner are difficult to evaluate because it is hard to tell whether they generalize beyond the particular examples used to motivate them. Recently however the field has turned to issues of robustness and the coverage of theories of particular phenomena with respect to specific types of data. This new empirical focus is supported by several recent advances: an increasing theoretical consensus on discourse models; a large amount of online dialogue and textual corpora available; and improvements in component technologies and tools for building and testing discourse and dialogue testbeds. This means that it is now possible to determine how representative particular discourse phenomena are, how frequently they occur, whether they are related to other phenomena, what percentage of the cases a particular model covers, the inherent difficulty of the problem, and how well an algorithm for processing or generating the phenomena should perform to be considered a good model. This issue brings together a collection of papers illustrating recent approaches to empirical research in discourse generation and interpretation. Section 2 gives a general overview of empirical studies in discourse and describes an empirical research strategy that leads from empirical findings to general theories. Section 3 discusses how each article exemplifies the empirical research strategy and how empirical methods have been employed in each research project."
            },
            "slug": "Empirical-Studies-in-Discourse-Walker-Moore",
            "title": {
                "fragments": [],
                "text": "Empirical Studies in Discourse"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This issue brings together a collection of papers illustrating recent approaches to empirical research in discourse generation and interpretation, and describes an empirical research strategy that leads from empirical findings to general theories."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144462875"
                        ],
                        "name": "G. Nunberg",
                        "slug": "G.-Nunberg",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Nunberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Nunberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 55124423,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "d3d46364a81edcaaa21379fe6bfdacf1ff0c196c",
            "isKey": false,
            "numCitedBy": 85,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The phenomenon of systematic polysemy offers a fruitful do\u00ad main forexamlnlng the theoretical differences between lexicological and lexicographic approaches to description. We consider here the process that provides for systematic conversion of count to mass nouns in English (a chicken ^ chicken, an oak ^ oak etc.). From the point of vlew of lexicology, we argue, standard syntactic and pragmatic tests suggest the phenomenon should be described by means of a single unlndlvlduated transfer function thatdoes notdlstlnguish between interpretations (rabbit = \"meat\" vs. \"fur\"). From thepointofviewoflexicography. however, these pragmatically determined \"sense precisions\" are made part of explicit description via the inclusion ofsemantlc \"llcenses,\"a mechanism distinct from lexical rules. 1. Systematic Polysemy It is well known that we can make productive generalizations about the relations among word uses, say in the form of implicational statements like: '1f a word has a use of type s, it also has a use of type s'.\" Thus a word that denotes a place or kind of place can be used to refer to the people who live there (The city /county /state votedfor Jones); a word that denotes a periodical publication or kind of periodical publication can be used to refer to its publisher (The newspaper I The Times opposed the project); and so on. In recent years these regularities have been increasingly prominent in lexical research. In this paper, we will describe the general phenomenon of transfer as \"systematic polysemy,\" and we will use the term \"transfer functions\" to describe the mappings from one class of words to another. Systematic polysemy raises a number of problems for lexicographers, particularly if dictionaries are to be modified to accommodate new types of users and new applications. Some of the difficulties are essentially structural or organizational. Conventional, itembased formats provide no obvious place for listing regularities like these. And even if devices are introduced to represent them, say via the kinds of codes that are used to represent syntactic classes, it is not a simple matter to accommodate their use to ordinary conceptions of sense-structure, or to coordinate their treatment in the defining process. For the purposes of this discussion, however, we will assume that lexicographers have available a format in which such rules can be represented, whether as designed for print or computational presentation. We will also assume some mechanism for achieving con-"
            },
            "slug": "Systematic-polysemy-in-lexicology-and-lexicography-Nunberg",
            "title": {
                "fragments": [],
                "text": "Systematic polysemy in lexicology and lexicography"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper will describe the general phenomenon of transfer as \"systematic polysemy,\" and the term \"transfer functions\" will be used to describe the mappings from one class of words to another."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701656"
                        ],
                        "name": "James L. McClelland",
                        "slug": "James-L.-McClelland",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "McClelland",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James L. McClelland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 56499839,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "4fa569625b5ab35e955a8d5be11a4aa9f59ca424",
            "isKey": false,
            "numCitedBy": 1468,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : This paper presents an alternative to the standard rule based account of a child's acquisition of the past tense in English. Children are typically said to pass through a three-phase acquisition process in which they first learn past tense by rote, then learn the past tense rule and over regularize, and then finally learn the exceptions to the rule. We show that the acquisition data can be accounted for in more detail by dispensing with the assumption that the child learns rules and substituting in its place a simple homogeneous learning procedure. We show how rule-like behavior can emerge from the interactions among a network of units encoding the root form to past tense mapping. A large computer simulation of the learning process demonstrates the operating principles of our alternative account, shows how details of the acquisition process not captured by the rule account emerge, and makes predictions about other details of the acquisition process not yet observed. Keywords: Learning; networks; Language; Verbs; Perceptions; Morphology."
            },
            "slug": "On-learning-the-past-tenses-of-English-verbs:-rules-Rumelhart-McClelland",
            "title": {
                "fragments": [],
                "text": "On learning the past-tenses of English verbs: implicit rules or parallel distributed processing"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown how rule-like behavior can emerge from the interactions among a network of units encoding the root form to past tense mapping, and how details of the acquisition process not captured by the rule account emerge."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797808"
                        ],
                        "name": "G. Salton",
                        "slug": "G.-Salton",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Salton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Salton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144321599"
                        ],
                        "name": "M. McGill",
                        "slug": "M.-McGill",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "McGill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. McGill"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 43685115,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "49af3e80343eb80c61e727ae0c27541628c7c5e2",
            "isKey": false,
            "numCitedBy": 12605,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Some people may be laughing when looking at you reading in your spare time. Some may be admired of you. And some may want be like you who have reading hobby. What about your own feel? Have you felt right? Reading is a need and a hobby at once. This condition is the on that will make you feel that you must read. If you know are looking for the book enPDFd introduction to modern information retrieval as the choice of reading, you can find here."
            },
            "slug": "Introduction-to-Modern-Information-Retrieval-Salton-McGill",
            "title": {
                "fragments": [],
                "text": "Introduction to Modern Information Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Reading is a need and a hobby at once and this condition is the on that will make you feel that you must read."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145848824"
                        ],
                        "name": "Karen Sp\u00e4rck Jones",
                        "slug": "Karen-Sp\u00e4rck-Jones",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Sp\u00e4rck Jones",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karen Sp\u00e4rck Jones"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2996187,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "4f09e6ec1b7d4390d23881852fd7240994abeb58",
            "isKey": false,
            "numCitedBy": 3206,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "The exhaustivity of document descriptions and the specificity of index terms are usually regarded as independent. It is suggested that specificity should be interpreted statistically, as a function of term use rather than of term meaning. The effects on retrieval of variations in term specificity are examined, experiments with three test collections showing in particular that frequently\u2010occurring terms are required for good overall performance. It is argued that terms should be weighted according to collection frequency, so that matches on less frequent, more specific, terms are of greater value than matches on frequent terms. Results for the test collections show that considerable improvements in performance are obtained with this very simple procedure."
            },
            "slug": "A-statistical-interpretation-of-term-specificity-in-Jones",
            "title": {
                "fragments": [],
                "text": "A statistical interpretation of term specificity and its application in retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is argued that terms should be weighted according to collection frequency, so that matches on less frequent, more specific, terms are of greater value than matches on frequent terms."
            },
            "venue": {
                "fragments": [],
                "text": "J. Documentation"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398489029"
                        ],
                        "name": "J. Tague-Sutcliffe",
                        "slug": "J.-Tague-Sutcliffe",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Tague-Sutcliffe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tague-Sutcliffe"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 27462517,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7d4563122e7d4c9bb45e2a56d2592f03d2449dcd",
            "isKey": false,
            "numCitedBy": 271,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Pragmatics-of-Information-Retrieval-Revisited-Tague-Sutcliffe",
            "title": {
                "fragments": [],
                "text": "The Pragmatics of Information Retrieval Experimentation Revisited"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Process. Manag."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2162080"
                        ],
                        "name": "M. Nagao",
                        "slug": "M.-Nagao",
                        "structuredName": {
                            "firstName": "Makoto",
                            "lastName": "Nagao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Nagao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18366233,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bc43f6bccb18a5a4892daa8e66756e0a684e7f5c",
            "isKey": false,
            "numCitedBy": 678,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Problems inherent in current machine translation systems have been reviewed and have been shown to be inherently inconsistent. The present paper defines a model based on a series of human language processing and in particular the use of analogical thinking. Machine translation systems developed so far have a kind of inherent contradiction in themselves. The more detailed a system has become by the additional improvements, the clearer the limitation and the boundary will be for the translation ability. To break through this difficulty we have to think about the mechanism of human translation, and have to build a model based on the fundamental function of language processing in the human brain. The following is an attempt to do this based on the ability of analogy finding in human beings."
            },
            "slug": "A-framework-of-a-mechanical-translation-between-and-Nagao",
            "title": {
                "fragments": [],
                "text": "A framework of a mechanical translation between Japanese and English by analogy principle"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "A model based on a series of human language processing and in particular the use of analogical thinking is defined, based on the ability of analogy finding in human beings."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341779"
                        ],
                        "name": "J. R. Quinlan",
                        "slug": "J.-R.-Quinlan",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Quinlan",
                            "middleNames": [
                                "Ross"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. R. Quinlan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5262555,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "807c1f19047f96083e13614f7ce20f2ac98c239a",
            "isKey": false,
            "numCitedBy": 21897,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nClassifier systems play a major role in machine learning and knowledge-based systems, and Ross Quinlan's work on ID3 and C4.5 is widely acknowledged to have made some of the most significant contributions to their development. This book is a complete guide to the C4.5 system as implemented in C for the UNIX environment. It contains a comprehensive guide to the system's use , the source code (about 8,800 lines), and implementation notes. The source code and sample datasets are also available on a 3.5-inch floppy diskette for a Sun workstation. \n \nC4.5 starts with large sets of cases belonging to known classes. The cases, described by any mixture of nominal and numeric properties, are scrutinized for patterns that allow the classes to be reliably discriminated. These patterns are then expressed as models, in the form of decision trees or sets of if-then rules, that can be used to classify new cases, with emphasis on making the models understandable as well as accurate. The system has been applied successfully to tasks involving tens of thousands of cases described by hundreds of properties. The book starts from simple core learning methods and shows how they can be elaborated and extended to deal with typical problems such as missing data and over hitting. Advantages and disadvantages of the C4.5 approach are discussed and illustrated with several case studies. \n \nThis book and software should be of interest to developers of classification-based intelligent systems and to students in machine learning and expert systems courses."
            },
            "slug": "C4.5:-Programs-for-Machine-Learning-Quinlan",
            "title": {
                "fragments": [],
                "text": "C4.5: Programs for Machine Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A complete guide to the C4.5 system as implemented in C for the UNIX environment, which starts from simple core learning methods and shows how they can be elaborated and extended to deal with typical problems such as missing data and over hitting."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751206"
                        ],
                        "name": "J. Zobel",
                        "slug": "J.-Zobel",
                        "structuredName": {
                            "firstName": "Justin",
                            "lastName": "Zobel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Zobel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144448479"
                        ],
                        "name": "Alistair Moffat",
                        "slug": "Alistair-Moffat",
                        "structuredName": {
                            "firstName": "Alistair",
                            "lastName": "Moffat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alistair Moffat"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14944466,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "50a1a8883ac9c8c70c7d8674f996fb9704d10cc4",
            "isKey": false,
            "numCitedBy": 418,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Ranked queries are used to locate relevant documents in text databases. In a ranked query a list of terms is specified, then the documents that most closely match the query are returned---in decreasing order of similarity---as answers. Crucial to the efficacy of ranked querying is the use of a similarity heuristic, a mechanism that assigns a numeric score indicating how closely a document and the query match. In this note we explore and categorise a range of similarity heuristics described in the literature. We have implemented all of these measures in a structured way, and have carried out retrieval experiments with a substantial subset of these measures.Our purpose with this work is threefold: first, in enumerating the various measures in an orthogonal framework we make it straightforward for other researchers to describe and discuss similarity measures; second, by experimenting with a wide range of the measures, we hope to observe which features yield good retrieval behaviour in a variety of retrieval environments; and third, by describing our results so far, to gather feedback on the issues we have uncovered. We demonstrate that it is surprisingly difficult to identify which techniques work best, and comment on the experimental methodology required to support any claims as to the superiority of one method over another."
            },
            "slug": "Exploring-the-similarity-space-Zobel-Moffat",
            "title": {
                "fragments": [],
                "text": "Exploring the similarity space"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It is demonstrated that it is surprisingly difficult to identify which techniques work best, and comment on the experimental methodology required to support any claims as to the superiority of one method over another."
            },
            "venue": {
                "fragments": [],
                "text": "SIGF"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705051"
                        ],
                        "name": "Y. Sakakibara",
                        "slug": "Y.-Sakakibara",
                        "structuredName": {
                            "firstName": "Yasubumi",
                            "lastName": "Sakakibara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Sakakibara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153856933"
                        ],
                        "name": "M. Brown",
                        "slug": "M.-Brown",
                        "structuredName": {
                            "firstName": "M",
                            "lastName": "Brown",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38849275"
                        ],
                        "name": "R. C. Underwood",
                        "slug": "R.-C.-Underwood",
                        "structuredName": {
                            "firstName": "Rebecca",
                            "lastName": "Underwood",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. C. Underwood"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1863326"
                        ],
                        "name": "I. Mian",
                        "slug": "I.-Mian",
                        "structuredName": {
                            "firstName": "I.",
                            "lastName": "Mian",
                            "middleNames": [
                                "Saira"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Mian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14466736,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0f9303627923c5be2a15fb55adda90662d6c1b63",
            "isKey": false,
            "numCitedBy": 115,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "Stochastic context-free grammars (SCFGs) are used to fold, align and model a family of homologous RNA sequences. SCFGs capture the sequences' common primary and secondary structure and generalize the hidden Markov models (HMMs) used in related work on protein and DNA. The novel aspect of this work is that SCFG parameters are learned automatically from unaligned, unfolded training sequences. A generalization of the HMM forward-backward algorithm is introduced The new algorithm, based on tree grammars and faster than the previously proposed SCFG inside-outside algorithm, is tested on the transfer RNA (tRNA) family. Results show the model can discern tRNA from similar-length RNA sequences, can find secondary structure of new tRNA sequences, and can give multiple alignments of large sets of tRNA sequences. The model is extended to handle introns in tRNA.<<ETX>>"
            },
            "slug": "Stochastic-context-free-grammars-for-modeling-RNA-Sakakibara-Brown",
            "title": {
                "fragments": [],
                "text": "Stochastic context-free grammars for modeling RNA"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A generalization of the HMM forward-backward algorithm is introduced, based on tree grammars and faster than the previously proposed SCFG inside-outside algorithm, and it can discern tRNA from similar-length RNA sequences, can find secondary structure of new tRNA sequences, and can give multiple alignments of large sets of t RNA sequences."
            },
            "venue": {
                "fragments": [],
                "text": "1994 Proceedings of the Twenty-Seventh Hawaii International Conference on System Sciences"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705051"
                        ],
                        "name": "Y. Sakakibara",
                        "slug": "Y.-Sakakibara",
                        "structuredName": {
                            "firstName": "Yasubumi",
                            "lastName": "Sakakibara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Sakakibara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153856933"
                        ],
                        "name": "M. Brown",
                        "slug": "M.-Brown",
                        "structuredName": {
                            "firstName": "M",
                            "lastName": "Brown",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144364614"
                        ],
                        "name": "R. Hughey",
                        "slug": "R.-Hughey",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Hughey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hughey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1863326"
                        ],
                        "name": "I. Mian",
                        "slug": "I.-Mian",
                        "structuredName": {
                            "firstName": "I.",
                            "lastName": "Mian",
                            "middleNames": [
                                "Saira"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Mian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5233893"
                        ],
                        "name": "K. Sj\u00f6lander",
                        "slug": "K.-Sj\u00f6lander",
                        "structuredName": {
                            "firstName": "Kimmen",
                            "lastName": "Sj\u00f6lander",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Sj\u00f6lander"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38849275"
                        ],
                        "name": "R. C. Underwood",
                        "slug": "R.-C.-Underwood",
                        "structuredName": {
                            "firstName": "Rebecca",
                            "lastName": "Underwood",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. C. Underwood"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2713387,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "1435c68bef30424e9f5e657abc3948a3ef2cf903",
            "isKey": false,
            "numCitedBy": 429,
            "numCiting": 90,
            "paperAbstract": {
                "fragments": [],
                "text": "Stochastic context-free grammars (SCFGs) are applied to the problems of folding, aligning and modeling families of tRNA sequences. SCFGs capture the sequences' common primary and secondary structure and generalize the hidden Markov models (HMMs) used in related work on protein and DNA. Results show that after having been trained on as few as 20 tRNA sequences from only two tRNA subfamilies (mitochondrial and cytoplasmic), the model can discern general tRNA from similar-length RNA sequences of other kinds, can find secondary structure of new tRNA sequences, and can produce multiple alignments of large sets of tRNA sequences. Our results suggest potential improvements in the alignments of the D- and T-domains in some mitochondrial tRNAs that cannot be fit into the canonical secondary structure."
            },
            "slug": "Stochastic-context-free-grammars-for-tRNA-modeling.-Sakakibara-Brown",
            "title": {
                "fragments": [],
                "text": "Stochastic context-free grammars for tRNA modeling."
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "Results show that after having been trained on as few as 20 tRNA sequences from only two tRNA subfamilies, the model can discern general tRNA from similar-length RNA sequences of other kinds, can find secondary structure of new t RNA sequences, and can produce multiple alignments of large sets of tRNAs."
            },
            "venue": {
                "fragments": [],
                "text": "Nucleic acids research"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145163573"
                        ],
                        "name": "A. Singhal",
                        "slug": "A.-Singhal",
                        "structuredName": {
                            "firstName": "Amit",
                            "lastName": "Singhal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Singhal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797808"
                        ],
                        "name": "G. Salton",
                        "slug": "G.-Salton",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Salton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Salton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144009691"
                        ],
                        "name": "C. Buckley",
                        "slug": "C.-Buckley",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Buckley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Buckley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 57395249,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1713b37666ebab21fbd675160a10f7fa8d9af80b",
            "isKey": false,
            "numCitedBy": 81,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Optical character recognition (OCR) is the most commonly used technique to convert printed material into electronic form. Using OCR, large repositories of machine readable text can be created in a short time. An information retrieval system can then be used to search through large information bases thus created. Many information retrieval systems use sophisticated term weighting functions to improve the effectiveness of a search. Term weighting schemes can be highly sensitive to the errors in the input text, introduced by the OCR process. This study examines the effects of the well known cosine normalization method in the presence of OCR errors and proposes a new, more robust, normalization method. Experiments show that the new scheme is less sensitive to OCR errors and facilitates use of more diverse basic weighting schemes. It also yields significant improvements in retrieval effectiveness over cosine normalization."
            },
            "slug": "Length-Normalization-in-Degraded-Text-Collections-Singhal-Salton",
            "title": {
                "fragments": [],
                "text": "Length Normalization in Degraded Text Collections"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This study examines the effects of the well known cosinenormalization method in the presence of OCR errors and proposes a new, more robust, normalization method that yields significant improvements in retrieval effectiveness over cosine normalization."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144160326"
                        ],
                        "name": "Craig Silverstein",
                        "slug": "Craig-Silverstein",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Silverstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Craig Silverstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34165212"
                        ],
                        "name": "Jan O. Pedersen",
                        "slug": "Jan-O.-Pedersen",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Pedersen",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jan O. Pedersen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2243637,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9336a75b53f6efe7121cfea0c109d1aa9a83ff11",
            "isKey": false,
            "numCitedBy": 72,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Methods exist for comtant-time clustering of corpus subsets selected via Scatter/Gather browsing [3]. In thii paper we expand on those techniqum, giving an algorithm for alrnostconstant-time clustering of arbitrary corpus subsets. This algorithm is never slower than clustering the document set from scratch, and for medium-sised and large sets it is significantly faster. ThE algorithm ia USSM for clustering arbitrary subsets of large corpora \u2014 obtained, for instance, by a boolean search \u2014 quickly enough to be useful in an interactive setting."
            },
            "slug": "Almost-constant-time-clustering-of-arbitrary-corpus-Silverstein-Pedersen",
            "title": {
                "fragments": [],
                "text": "Almost-constant-time clustering of arbitrary corpus subsets4"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "An algorithm for alrnostconstant-time clustering of arbitrary corpus subsets of large corpora is given, which is never slower than clustering the document set from scratch, and for medium-sised and large sets it is significantly faster."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '97"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746807"
                        ],
                        "name": "Dan Jurafsky",
                        "slug": "Dan-Jurafsky",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Jurafsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dan Jurafsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10796472"
                        ],
                        "name": "James H. Martin",
                        "slug": "James-H.-Martin",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Martin",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James H. Martin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5073927,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "894149cb66e8af4a20c82840ea3f774888644fa6",
            "isKey": false,
            "numCitedBy": 3257,
            "numCiting": 356,
            "paperAbstract": {
                "fragments": [],
                "text": "is one of the most recognizablecharacters in 20th century cinema. HAL is an arti\ufb01cial agent capable of such advancedlanguage behavior as speaking and understanding English, and at a crucial moment inthe plot, even reading lips. It is now clear that HAL\u2019s creator, Arthur C. Clarke, wasa little optimistic in predicting when an arti\ufb01cial agent such as HAL would be avail-able. But just how far off was he? What would it take to create at least the language-relatedpartsofHAL?WecallprogramslikeHALthatconversewithhumansinnatural"
            },
            "slug": "Speech-and-Language-Processing-Jurafsky-Martin",
            "title": {
                "fragments": [],
                "text": "Speech and Language Processing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145322333"
                        ],
                        "name": "H. Ney",
                        "slug": "H.-Ney",
                        "structuredName": {
                            "firstName": "Hermann",
                            "lastName": "Ney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48873196"
                        ],
                        "name": "Sven C. Martin",
                        "slug": "Sven-C.-Martin",
                        "structuredName": {
                            "firstName": "Sven",
                            "lastName": "Martin",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sven C. Martin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782283"
                        ],
                        "name": "F. Wessel",
                        "slug": "F.-Wessel",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Wessel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Wessel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 78
                            }
                        ],
                        "text": "Other discussions of estimation techniques can be found in (Jelinek 1997) and (Ney et al. 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 4
                            }
                        ],
                        "text": "See (Ney et al. 1997) for a proof that the relative frequency really is the maximum likelihood estimate."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 118943002,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "382c5a5f938d426fb7734994933804b5b8b3fddd",
            "isKey": false,
            "numCitedBy": 120,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The need for a stochastic language model in speech recognition arises from Bayes\u2019 decision rule for minimum error rate (Bahl et al., 1983). The word sequence w1 ... w N to be recognized from the sequence of acoustic observations x 1 ... x T is determined as that word sequence w 1 ... w N for which the posterior probability Pr(w 1 ... w N |x 1 ... x T ) attains its maximum. This rule can be rewritten in the form: \n \n$$ \\mathop {\\arg \\,\\max }\\limits_{{\\omega _1} \\cdot \\cdot \\cdot {\\omega _N}} \\{ \\Pr ({\\omega _1} \\cdot \\cdot \\cdot {\\omega _N}) \\cdot \\Pr ({x_1} \\cdot \\cdot \\cdot {x_T}\\left| {{\\omega _1} \\cdot \\cdot \\cdot {\\omega _N}} \\right.)\\} , $$ \n \n, where Pr(x 1 ... x T |w 1 ... w N ) is the conditional probability of, given the word sequence w 1 ... w N , observing the sequence of acoustic measurements x 1 ... x T and where Pr(w 1 ... w N ) is the prior probability of producing the word sequence w 1 ... w N ."
            },
            "slug": "Statistical-Language-Modeling-Using-Leaving-One-Out-Ney-Martin",
            "title": {
                "fragments": [],
                "text": "Statistical Language Modeling Using Leaving-One-Out"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "The need for a stochastic language model in speech recognition arises from Bayes\u2019 decision rule for minimum error rate and the conditional probability of the word sequence w 1 ... w N is determined."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9419406"
                        ],
                        "name": "I. Witten",
                        "slug": "I.-Witten",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Witten",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Witten"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49014513"
                        ],
                        "name": "T. Bell",
                        "slug": "T.-Bell",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Bell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Bell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10314497,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f5d21625f8264f455591b3c7cbdac18b983b3c0",
            "isKey": false,
            "numCitedBy": 848,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Approaches to the zero-frequency problem in adaptive text compression are discussed. This problem relates to the estimation of the likelihood of a novel event occurring. Although several methods have been used, their suitability has been on empirical evaluation rather than a well-founded model. The authors propose the application of a Poisson process model of novelty. Its ability to predict novel tokens is evaluated, and it consistently outperforms existing methods. It is applied to a practical statistical coding scheme, where a slight modification is required to avoid divergence. The result is a well-founded zero-frequency model that explains observed differences in the performance of existing methods, and offers a small improvement in the coding efficiency of text compression over the best method previously known. >"
            },
            "slug": "The-zero-frequency-problem:-Estimating-the-of-novel-Witten-Bell",
            "title": {
                "fragments": [],
                "text": "The zero-frequency problem: Estimating the probabilities of novel events in adaptive text compression"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The authors propose the application of a Poisson process model of novelty, which ability to predict novel tokens is evaluated, and it consistently outperforms existing methods and offers a small improvement in the coding efficiency of text compression over the best method previously known."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2889602"
                        ],
                        "name": "Erik D. Wiener",
                        "slug": "Erik-D.-Wiener",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Wiener",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Erik D. Wiener"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34165212"
                        ],
                        "name": "Jan O. Pedersen",
                        "slug": "Jan-O.-Pedersen",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Pedersen",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jan O. Pedersen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2024710"
                        ],
                        "name": "A. Weigend",
                        "slug": "A.-Weigend",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Weigend",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Weigend"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17503448,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "abbe40b7503f51971c92f9f9b20ebea6c0b36d77",
            "isKey": false,
            "numCitedBy": 459,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an application of nonlinear neural networks to topic spotting. Neural networks allow us to model higher-order interaction between document terms and to simultaneously predict multiple topics using shared hidden features. In the context of this model, we compare two approaches to dimensionality reduction in representation: one based on term selection and another based on Latent Semantic Indexing (LSI). Two diierent methods are proposed for improving LSI representations for the topic spotting task. We nd that term selection and our modiied LSI representations lead to similar topic spotting performance, and that this performance is equal to or better than other published results on the same corpus."
            },
            "slug": "A-neural-network-approach-to-topic-spotting-Wiener-Pedersen",
            "title": {
                "fragments": [],
                "text": "A neural network approach to topic spotting"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "It is shown that term selection and the modiied LSI representations lead to similar topic spotting performance, and that this performance is equal to or better than other published results on the same corpus."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144847246"
                        ],
                        "name": "Andrei Mikheev",
                        "slug": "Andrei-Mikheev",
                        "structuredName": {
                            "firstName": "Andrei",
                            "lastName": "Mikheev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrei Mikheev"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15062071,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce007f24793c37804bfa990f27d729f082bdc234",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Maximum entropy framework proved to be expressive and powerful for the statistical language modelling, but it suffers from the computational expensiveness of the model building. The iterative scaling algorithm that is used for the parameter estimation is computationally expensive while the feature selection process might require to estimate parameters for many candidate features many times. In this paper we present a novel approach for building maximum entropy models. Our approach uses the feature collocation lattice and builds complex candidate features without resorting to iterative scaling."
            },
            "slug": "Feature-Lattices-for-Maximum-Entropy-Modelling-Mikheev",
            "title": {
                "fragments": [],
                "text": "Feature Lattices for Maximum Entropy Modelling"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper presents a novel approach for building maximum entropy models that uses the feature collocation lattice and builds complex candidate features without resorting to iterative scaling."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145163573"
                        ],
                        "name": "A. Singhal",
                        "slug": "A.-Singhal",
                        "structuredName": {
                            "firstName": "Amit",
                            "lastName": "Singhal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Singhal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7913028,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e5a4f203dcc8ee607a3d41c1f96c5e91f4a66117",
            "isKey": false,
            "numCitedBy": 379,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "We discuss two learning algorithms for text filtering: modified Rocchio and a boosting algorithm called AdaBoost. We show how both algorithms can be adapted to maximize any general utility matrix that associates cost (or gain) for each pair of machine prediction and correct label. We first show that AdaBoost significantly outperforms another highly effective text filtering algorithm. We then compare AdaBoost and Rocchio over three large text filtering tasks. Overall both algorithms are comparable and are quite effective. AdaBoost produces better classifiers than Rocchio when the training collection contains a very large number of relevant documents. However, on these tasks, Rocchio runs much faster than AdaBoost."
            },
            "slug": "Boosting-and-Rocchio-applied-to-text-filtering-Schapire-Singer",
            "title": {
                "fragments": [],
                "text": "Boosting and Rocchio applied to text filtering"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "This paper discusses two learning algorithms for text filtering: modified Rocchio and a boosting algorithm called AdaBoost, and shows how both algorithms can be adapted to maximize any general utility matrix that associates cost for each pair of machine prediction and correct label."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '98"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144418438"
                        ],
                        "name": "Hinrich Sch\u00fctze",
                        "slug": "Hinrich-Sch\u00fctze",
                        "structuredName": {
                            "firstName": "Hinrich",
                            "lastName": "Sch\u00fctze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hinrich Sch\u00fctze"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6673092,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8559f2b6e61a07da52ce4dfc33a4b29e77a06e5f",
            "isKey": false,
            "numCitedBy": 74,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new approach to disambiguating syntactically ambiguous words in context, based on Variable Memory Markov (VMM) models. In contrast to fixed-length Markov models, which predict based on fixed-lenth histories, variable memory Markov models dynamically adapt their history length based on the training data, and hence may use fewer parameters. In a test of a VMM based tagger on the Brown corpus, 95.81% of tokens are correctly classified."
            },
            "slug": "Part-of-Speech-Tagging-Using-a-Variable-Memory-Sch\u00fctze-Singer",
            "title": {
                "fragments": [],
                "text": "Part-of-Speech Tagging Using a Variable Memory Markov Model"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "A new approach to disambiguating syntactically ambiguous words in context, based on Variable Memory Markov (VMM) models, which dynamically adapt their history length based on the training data, and hence may use fewer parameters."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37669047"
                        ],
                        "name": "E. Sapir",
                        "slug": "E.-Sapir",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Sapir",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Sapir"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62356396,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "630252a7302663faf6ea47ac01dcd6b611cea65b",
            "isKey": false,
            "numCitedBy": 1843,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface 1. Introductory: language defined 2. The elements of speech 3. The sounds of language 4. Form in language: grammatical processes 5. Form in language: grammatical concepts 6. Types of linguistic structure 7. Language as a historical product: drift 8. Language as a historical product: phonetic law 9. How languages influence each other 10. Language, race, and culture 11. Language and literature Index."
            },
            "slug": "Language:-An-Introduction-to-the-Study-of-Speech-Sapir",
            "title": {
                "fragments": [],
                "text": "Language: An Introduction to the Study of Speech"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The author examines how languages influence each other and investigates the role of language, race, and culture in the development of language and literature."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1949
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2086303"
                        ],
                        "name": "R. L. Humphreys",
                        "slug": "R.-L.-Humphreys",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Humphreys",
                            "middleNames": [
                                "Lee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. L. Humphreys"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 35779703,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "eff4df5300e357925047fe4c01c06481c4b89b1e",
            "isKey": false,
            "numCitedBy": 69,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "There are three sorts of book on punctuation. The first, prototypically authored by retired schoolmasters, is selflessly dedicated to the the task of bringing Punctuation to the Peasantry. Somewhat \u201chobby-horsical\u201d in tone, they usually contain lengthy pleas for the better treatment of the semi-colon, a stop which the author invariably considers to have been \u201cwantonly neglected\u201d of late. The second sort is the Style Guide, written by editors and printers for the private pleasure of fellow professionals and, in consequence, rarely seen in the open. The most important function of the guides is to state which punctuation convention is to be adopted in those places where the punctuation system as a whole leaves open the possibility of alternatives (e.g. the appropriate handling of full-stops within embedded quotes). The third sort, on the linguistics of the punctuation system, is much the rarest of all. In fact, in addition to the present work, the only other I know is that of Meyer (1987), a careful study based on searches of the Brown corpus. Presumably this paucity reflects a widespread belief that, mere stipulation aside, there really is nothing much to be said of the topic. I would be lying if I said that Myers and Nunberg have stumbled upon the shores of a linguistic America, returning laden with treasure and tales of vast new territories awaiting the attentions of the intrepid researcher; for it is not so. But there are certainly some discoveries to be made. At the most formal level, the rules governing the writing out of punctuation marks must be explored. For example, dashes generally come in pairs like round brackets except where the the second dash clashes with a fullstop; on the other hand, when it clashes with a would-be comma, these days"
            },
            "slug": "The-linguistics-of-punctuation-Humphreys",
            "title": {
                "fragments": [],
                "text": "The linguistics of punctuation"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "There are three sorts of book on punctuation, and the linguistics of the punctuation system, based on searches of the Brown corpus, is much the rarest of all."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Translation"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341779"
                        ],
                        "name": "J. R. Quinlan",
                        "slug": "J.-R.-Quinlan",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Quinlan",
                            "middleNames": [
                                "Ross"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. R. Quinlan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13252401,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bcee7c85d237b79491a773ef51e746bbbcf48e35",
            "isKey": false,
            "numCitedBy": 13488,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "The technology for building knowledge-based systems by inductive inference from examples has been demonstrated successfully in several practical applications. This paper summarizes an approach to synthesizing decision trees that has been used in a variety of systems, and it describes one such system, ID3, in detail. Results from recent studies show ways in which the methodology can be modified to deal with information that is noisy and/or incomplete. A reported shortcoming of the basic algorithm is discussed and two means of overcoming it are compared. The paper concludes with illustrations of current research directions."
            },
            "slug": "Induction-of-Decision-Trees-Quinlan",
            "title": {
                "fragments": [],
                "text": "Induction of Decision Trees"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper summarizes an approach to synthesizing decision trees that has been used in a variety of systems, and it describes one such system, ID3, in detail, which is described in detail."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144096985"
                        ],
                        "name": "G. Miller",
                        "slug": "G.-Miller",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Miller",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34729490"
                        ],
                        "name": "W. Charles",
                        "slug": "W.-Charles",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Charles",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Charles"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 145580646,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "402627e4eb8c95e4aae3026fd921aa08cd792006",
            "isKey": false,
            "numCitedBy": 1678,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract The relationship between semantic and contextual similarity is investigated for pairs of nouns that vary from high to low semantic similarity. Semantic similarity is estimated by subjective ratings; contextual similarity is estimated by the method of sorting sentential contexts. The results show an inverse linear relationship between similarity of meaning and the discriminability of contexts. This relation, is obtained for two separate corpora of sentence contexts. It is concluded that, on average, for words in the same language drawn from the same syntactic and semantic categories, the more often two words can be substituted into the same contexts the more similar in meaning they are judged to be."
            },
            "slug": "Contextual-correlates-of-semantic-similarity-Miller-Charles",
            "title": {
                "fragments": [],
                "text": "Contextual correlates of semantic similarity"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52628848"
                        ],
                        "name": "P. Roget",
                        "slug": "P.-Roget",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Roget",
                            "middleNames": [
                                "Mark"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Roget"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 105
                            }
                        ],
                        "text": "Thesaurus-based disambiguation exploits the semantic categorization provided by a thesaurus like Roget\u2019s (Roget 1946) or a dictionary with subject categories like Longman\u2019s (Procter 1978)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 104
                            }
                        ],
                        "text": "Yarowsky (1992) shows how to apply the semantic categorization of words (derived from the categories in Roget\u2019s thesaurus) to the semantic categorization and disambiguation of contexts. In Dagan and Itai\u2019s method (1994), translations of the different senses are extracted from a bilingual dictionary and their distribution in a foreign language corpus is analyzed for disambiguation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60883708,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "50857b6d8680200147a91e6de87e8ef8d4e1cdb9",
            "isKey": false,
            "numCitedBy": 237,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This thesaurus is based on the system devised by Peter Roget for his original thesaurus of 1852. Words and phrases are arranged according to the ideas or concepts that they express within an overall system of categories and classes. Thus the user can start from an idea and explore different ways to express it. This fifth edition brings Roget's original category concept towards the 21st century. 325,000 words and phrases are grouped by concept into 1073 categories with all synonyms, antonyms and related words presented together for quick and easy reference. The text has been fully revised and expanded, including 31 entirely new categories, such as computers, fitness and exercise, and all kinds of sport. The whole range of language from archaisms to the latest slang (labelled) is included."
            },
            "slug": "Roget's-International-Thesaurus-Roget",
            "title": {
                "fragments": [],
                "text": "Roget's International Thesaurus"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This fifth edition brings Roget's original category concept towards the 21st century, including 31 entirely new categories, such as computers, fitness and exercise, and all kinds of sport."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145764662"
                        ],
                        "name": "S. McGrath",
                        "slug": "S.-McGrath",
                        "structuredName": {
                            "firstName": "Sean",
                            "lastName": "McGrath",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. McGrath"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60210763,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5c0f683ac2a9bee74070ff6527ebdb7e18942198",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "PARSEME.1ST is the first developer's guide to both SGML's practicalities and its worldview. It will tell you everything you need to build world-class SGML systems and products. Start by walking through all the basics of SGML - from the perspective of an experienced developer. Understand the structure of SGML documents and systems - and how SGML can solve a remarkable number of real-world problems. Then, get the detailed information and worked examples you need to implement practical solutions."
            },
            "slug": "PARSEME.1st-:-SGML-for-software-developers-McGrath",
            "title": {
                "fragments": [],
                "text": "PARSEME.1st : SGML for software developers"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "SGML.PARSEME.1ST is the first developer's guide to both SGML's practicalities and its worldview, and will tell you everything you need to build world-class SGML systems and products."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762744"
                        ],
                        "name": "A. Stolcke",
                        "slug": "A.-Stolcke",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Stolcke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Stolcke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808760"
                        ],
                        "name": "S. Omohundro",
                        "slug": "S.-Omohundro",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Omohundro",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Omohundro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1985596,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "204f6148bc6aba37eb5a7c5686d80547a99425b1",
            "isKey": false,
            "numCitedBy": 175,
            "numCiting": 73,
            "paperAbstract": {
                "fragments": [],
                "text": "This report describes a new technique for inducing the structure of Hidden Markov Models from data which is based on the general `model merging' strategy (Omohundro 1992). The process begins with a maximum likelihood HMM that directly encodes the training data. Successively more general models are produced by merging HMM states. A Bayesian posterior probability criterion is used to determine which states to merge and when to stop generalizing. The procedure may be considered a heuristic search for the HMM structure with the highest posterior probability. We discuss a variety of possible priors for HMMs, as well as a number of approximations which improve the computational efficiency of the algorithm. We studied three applications to evaluate the procedure. The first compares the merging algorithm with the standard Baum-Welch approach in inducing simple finite-state languages from small, positive-only training samples. We found that the merging procedure is more robust and accurate, particularly with a small amount of training data. The second application uses labelled speech data from the TIMIT database to build compact, multiple-pronunciation word models that can be used in speech recognition. Finally, we describe how the algorithm was incorporated in an operational speech understanding system, where it is combined with neural network acoustic likelihood estimators to improve performance over single-pronunciation word models."
            },
            "slug": "Best-first-Model-Merging-for-Hidden-Markov-Model-Stolcke-Omohundro",
            "title": {
                "fragments": [],
                "text": "Best-first Model Merging for Hidden Markov Model Induction"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A new technique for inducing the structure of Hidden Markov Models from data which is based on the general `model merging' strategy, and how the algorithm was incorporated in an operational speech understanding system, where it was combined with neural network acoustic likelihood estimators to improve performance over single-pronunciation word models."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103926434"
                        ],
                        "name": "R. G. Kent",
                        "slug": "R.-G.-Kent",
                        "structuredName": {
                            "firstName": "Roland",
                            "lastName": "Kent",
                            "middleNames": [
                                "Grubb"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. G. Kent"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4632093"
                        ],
                        "name": "G. Zipf",
                        "slug": "G.-Zipf",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Zipf",
                            "middleNames": [
                                "Kingsley"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Zipf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 149728126,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "7c6f403323f88770db22e593f544e59ad0dbc13b",
            "isKey": false,
            "numCitedBy": 198,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "BEFORE presenting in detail this investigation, I think that it may not be amiss to give a brief account of its origin and development. Some time ago it occurred to me that a great deal of light might be thrown on the history of the languages of the past by an extensive examination of modern, spoken tongues from a new point of view. It has, of course, ever been emphasized that sound-changes similar to those of remoter times are constantly taking place in the present, and thus affording not only the advantage of more direct observation, but also data otherwise obtainable or suggested only by subjective analysis. As time passed, however, and collateral facts began to accumulate rapidly, the firm outlines of a powerful law of language became ever more manifest, until, after having presented my data to others better qualified in physics, psychology, and biology, I was encouraged to take up the whole question of mutation in language itself. With my a priori theory in mind, I accordingly proceeded to collect evidence, pursuing the following course. I argued that, if my theory were valid, it should be demonstrable in modern speech. Upon research in ten modern languages I began to be convinced, to my own amazement and that of my scientific friends, of the validity of my theory. Emboldened by this discovery, I proceeded to consider phonetic and accentual changes in the so-called \"dead\" languages, Greek, Latin, Sanskrit, and Gothic. Obviously, the scope of such a thesis will not admit, within the limits of a single article, a discussion of every ramification of the subject. Hence, I have been content to give analyses only of the most salient sound-changes and shifts in accent, for the sole purpose of putting my theory in a clear, definite light before scholars, whose criticism I invite as an aid for future study. None the less, the arrangement of the presentation is such that I believe any reader, at home in Indo-European"
            },
            "slug": "Relative-Frequency-as-a-Determinant-of-Phonetic-Kent-Zipf",
            "title": {
                "fragments": [],
                "text": "Relative Frequency as a Determinant of Phonetic Change"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1929
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33190254"
                        ],
                        "name": "J. Sinclair",
                        "slug": "J.-Sinclair",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Sinclair",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sinclair"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 162168241,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "b2ee6161d3d3e2026906b4e2cc3ac13119b633ba",
            "isKey": false,
            "numCitedBy": 262,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Cobuild offers a revolutionary approach to understanding, learning, and teaching today's English. This new Cobuild dictionary has been written by an experienced editorial team at the University of Birmingham to meet the needs of all intermediate and advanced students and teachers of English."
            },
            "slug": "Collins-Cobuild-English-dictionary-Sinclair",
            "title": {
                "fragments": [],
                "text": "Collins Cobuild English dictionary"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2070964086"
                        ],
                        "name": "Simon St. Laurent",
                        "slug": "Simon-St.-Laurent",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Laurent",
                            "middleNames": [
                                "St."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon St. Laurent"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61095803,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b712cf5759f068be832d346bcfcc9f3f7c88b760",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nYou spend hours designing and formatting your Web pages to look just right, but when users log on to your site -- wham! -- all your carefully formatted data is scattered helter-skelter across the page! Now there\u0092s no need to curse the quirks of HMTL. Instead, skip over all those cumbersome and redundant HTML tags and jump to the head of the class with XML (eXtensible Markup Language), the new technology being developed by the World Wide Web Consortium. XML offers all the power of HTML and SGML scripting without all the headaches, giving developers a robust new tool in managing information and page formatting with increasing flexibility and usability that's just not possible with today's HMTL. \n \nXML: A Primer takes Web developers through the ins and outs of XML, including tips on integrating XML with Dynamic HTML and Cascading Style Sheets; creating custom search tools, Document Type Definitions (DTDs), customized tips, and commercial Web solutions; managing documents with XML; and using XML for data-driven applications. XML offers developers the opportunity to create documents with built-in frameworks that make getting consistent results much easier, time after time. Best of all, XML is backward-compatible to help ease your transition from HTML into the next phase of Web-based formatting and architecture. \n \nXML combines the strength of SGML with simplicity, versatility, and readability by people and machines. Now designers and developers can create and manage their own formatting tags, content, and hyperlinks, instead of relying on the idiosyncrasies of HTML. \"XML,\" says author Simon St. Laurent, \"is HTML done right.\""
            },
            "slug": "XML:-A-Primer-Laurent",
            "title": {
                "fragments": [],
                "text": "XML: A Primer"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "XML: A Primer takes Web developers through the ins and outs of XML, including tips on integrating XML with Dynamic HTML and Cascading Style Sheets; creating custom search tools, Document Type Definitions, customized tips, and commercial Web solutions; managing documents with XML; and using XML for data-driven applications."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152187551"
                        ],
                        "name": "R. Quirk",
                        "slug": "R.-Quirk",
                        "structuredName": {
                            "firstName": "Randolph",
                            "lastName": "Quirk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Quirk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143998726"
                        ],
                        "name": "G. Leech",
                        "slug": "G.-Leech",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Leech",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Leech"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2867865"
                        ],
                        "name": "Jan Svartvik",
                        "slug": "Jan-Svartvik",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Svartvik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jan Svartvik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Quirk et al. (1985) provide a comprehensive grammar of English."
                    },
                    "intents": []
                }
            ],
            "corpusId": 59805466,
            "fieldsOfStudy": [
                "Education",
                "Linguistics"
            ],
            "id": "9998130785e57709fe9a7bbb92fed7daa4304faf",
            "isKey": false,
            "numCitedBy": 7565,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "An indispensable store of information on the English language, written by some of the best-known grammarians in the world."
            },
            "slug": "A-__-comprehensive-grammar-of-the-English-language-Quirk-Leech",
            "title": {
                "fragments": [],
                "text": "A __ comprehensive grammar of the English language"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "An indispensable store of information on the English language, written by some of the best-known grammarians in the world."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144610645"
                        ],
                        "name": "P. Willett",
                        "slug": "P.-Willett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Willett",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Willett"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17113895,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1fb58b6de34ae18174a111a9f32efaf79bbb0bbe",
            "isKey": false,
            "numCitedBy": 877,
            "numCiting": 112,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Recent-trends-in-hierarchic-document-clustering:-A-Willett",
            "title": {
                "fragments": [],
                "text": "Recent trends in hierarchic document clustering: A critical review"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Process. Manag."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059199068"
                        ],
                        "name": "H. J. Arnold",
                        "slug": "H.-J.-Arnold",
                        "structuredName": {
                            "firstName": "Harvey",
                            "lastName": "Arnold",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. J. Arnold"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122891525,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f0f8114363d6ec9526a454146a63cbf012279673",
            "isKey": false,
            "numCitedBy": 2955,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "PART I: LOOKING AT DATA Looking at Data-Distributions Looking at Data-Relationships Producing Data PART II: PROBABILITY AND INFERENCE Probability: The Study of Randomness Sampling Distributions Introduction to Inference Inference for Distributions Inference for Proportions PART III: TOPICS ON INFERENCE Analysis of Two-Way Tables Inference for Regression Multiple Regression One-Way Analysis of Variance Two-Way Analysis of Variance Additional chapters available on the CD-ROM and the website: Logistic Regression Nonparametric Tests Bootstrap Methods and Permutation Tests Statistics for Quality: Control and Capability"
            },
            "slug": "Introduction-to-the-Practice-of-Statistics-Arnold",
            "title": {
                "fragments": [],
                "text": "Introduction to the Practice of Statistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2222167"
                        ],
                        "name": "E. Ristad",
                        "slug": "E.-Ristad",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Ristad",
                            "middleNames": [
                                "Sven"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Ristad"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6734137,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5b038983b55f8f1a65923cd3c01fc0703bcfa47a",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "The Maximum Entropy Modeling Toolkit supports parameter estimation and prediction for statistical language models in the maximum entropy framework. The maximum entropy framework provides a constructive method for obtaining the unique conditional distribution p*(y|x) that satisfies a set of linear constraints and maximizes the conditional entropy H(p|f) with respect to the empirical distribution f(x). The maximum entropy distribution p*(y|x) also has a unique parametric representation in the class of exponential models, as m(y|x) = r(y|x)/Z(x) where the numerator m(y|x) = prod_i alpha_i^g_i(x,y) is a product of exponential weights, with alpha_i = exp(lambda_i), and the denominator Z(x) = sum_y r(y|x) is required to satisfy the axioms of probability. \nThis manual explains how to build maximum entropy models for discrete domains with the Maximum Entropy Modeling Toolkit (MEMT). First we summarize the steps necessary to implement a language model using the toolkit. Next we discuss the executables provided by the toolkit and explain the file formats required by the toolkit. Finally, we review the maximum entropy framework and apply it to the problem of statistical language modeling. \nKeywords: statistical language models, maximum entropy, exponential models, improved iterative scaling, Markov models, triggers."
            },
            "slug": "Maximum-Entropy-Modeling-Toolkit-Ristad",
            "title": {
                "fragments": [],
                "text": "Maximum Entropy Modeling Toolkit"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This manual explains how to build maximum entropy models for discrete domains with the Maximum Entropy Modeling Toolkit (MEMT), and summarizes the steps necessary to implement a language model using the toolkit."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760530"
                        ],
                        "name": "M. Walker",
                        "slug": "M.-Walker",
                        "structuredName": {
                            "firstName": "Marilyn",
                            "lastName": "Walker",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Walker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144436858"
                        ],
                        "name": "Jeanne Fromer",
                        "slug": "Jeanne-Fromer",
                        "structuredName": {
                            "firstName": "Jeanne",
                            "lastName": "Fromer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeanne Fromer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145254843"
                        ],
                        "name": "Shrikanth S. Narayanan",
                        "slug": "Shrikanth-S.-Narayanan",
                        "structuredName": {
                            "firstName": "Shrikanth",
                            "lastName": "Narayanan",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shrikanth S. Narayanan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11308277,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8332944fb0cd6571095c371d3a65621c6ae77cd4",
            "isKey": false,
            "numCitedBy": 157,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a novel method by which a dialogue agent can learn to choose an optimal dialogue strategy. While it is widely agreed that dialogue strategies should be formulated in terms of communicative intentions, there has been little work on automatically optimizing an agent's choices when there are multiple ways to realize a communicative intention. Our method is based on a combination of learning algorithms and empirical evaluation techniques. The learning component of our method is based on algorithms for reinforcement learning, such as dynamic programming and Q-learning. The empirical component uses the PARADISE evaluation framework (Walker et al., 1997) to identify the important performance factors and to provide the performance function needed by the learning algorithm. We illustrate our method with a dialogue agent named ELVIS (EmaiL Voice Interactive System), that supports access to email over the phone. We show how ELVIS can learn to choose among alternate strategies for agent initiative, for reading messages, and for summarizing email folders."
            },
            "slug": "Learning-Optimal-Dialogue-Strategies:-A-Case-Study-Walker-Fromer",
            "title": {
                "fragments": [],
                "text": "Learning Optimal Dialogue Strategies: A Case Study of a Spoken Dialogue Agent for Email"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "A novel method by which a dialogue agent can learn to choose an optimal dialogue strategy is described, based on a combination of learning algorithms and empirical evaluation techniques."
            },
            "venue": {
                "fragments": [],
                "text": "COLING-ACL"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144418438"
                        ],
                        "name": "Hinrich Sch\u00fctze",
                        "slug": "Hinrich-Sch\u00fctze",
                        "structuredName": {
                            "firstName": "Hinrich",
                            "lastName": "Sch\u00fctze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hinrich Sch\u00fctze"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35211746"
                        ],
                        "name": "David A. Hull",
                        "slug": "David-A.-Hull",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Hull",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David A. Hull"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34165212"
                        ],
                        "name": "Jan O. Pedersen",
                        "slug": "Jan-O.-Pedersen",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Pedersen",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jan O. Pedersen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9131020,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "92062ccb796efbf56fe1ae2dcc8b3a943a2c989b",
            "isKey": false,
            "numCitedBy": 566,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we compare learning techniques based on statistical classification to traditional methods of relevance feedback for the document routing problem. We consider three classification techniques which have decision rules that are derived via explicit error minimization: linear discriminant analysis, logistic regression, and neural networks. We demonstrate that the classifiers perform 1015% better than relevance feedback via Rocchio expansion for the TREC-2 and TREC-3 routing tasks. Error minimization is difficult in high-dimensional feature spaces because the convergence process is slow and the models are prone to overfitting. We use two different strategies, latent semantic indexing and optimal term selection, to reduce the number of features. Our results indicate that features based on latent semantic indexing are more effective for techniques such as linear discriminant analysis and logistic regression, which have no way to protect against overfitting. Neural networks perform equally well with either set of features and can take advantage of the additional information available when both feature sets are used as input."
            },
            "slug": "A-comparison-of-classifiers-and-document-for-the-Sch\u00fctze-Hull",
            "title": {
                "fragments": [],
                "text": "A comparison of classifiers and document representations for the routing problem"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "This paper compares learning techniques based on statistical classification to traditional methods of relevance feedback for the document routing problem and indicates that features based on latent semantic indexing are more effective for techniques such as linear discriminant analysis and logistic regression, which have no way to protect against overfitting."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '95"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2424234"
                        ],
                        "name": "Beatrice Santorini",
                        "slug": "Beatrice-Santorini",
                        "structuredName": {
                            "firstName": "Beatrice",
                            "lastName": "Santorini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Beatrice Santorini"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18146635,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "c2fbd6cead3815b8e7038fda6f0f0254a2218ca7",
            "isKey": false,
            "numCitedBy": 385,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "An enclosed-type magnetic disc recording and/or reproducing apparatus according to this invention, having a pressure chamber to enclose a bearing unit of a flange rotating along with a magnetic disc, introduces an air pressure at a high-pressure portion, created by an air flow produced by rotation of the magnetic disc and the flange, into the pressure chamber so as to raise the pressure within the pressure chamber to a level higher than 1 atm., thereby preventing the lubricant evaporated from the bearing member from sticking to the magnetic disc or head to lower the faculty thereof."
            },
            "slug": "Part-of-speech-tagging-guidelines-for-the-penn-Santorini",
            "title": {
                "fragments": [],
                "text": "Part-of-speech tagging guidelines for the penn treebank project"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762744"
                        ],
                        "name": "A. Stolcke",
                        "slug": "A.-Stolcke",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Stolcke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Stolcke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808760"
                        ],
                        "name": "S. Omohundro",
                        "slug": "S.-Omohundro",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Omohundro",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Omohundro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2798755,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "837fcdfe8fdcc9c7f2f8a8c58b2afd7e64b43ee0",
            "isKey": false,
            "numCitedBy": 317,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a technique for learning both the number of states and the topology of Hidden Markov Models from examples. The induction process starts with the most specific model consistent with the training data and generalizes by successively merging states. Both the choice of states to merge and the stopping criterion are guided by the Bayesian posterior probability. We compare our algorithm with the Baum-Welch method of estimating fixed-size models, and find that it can induce minimal HMMs from data in cases where fixed estimation does not converge or requires redundant parameters to converge."
            },
            "slug": "Hidden-Markov-Model}-Induction-by-Bayesian-Model-Stolcke-Omohundro",
            "title": {
                "fragments": [],
                "text": "Hidden Markov Model} Induction by Bayesian Model Merging"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The algorithm is compared with the Baum-Welch method of estimating fixed-size models, and it is found that it can induce minimal HMMs from data in cases where fixed estimation does not converge or requires redundant parameters to converge."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1389909404"
                        ],
                        "name": "C. Nevill-Manning",
                        "slug": "C.-Nevill-Manning",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Nevill-Manning",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Nevill-Manning"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9419406"
                        ],
                        "name": "I. Witten",
                        "slug": "I.-Witten",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Witten",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Witten"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2171407"
                        ],
                        "name": "G. Paynter",
                        "slug": "G.-Paynter",
                        "structuredName": {
                            "firstName": "Gordon",
                            "lastName": "Paynter",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Paynter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2958213,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "98c61e1f715f48d08b2b804ff313c4f64d718321",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "A key question for digital libraries is this: how should one go about becoming familiar with a digital collection, as opposed to a physical one? Digital collections generally present an appearance which is extremely opaque-a screen, typically a Web page, with no indication of what, or how much, lies beyond: whether a carefully-selected collection or a morass of worthless ephemera; whether half a dozen documents or many millions. At least physical collections occupy physical space, present a physical appearance, and exhibit tangible physical organizatiqn. When standing on the threshold of a large library one gains a sense of presence and permanence that reflects the care taken in building and maintaining the collection inside. No-one could confuse it with a dung-heap! Yet in the digital world the difference is not so palpable."
            },
            "slug": "Browsing-in-digital-libraries:-a-phrase-based-Nevill-Manning-Witten",
            "title": {
                "fragments": [],
                "text": "Browsing in digital libraries: a phrase-based approach"
            },
            "venue": {
                "fragments": [],
                "text": "DL '97"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4464121"
                        ],
                        "name": "J. Jaccard",
                        "slug": "J.-Jaccard",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Jaccard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Jaccard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 60478576,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "b797c9ae5037b0a0176b4e584aa89b16153c373c",
            "isKey": false,
            "numCitedBy": 909,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Now your students can become intelligent consumers of scientific research, without being overwhelmed by the statistics! Jaccard and Becker's text teaches students the basic skills for analyzing data and helps them become intelligent consumers of scientific information. Praised for its real-life applications, the text tells students when to use a particular statistic, why they should use it, and how the statistic should be computed and interpreted. Because many students, given a set of data, cannot determine where to begin in answering relevant research questions, the authors explicate the issues involved in selecting a statistical test. Each statistical technique is introduced by giving instances where the test is most typically applied followed by an interesting research example (each example is taken from psychology literature)."
            },
            "slug": "Statistics-for-the-Behavioral-Sciences-Jaccard",
            "title": {
                "fragments": [],
                "text": "Statistics for the Behavioral Sciences"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "Jaccard and Becker's text teaches students the basic skills for analyzing data and helps them become intelligent consumers of scientific information."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5730960"
                        ],
                        "name": "D. Paul",
                        "slug": "D.-Paul",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Paul",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Paul"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 59739469,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0be6ab93a635f1582f32dd9cc255cc7450ee581d",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "The Lincoln robust hidden Markov model speech recognizer currently provides state-of-the-art performance for both speaker-dependent and speaker-independent large-vocabulary continuous-speech recognition. An early isolated-word version similarly improved the state of the art on a speaker-stress-robustness isolated-word task. This article combines hidden Markov model and speech recognition tutorials with a description of the above recognition systems."
            },
            "slug": "Speech-Recognition-Using-Hidden-Markov-Models-Paul",
            "title": {
                "fragments": [],
                "text": "Speech Recognition Using Hidden Markov Models"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The Lincoln robust hidden Markov model speech recognizer currently provides state-of-the-art performance for both speaker-dependent and speaker-independent large-vocabulary continuous-speech recognition."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102429215"
                        ],
                        "name": "Kai-Fu Lee",
                        "slug": "Kai-Fu-Lee",
                        "structuredName": {
                            "firstName": "Kai-Fu",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai-Fu Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 57420724,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "0071c960f49d8279e7a5503214a3567cb2237505",
            "isKey": false,
            "numCitedBy": 586,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Readings-in-speech-recognition-Waibel-Lee",
            "title": {
                "fragments": [],
                "text": "Readings in speech recognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2403128"
                        ],
                        "name": "C. Samuelsson",
                        "slug": "C.-Samuelsson",
                        "structuredName": {
                            "firstName": "Christer",
                            "lastName": "Samuelsson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Samuelsson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 686,
                                "start": 48
                            }
                        ],
                        "text": "Since space is less of an issue with electronic dictionaries plenty of corpus examples can be integrated into a dictionary entry for the interested user. What we have said about the value of statistical corpus analysis for monolingual dictionaries applies equally to bilingual dictionaries, at least if an aligned corpus is available (Smadja et al. 1996). Another important application of collocations is Information Retrieval Accuracy of retrieval can be improved if the similarity between a user query and a document is determined based on common collocations phrases) instead of common words 1989; Evans et al. 1991; Strzalkowski 1995; Mitra et al. 1997). See Lewis and Jones (1996) and Krovetz (1991) for further discussion of the question of using collocation discovery and NLP in Information Retrieval and Nevill-Manning et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3714725,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "91d7c7f4e5d094f45428c31e6122aa1c6dd44e10",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "A general, practical method for handling sparse data that avoids held-out data and iterative reestimation is derived from first principles. It has been tested on a part-of-speech tagging task and out-performed (deleted) interpolation with context-independent weights, even when the latter used a globally optimal parameter setting determined a posteriori."
            },
            "slug": "Handling-Sparse-Data-by-Successive-Abstraction-Samuelsson",
            "title": {
                "fragments": [],
                "text": "Handling Sparse Data by Successive Abstraction"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "A general, practical method for handling sparse data that avoids held-out data and iterative reestimation is derived from first principles and has been tested on a part-of-speech tagging task and out-performed interpolation with context-independent weights."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763321"
                        ],
                        "name": "E. Saund",
                        "slug": "E.-Saund",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Saund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Saund"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 40878484,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "586c3d62094eb72bc48da44bf6f899720244ed48",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a formulation for unsupervised learning of clusters reflecting multiple causal structure in binary data. Unlike the standard mixture model, a multiple cause model accounts for observed data by combining assertions from many hidden causes, each of which can pertain to varying degree to any subset of the observable dimensions. A crucial issue is the mixing-function for combining beliefs from different cluster-centers in order to generate data reconstructions whose errors are minimized both during recognition and learning. We demonstrate a weakness inherent to the popular weighted sum followed by sigmoid squashing, and offer an alternative form of the nonlinearity. Results are presented demonstrating the algorithm's ability successfully to discover coherent multiple causal representations of noisy test data and in images of printed characters."
            },
            "slug": "Unsupervised-Learning-of-Mixtures-of-Multiple-in-Saund",
            "title": {
                "fragments": [],
                "text": "Unsupervised Learning of Mixtures of Multiple Causes in Binary Data"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This paper presents a formulation for unsupervised learning of clusters reflecting multiple causal structure in binary data, and demonstrates the algorithm's ability to discover coherent multiple causal representations of noisy test data and in images of printed characters."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895771"
                        ],
                        "name": "D. Zipser",
                        "slug": "D.-Zipser",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Zipser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Zipser"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 18100780,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bb4bad84a2fd896edfa4f5c22061b2913fec500d",
            "isKey": false,
            "numCitedBy": 1445,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Feature-discovery-by-competitive-learning-Rumelhart-Zipser",
            "title": {
                "fragments": [],
                "text": "Feature discovery by competitive learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2304881"
                        ],
                        "name": "M. Oaksford",
                        "slug": "M.-Oaksford",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Oaksford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Oaksford"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1803359"
                        ],
                        "name": "N. Chater",
                        "slug": "N.-Chater",
                        "structuredName": {
                            "firstName": "Nick",
                            "lastName": "Chater",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Chater"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1983,
                                "start": 4
                            }
                        ],
                        "text": "SW (Oaksford and Chater 1998) for a recent collection describing different cognitive architectures, including connectionism. The view that language is best explained as a cognitive phenomenon is the central tenet of cognitive linguistics (Lakoff 1987; Langacker 1987, 19911, but many cognitive linguists would not endorse probability theory as a formalization of cognitive linguistics. See also (Schutze 1997). The novel Tom Sawyer is available in the public domain on the internet, currently from sources including the Virginia Electronic Text Center (see the website). Zipf\u2019s work began with (Zipf 1929), his doctoral thesis. His two ma,jor books are (Zipf 1935) and (Zipf 1949). It is interesting to note that Zipf was reviewed harshly by linguists in his day (see, for instance, (Kent 1930) and (Prokosch 1933)). In part these criticisms correctly focussed on the grandiosity of Zipf\u2019s claims (Kent (1930: 88) writes: \u201cproblems of phonology and morphology are not to be solved en musse by one grand general formula\u201d), but they also reflected, even then, a certain ambivalence to the application of statistical methods in linguistics. Nevertheless, prominent American structuralists, such as Martin Joos and Morris Swadesh, did become involved in data collection for statistical studies, with Joos (1936) emphasizing that the question of whether to use statistical methods in linguistics should be evaluated separately from Zipf\u2019s particular claims. As well as (Mandelbrot 1954), Mandelbrot\u2019s investigation of Zipf\u2019s law is summarized in (Mandelbrot 1983) - see especially chapters 38, 40, and 42. Mandelbrot attributes the direction of his life\u2019s work (leading to his well known work on fractals and the Mandelbrot set) to reading a review of (Zipf 1949). Concordances were first constructed by hand for important literary and religious works. Computer concordancing began in the late 1950s for the purposes of categorizing and indexing article titles and abstracts. Luhn (1960) developed the first computer concordancer and coined the term KWIC KWC."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1308,
                                "start": 4
                            }
                        ],
                        "text": "SW (Oaksford and Chater 1998) for a recent collection describing different cognitive architectures, including connectionism. The view that language is best explained as a cognitive phenomenon is the central tenet of cognitive linguistics (Lakoff 1987; Langacker 1987, 19911, but many cognitive linguists would not endorse probability theory as a formalization of cognitive linguistics. See also (Schutze 1997). The novel Tom Sawyer is available in the public domain on the internet, currently from sources including the Virginia Electronic Text Center (see the website). Zipf\u2019s work began with (Zipf 1929), his doctoral thesis. His two ma,jor books are (Zipf 1935) and (Zipf 1949). It is interesting to note that Zipf was reviewed harshly by linguists in his day (see, for instance, (Kent 1930) and (Prokosch 1933)). In part these criticisms correctly focussed on the grandiosity of Zipf\u2019s claims (Kent (1930: 88) writes: \u201cproblems of phonology and morphology are not to be solved en musse by one grand general formula\u201d), but they also reflected, even then, a certain ambivalence to the application of statistical methods in linguistics. Nevertheless, prominent American structuralists, such as Martin Joos and Morris Swadesh, did become involved in data collection for statistical studies, with Joos (1936) emphasizing that the question of whether to use statistical methods in linguistics should be evaluated separately from Zipf\u2019s particular claims."
                    },
                    "intents": []
                }
            ],
            "corpusId": 144310905,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "45900a12a60544ad5ba477f880aa98172466ba96",
            "isKey": false,
            "numCitedBy": 229,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "1. An introduction to rational models of cognition Section 1: General Issues 2. Connectionist models and Bayesian inference 3. Normative and descriptive models of decision making: time discounting and risk sensitivity Section 2: Memory 4. The effectiveness of retrieval from memory 5. Predictions of a Bayesian recognition memory model (and a class of mdels including it) 6. Cueing for context: an alternative to global matching models of recognition memory 7. Sorting out core memory processes 8. Rational and non-rational aspects of forgetting 9. Adaptive analysis of sequential behaviour: oscillators as rational mechanisms Section 3: Categorization & Induction 10. The rational analysis of categorization and the ACT-R architecture 11. Optimum performance and exemplar models of classificaiton 12. A Bayesian analysis of some forms of inductive reasoning 13. Dynamics of dimension weight distribution and flexibility in categorization Section 4: Reasoning 14. Causal mechanism and probability: a normative approach 15. The rational analysis of human contingency judgement 16. Rationality assumption of game theory and the backward induction paradox 17. A revised rational analysis of the selection task: exceptions and sequential sampling 18. Rational analysis of causal conditionals and the selection task 19. The practice of mathematics and science: from calculus to the clothesline problem Section 5: Search 20. The rational analysis of inquiry: the case of parsing 21. Rational analysis of exploratory choice 22. Rationality as optimised cognitive self-regulation"
            },
            "slug": "Rational-models-of-cognition-Oaksford-Chater",
            "title": {
                "fragments": [],
                "text": "Rational models of cognition"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3530486"
                        ],
                        "name": "R. Darnell",
                        "slug": "R.-Darnell",
                        "structuredName": {
                            "firstName": "R\u00e9gna",
                            "lastName": "Darnell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Darnell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 215102261,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "5839ad026ee43f3b72493c416dddb4203791714d",
            "isKey": false,
            "numCitedBy": 861,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "?well worth reproducing in English.?Edts., I. M. Gazette.] On the loss of the epithelium of the intestinal canal, consequent on the excessive secretion of fluid from its surface. We are all well acquainted with the fact, that in certain diseases the outer layers of the epithelial cells protecting the skin are thrown off in flakes ; and I believe that it is the same in Asiatic cholera as regards epithelial cells lining the surface of the jntestinal mucous membrane?a matter of greater pathological significance than that concerning the skin, because the intestinal epithelium is intended to guard more delicate and important structures than the cells that cover the cutis. The symptoms of cholera, however, are very much dependent on this desquamation of the epithelia?a fact which may be demonstrated by the aid of the microscope; but we are not to suppose that all parts of the intestinal canal are equally affected in cholera. The epithelium of the stomach suffers less than that of the intestines, and the upper part of the small intestines is not so deeply involved in the disease as the lower part of the ileum. In the duodenum, where the peristaltic action of the canal is not very strong, you often find the epithelial cells lining the mucous membrane ; the cells are loosened, but riot detached, because this part of the canal has less mechanical work to do than the lower portion of the gut. The valvulae conniventes (kerkring), which are large and closely approximated in the second part of the duodenum, protect by covering in the epithelial celh that lie between them, but on the surface of these folds wo shall observe the commencement of the desquamative process which is so marked in the ileum. We shall see with the naked eye that the epithelium, which should cover the valvulae conniventes, has disappeared in places, leaving small isolated patches of the denuded mucous membrane. In their early stages, these spots are distinguishable by their whiter colour, and by a soft velvet-like texture, which may be well demonstrated if a spot of this kind is isolates1, and fixed on a plate under the object glass of the microscope, little water being allowed to trickle over it. You may also in this way examine the villi, which are clearly denuded of epithelial cells in the patches of the valvulae conniventes above referred to. In some parts wo notice that a space evidently extends through the length of the villi, and externally the villi are covered"
            },
            "slug": "Translation-Darnell",
            "title": {
                "fragments": [],
                "text": "Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "This chapter discusses the loss of the epithelium of the intestinal canal, consequent on the excessive secretion of fluid from its surface, and examines the villi, which are clearly denuded of epithelial cells in the patches of the valvulae conniventes above referred to."
            },
            "venue": {
                "fragments": [],
                "text": "The Indian medical gazette"
            },
            "year": 1873
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "134211067"
                        ],
                        "name": "J. Rocchio",
                        "slug": "J.-Rocchio",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Rocchio",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rocchio"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61859400,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4083ad1066cfa2ff0d65866ef4b011399d6873d1",
            "isKey": false,
            "numCitedBy": 3242,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Relevance-feedback-in-information-retrieval-Rocchio",
            "title": {
                "fragments": [],
                "text": "Relevance feedback in information retrieval"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688124"
                        ],
                        "name": "W. Hersh",
                        "slug": "W.-Hersh",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Hersh",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Hersh"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 27378203,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce9e144bd3ccf80f117af2226bb086f330c1b87e",
            "isKey": false,
            "numCitedBy": 83,
            "numCiting": 197,
            "paperAbstract": {
                "fragments": [],
                "text": "As the health care industry becomes increasingly dependent on electronic information, the need for sophisticated information retrieval systems and for knowledgeable people to design, purchase and use them also increases. This book provides an overview of the theory, practical applications, evaluation and research directions of these systems. In addition to bibliographic and full-text literature retrieval, the author discusses clinical records, multimedia and networked applications."
            },
            "slug": "Information-Retrieval:-A-Health-Care-Perspective-Hersh",
            "title": {
                "fragments": [],
                "text": "Information Retrieval: A Health Care Perspective"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This book provides an overview of the theory, practical applications, evaluation and research directions of information retrieval systems and discusses clinical records, multimedia and networked applications."
            },
            "venue": {
                "fragments": [],
                "text": "Computers and Medicine"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21682565"
                        ],
                        "name": "F. Ramsey",
                        "slug": "F.-Ramsey",
                        "structuredName": {
                            "firstName": "Fred",
                            "lastName": "Ramsey",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Ramsey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38506359"
                        ],
                        "name": "D. W. Schafer",
                        "slug": "D.-W.-Schafer",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Schafer",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. W. Schafer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 116951508,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d93108360a9e70c756f3040a8e2e8d554de71ccc",
            "isKey": false,
            "numCitedBy": 1301,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Drawing Statistical Conclusions. 2. Inference Using t-Distributions. 3. A Closer Look at Assumptions. 4. Alternatives to the t-Tools. 5. Comparisons among Several Samples. 6. Linear Combinations and Multiple Comparisons of Means. 7. Simple Linear Regression: A Model for the Mean. 8. A Closer Look at Assumptions for Simple Linear Regression. 9. Multiple Regression. 10. Inferential Tools for Multiple Regression. 11. Model Checking and Refinement. 12. Strategies for Variable Selection. 13. The Analysis of Variance for Two-Way Classifications. 14. Multifactor Studies Without Replication. 15. Adjustment for Serial Correlation. 16. Repeated Measures and Other Multivariate Responses. 17. Exploratory Tools for Summarizing Multivariate Responses. 18. Comparisons of Proportions or Odds. 19. More Tools for Tables of Counts. 20. Logistics Regression for Binary Response Variables. 21. Logistic Regression for Binomial Counts. 22. Log-Linear Regression for Poisson Counts. 23. Elements of Research Design. 24. Factorial Treatment Arrangements and Blocking Designs. Appendix A. Tables. Appendix B. References. Bibliography. Index."
            },
            "slug": "The-statistical-sleuth-:-a-course-in-methods-of-Ramsey-Schafer",
            "title": {
                "fragments": [],
                "text": "The statistical sleuth : a course in methods of data analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144610645"
                        ],
                        "name": "P. Willett",
                        "slug": "P.-Willett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Willett",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Willett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3103491"
                        ],
                        "name": "Vivienne Winterman",
                        "slug": "Vivienne-Winterman",
                        "structuredName": {
                            "firstName": "Vivienne",
                            "lastName": "Winterman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vivienne Winterman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 120307515,
            "fieldsOfStudy": [
                "Chemistry"
            ],
            "id": "b69b9ab2e2ee35dc287edddb91c5bc8cdf126679",
            "isKey": false,
            "numCitedBy": 176,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Six weighting schemes and six similarity coefficients have been used in a systematic comparison of measures for the determination of inter-molecular structural similarity. The effectiveness of the measures for molecular property prediction was tested using sixteen small sets of compounds for which property data was available. The comparison suggests that the weighting of fragments by their frequency of occurrence in molecules is often useful, but that there are only small differences in performance between the similarity coefficients tested."
            },
            "slug": "A-Comparison-of-Some-Measures-for-the-Determination-Willett-Winterman",
            "title": {
                "fragments": [],
                "text": "A Comparison of Some Measures for the Determination of Inter\u2010Molecular Structural Similarity Measures of Inter\u2010Molecular Structural Similarity"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2222167"
                        ],
                        "name": "E. Ristad",
                        "slug": "E.-Ristad",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Ristad",
                            "middleNames": [
                                "Sven"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Ristad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110942396"
                        ],
                        "name": "Robert G. Thomas",
                        "slug": "Robert-G.-Thomas",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Thomas",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert G. Thomas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2144,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6e218599333e3042eba298708c75c883d705aea5",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a simple variant of the interpolated Markov model with non-emitting state transitions and prove that it is strictly more powerful than any Markov model. Empirical results demonstrate that the non-emitting model outperforms the interpolated model on the Brown corpus and on the Wall Street Journal under a wide range of experimental conditions. The non-emitting model is also much less prone to overtraining."
            },
            "slug": "Hierarchical-Non-Emitting-Markov-Models-Ristad-Thomas",
            "title": {
                "fragments": [],
                "text": "Hierarchical Non-Emitting Markov Models"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "A simple variant of the interpolated Markov model with non-emitting state transitions is described and it is proved that it is strictly more powerful than any Markovmodel."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341779"
                        ],
                        "name": "J. R. Quinlan",
                        "slug": "J.-R.-Quinlan",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Quinlan",
                            "middleNames": [
                                "Ross"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. R. Quinlan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 937841,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "79ea6a5a68e05065f82acd11a478aa7eac5f6c06",
            "isKey": false,
            "numCitedBy": 1657,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Breiman's bagging and Freund and Schapire's boosting are recent methods for improving the predictive power of classifier learning systems. Both form a set of classifiers that are combined by voting, bagging by generating replicated bootstrap samples of the data, and boosting by adjusting the weights of training instances. This paper reports results of applying both techniques to a system that learns decision trees and testing on a representative collection of datasets. While both approaches substantially improve predictive accuracy, boosting shows the greater benefit. On the other hand, boosting also produces severe degradation on some datasets. A small change to the way that boosting combines the votes of learned classifiers reduces this downside and also leads to slightly better results on most of the datasets considered."
            },
            "slug": "Bagging,-Boosting,-and-C4.5-Quinlan",
            "title": {
                "fragments": [],
                "text": "Bagging, Boosting, and C4.5"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "Results of applying Breiman's bagging and Freund and Schapire's boosting to a system that learns decision trees and testing on a representative collection of datasets show boosting shows the greater benefit."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI, Vol. 1"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69857560"
                        ],
                        "name": "A. Mood",
                        "slug": "A.-Mood",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Mood",
                            "middleNames": [
                                "McFarlane"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Mood"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3973811,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "3da8684050df72c07731c030f7669a920ccfffde",
            "isKey": false,
            "numCitedBy": 2373,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "1 probability 2 Random variables, distribution functions, and expectation 3 Special parametric families of univariate distributions 4 Joint and conditional distributions, stochastic independence, more expectation 5 Distributions of functions of random variables 6 Sampling and sampling distributions 7 Parametric point estimation 8 Parametric interval estimation 9 Tests of hypotheses 10 Linear models 11 Nonparametric method Appendix A Mathematical Addendum Appendix B tabular summary of parametric families of distributions Appendix C References and related reading Appendix D Tables"
            },
            "slug": "An-Introduction-to-the-Theory-of-Statistics-Mood",
            "title": {
                "fragments": [],
                "text": "An Introduction to the Theory of Statistics"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1911
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2256718"
                        ],
                        "name": "W. Press",
                        "slug": "W.-Press",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Press",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Press"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48590121"
                        ],
                        "name": "S. Teukolsky",
                        "slug": "S.-Teukolsky",
                        "structuredName": {
                            "firstName": "Saul",
                            "lastName": "Teukolsky",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Teukolsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2608024"
                        ],
                        "name": "W. Vetterling",
                        "slug": "W.-Vetterling",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Vetterling",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Vetterling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35046585"
                        ],
                        "name": "B. Flannery",
                        "slug": "B.-Flannery",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Flannery",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Flannery"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 79
                            }
                        ],
                        "text": "For instance, Chen and Goodman (1996) use Powell\u2019s algorithm, as presented in (Press et al. 1988). Chen and Goodman (1996) show that this simple"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 78
                            }
                        ],
                        "text": "For instance, Chen and Goodman (1996) use Powell\u2019s algorithm, as presented in (Press et al. 1988)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61769312,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "ca2832d2c30287a9ee5b8584cc498d2b1cb14753",
            "isKey": false,
            "numCitedBy": 16689,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Note: Includes bibliographical references, 3 appendixes and 2 indexes.- Diskette v 2.06, 3.5''[1.44M] for IBM PC, PS/2 and compatibles [DOS] Reference Record created on 2004-09-07, modified on 2016-08-08"
            },
            "slug": "Numerical-recipes-in-C-Press-Teukolsky",
            "title": {
                "fragments": [],
                "text": "Numerical recipes in C"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The Diskette v 2.06, 3.5''[1.44M] for IBM PC, PS/2 and compatibles [DOS] Reference Record created on 2004-09-07, modified on 2016-08-08."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "97301357"
                        ],
                        "name": "F. David",
                        "slug": "F.-David",
                        "structuredName": {
                            "firstName": "F.",
                            "lastName": "David",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. David"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "92448292"
                        ],
                        "name": "S. Siegel",
                        "slug": "S.-Siegel",
                        "structuredName": {
                            "firstName": "Sidney",
                            "lastName": "Siegel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Siegel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 124165178,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "53b9b242f8cb2007e8e3dd9db5cd11b88fa6c4a7",
            "isKey": false,
            "numCitedBy": 33564,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This is the revision of the classic text in the field, adding two new chapters and thoroughly updating all others. The original structure is retained, and the book continues to serve as a combined text/reference."
            },
            "slug": "Nonparametric-Statistics-for-the-Behavioral-David-Siegel",
            "title": {
                "fragments": [],
                "text": "Nonparametric Statistics for the Behavioral Sciences."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1956
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1847175"
                        ],
                        "name": "M. Minsky",
                        "slug": "M.-Minsky",
                        "structuredName": {
                            "firstName": "Marvin",
                            "lastName": "Minsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Minsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2434678"
                        ],
                        "name": "S. Papert",
                        "slug": "S.-Papert",
                        "structuredName": {
                            "firstName": "Seymour",
                            "lastName": "Papert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Papert"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5400596,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f74ded11f72099d16591a1191d72262ae6b5f14a",
            "isKey": false,
            "numCitedBy": 3040,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Cambridge, Mass.: MIT Press, 1972. 2nd. ed. The book's aim is to seek general results from the close study of abstract version of devices known as perceptrons"
            },
            "slug": "Perceptrons-an-introduction-to-computational-Minsky-Papert",
            "title": {
                "fragments": [],
                "text": "Perceptrons - an introduction to computational geometry"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "The aim of this book is to seek general results from the close study of abstract version of devices known as perceptrons."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1969
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51083130"
                        ],
                        "name": "F. Rosenblatt",
                        "slug": "F.-Rosenblatt",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Rosenblatt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Rosenblatt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 62710001,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "9b486c647916df9f8be0f8d4fc5c94c493bfaa80",
            "isKey": false,
            "numCitedBy": 1904,
            "numCiting": 77,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : Part I attempts to review the background, basic sources of data, concepts, and methodology to be employed in the study of perceptrons. In Chapter 2, a brief review of the main alternative approaches to the development of brain models is presented. Chapter 3 considers the physiological and psychological criteria for a suitable model, and attempts to evaluate the empirical evidence which is available on several important issues. Chapter 4 contains basic definitions and some of the notation to be used in later sections are presented. Parts II and III are devoted to a summary of the established theoretical results obtained to date. Part II (Chapters 5 through 14) deals with the theory of three-layer series-coupled perceptrons, on which most work has been done to date. Part III (Chapters 15 through 20) deals with the theory of multi-layer and cross-coupled perceptrons. Part IV is concerned with more speculative models and problems for future analysis. Of necessity, the final chapters become increasingly heuristic in character, as the theory of perceptrons is not yet complete, and new possibilities are continually coming to light."
            },
            "slug": "PRINCIPLES-OF-NEURODYNAMICS.-PERCEPTRONS-AND-THE-OF-Rosenblatt",
            "title": {
                "fragments": [],
                "text": "PRINCIPLES OF NEURODYNAMICS. PERCEPTRONS AND THE THEORY OF BRAIN MECHANISMS"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "The background, basic sources of data, concepts, and methodology to be employed in the study of perceptrons are reviewed, and some of the notation to be used in later sections are presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1963
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2222167"
                        ],
                        "name": "E. Ristad",
                        "slug": "E.-Ristad",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Ristad",
                            "middleNames": [
                                "Sven"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Ristad"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17051446,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "d4af62eb3f8fa99850df061ebaf40f2b110c5ed7",
            "isKey": false,
            "numCitedBy": 75,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new solution to multinomial estimation and demonstrate that our solution outperforms standard solutions both in theory and in practice. The novelty of our approach lies in our use of combinatorial priors on strings."
            },
            "slug": "A-natural-law-of-succession-Ristad",
            "title": {
                "fragments": [],
                "text": "A natural law of succession"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A new solution to multinomial estimation using combinatorial priors on strings is presented and it is demonstrated that this solution outperforms standard solutions both in theory and in practice."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1998 IEEE International Symposium on Information Theory (Cat. No.98CH36252)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2928927"
                        ],
                        "name": "C. Stanfill",
                        "slug": "C.-Stanfill",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Stanfill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Stanfill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788375"
                        ],
                        "name": "D. Waltz",
                        "slug": "D.-Waltz",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Waltz",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Waltz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 16624499,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "123a726f6feb2bce29708b68ab2db5cdf9fcdaf4",
            "isKey": false,
            "numCitedBy": 1436,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "The intensive use of memory to recall specific episodes from the past\u2014rather than rules\u2014should be the foundation of machine reasoning."
            },
            "slug": "Toward-memory-based-reasoning-Stanfill-Waltz",
            "title": {
                "fragments": [],
                "text": "Toward memory-based reasoning"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The intensive use of memory to recall specific episodes from the past\u2014rather than rules\u2014should be the foundation of machine reasoning."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102639778"
                        ],
                        "name": "M. Hoffmann",
                        "slug": "M.-Hoffmann",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Hoffmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hoffmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5434452"
                        ],
                        "name": "S. Bernstein",
                        "slug": "S.-Bernstein",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Bernstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Bernstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47377511"
                        ],
                        "name": "E. Weinthal",
                        "slug": "E.-Weinthal",
                        "structuredName": {
                            "firstName": "Erika",
                            "lastName": "Weinthal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Weinthal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 208688438,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "1d5e998761de0e7bb8c9093bcc30d87941d7b58b",
            "isKey": false,
            "numCitedBy": 1543,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This issue of Global Environmental Politics brings together a wide array of articles on topics central to the study of global environmental politics, that range from the importance of international environmental agreements and transparency to climate change and the fossil fuel industry. The issue begins with a fascinating and frightening forum article by Sarah M. Jordaan, Afreen Siddiqi, William Kakenmaster, and Alice C. Hill, titled \u201cThe Climate Vulnerabilities of Global Nuclear Power.\u201d Most discussions of nuclear power and climate change involve the debate over the positives of nuclear power as a lowgreenhouse-gas energy source versus the negatives of nuclear waste. This forum, however, delves into a more troubling relationship\u2014the impact of climate change on nuclear power\u2014detailing the threats to nuclear power facilities from climate change impacts like heat waves and sea level rise. These threats demand more and better international cooperation and standards around nuclear plants, but they have emerged at a nadir of global cooperative efforts around nuclear energy. This forum lays out the challenges and possible directions forward on this crucial issue. A long-standing question in global environmental politics is whether treaties matter. This question is usually applied to multilateral environmental agreements, but Clara Brandi, Dominique Bl\u00fcmer, and Jean-Fr\u00e9d\u00e9ric Morin, in \u201cWhen Do International Treaties Matter for Domestic Environmental Legislation?\u201d broaden its scope to examine preferential trade agreements with environmental provisions alongside MEAs, finding the former have more robust impacts on domestic environmental legislation. Leveraging their fine-grained dataset on trade agreements (see Jean-Fr\u00e9d\u00e9ric Morin, Andreas D\u00fcr, and Lisa Lechner, \u201cMapping the Trade and Environment Nexus: Insights from a New Data Set\u201d in Global Environmental Politics 2018 18 (1): 122-139), they identify several additional relationships that advance our understanding of how international agreements affect domestic environmental legislation, including variation in impacts across specific issues as well as developed and developing countries. In their article on the Paris Agreement and the US fossil fuel industry, Lukas Hermwille and Lisa Sanderink offer a timely analysis of climate change and its effects on global economies and societies. The authors examine how the Paris Agreement provides a strong signal for shifting decision makers in the US fossil fuel industry to take action regarding climate change policy, most notably at a time when the US government under President Donald Trump has pulled"
            },
            "slug": "Introduction-Hoffmann-Bernstein",
            "title": {
                "fragments": [],
                "text": "Introduction"
            },
            "venue": {
                "fragments": [],
                "text": "Brain and Language"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145500689"
                        ],
                        "name": "A. Viterbi",
                        "slug": "A.-Viterbi",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Viterbi",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Viterbi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15843983,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "145c0b53514b02bdc3dadfb2e1cea124f2abd99b",
            "isKey": false,
            "numCitedBy": 5209,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The probability of error in decoding an optimal convolutional code transmitted over a memoryless channel is bounded from above and below as a function of the constraint length of the code. For all but pathological channels the bounds are asymptotically (exponentially) tight for rates above R_{0} , the computational cutoff rate of sequential decoding. As a function of constraint length the performance of optimal convolutional codes is shown to be superior to that of block codes of the same length, the relative improvement increasing with rate. The upper bound is obtained for a specific probabilistic nonsequential decoding algorithm which is shown to be asymptotically optimum for rates above R_{0} and whose performance bears certain similarities to that of sequential decoding algorithms."
            },
            "slug": "Error-bounds-for-convolutional-codes-and-an-optimum-Viterbi",
            "title": {
                "fragments": [],
                "text": "Error bounds for convolutional codes and an asymptotically optimum decoding algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The upper bound is obtained for a specific probabilistic nonsequential decoding algorithm which is shown to be asymptotically optimum for rates above R_{0} and whose performance bears certain similarities to that of sequential decoding algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1967
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712517"
                        ],
                        "name": "L. Rabiner",
                        "slug": "L.-Rabiner",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Rabiner",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rabiner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13618539,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "8fe2ea0a67954f1380b3387e3262f1cdb9f9b3e5",
            "isKey": false,
            "numCitedBy": 24804,
            "numCiting": 98,
            "paperAbstract": {
                "fragments": [],
                "text": "The fabric comprises a novel type of netting which will have particular utility in screening out mosquitoes and like insects and pests. The fabric is defined of voids having depth as well as width and length. The fabric is usable as a material from which to form clothing for wear, or bed coverings, or sleeping bags, etc., besides use simply as a netting."
            },
            "slug": "A-Tutorial-on-Hidden-Markov-Models-and-Selected-Rabiner",
            "title": {
                "fragments": [],
                "text": "A Tutorial on Hidden Markov Models and Selected Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The fabric comprises a novel type of netting which will have particular utility in screening out mosquitoes and like insects and pests."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153281777"
                        ],
                        "name": "D. Marr",
                        "slug": "D.-Marr",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Marr",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Marr"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 53917384,
            "fieldsOfStudy": [
                "Materials Science"
            ],
            "id": "52df763d69ed5fe609a4a88f07e97a6cfaadf59f",
            "isKey": false,
            "numCitedBy": 3089,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "An additive composition which can be adapted into conventional automatic dishwasher detergents for reducing foam during use thereof, comprising:"
            },
            "slug": "Vision:-A-computational-investigation-into-the-Marr",
            "title": {
                "fragments": [],
                "text": "Vision: A computational investigation into the human representation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 24
                            }
                        ],
                        "text": "The application suggested by Church and Hanks (1989) for this form of the t test was lexicography."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 16
                            }
                        ],
                        "text": "For instance Hindle and Rooth (1993) note that a superlative adjective preceding the noun highly biased things towards an NP attachment (in their data)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 85
                            }
                        ],
                        "text": "Foundations of Statistical Natural Language Processing by Christopher D. Manning and Hinrich Schiitze \nMIT Press, 1999 620 pages, hardcover, list price $70.00 ISBN 0-262-13360-1 Review by: Gerhard Weikurn, \nUniversity of the Saarland, Saarbrticken, Germany weikum@cs.uni-sb.de It is a pleasure to write this \nreview of an excellent textbook."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 84
                            }
                        ],
                        "text": "Two proponents of an important role for probabilistic mechanisms in cognition are Anderson (1983, 1990) and Suppes (1984). (Oaksford and Chater 1998) for a recent collection describing different cognitive architectures, including connectionism."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 559,
                                "start": 3
                            }
                        ],
                        "text": "In-depth overview articles of a large number of the subfields of linguistics can be found in (Newmeyer 1988). In many of these areas, the influence of Statistical NLP can now be felt, be it in the widespread use of corpora, or in the adoption of quantitative methods from Statistical NLP. De Saussure 1962 is a landmark work in structuralist linguistics. An excellent in-depth overview of the field of linguistics for non-linguists is provided by the Cambridge Encyclopedia of Language (Crystal 1987). See also (Pinker 1994) for a recent popular book. (1969) presents an extremely thorough study of the possibilities for word derivation in English."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The impact on retrieval effectiveness of the skewed frequency distribution of a word senses"
            },
            "venue": {
                "fragments": [],
                "text": "A C M Transactions on Information Systems. To appear."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145322333"
                        ],
                        "name": "H. Ney",
                        "slug": "H.-Ney",
                        "structuredName": {
                            "firstName": "Hermann",
                            "lastName": "Ney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2037240"
                        ],
                        "name": "U. Essen",
                        "slug": "U.-Essen",
                        "structuredName": {
                            "firstName": "Ute",
                            "lastName": "Essen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Essen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 779,
                                "start": 79
                            }
                        ],
                        "text": "Other discussions of estimation techniques can be found in (Jelinek 1997) and (Ney et al. 1997). Gale and Church (1994) provide detailed coverage of the problems with \u201cadding one.\u201d An approachable account of Good-Turing estimation can be found in (Gale and Sampson 1995). The extensive empirical comparison of various smoothing methods in (Chen and Goodman 1996, 1998) are particularly recommended. The notion of maximum likelihood across the values of a parameter was first defined in (Fisher 1922). See (Ney et al. 1997) for a proof that the relative frequency really is the maximum likelihood estimate. Recently, there has been increasing use of maximum entropy methods for combining models. We defer coverage of maximum entropy models until chapter 16. See Lau et al. (1993) and Rosenfeld (1994, 1996) for applications to language models."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 25
                            }
                        ],
                        "text": "Ney and Essen (1993) and Ney et al. (1994) propose two discounting models: in the absolute discounting model, all non-zero MLE frequencies are discounted by a small constant amount 6 and the frequency so gained is uniformly distributed over unseen events:"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 79
                            }
                        ],
                        "text": "Other discussions of estimation techniques can be found in (Jelinek 1997) and (Ney et al. 1997). Gale and Church (1994) provide detailed coverage of the problems with \u201cadding one."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 14
                            }
                        ],
                        "text": "In particular Ney et al. LEAVING-ONE-OUT (1997) explore a method that they call Leaving-One-Out where the primary training corpus is of size N - 1 tokens, while 1 token is used as held out data for a sort of simulated testing."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 39772363,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cb289d47d9333dbdb469375255ef12c17b76e860",
            "isKey": true,
            "numCitedBy": 33,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Estimating-'small'-probabilities-by-leaving-one-out-Ney-Essen",
            "title": {
                "fragments": [],
                "text": "Estimating 'small' probabilities by leaving-one-out"
            },
            "venue": {
                "fragments": [],
                "text": "EUROSPEECH"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 8
                            }
                        ],
                        "text": "Ney and Essen (1993) and Ney et al. (1994) propose two discounting models: in the absolute discounting model, all non-zero MLE frequencies are discounted by a small constant amount and the frequency so gained is uniformly distributed over unseen events:"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 942,
                                "start": 138
                            }
                        ],
                        "text": "Important research studies on statistical estimation in the context of language modeling include (Katz (Jelinek (Church and Gale (Ney and Essen and (Ristad 1995). Other discussions of estimation techniques can be found in (Jelinek 1997) and (Ney et al. 1997). Gale and Church (1994) provide detailed coverage of the problems with \u201cadding one.\u201d An approachable account of Good-Turing estimation can be found in (Gale and Sampson 1995). The extensive empirical comparison of various smoothing methods in (Chen and Goodman 1996, 1998) are particularly recommended. The notion of maximum likelihood across the values of a parameter was first defined in (Fisher 1922). See (Ney et al. 1997) for a proof that the relative frequency really is the maximum likelihood estimate. Recently, there has been increasing use of maximum entropy methods for combining models. We defer coverage of maximum entropy models until chapter 16. See Lau et al. (1993) and Rosenfeld (1994, 1996) for applications to language models."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 283,
                                "start": 138
                            }
                        ],
                        "text": "Important research studies on statistical estimation in the context of language modeling include (Katz (Jelinek (Church and Gale (Ney and Essen and (Ristad 1995). Other discussions of estimation techniques can be found in (Jelinek 1997) and (Ney et al. 1997). Gale and Church (1994) provide detailed coverage of the problems with \u201cadding one."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Estimating mall probabilities by one-out"
            },
            "venue": {
                "fragments": [],
                "text": "In Eurospeech 3,"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 219306359,
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning Optimal Dialogue Strategies: A Case Study of a Spoken Dialogue Agent for Email"
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            }
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4051785"
                        ],
                        "name": "F. Newmeyer",
                        "slug": "F.-Newmeyer",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Newmeyer",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Newmeyer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 222239531,
            "fieldsOfStudy": [
                "History"
            ],
            "id": "fbee8cb104c75f88f34372905da16e842eed9a5b",
            "isKey": false,
            "numCitedBy": 349,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Linguistics:-The-Cambridge-Survey-Newmeyer",
            "title": {
                "fragments": [],
                "text": "Linguistics: The Cambridge Survey"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40201738"
                        ],
                        "name": "G. Youmans",
                        "slug": "G.-Youmans",
                        "structuredName": {
                            "firstName": "Gilbert",
                            "lastName": "Youmans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Youmans"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 144078203,
            "fieldsOfStudy": [
                "Sociology"
            ],
            "id": "cbc437f580ef36d80a9aacac3b1a88a88cf72080",
            "isKey": false,
            "numCitedBy": 117,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-New-Tool-for-Discourse-Analysis:-The-Profile.-Youmans",
            "title": {
                "fragments": [],
                "text": "A New Tool for Discourse Analysis: The Vocabulary-Management Profile."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "16709314"
                        ],
                        "name": "G. \u0100llport",
                        "slug": "G.-\u0100llport",
                        "structuredName": {
                            "firstName": "Gordon",
                            "lastName": "\u0100llport",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. \u0100llport"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 141091906,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "b2c64d75c6cab419da9f876e2f55ff54480f07e8",
            "isKey": false,
            "numCitedBy": 408,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Psycho-Biology-of-Language.-\u0100llport",
            "title": {
                "fragments": [],
                "text": "The Psycho-Biology of Language."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1936
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144246983"
                        ],
                        "name": "H. Barlow",
                        "slug": "H.-Barlow",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Barlow",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Barlow"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 124937374,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d4c74862e9b0011ea922e3ee171ebd06af568bf5",
            "isKey": false,
            "numCitedBy": 1503,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Vision:-A-computational-investigation-into-the-and-Barlow",
            "title": {
                "fragments": [],
                "text": "Vision: A computational investigation into the human representation and processing of visual information: David Marr. San Francisco: W. H. Freeman, 1982. pp. xvi + 397"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2071154765"
                        ],
                        "name": "S. Waterman",
                        "slug": "S.-Waterman",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Waterman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Waterman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 64335447,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "5e59e9089c49078ad2d4e564600fd86e65beedbc",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Distinguished-usage-Waterman",
            "title": {
                "fragments": [],
                "text": "Distinguished usage"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49637194"
                        ],
                        "name": "A. Mullin",
                        "slug": "A.-Mullin",
                        "structuredName": {
                            "firstName": "Albert",
                            "lastName": "Mullin",
                            "middleNames": [
                                "A."
                            ],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Mullin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51083130"
                        ],
                        "name": "F. Rosenblatt",
                        "slug": "F.-Rosenblatt",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Rosenblatt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Rosenblatt"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61566132,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cccc0a4817fd5f6d8758c66b4065a23897d49f1d",
            "isKey": false,
            "numCitedBy": 2369,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Principles-of-neurodynamics-Mullin-Rosenblatt",
            "title": {
                "fragments": [],
                "text": "Principles of neurodynamics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1962
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69330933"
                        ],
                        "name": "N. Webster",
                        "slug": "N.-Webster",
                        "structuredName": {
                            "firstName": "Noah",
                            "lastName": "Webster",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Webster"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61233785,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "80409624e9c44fcafa7621a7bae4ce26b69307e7",
            "isKey": false,
            "numCitedBy": 807,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Webster's-new-collegiate-dictionary-Webster",
            "title": {
                "fragments": [],
                "text": "Webster's new collegiate dictionary"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1936
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1405337742"
                        ],
                        "name": "I. Mel'cuk",
                        "slug": "I.-Mel'cuk",
                        "structuredName": {
                            "firstName": "Igor",
                            "lastName": "Mel'cuk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Mel'cuk"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 203672231,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "1215415ac4e5abb82d7596538bc81e6247d4f020",
            "isKey": false,
            "numCitedBy": 1326,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Dependency-Syntax:-Theory-and-Practice-Mel'cuk",
            "title": {
                "fragments": [],
                "text": "Dependency Syntax: Theory and Practice"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3215185"
                        ],
                        "name": "G. Sampson",
                        "slug": "G.-Sampson",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Sampson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Sampson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62607414,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cb2e7caade637e2fd8b348ab9f0a965245365a0b",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "How-Fully-Does-a-Machine-Usable-Dictionary-Cover-Sampson",
            "title": {
                "fragments": [],
                "text": "How Fully Does a Machine-Usable Dictionary Cover English Text?"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4632093"
                        ],
                        "name": "G. Zipf",
                        "slug": "G.-Zipf",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Zipf",
                            "middleNames": [
                                "Kingsley"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Zipf"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 141120597,
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "id": "2bcf3e6c2b45c052a0bd0183cc29c03acc4b49ac",
            "isKey": false,
            "numCitedBy": 7038,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Human-behavior-and-the-principle-of-least-effort-Zipf",
            "title": {
                "fragments": [],
                "text": "Human behavior and the principle of least effort"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1949
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144418438"
                        ],
                        "name": "Hinrich Sch\u00fctze",
                        "slug": "Hinrich-Sch\u00fctze",
                        "structuredName": {
                            "firstName": "Hinrich",
                            "lastName": "Sch\u00fctze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hinrich Sch\u00fctze"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60611449,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "8409ac798efbb2730d8185509f86ca6d8087c73a",
            "isKey": false,
            "numCitedBy": 82,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Ambiguity-resolution-in-language-learning-Sch\u00fctze",
            "title": {
                "fragments": [],
                "text": "Ambiguity resolution in language learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797808"
                        ],
                        "name": "G. Salton",
                        "slug": "G.-Salton",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Salton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Salton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61113802,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "939d409a615d4b88c0a84e1f9b99ed67a9053208",
            "isKey": false,
            "numCitedBy": 3147,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-SMART-Retrieval-System\u2014Experiments-in-Automatic-Salton",
            "title": {
                "fragments": [],
                "text": "The SMART Retrieval System\u2014Experiments in Automatic Document Processing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47976263"
                        ],
                        "name": "F. Mosteller",
                        "slug": "F.-Mosteller",
                        "structuredName": {
                            "firstName": "Frederick",
                            "lastName": "Mosteller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Mosteller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144087709"
                        ],
                        "name": "D. L. Wallace",
                        "slug": "D.-L.-Wallace",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Wallace",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. L. Wallace"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60749965,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "af0c05cad7d09a5d7bd296dd24f6172ca8e84cf7",
            "isKey": false,
            "numCitedBy": 388,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Applied-Bayesian-and-classical-inference-:-the-case-Mosteller-Wallace",
            "title": {
                "fragments": [],
                "text": "Applied Bayesian and classical inference : the case of the Federalist papers"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121300608"
                        ],
                        "name": "J. Allan",
                        "slug": "J.-Allan",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Allan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Allan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60250556,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ed22fb29a43401238928cd0c2dcc21e317211a5a",
            "isKey": false,
            "numCitedBy": 506,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Readings-in-information-retrieval.-Allan",
            "title": {
                "fragments": [],
                "text": "Readings in information retrieval."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144120136"
                        ],
                        "name": "M. Stubbs",
                        "slug": "M.-Stubbs",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Stubbs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Stubbs"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 56650351,
            "fieldsOfStudy": [
                "Sociology"
            ],
            "id": "8f04c4fc11ec82e61070e305fee963071ef95542",
            "isKey": false,
            "numCitedBy": 527,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Text-and-corpus-analysis-Stubbs",
            "title": {
                "fragments": [],
                "text": "Text and corpus analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31411799"
                        ],
                        "name": "V. Hansen",
                        "slug": "V.-Hansen",
                        "structuredName": {
                            "firstName": "Vagn",
                            "lastName": "Hansen",
                            "middleNames": [
                                "Lundsgaard"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Hansen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 118976935,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "42b352335a52d2f6b131341e7f0d230c0f366df2",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Geometry-in-Nature-Hansen",
            "title": {
                "fragments": [],
                "text": "Geometry in Nature"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797808"
                        ],
                        "name": "G. Salton",
                        "slug": "G.-Salton",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Salton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Salton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 6699526,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e154ccb2619019c85c9b8aebc7318ec1c6a75463",
            "isKey": false,
            "numCitedBy": 80,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Experiments-in-Automatic-Thesaurus-Construction-for-Salton",
            "title": {
                "fragments": [],
                "text": "Experiments in Automatic Thesaurus Construction for Information Retrieval"
            },
            "venue": {
                "fragments": [],
                "text": "IFIP Congress"
            },
            "year": 1971
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1754834"
                        ],
                        "name": "E. Gurari",
                        "slug": "E.-Gurari",
                        "structuredName": {
                            "firstName": "Eitan",
                            "lastName": "Gurari",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Gurari"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 785898,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "71afeaafe028429fb365f509b66c4951bad8de00",
            "isKey": false,
            "numCitedBy": 1497,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Introduction-to-the-theory-of-computation-Gurari",
            "title": {
                "fragments": [],
                "text": "Introduction to the theory of computation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107261059"
                        ],
                        "name": "S. K. Wong",
                        "slug": "S.-K.-Wong",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Wong",
                            "middleNames": [
                                "K.",
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. K. Wong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698696"
                        ],
                        "name": "Yiyu Yao",
                        "slug": "Yiyu-Yao",
                        "structuredName": {
                            "firstName": "Yiyu",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yiyu Yao"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 21089601,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cda3e3a640f21c6859e5a5ec6955399517de77fd",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-Information-Theoretic-Measure-of-Term-Wong-Yao",
            "title": {
                "fragments": [],
                "text": "An Information-Theoretic Measure of Term Specificity"
            },
            "venue": {
                "fragments": [],
                "text": "J. Am. Soc. Inf. Sci."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797808"
                        ],
                        "name": "G. Salton",
                        "slug": "G.-Salton",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Salton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Salton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705950"
                        ],
                        "name": "E. Fox",
                        "slug": "E.-Fox",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Fox",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Fox"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112226698"
                        ],
                        "name": "Harry Wu",
                        "slug": "Harry-Wu",
                        "structuredName": {
                            "firstName": "Harry",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Harry Wu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207180535,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8b7cbdd9e7fca94e686d726442d94635378ae04f",
            "isKey": false,
            "numCitedBy": 1079,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "In conventional information retrieval Boolean combinations of index terms are used to formulate the users'' information requests. While any document is in principle retrievable by a Boolean query, the amount of output obtainable by Boolean processing is difficult to control, and the retrieved items are not ranked in any presumed order of importance to the user population. In the vector processing model of retrieval, the retrieved items are easily ranked in decreasing order of the query-record similarity, but the queries themselves are unstructured and expressed as simple sets of weighted index terms. A new, extended Boolean information retrieval system is introduced which is intermediate between the Boolean system of query processing and the vector processing model. The query structure inherent in the Boolean system is preserved, while at the same time weighted terms may be incorporated into both queries and stored documents; the retrieved output can also be ranked in strict similarity order with the user queries. A conventional retrieval system can be modified to make use of the extended system. Laboratory tests indicate that the extended system produces better retrieval output than either the Boolean or the vector processing systems."
            },
            "slug": "Extended-Boolean-information-retrieval-Salton-Fox",
            "title": {
                "fragments": [],
                "text": "Extended Boolean information retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A new, extended Boolean information retrieval system is introduced which is intermediate between the Boolean system of query processing and the vector processing model, and Laboratory tests indicate that the extended system produces better retrieval output than either the Boolean or thevector processing systems."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797808"
                        ],
                        "name": "G. Salton",
                        "slug": "G.-Salton",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Salton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Salton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 34382228,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3f2f6772d96d972e3b2da5aaa8a0f2feefdf827f",
            "isKey": false,
            "numCitedBy": 3884,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Automatic-Text-Processing:-The-Transformation,-and-Salton",
            "title": {
                "fragments": [],
                "text": "Automatic Text Processing: The Transformation, Analysis, and Retrieval of Information by Computer"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693256"
                        ],
                        "name": "D. Sankoff",
                        "slug": "D.-Sankoff",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Sankoff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Sankoff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 124314945,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0f5524fd936d692beac6ab0bfcfcfa656cc3178b",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Branching-processes-with-terminal-types:-to-Sankoff",
            "title": {
                "fragments": [],
                "text": "Branching processes with terminal types: application to context-free grammars"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3047425"
                        ],
                        "name": "Amy M. Steier",
                        "slug": "Amy-M.-Steier",
                        "structuredName": {
                            "firstName": "Amy",
                            "lastName": "Steier",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Amy M. Steier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682655"
                        ],
                        "name": "R. Belew",
                        "slug": "R.-Belew",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Belew",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Belew"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60629905,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e3f1bcd14c1b9108918c998503c44b84bb25a714",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Exporting-phrases:-a-statistical-analysis-of-Steier-Belew",
            "title": {
                "fragments": [],
                "text": "Exporting phrases: a statistical analysis of topical language"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403215454"
                        ],
                        "name": "C. M. Sperberg-McQueen",
                        "slug": "C.-M.-Sperberg-McQueen",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Sperberg-McQueen",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. M. Sperberg-McQueen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1776627"
                        ],
                        "name": "L. Burnard",
                        "slug": "L.-Burnard",
                        "structuredName": {
                            "firstName": "Lou",
                            "lastName": "Burnard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Burnard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69348601"
                        ],
                        "name": "Linguistic Computing",
                        "slug": "Linguistic-Computing",
                        "structuredName": {
                            "firstName": "Linguistic",
                            "lastName": "Computing",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Linguistic Computing"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 152874370,
            "fieldsOfStudy": [
                "Political Science"
            ],
            "id": "59b19bfe925d53261133c7e69e043f17b73261f6",
            "isKey": false,
            "numCitedBy": 954,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Guidelines-for-electronic-text-encoding-and-Sperberg-McQueen-Burnard",
            "title": {
                "fragments": [],
                "text": "Guidelines for electronic text encoding and interchange"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2038307"
                        ],
                        "name": "S. Nie\u00dfen",
                        "slug": "S.-Nie\u00dfen",
                        "structuredName": {
                            "firstName": "Sonja",
                            "lastName": "Nie\u00dfen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nie\u00dfen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145247319"
                        ],
                        "name": "S. Vogel",
                        "slug": "S.-Vogel",
                        "structuredName": {
                            "firstName": "Stephan",
                            "lastName": "Vogel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Vogel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145322333"
                        ],
                        "name": "H. Ney",
                        "slug": "H.-Ney",
                        "structuredName": {
                            "firstName": "Hermann",
                            "lastName": "Ney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2324070"
                        ],
                        "name": "C. Tillmann",
                        "slug": "C.-Tillmann",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Tillmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Tillmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13442531,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0991b1a89f9046cdc37e1db1f3f8d2d56b00162c",
            "isKey": false,
            "numCitedBy": 69,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a novel search algorithm for statistical machine translation based on dynamic programming (DP). During the search process two statistical knowledge sources are combined: a translation model and a bigram language model. This search algorithm expands hypotheses along the positions of the target string while guaranteeing progressive coverage of the words in the source string. We present experimental results on the Verbmobil task."
            },
            "slug": "A-DP-based-Search-Algorithm-for-Statistical-Machine-Nie\u00dfen-Vogel",
            "title": {
                "fragments": [],
                "text": "A DP based Search Algorithm for Statistical Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "A novel search algorithm for statistical machine translation based on dynamic programming (DP) that expands hypotheses along the positions of the target string while guaranteeing progressive coverage of the words in the source string."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The application of syntactic language processing to effective phrase matching"
            },
            "venue": {
                "fragments": [],
                "text": "Information Processing &Management"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Together with John Lafferty, she has led two AAAI tutorials on statistical methods in natural language processing"
            },
            "venue": {
                "fragments": [],
                "text": "She received the Stephen and Marilyn Miles Excellence in Teaching Award in 1999 from Cornell\u2019s College of Engineering. Lee\u2019s address is: Department of Computer Science,"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "press. Speech and Language Processing"
            },
            "venue": {
                "fragments": [],
                "text": "press. Speech and Language Processing"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Using multiple knowledge sources for word sense disambiguation"
            },
            "venue": {
                "fragments": [],
                "text": "Computational Linguistics 181-30."
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "SPATTER HMM, see Hidden Markov model"
            },
            "venue": {
                "fragments": [],
                "text": "holonym,"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Deterministic left corner parser"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Conference Record of the 11 th Annual Syposium on Switching and Automata,"
            },
            "year": 1970
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Linear algebra and its applications, 3rd edition"
            },
            "venue": {
                "fragments": [],
                "text": "San Diego: Harcourt, Brace, Jovanovich."
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Recognition and parsing of context free languages in time Information and Control"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1967
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Plausibility and syntactic ambiguity resolution"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 14th Annual Conference of the Cognitive Society."
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Part-of-speech tagging guidelines for the Penn project"
            },
            "venue": {
                "fragments": [],
                "text": "3rd Revision, 2nd printing, Feb. 1995. University of Pennsylvania."
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 105
                            }
                        ],
                        "text": "Thesaurus-based disambiguation exploits the semantic categorization provided by a thesaurus like Roget\u2019s (Roget 1946) or a dictionary with subject categories like Longman\u2019s (Procter 1978)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Roget International Thesaurus"
            },
            "venue": {
                "fragments": [],
                "text": "New York: Thomas Y. ell."
            },
            "year": 1946
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Inflectional morphology needs to be authenticated by hand"
            },
            "venue": {
                "fragments": [],
                "text": "Working Notes of the AAAI Spring Syposium on Building Lexicons for Machine Translation, pp. 99-99, Stanford, CA. AAAI Press."
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Synracric Innovarion: A Connecrionisr Model"
            },
            "venue": {
                "fragments": [],
                "text": "PhD thesis, Stanford."
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Contextual correlates"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Grammarless extraction of phrasal examples from parallel texts"
            },
            "venue": {
                "fragments": [],
                "text": "Sixth International Conference on Theoretical and Methodological Issues in Machine Translation."
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "machine-readable dictionaries in lexical acquisition, 3 14 MacKay and"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "402 active voice, 102 ad-hoc retrieval problem, 530 adding one, 202,225 additivity"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "19981, 608 boundary selector"
            },
            "venue": {
                "fragments": [],
                "text": "Box and Tiao (1973),"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 194
                            }
                        ],
                        "text": "In fact, there are linguistic theories claiming that all linguistic knowledge is knowledge about words (Dependency Grammar (Mel\u2019?uk 1988), Categorial Grammar (Wood 1993), Tree Adjoining Grammar (Schabes et al. 1988; Joshi 1993), \u2018Radical Lexicalism\u2019 (Karttunen 1986)) and all there is to know about a language is the lexicon, thus completely dispensing with grammar as an independent entity."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Parsing strategies with lexicalized grammars: Tree adjoining grammars"
            },
            "venue": {
                "fragments": [],
                "text": "COLING 12, pp. 578-583."
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Identifying unknown proper names"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Train1 vs"
            },
            "venue": {
                "fragments": [],
                "text": "train? Tagging word sense in corpus. In Uri Zernik (ed.), Lexical Acquisition: Exploiting On-Line Resources to Build a Lexicon, pp. 9 1 - 112. Hillsdale, NJ: Lawrence Erlbaum."
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Some applications of tree-based modeling to speech and language indexing"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the DARPA Speech and Natural Language Workshop, pp. 339-352. Morgan Kaufmann."
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A new tool for discourse analysis: The management profile"
            },
            "venue": {
                "fragments": [],
                "text": "Language"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An example of statistical investigation in the text"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1913
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Unsupervised statistical models for prepositional phrase attachment"
            },
            "venue": {
                "fragments": [],
                "text": "ACL 36,KOLING 17, pp. 1079-1085."
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Cognitive models for text filtering"
            },
            "venue": {
                "fragments": [],
                "text": "Manuscript, University of Maryland, College Park."
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 0
                            }
                        ],
                        "text": "Kneser and Ney (1995) develop a back-off model based on an extension of absolute discounting which provides a new more accurate way of estimating the distribution to which one backs off. Chen and Goodman (1998) find that both this method and an extension of it that they propose provide excellent smoothing performance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "Kneser and Ney (1995) develop a back-off model based on an extension of absolute discounting which provides a new more accurate way of estimating the distribution to which one backs off."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On structuring probabilistic dependencies in stochastic language modeling"
            },
            "venue": {
                "fragments": [],
                "text": "Computer Speech and Language 8:1-28. Ney,"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Read Foundations of Statistical Natural Language Processing By Christopher Manning, Hinrich Schuetze for online ebook"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The problem with mutual information"
            },
            "venue": {
                "fragments": [],
                "text": "Manuscript, Xerox Palo Alto Research Center, September 15, 1992."
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "LOB, 20 LOB corpus, see Lancaster-Oslo-Bergen corpus local coherence"
            },
            "venue": {
                "fragments": [],
                "text": "(in clustering),"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Using a probablistic translation model for cross-language information retrieval"
            },
            "venue": {
                "fragments": [],
                "text": "WVLC 6, pp. 18-27."
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Tag selection using probabilistic methods"
            },
            "venue": {
                "fragments": [],
                "text": "Roger Garside, Geoffrey Sampson, and Geoffrey Leech teds.), The Computational anaZysis of English: a corpus-based approach, pp. 42-65. London: Longman."
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Inflectional morphology needs to be authenticated by hand"
            },
            "venue": {
                "fragments": [],
                "text": "Working Notes of the Spring Syposium on Building Lexicons for Machine Translation,"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The use of machine-readable dictionaries in sublanguage analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Ralph Grishman and Richard Kittredge (eds.), Analyzing language in restricted domains: sublanguage description and processing, pp. 69-84. Hillsdale, NJ: Lawrence Erlbaum."
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Subsymbolic Natural Language Processing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Probabilisric Metaphysics"
            },
            "venue": {
                "fragments": [],
                "text": "Oxford: Blackwell."
            },
            "year": 1984
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Introduction to these volumes. In John Wilder Tukey Index to Statistics and Probability, pp. iv-x"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Every time I fire a linguist, my performance goes up, \" and other myths of the statistical natural language processing revolution"
            },
            "venue": {
                "fragments": [],
                "text": "Invited talk, Fifteenth National Conference on Artificial Intelligence (AAAI-98"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Guidelines for Electronic"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The impact on retrieval effectiveness of the skewed frequency distribution of a word s"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalized Linear Models, 2nd edition, chapter 4, pp"
            },
            "venue": {
                "fragments": [],
                "text": "101-123. Chapman and Hall."
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Lexicalization patterns: Semantic structure in lexical form"
            },
            "venue": {
                "fragments": [],
                "text": "Timothy Shopen ted.), Language Typology and Syntactic Descriprion III: Grammatical Categories and the Lexicon, pp. 57-149. Cambridge, MA: Cambridge University Press."
            },
            "year": 1985
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Categories and types of present-day English formation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1969
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Categories and types of present-day English formation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1969
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Structure formelle des textes et"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1954
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On structuring probabilistic dependencies in stochastic language modeling"
            },
            "venue": {
                "fragments": [],
                "text": "Computer Speech and Language 8:1-28."
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Introduction to these volumes"
            },
            "venue": {
                "fragments": [],
                "text": "John Wilder Tukey (ed.), Index to Statistics and Probability, pp. iv-x. Los Altos, CA: R & D Press."
            },
            "year": 1975
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Decoding algorithm in statistical machine"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Natural language processing has long seemed to be the magic bullet that will bring information retrieval much closer to human capabilities. It is rather frustrating that 20 years"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Lexicalization patterns: Semantic structure in lexical form"
            },
            "venue": {
                "fragments": [],
                "text": "Talmy, Leonard"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Automatic acquisition of a large tion dictionary from corpora"
            },
            "venue": {
                "fragments": [],
                "text": "ACL 31, pp. 23 5-242."
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Linear algebra and applications, 3rd edition"
            },
            "venue": {
                "fragments": [],
                "text": "San Diego: Harcourt, Brace, Jovanovich."
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Language Insrincr"
            },
            "venue": {
                "fragments": [],
                "text": "New York: William Morrow."
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Word-sense disambiguation using statistical models of Roget s"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The statistical sleuth: a course"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalized Linear Models, 2nd edition, chapter 4, pp"
            },
            "venue": {
                "fragments": [],
                "text": "101-123. Chapman and Hall."
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Word sense disambiguation using optimized combination of knowledge sources"
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Loan 561 Good-Turing estimator, 2 12 Good (19531,212"
            },
            "venue": {
                "fragments": [],
                "text": "Good"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Webster new collegiate dictionary Springfield, MA: G"
            },
            "venue": {
                "fragments": [],
                "text": "C. Merriam Co."
            },
            "year": 1973
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Lillian Lee is an assistant professor in the Department of Computer Science at Cornell University"
            },
            "venue": {
                "fragments": [],
                "text": "Lillian Lee is an assistant professor in the Department of Computer Science at Cornell University"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An approach to the segmentation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1962
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Philosophical Investigations tersuchungen], 3rd edition"
            },
            "venue": {
                "fragments": [],
                "text": "Oxford: Basil Blackwell. Translated by G. E. M. Anscombe."
            },
            "year": 1968
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "402 KL divergence as a measure of selectional"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Artificial Intelligence: A Modem Approach"
            },
            "venue": {
                "fragments": [],
                "text": "Englewood Cliffs, NJ: Prentice Hall."
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "19861, 79 algorithmic complexity, 506 alignment, see text alignment and word alignment"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Philosophical Investigations [Philosophische Untersuchungen], 3rd edition"
            },
            "venue": {
                "fragments": [],
                "text": "Oxford: Basil Blackwell. Translated by G. E. M. Anscombe."
            },
            "year": 1968
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "FundamenfaIs of Speech Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Englewood Cliffs, NJ: PTR Prentice-Hall."
            },
            "year": 1993
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 13,
            "methodology": 9
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 294,
        "totalPages": 30
    },
    "page_url": "https://www.semanticscholar.org/paper/Foundations-of-statistical-natural-language-Manning-Sch\u00fctze/084c55d6432265785e3ff86a2e900a49d501c00a?sort=total-citations"
}