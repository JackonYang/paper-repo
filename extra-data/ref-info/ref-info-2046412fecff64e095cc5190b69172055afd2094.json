{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "A future paper will discuss the application of the same ideas to classification problems ( MacKay 1992 ~)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6530745,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7abda1941534d3bb558dd959025d67f1df526303",
            "isKey": false,
            "numCitedBy": 792,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Three Bayesian ideas are presented for supervised adaptive classifiers. First, it is argued that the output of a classifier should be obtained by marginalizing over the posterior distribution of the parameters; a simple approximation to this integral is proposed and demonstrated. This involves a \"moderation\" of the most probable classifier's outputs, and yields improved performance. Second, it is demonstrated that the Bayesian framework for model comparison described for regression models in MacKay (1992a,b) can also be applied to classification problems. This framework successfully chooses the magnitude of weight decay terms, and ranks solutions found using different numbers of hidden units. Third, an information-based data selection criterion is derived and demonstrated within this framework."
            },
            "slug": "The-Evidence-Framework-Applied-to-Classification-Mackay",
            "title": {
                "fragments": [],
                "text": "The Evidence Framework Applied to Classification Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is demonstrated that the Bayesian framework for model comparison described for regression models in MacKay (1992a,b) can also be applied to classification problems and an information-based data selection criterion is derived and demonstrated within this framework."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15883988,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0f2433c088591d265891231f1c22424047f1bc1",
            "isKey": false,
            "numCitedBy": 254,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks. The framework makes possible: (1) objective comparisons between solutions using alternative network architectures; (2) objective stopping rules for deletion of weights; (3) objective choice of magnitude and type of weight decay terms or additive regularisers (for penalising large weights, etc.); (4) a measure of the e ective number of well{determined parameters in a model; (5) quanti ed estimates of the error bars on network parameters and on network output; (6) objective comparisons with alternative learning and interpolation models such as splines and radial basis functions. The Bayesian `evidence' automatically embodies `Occam's razor,' penalising over{ exible and over{complex architectures. The Bayesian approach helps detect poor underlying assumptions in learning models. For learning models well{ matched to a problem, a good correlation between generalisation ability and the Bayesian evidence is obtained."
            },
            "slug": "A-Practical-Bayesian-Framework-for-Backprop-Mackay",
            "title": {
                "fragments": [],
                "text": "A Practical Bayesian Framework for Backprop Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks and a good correlation between generalisation ability and the Bayesian evidence is obtained."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1762283,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e68c54f39e87daf3a8bdc0ee005aece3c652d11",
            "isKey": false,
            "numCitedBy": 3960,
            "numCiting": 73,
            "paperAbstract": {
                "fragments": [],
                "text": "Although Bayesian analysis has been in use since Laplace, the Bayesian method of model-comparison has only recently been developed in depth. In this paper, the Bayesian approach to regularization and model-comparison is demonstrated by studying the inference problem of interpolating noisy data. The concepts and methods described are quite general and can be applied to many other data modeling problems. Regularizing constants are set by examining their posterior probability distribution. Alternative regularizers (priors) and alternative basis sets are objectively compared by evaluating the evidence for them. Occam's razor is automatically embodied by this process. The way in which Bayes infers the values of regularizing constants and noise levels has an elegant interpretation in terms of the effective number of parameters determined by the data set. This framework is due to Gull and Skilling."
            },
            "slug": "Bayesian-Interpolation-Mackay",
            "title": {
                "fragments": [],
                "text": "Bayesian Interpolation"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The Bayesian approach to regularization and model-comparison is demonstrated by studying the inference problem of interpolating noisy data by examining the posterior probability distribution of regularizing constants and noise levels."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "117216101"
                        ],
                        "name": "M. El-Gamal",
                        "slug": "M.-El-Gamal",
                        "structuredName": {
                            "firstName": "Mahmoud",
                            "lastName": "El-Gamal",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. El-Gamal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 119
                            }
                        ],
                        "text": "The problem of \"active learning\" or \"sequential design\" has been extensively studied in economic theory and statistics (El-Gamal 1991; Fedorov 1972)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 288,
                                "start": 120
                            }
                        ],
                        "text": "The problem of \"active learning\" or \"sequential design\" has been extensively studied in economic theory and statistics (El-Gamal 1991; Fedorov 1972). Experimental design within a Bayesian framework using the Shannon information as an objective function has been studied by Lindley (1956) and by Luttrell (1985)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 311,
                                "start": 120
                            }
                        ],
                        "text": "The problem of \"active learning\" or \"sequential design\" has been extensively studied in economic theory and statistics (El-Gamal 1991; Fedorov 1972). Experimental design within a Bayesian framework using the Shannon information as an objective function has been studied by Lindley (1956) and by Luttrell (1985). A distinctive feature of this approach is that it renders the optimization of the experimental design independent of the \"tests\" that are to be applied to the data and the loss functions associated with any decisions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 150635102,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "817f2d566400df86db2a8ecf4a7ffc2afe3193a5",
            "isKey": true,
            "numCitedBy": 13,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter introduces the notions of rational expectations and optimal learning extensively used in economic theory.* It has become well known from recent literature that in active learning situations (where the actions of the statistician, or the person learning about some parameters, influences the draws from the distribution about which he/she is learning), full learning may not take place. This challenges the use of the rational expectations hypothesis which is justified on the basis that agents operating in an economy eventually all learn the true structure of the economy and optimize accordingly. In El-Gamal and Sundaram (1989, 1990) we presented a framework where a Bayesian economist imposes priors on agent-priors and we then study the evolution of those economist beliefs. We showed that generically, the economist limit beliefs generically do not have point mass at any particular agent-belief, let alone the true rational expectations belief. We show, however, that in most cases where there is sufficient variability in the law of motion that the agents are trying to learn, in sequential models that are extensively used in the economic literature, the rational expectations hypothesis may indeed be justified on the basis of optimizing and optimally updating agents."
            },
            "slug": "The-Role-of-Priors-in-Active-Bayesian-Learning-in-El-Gamal",
            "title": {
                "fragments": [],
                "text": "The Role of Priors in Active Bayesian Learning in the Sequential Statistical Decision Framework"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "It is shown that in most cases where there is sufficient variability in the law of motion that the agents are trying to learn, in sequential models that are extensively used in the economic literature, the rational expectations hypothesis may indeed be justified on the basis of optimizing and optimally updating agents."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35206065"
                        ],
                        "name": "E. Jaynes",
                        "slug": "E.-Jaynes",
                        "structuredName": {
                            "firstName": "Edwin",
                            "lastName": "Jaynes",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Jaynes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 13
                            }
                        ],
                        "text": "According to Jaynes (1986), Bayesian reasoning was rst applied to this problem two centuries ago by Laplace, who in consequence made more important discoveries in celestial mechanics than anyone else."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8154444,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c6ba8a74338ccb89d8b7242884289753653b86e7",
            "isKey": false,
            "numCitedBy": 184,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We note the main points of history, as a framework on which to hang many background remarks concerning the nature and motivation of Bayesian/Maximum Entropy methods. Experience has shown that these are needed in order to understand recent work and problems. A more complete account of the history, with many more details and references, is given in Jaynes (1978). The following discussion is essentially nontechnical; the aim is only to convey a little introductory \\feel\" for our outlook, purpose, and terminology, and to alert newcomers to common pitfalls of misunderstanding."
            },
            "slug": "Maximum-Entropy-and-Bayesian-Methods-in-Applied-Jaynes",
            "title": {
                "fragments": [],
                "text": "Maximum Entropy and Bayesian Methods in Applied Statistics: Bayesian Methods: General Background"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "The main points of history are noted, as a framework on which to hang many background remarks concerning the nature and motivation of Bayesian/Maximum Entropy methods."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852650"
                        ],
                        "name": "D. Mackay",
                        "slug": "D.-Mackay",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mackay",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mackay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16543854,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b959164d1efca4b73986ba5d21e664aadbbc0457",
            "isKey": false,
            "numCitedBy": 2590,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks. The framework makes possible (1) objective comparisons between solutions using alternative network architectures, (2) objective stopping rules for network pruning or growing procedures, (3) objective choice of magnitude and type of weight decay terms or additive regularizers (for penalizing large weights, etc.), (4) a measure of the effective number of well-determined parameters in a model, (5) quantified estimates of the error bars on network parameters and on network output, and (6) objective comparisons with alternative learning and interpolation models such as splines and radial basis functions. The Bayesian \"evidence\" automatically embodies \"Occam's razor,\" penalizing overflexible and overcomplex models. The Bayesian approach helps detect poor underlying assumptions in learning models. For learning models well matched to a problem, a good correlation between generalization ability and the Bayesian evidence is obtained."
            },
            "slug": "A-Practical-Bayesian-Framework-for-Backpropagation-Mackay",
            "title": {
                "fragments": [],
                "text": "A Practical Bayesian Framework for Backpropagation Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks that automatically embodies \"Occam's razor,\" penalizing overflexible and overcomplex models."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145159381"
                        ],
                        "name": "Jenq-Neng Hwang",
                        "slug": "Jenq-Neng-Hwang",
                        "structuredName": {
                            "firstName": "Jenq-Neng",
                            "lastName": "Hwang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jenq-Neng Hwang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "28184513"
                        ],
                        "name": "J. J. Choi",
                        "slug": "J.-J.-Choi",
                        "structuredName": {
                            "firstName": "Jai",
                            "lastName": "Choi",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. J. Choi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3235479"
                        ],
                        "name": "Seho Oh",
                        "slug": "Seho-Oh",
                        "structuredName": {
                            "firstName": "Seho",
                            "lastName": "Oh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seho Oh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47398186"
                        ],
                        "name": "R. Marks",
                        "slug": "R.-Marks",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Marks",
                            "middleNames": [
                                "J."
                            ],
                            "suffix": "Jr"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Marks"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 145
                            }
                        ],
                        "text": "\u2026literature on active data selection, also known as`query learning,' has concentrated on slightly diierent problems: The work of Baum (1991) and Hwang et al. (1991) relates to perfectly separable classiication problems only; in both these papers a sensible query{based learning algorithm is\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 26254146,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0bd6e0c494ca277aeaa5ef00da41da5b2600b9c",
            "isKey": false,
            "numCitedBy": 174,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "An approach is presented for query-based neural network learning. A layered perceptron partially trained for binary classification is considered. The single-output neuron is trained to be either a zero or a one. A test decision is made by thresholding the output at, for example, one-half. The set of inputs that produce an output of one-half forms the classification boundary. The authors adopted an inversion algorithm for the neural network that allows generation of this boundary. For each boundary point, the classification gradient can be generated. The gradient provides a useful measure of the steepness of the multidimensional decision surfaces. Conjugate input pairs are generated using the boundary point and gradient information and presented to an oracle for proper classification. These data are used to refine further the classification boundary, thereby increasing the classification accuracy. The result can be a significant reduction in the training set cardinality in comparison with, for example, randomly generated data points. An application example to power system security assessment is given."
            },
            "slug": "Query-based-learning-applied-to-partially-trained-Hwang-Choi",
            "title": {
                "fragments": [],
                "text": "Query-based learning applied to partially trained multilayer perceptrons"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "An approach is presented for query-based neural network learning that combines a layered perceptron partially trained for binary classification with an inversion algorithm for the neural network that allows generation of this boundary."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2403454"
                        ],
                        "name": "E. Baum",
                        "slug": "E.-Baum",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Baum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Baum"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 149
                            }
                        ],
                        "text": "\u2026the neural networks literature on active data selection, also known as`query learning,' has concentrated on slightly diierent problems: The work of Baum (1991) and Hwang et al. (1991) relates to perfectly separable classiication problems only; in both these papers a sensible query{based learning\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 19460515,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "a7404527c3a6aa542ea183da9c821efda05a2afc",
            "isKey": false,
            "numCitedBy": 183,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "An algorithm which trains networks using examples and queries is proposed. In a query, the algorithm supplies a y and is told t(y) by an oracle. Queries appear to be available in practice for most problems of interest, e.g. by appeal to a human expert. The author's algorithm is proved to PAC learn in polynomial time the class of target functions defined by layered, depth two, threshold nets having n inputs connected to k hidden threshold units connected to one or more output units, provided k=/<4. While target functions and input distributions can be described for which the algorithm will fail for larger k, it appears likely to work well in practice. Tests of a variant of the algorithm have consistently and rapidly learned random nets of this type. Computational efficiency figures are given. The algorithm can also be proved to learn intersections of k half-spaces in R(n) in time polynomial in both n and k. A variant of the algorithm can learn arbitrary depth layered threshold networks with n inputs and k units in the first hidden layer in time polynomial in the larger of n and k but exponential in the smaller of the two."
            },
            "slug": "Neural-net-algorithms-that-learn-in-polynomial-time-Baum",
            "title": {
                "fragments": [],
                "text": "Neural net algorithms that learn in polynomial time from examples and queries"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The author's algorithm is proved to PAC learn in polynomial time the class of target functions defined by layered, depth two, threshold nets having n inputs connected to k hidden threshold units connected to one or more output units, provided k=/<4."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2010050"
                        ],
                        "name": "J. Skilling",
                        "slug": "J.-Skilling",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Skilling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Skilling"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 89
                            }
                        ],
                        "text": "This work was directly stimulated by a presentation given by John Skilling at Maxent 91 (Skilling, 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 88
                            }
                        ],
                        "text": "This work was directly stimulated by a presentation given by John Skilling at Maxent 91 (Skilling 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 116220029,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "4cebfe6472d23f62ff21182c7bb503a07379c651",
            "isKey": false,
            "numCitedBy": 94,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "In the numerical solution of ordinary differential equations, a function y(x) is to be reconstructed from knowledge of the functional form of its derivative: dy/dx = f (x, y), together with an appropriate boundary condition. The derivative f is evaluated at a sequence of suitably chosen points (xk, yk), from which the form of y(\u2022) is estimated. This is an inference problem, which can and perhaps should be treated by Bayesian techniques. As always, the inference appears as a probability distribution prob(y(\u2022)), from which random samples show the probabilistic reliability of the results. Examples are given."
            },
            "slug": "Bayesian-Solution-of-Ordinary-Differential-Skilling",
            "title": {
                "fragments": [],
                "text": "Bayesian Solution of Ordinary Differential Equations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "65793277"
                        ],
                        "name": "T. Loredo",
                        "slug": "T.-Loredo",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Loredo",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Loredo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 120668555,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "065803f662547c47fc46bf7e649847f90e3e90e9",
            "isKey": false,
            "numCitedBy": 203,
            "numCiting": 227,
            "paperAbstract": {
                "fragments": [],
                "text": "The Bayesian approach to probability theory is presented as an alternative to the currently used long-run relative frequency approach, which does not offer clear, compelling criteria for the design of statistical methods. Bayesian probability theory offers unique and demonstrably optimal solutions to well-posed statistical problems, and is historically the original approach to statistics. The reasons for earlier rejection of Bayesian methods are discussed, and it is noted that the work of Cox, Jaynes, and others answers earlier objections, giving Bayesian inference a firm logical and mathematical foundation as the correct mathematical language for quantifying uncertainty. The Bayesian approaches to parameter estimation and model comparison are outlined and illustrated by application to a simple problem based on the gaussian distribution. As further illustrations of the Bayesian paradigm, Bayesian solutions to two interesting astrophysical problems are outlined: the measurement of weak signals in a strong background, and the analysis of the neutrinos detected from supernova SN 1987A. A brief bibliography of astrophysically interesting applications of Bayesian inference is provided."
            },
            "slug": "From-Laplace-to-Supernova-SN-1987A:-Bayesian-in-Loredo",
            "title": {
                "fragments": [],
                "text": "From Laplace to Supernova SN 1987A: Bayesian Inference in Astrophysics"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "The Bayesian approach to probability theory is presented as an alternative to the currently used long-run relative frequency approach, and Bayesian solutions to two interesting astrophysical problems are outlined: the measurement of weak signals in a strong background and the analysis of the neutrinos detected from supernova SN 1987A."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39201543"
                        ],
                        "name": "J. Berger",
                        "slug": "J.-Berger",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Berger",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Berger"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 278,
                                "start": 266
                            }
                        ],
                        "text": "\u2026but didn't. Bayesian inference is consistent with this principle; there is no need to undo biases introduced by the data collecting strategy, because it is not possible for such biases to be introduced | as long as we perform inference using all the data gathered (Berger, 1985, Loredo, 1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 120366929,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b9dd05b69d6906fff6ea6c4ba3609a6d97c9b8a3",
            "isKey": false,
            "numCitedBy": 7326,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "An overview of statistical decision theory, which emphasizes the use and application of the philosophical ideas and mathematical structure of decision theory. The text assumes a knowledge of basic probability theory and some advanced calculus is also required."
            },
            "slug": "Statistical-Decision-Theory-and-Bayesian-Analysis-Berger",
            "title": {
                "fragments": [],
                "text": "Statistical Decision Theory and Bayesian Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "An overview of statistical decision theory, which emphasizes the use and application of the philosophical ideas and mathematical structure of decision theory."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694254"
                        ],
                        "name": "S. Luttrell",
                        "slug": "S.-Luttrell",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Luttrell",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Luttrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 120981660,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d4264a1253a878ec1d6c4d2566646d9072f2d03b",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "The author analyses the average useful information content of data samples by using the transinformation entropy (rate of transmission) of Shannon's information theory. The author derives a simple expression for the transinformation in linear experiments with Gaussian a priori distributions. The author uses this expression to examine various schemes for sampling the image spaces of a translation invariant (sinc) and a conformally invariant (Laplace) mapping. The optimum sampling scheme is found to be considerably better than the naive sampling scheme (e.g. Nyquist) when the number of samples is small and the a priori knowledge is non-trivial."
            },
            "slug": "The-use-of-transinformation-in-the-design-of-data-Luttrell",
            "title": {
                "fragments": [],
                "text": "The use of transinformation in the design of data sampling schemes for inverse problems"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "The author analyses the average useful information content of data samples by using the transinformation entropy (rate of transmission) of Shannon's information theory to examine various schemes for sampling the image spaces of a translation invariant and a conformally invariant mapping."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40325722"
                        ],
                        "name": "V. Fedorov",
                        "slug": "V.-Fedorov",
                        "structuredName": {
                            "firstName": "Valerii",
                            "lastName": "Fedorov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Fedorov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2636336"
                        ],
                        "name": "W. J. Studden",
                        "slug": "W.-J.-Studden",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Studden",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. J. Studden"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "98701042"
                        ],
                        "name": "E. Klimko",
                        "slug": "E.-Klimko",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Klimko",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Klimko"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 230,
                                "start": 79
                            }
                        ],
                        "text": "The data were generated from a smooth function by adding noise with standard deviation CJ,, = 0.05. The neural network was adapted to the data using weight decay terms ac, which were controlled using the methods of MacKay (1992b) and noise level p fixed to l/& The data and the resulting interpolant, with error bars, are shown in Figure la."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 52
                            }
                        ],
                        "text": "Thus the two candidate information measures are equivalent for our purposes. This proof also implicitly demonstrates that E(AS) is independent of the measure m(w). Other properties of E(AS) are proved in Lindley (1956). The rest of this paper will use AS as the information measure, with m(w) set to a constant."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 631,
                                "start": 76
                            }
                        ],
                        "text": "This paper will study each of these tasks for the case in which we wish to evaluate the utility as a function of xN+', the input location at which a single measurement of a scalar tN+' will be made. The more complex task of selecting multiple new data points will not be addressed here, but the methods used can be generalized to solve this task, as is discussed in Fedorov (1972) and Luttrell (1985). The similar problem of choosing the x N + ] at which a vector of outputs tN+' is measured will not be addressed either. The first and third definitions of information gain have both been studied in the abstract by Lindley (1956). All three cases have been studied by Fedorov (1972), mainly in non-Bayesian terms."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 401,
                                "start": 76
                            }
                        ],
                        "text": "This paper will study each of these tasks for the case in which we wish to evaluate the utility as a function of xN+', the input location at which a single measurement of a scalar tN+' will be made. The more complex task of selecting multiple new data points will not be addressed here, but the methods used can be generalized to solve this task, as is discussed in Fedorov (1972) and Luttrell (1985). The similar problem of choosing the x N + ] at which a vector of outputs tN+' is measured will not be addressed either."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 326,
                                "start": 84
                            }
                        ],
                        "text": "The solutions apply to linear and nonlinear interpolation models, but depend on the validity of a local gaussian approximation. Each solution has an analog in the non-Bayesian literature (Fedorov 19721, and generalizations to multiple measurements and multiple output variables can be found there, and also in Luttrell (1985). In each case a function of x has been derived that predicts the information gain for a measurement at that x."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 381,
                                "start": 76
                            }
                        ],
                        "text": "This paper will study each of these tasks for the case in which we wish to evaluate the utility as a function of xN+', the input location at which a single measurement of a scalar tN+' will be made. The more complex task of selecting multiple new data points will not be addressed here, but the methods used can be generalized to solve this task, as is discussed in Fedorov (1972) and Luttrell (1985)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 339,
                                "start": 73
                            }
                        ],
                        "text": "2This measure rn will be unimportant in what follows but is included to avoid committing dimensional crimes. Note that the sign of AS has been defined so that our information gain corresponds to positive AS. 'If the Occam factor is O.F. = (2~)~/~det - ' ' *A exp(-crE$p)/Z~(a), then SN = log O.F. + y/2, using notation from MacKay (1992a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 252,
                                "start": 71
                            }
                        ],
                        "text": "These two terms correspond precisely to the two expectations stated above. The first term favors measurements where pl and p2 are well separated; the second term favors places where 0: and 0; differ. Thus the third task has been solved. Fedorov (1972) makes a similar derivation but he uses a poor approximation that loses the second term."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 11
                            }
                        ],
                        "text": "where we have used the approximation VVi[t - y(x;w)I2 N gg'. This expression neglects terms in d2y/dw,dwk; those terms are exactly zero for the linear models discussed in MacKay (1992a), but they are not necessarily negligible for nonlinear models such as neural networks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 121909870,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "65c1a71c5307492782177a9799bef2cdf539ea00",
            "isKey": true,
            "numCitedBy": 2567,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Theory-Of-Optimal-Experiments-Fedorov-Studden",
            "title": {
                "fragments": [],
                "text": "Theory Of Optimal Experiments"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144537223"
                        ],
                        "name": "D. Lindley",
                        "slug": "D.-Lindley",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lindley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lindley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 123
                            }
                        ],
                        "text": "Experimental design within a Bayesian framework using the Shannon information as an objective function has been studied by Lindley (1956) and by Luttrell (1985)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 100
                            }
                        ],
                        "text": "This means that the change in entropy of P(fy (u) g) is identical to the change in entropy of P(w) (Lindley, 1956 )."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 91
                            }
                        ],
                        "text": "The rst and third deenitions of information gain have both been studied in the abstract by Lindley (1956)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 41
                            }
                        ],
                        "text": "Other properties of E((S) are proved in (Lindley, 1956)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 123582195,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ff5ff13ba3155cb24d8ff88607b816f8174c6bad",
            "isKey": true,
            "numCitedBy": 1326,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-a-Measure-of-the-Information-Provided-by-an-Lindley",
            "title": {
                "fragments": [],
                "text": "On a Measure of the Information Provided by an Experiment"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1956
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 48
                            }
                        ],
                        "text": "This determinant can be analytically evaluated (Fedorov, 1972), using the identities from which we obtain: Total information gain = 1 2 log m 2 det A = 1 2 log(1 + g T A ?"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 57
                            }
                        ],
                        "text": "Most of the results in this paper have direct analogs in Fedorov (1972), though the quantities involved have diierent interpretations: for example, Fedorov's dispersion of an estimator becomes the Bayesian's posterior variance of the parameter."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 133
                            }
                        ],
                        "text": "The problem of\u00e0ctive learning' or`sequential design' has been extensively studied in economic theory and statistics (El{Gamal, 1991, Fedorov, 1972)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 168
                            }
                        ],
                        "text": "The more complex task of selecting multiple new data points will not be addressed here, but the methods used can be generalised to solve this task, as is discussed in (Fedorov, 1972, Luttrell, 1985)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 223,
                                "start": 209
                            }
                        ],
                        "text": "This case also includes the generalisation to more than one output variable y; however the full generalisation, to optimisation of an experiment in which many measurements are made, will not be made here (see Fedorov , 1972 and Luttrell, 1985)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 89
                            }
                        ],
                        "text": "This rule is the same as that resulting from th\u00e8D{optimal' and`minimax' design criteria (Fedorov, 1972)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 42
                            }
                        ],
                        "text": "This is the same as th\u00e8Q{optimal' design (Fedorov, 1972)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 37
                            }
                        ],
                        "text": "All three cases have been studied by Fedorov (1972), mainly in non{Bayesian terms."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 0
                            }
                        ],
                        "text": "Fedorov (1972) makes a similar derivation but he uses a poor approximation which loses the second term."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theory of optimal experiments', Academic press"
            },
            "venue": {
                "fragments": [],
                "text": "Theory of optimal experiments', Academic press"
            },
            "year": 1972
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 165
                            }
                        ],
                        "text": "Recent work in the neural networks literature on active data selection, also known as \u201cquery learning,\u201d has concentrated on slightly different problems: The work of Baum (1991) and Hwang et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 920,
                                "start": 165
                            }
                        ],
                        "text": "Recent work in the neural networks literature on active data selection, also known as \u201cquery learning,\u201d has concentrated on slightly different problems: The work of Baum (1991) and Hwang et al. (1991) relates to perfectly separable classification problems only; in both these papers a sensible query-based learning algorithm is proposed, and empirical results of the algorithm are reported; Baum also gives a convergence proof. But since the algorithms are both human designed, it is not clear what objective function their querying strategy optimizes, nor how the algorithms could be improved. In contrast, this paper (which discusses noisy interpolation problems) derives criteria from defined objective functions; each objective function leads to a different data selection criterion. A future paper will discuss the application of the same ideas to classification problems (MacKay 1992~). Plutowski and White (1991) study a different problem from the above, in the context of noise-free interpolation: they assume that a large amount of data has already been gathered, and work on principles for selecting a subset of that data for efficient training; the entire data set (inputs and targets) is consulted at each iteration to decide which example to add to the training subset, an option that is not permitted in this paper."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 165
                            }
                        ],
                        "text": "Recent work in the neural networks literature on active data selection, also known as \u201cquery learning,\u201d has concentrated on slightly different problems: The work of Baum (1991) and Hwang et al. (1991) relates to perfectly separable classification problems only; in both these papers a sensible query-based learning algorithm is proposed, and empirical results of the algorithm are reported; Baum also gives a convergence proof."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 149
                            }
                        ],
                        "text": "\u2026the neural networks literature on active data selection, also known as`query learning,' has concentrated on slightly diierent problems: The work of Baum (1991) and Hwang et al. (1991) relates to perfectly separable classiication problems only; in both these papers a sensible query{based learning\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural net algorithms that learn in polynomial time from Berger, J"
            },
            "venue": {
                "fragments": [],
                "text": "1985. Statistical Decision Theory and Bayesian Analysis. Springer, New"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 70
                            }
                        ],
                        "text": "If M has other minima, those can be treated as distinct models as in (MacKay, 1991b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 21
                            }
                        ],
                        "text": "Two previous papers (MacKay, 1991a, 1991b) described the Bayesian framework for tting and comparing such models, assuming a xed data set."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 192
                            }
                        ],
                        "text": "This paper uses similar information{based objective functions and discusses the problem of optimal data selection within the Bayesian framework for interpolation described in previous papers (MacKay, 1991a, 1991b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 89
                            }
                        ],
                        "text": "A future paper will discuss the application of the same ideas to classiication problems (MacKay, 1991d)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 50
                            }
                        ],
                        "text": "In the case of the quadratic models discussed in (MacKay, 1991a), if we set the measure m(w) equal to the prior P 0 (w), the quantity S N is closely related to the log of th\u00e8Occam factor."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 117
                            }
                        ],
                        "text": "This expression neglects terms in @ 2 y @w j @w k ; those terms are exactly zero for the linear models discussed in (MacKay, 1991a), but they are not necessarily negligible for non{linear models such as neural networks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 114
                            }
                        ],
                        "text": "The neural network was adapted to the data using weight decay terms c which were controlled using the methods of (MacKay, 1991b) and noise level xed to 1=="
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian interpolation', Neural Computation, this volume"
            },
            "venue": {
                "fragments": [],
                "text": "Bayesian interpolation', Neural Computation, this volume"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 70
                            }
                        ],
                        "text": "If M has other minima, those can be treated as distinct models as in (MacKay, 1991b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 21
                            }
                        ],
                        "text": "Two previous papers (MacKay, 1991a, 1991b) described the Bayesian framework for tting and comparing such models, assuming a xed data set."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 192
                            }
                        ],
                        "text": "This paper uses similar information{based objective functions and discusses the problem of optimal data selection within the Bayesian framework for interpolation described in previous papers (MacKay, 1991a, 1991b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 89
                            }
                        ],
                        "text": "A future paper will discuss the application of the same ideas to classiication problems (MacKay, 1991d)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 50
                            }
                        ],
                        "text": "In the case of the quadratic models discussed in (MacKay, 1991a), if we set the measure m(w) equal to the prior P 0 (w), the quantity S N is closely related to the log of th\u00e8Occam factor."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 117
                            }
                        ],
                        "text": "This expression neglects terms in @ 2 y @w j @w k ; those terms are exactly zero for the linear models discussed in (MacKay, 1991a), but they are not necessarily negligible for non{linear models such as neural networks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 114
                            }
                        ],
                        "text": "The neural network was adapted to the data using weight decay terms c which were controlled using the methods of (MacKay, 1991b) and noise level xed to 1=="
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The evidence framework applied to classiication networks', in preparation"
            },
            "venue": {
                "fragments": [],
                "text": "The evidence framework applied to classiication networks', in preparation"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145409498"
                        ],
                        "name": "J. Justice",
                        "slug": "J.-Justice",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Justice",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Justice"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 13
                            }
                        ],
                        "text": "According to Jaynes (1986), Bayesian reasoning was rst applied to this problem two centuries ago by Laplace, who in consequence made more important discoveries in celestial mechanics than anyone else."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 115305104,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8150272d9b5aa033273161611664f6a611c355b7",
            "isKey": false,
            "numCitedBy": 166,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Maximum-entropy-and-bayesian-methods-in-applied-Justice",
            "title": {
                "fragments": [],
                "text": "Maximum entropy and bayesian methods in applied statistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 220119561,
            "fieldsOfStudy": [],
            "id": "0a55b22bc98bc997bc31af0244038643e2bae74a",
            "isKey": false,
            "numCitedBy": 6373,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Received",
            "title": {
                "fragments": [],
                "text": "Received"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1968
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 1
                            }
                        ],
                        "text": "Neural Computation 4,590-604 (1992) @ 1992 Massachusetts Institute of Technology"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian methods: General background"
            },
            "venue": {
                "fragments": [],
                "text": "Maximum Entropy and Bayesian Methods in Applied Statistics, J. H. Justice, ed., pp. 1-25. Cambridge University Press, Cambridge."
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 0
                            }
                        ],
                        "text": "Plutowski and White (1991) study a diierent problem from the above, in the context of noise{free interpolation: they assume that a large amount of data has already been gathered, and work on principles for selecting a subset of that data for eecient training; the entire data set (inputs and\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Active selection of training examples for network learning in noiseless environments"
            },
            "venue": {
                "fragments": [],
                "text": "Active selection of training examples for network learning in noiseless environments"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Marcus Mitchell and the referees for helpful feedback. This work was supported by a Caltech Fellowship and a Studentship from SERC"
            },
            "venue": {
                "fragments": [],
                "text": "Marcus Mitchell and the referees for helpful feedback. This work was supported by a Caltech Fellowship and a Studentship from SERC"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "` Bayesian methods : general background ' , in Maximum Entropy and BayesianMethods in applied statistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 13
                            }
                        ],
                        "text": "According to Jaynes (1986), Bayesian reasoning was rst applied to this problem two centuries ago by Laplace, who in consequence made more important discoveries in celestial mechanics than anyone else."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian methods: General background"
            },
            "venue": {
                "fragments": [],
                "text": "In Maximum Entropy and Bayesian Methods in Applied Statistics"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 163
                            }
                        ],
                        "text": "Recent work in the neural networks literature on active data selection, also known as`query learning,' has concentrated on slightly diierent problems: The work of Baum (1991) and Hwang et al. (1991) relates to perfectly separable classiication problems only; in both these papers a sensible query{based learning algorithm is proposed, and empirical results of the algorithm are reported; Baum also gives a convergence proof."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 149
                            }
                        ],
                        "text": "\u2026the neural networks literature on active data selection, also known as`query learning,' has concentrated on slightly diierent problems: The work of Baum (1991) and Hwang et al. (1991) relates to perfectly separable classiication problems only; in both these papers a sensible query{based learning\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 150
                            }
                        ],
                        "text": "\u2026neural networks literature on active data selection, also known as`query learning,' has concentrated on slightly diierent problems: The work of Baum (1991) and Hwang et al. (1991) relates to perfectly separable classiication problems only; in both these papers a sensible query{based learning\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural net algorithms that learn in polynomial time from Berger, J. 1985. Statistical Decision Theory and Bayesian Analysis New examples and queries"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian solution of ordinary diierential equations', in Maximum Entropy and Bayesian Methods"
            },
            "venue": {
                "fragments": [],
                "text": "Bayesian solution of ordinary diierential equations', in Maximum Entropy and Bayesian Methods"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 0
                            }
                        ],
                        "text": "Plutowski and White (1991) study a diierent problem from the above, in the context of noise{free interpolation: they assume that a large amount of data has already been gathered, and work on principles for selecting a subset of that data for eecient training; the entire data set (inputs and\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Active selection of training examples for network learning in noiseless environments Bayesian solution of ordinary differential equations"
            },
            "venue": {
                "fragments": [],
                "text": "Dept. Computer Science, UCSD, Skilling, J"
            },
            "year": 1991
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 14,
            "methodology": 11,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 28,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Information-Based-Objective-Functions-for-Active-Mackay/2046412fecff64e095cc5190b69172055afd2094?sort=total-citations"
}