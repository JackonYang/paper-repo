{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145307652"
                        ],
                        "name": "Li Dong",
                        "slug": "Li-Dong",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Dong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Dong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144610884"
                        ],
                        "name": "Nan Yang",
                        "slug": "Nan-Yang",
                        "structuredName": {
                            "firstName": "Nan",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nan Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51456429"
                        ],
                        "name": "Wenhui Wang",
                        "slug": "Wenhui-Wang",
                        "structuredName": {
                            "firstName": "Wenhui",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenhui Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49807919"
                        ],
                        "name": "Furu Wei",
                        "slug": "Furu-Wei",
                        "structuredName": {
                            "firstName": "Furu",
                            "lastName": "Wei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Furu Wei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46522098"
                        ],
                        "name": "Xiaodong Liu",
                        "slug": "Xiaodong-Liu",
                        "structuredName": {
                            "firstName": "Xiaodong",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaodong Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72682749"
                        ],
                        "name": "Yu Wang",
                        "slug": "Yu-Wang",
                        "structuredName": {
                            "firstName": "Yu",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yu Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800422"
                        ],
                        "name": "Jianfeng Gao",
                        "slug": "Jianfeng-Gao",
                        "structuredName": {
                            "firstName": "Jianfeng",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianfeng Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143849609"
                        ],
                        "name": "M. Zhou",
                        "slug": "M.-Zhou",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145058181"
                        ],
                        "name": "H. Hon",
                        "slug": "H.-Hon",
                        "structuredName": {
                            "firstName": "Hsiao-Wuen",
                            "lastName": "Hon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 12
                            }
                        ],
                        "text": "Inspired by (Dong et al., 2019), we propose a pseudomasked language model (PMLM) to jointly pre-train a bidirectional LM for language understanding (e."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 35
                            }
                        ],
                        "text": "Among the compared results, UNILM (Dong et al., 2019) is based on pretrained models, while the other three methods are sequenceto-sequence models enhanced with manual features (Du & Cardie, 2018), gated self-attention (Zhao et al., 2018), and reinforcement learning (Zhang & Bansal, 2019)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 56
                            }
                        ],
                        "text": "72 Fine-tuning LARGE-size pre-trained models UNILMLARGE (Dong et al., 2019) 340M 43."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 13
                            }
                        ],
                        "text": "Inspired by (Dong et al., 2019), we propose a pseudomasked language model (PMLM) to jointly pre-train a bidirectional LM for language understanding (e.g., text classification, and question answering) and a sequence-to-sequence LM for language generation (e.g., document summarization, and response\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 78
                            }
                        ],
                        "text": "During decoding, we use beam search to generate the target tokens one by one (Dong et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 119
                            }
                        ],
                        "text": "The first one jointly pretrains a shared Transformer network for language understanding and generation, such as UniLM (Dong et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 113
                            }
                        ],
                        "text": "Second, the unified pre-training framework learns models for both natural language understanding and generation (Dong et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 11
                            }
                        ],
                        "text": "Following (Dong et al., 2019), we fine-tune the pre-trained PMLM (with additional task-specific layers if necessary) to both natural language understanding (NLU) and natural language generation (NLG) tasks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 34
                            }
                        ],
                        "text": "Among the compared results, UNILM (Dong et al., 2019) is based on pretrained models, while the other three methods are sequenceto-sequence models enhanced with manual features (Du & Cardie, 2018), gated self-attention (Zhao et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 147704286,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1c71771c701aadfd72c5866170a9f5d71464bb88",
            "isKey": false,
            "numCitedBy": 732,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new Unified pre-trained Language Model (UniLM) that can be fine-tuned for both natural language understanding and generation tasks. The model is pre-trained using three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction. The unified modeling is achieved by employing a shared Transformer network and utilizing specific self-attention masks to control what context the prediction conditions on. UniLM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0 and CoQA question answering tasks. Moreover, UniLM achieves new state-of-the-art results on five natural language generation datasets, including improving the CNN/DailyMail abstractive summarization ROUGE-L to 40.51 (2.04 absolute improvement), the Gigaword abstractive summarization ROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question answering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question generation BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7 document-grounded dialog response generation NIST-4 to 2.67 (human performance is 2.65). The code and pre-trained models are available at this https URL."
            },
            "slug": "Unified-Language-Model-Pre-training-for-Natural-and-Dong-Yang",
            "title": {
                "fragments": [],
                "text": "Unified Language Model Pre-training for Natural Language Understanding and Generation"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A new Unified pre-trained Language Model (UniLM) that can be fine-tuned for both natural language understanding and generation tasks that compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0 and CoQA question answering tasks."
            },
            "venue": {
                "fragments": [],
                "text": "NeurIPS"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109512754"
                        ],
                        "name": "Zhilin Yang",
                        "slug": "Zhilin-Yang",
                        "structuredName": {
                            "firstName": "Zhilin",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhilin Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3422912"
                        ],
                        "name": "Zihang Dai",
                        "slug": "Zihang-Dai",
                        "structuredName": {
                            "firstName": "Zihang",
                            "lastName": "Dai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zihang Dai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35729970"
                        ],
                        "name": "Yiming Yang",
                        "slug": "Yiming-Yang",
                        "structuredName": {
                            "firstName": "Yiming",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yiming Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143712374"
                        ],
                        "name": "J. Carbonell",
                        "slug": "J.-Carbonell",
                        "structuredName": {
                            "firstName": "Jaime",
                            "lastName": "Carbonell",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Carbonell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 87
                            }
                        ],
                        "text": "During pre-training, we randomly sample one factorization order M for each input text (Yang et al., 2019), rather than computing the exact expectation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 15
                            }
                        ],
                        "text": ", 2018), XLNet (Yang et al., 2019), and RoBERTa (Liu et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 19
                            }
                        ],
                        "text": "Notice that XLNet (Yang et al., 2019) is an autoregressive MLM augmented with more advanced relative position embeddings, and long-context memory."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 24
                            }
                        ],
                        "text": "2019; Liu et al., 2019; Yang et al., 2019; Lewis et al., 2019; Lan et al., 2019; Raffel et al., 2019; Chi et al., 2020)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 103
                            }
                        ],
                        "text": "The second type of pretraining uses autoregressive modeling (Radford et al., 2018; Lewis et al., 2019; Yang et al., 2019; Raffel et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 108
                            }
                        ],
                        "text": "The randomly sampled factorization orders are similar to permutation-based language modeling used by XLNet (Yang et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 95
                            }
                        ],
                        "text": "We compare PMLM with three strong pre-trained models, i.e., BERT (Devlin et al., 2018), XLNet (Yang et al., 2019), and RoBERTa (Liu et al., 2019), in the single task fine-tuning setting."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 56
                            }
                        ],
                        "text": "The most relevant work for the proposed model is XLNet (Yang et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 195069387,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
            "isKey": true,
            "numCitedBy": 4226,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking."
            },
            "slug": "XLNet:-Generalized-Autoregressive-Pretraining-for-Yang-Dai",
            "title": {
                "fragments": [],
                "text": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "XLNet is proposed, a generalized autoregressive pretraining method that enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and overcomes the limitations of BERT thanks to its autore progressive formulation."
            },
            "venue": {
                "fragments": [],
                "text": "NeurIPS"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50982078"
                        ],
                        "name": "Kaitao Song",
                        "slug": "Kaitao-Song",
                        "structuredName": {
                            "firstName": "Kaitao",
                            "lastName": "Song",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaitao Song"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48391466"
                        ],
                        "name": "Xu Tan",
                        "slug": "Xu-Tan",
                        "structuredName": {
                            "firstName": "Xu",
                            "lastName": "Tan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xu Tan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143826491"
                        ],
                        "name": "Tao Qin",
                        "slug": "Tao-Qin",
                        "structuredName": {
                            "firstName": "Tao",
                            "lastName": "Qin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tao Qin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145313246"
                        ],
                        "name": "Jianfeng Lu",
                        "slug": "Jianfeng-Lu",
                        "structuredName": {
                            "firstName": "Jianfeng",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianfeng Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110264337"
                        ],
                        "name": "Tie-Yan Liu",
                        "slug": "Tie-Yan-Liu",
                        "structuredName": {
                            "firstName": "Tie-Yan",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tie-Yan Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 146808476,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "145b8b5d99a2beba6029418ca043585b90138d12",
            "isKey": false,
            "numCitedBy": 599,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "Pre-training and fine-tuning, e.g., BERT, have achieved great success in language understanding by transferring knowledge from rich-resource pre-training task to the low/zero-resource downstream tasks. Inspired by the success of BERT, we propose MAsked Sequence to Sequence pre-training (MASS) for the encoder-decoder based language generation tasks. MASS adopts the encoder-decoder framework to reconstruct a sentence fragment given the remaining part of the sentence: its encoder takes a sentence with randomly masked fragment (several consecutive tokens) as input, and its decoder tries to predict this masked fragment. In this way, MASS can jointly train the encoder and decoder to develop the capability of representation extraction and language modeling. By further fine-tuning on a variety of zero/low-resource language generation tasks, including neural machine translation, text summarization and conversational response generation (3 tasks and totally 8 datasets), MASS achieves significant improvements over the baselines without pre-training or with other pre-training methods. Specially, we achieve the state-of-the-art accuracy (37.5 in terms of BLEU score) on the unsupervised English-French translation, even beating the early attention-based supervised model."
            },
            "slug": "MASS:-Masked-Sequence-to-Sequence-Pre-training-for-Song-Tan",
            "title": {
                "fragments": [],
                "text": "MASS: Masked Sequence to Sequence Pre-training for Language Generation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work proposes MAsked Sequence to Sequence pre-training (MASS) for the encoder-decoder based language generation tasks, which achieves the state-of-the-art accuracy on the unsupervised English-French translation, even beating the early attention-based supervised model."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46221722"
                        ],
                        "name": "Zewen Chi",
                        "slug": "Zewen-Chi",
                        "structuredName": {
                            "firstName": "Zewen",
                            "lastName": "Chi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zewen Chi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145307652"
                        ],
                        "name": "Li Dong",
                        "slug": "Li-Dong",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Dong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Dong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49807919"
                        ],
                        "name": "Furu Wei",
                        "slug": "Furu-Wei",
                        "structuredName": {
                            "firstName": "Furu",
                            "lastName": "Wei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Furu Wei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51456429"
                        ],
                        "name": "Wenhui Wang",
                        "slug": "Wenhui-Wang",
                        "structuredName": {
                            "firstName": "Wenhui",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenhui Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "134880677"
                        ],
                        "name": "Xian-Ling Mao",
                        "slug": "Xian-Ling-Mao",
                        "structuredName": {
                            "firstName": "Xian-Ling",
                            "lastName": "Mao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xian-Ling Mao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4590286"
                        ],
                        "name": "Heyan Huang",
                        "slug": "Heyan-Huang",
                        "structuredName": {
                            "firstName": "Heyan",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Heyan Huang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 102
                            }
                        ],
                        "text": "2019; Liu et al., 2019; Yang et al., 2019; Lewis et al., 2019; Lan et al., 2019; Raffel et al., 2019; Chi et al., 2020)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 202718969,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aa2e8b6eecaed5c4af018c03abe5eb8adcabaf47",
            "isKey": false,
            "numCitedBy": 77,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work we focus on transferring supervision signals of natural language generation (NLG) tasks between multiple languages. We propose to pretrain the encoder and the decoder of a sequence-to-sequence model under both monolingual and cross-lingual settings. The pre-training objective encourages the model to represent different languages in the shared space, so that we can conduct zero-shot cross-lingual transfer. After the pre-training procedure, we use monolingual data to fine-tune the pre-trained model on downstream NLG tasks. Then the sequence-to-sequence model trained in a single language can be directly evaluated beyond that language (i.e., accepting multi-lingual input and producing multi-lingual output). Experimental results on question generation and abstractive summarization show that our model outperforms the machine-translation-based pipeline methods for zero-shot cross-lingual generation. Moreover, cross-lingual transfer improves NLG performance of low-resource languages by leveraging rich-resource language data. Our implementation and data are available at this https URL."
            },
            "slug": "Cross-Lingual-Natural-Language-Generation-via-Chi-Dong",
            "title": {
                "fragments": [],
                "text": "Cross-Lingual Natural Language Generation via Pre-Training"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Experimental results on question generation and abstractive summarization show that the model outperforms the machine-translation-based pipeline methods for zero-shot cross-lingual generation and improves NLG performance of low-resource languages by leveraging rich-resource language data."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39172707"
                        ],
                        "name": "Jacob Devlin",
                        "slug": "Jacob-Devlin",
                        "structuredName": {
                            "firstName": "Jacob",
                            "lastName": "Devlin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jacob Devlin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744179"
                        ],
                        "name": "Ming-Wei Chang",
                        "slug": "Ming-Wei-Chang",
                        "structuredName": {
                            "firstName": "Ming-Wei",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming-Wei Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2544107"
                        ],
                        "name": "Kenton Lee",
                        "slug": "Kenton-Lee",
                        "structuredName": {
                            "firstName": "Kenton",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenton Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3259253"
                        ],
                        "name": "Kristina Toutanova",
                        "slug": "Kristina-Toutanova",
                        "structuredName": {
                            "firstName": "Kristina",
                            "lastName": "Toutanova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kristina Toutanova"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 35
                            }
                        ],
                        "text": "We follow the format used by BERT (Devlin et al., 2018)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 104
                            }
                        ],
                        "text": "We compare previous BASE-size models with PMLM. Notice that the publicly available BERTBASE checkpoint (Devlin et al., 2018) is pre-trained on 13GB corpora with 256 batch size, while XLNetBASE and RoBERTaBASE are more directly comparable."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 45
                            }
                        ],
                        "text": "We followed the same model size as BERTBASE (Devlin et al., 2018) for comparison purposes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 49
                            }
                        ],
                        "text": "The evaluations follow the settings3 as in BERT (Devlin et al., 2018), so that the results in Table 6 can be directly compared with each other."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 29
                            }
                        ],
                        "text": "So if masked language models (Devlin et al., 2018) [M] x1 [M] x3 [M] x6 x2 x5 x4"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 53
                            }
                        ],
                        "text": "The first strand of work relies on autoencoding LMs (Devlin et al., 2018; Liu et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 61
                            }
                        ],
                        "text": "For example, the masked language modeling task used by BERT (Devlin et al., 2018) randomly masks some tokens in a text sequence, and then independently recovers the masked tokens by conditioning on the encoding vectors obtained by a bidirectional Transformer (Vaswani et al., 2017)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 30
                            }
                        ],
                        "text": "So if masked language models (Devlin et al., 2018)\nare directly used, we have to construct a new cloze instance (as shown in Figure 3) for each factorization step, which renders partially autoregressive pre-training infeasible."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 102
                            }
                        ],
                        "text": "2skylion007.github.io/OpenWebTextCorpus\nthe uncased WordPiece (Wu et al., 2016) tokenization used in (Devlin et al., 2018)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 42
                            }
                        ],
                        "text": "We use the masked language modeling (MLM; Devlin et al. 2018) task to pre-train a Transformer network, which is also known as the cloze task (Taylor, 1953)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 60
                            }
                        ],
                        "text": "For example, the masked language modeling task used by BERT (Devlin et al., 2018) randomly masks some tokens in a text sequence, and then independently recovers the masked tokens by conditioning on the encoding vectors obtained by a bidirectional Transformer (Vaswani et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 226,
                                "start": 207
                            }
                        ],
                        "text": "Language model (LM) pre-training on large-scale text corpora has substantially advanced the state of the art across a variety of natural language processing tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018; Dong et al.,\n* Contribution during internship at Microsoft."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 55
                            }
                        ],
                        "text": "Notice that the publicly available BERTBASE checkpoint (Devlin et al., 2018) is pre-trained on 13GB corpora with 256 batch size, while XLNetBASE and RoBERTaBASE are more directly comparable."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 66
                            }
                        ],
                        "text": "We compare PMLM with three strong pre-trained models, i.e., BERT (Devlin et al., 2018), XLNet (Yang et al., 2019), and RoBERTa (Liu et al., 2019), in the single task fine-tuning setting."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 63
                            }
                        ],
                        "text": "Pre-Training Setup We followed the same model size as BERTBASE (Devlin et al., 2018) for comparison purposes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 52967399,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
            "isKey": true,
            "numCitedBy": 33744,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
            },
            "slug": "BERT:-Pre-training-of-Deep-Bidirectional-for-Devlin-Chang",
            "title": {
                "fragments": [],
                "text": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "A new language representation model, BERT, designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers, which can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35084211"
                        ],
                        "name": "M. Lewis",
                        "slug": "M.-Lewis",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Lewis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lewis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11323179"
                        ],
                        "name": "Yinhan Liu",
                        "slug": "Yinhan-Liu",
                        "structuredName": {
                            "firstName": "Yinhan",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yinhan Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39589154"
                        ],
                        "name": "Naman Goyal",
                        "slug": "Naman-Goyal",
                        "structuredName": {
                            "firstName": "Naman",
                            "lastName": "Goyal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Naman Goyal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2320509"
                        ],
                        "name": "Marjan Ghazvininejad",
                        "slug": "Marjan-Ghazvininejad",
                        "structuredName": {
                            "firstName": "Marjan",
                            "lastName": "Ghazvininejad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marjan Ghazvininejad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "113947684"
                        ],
                        "name": "Abdelrahman Mohamed",
                        "slug": "Abdelrahman-Mohamed",
                        "structuredName": {
                            "firstName": "Abdelrahman",
                            "lastName": "Mohamed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Abdelrahman Mohamed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39455775"
                        ],
                        "name": "Omer Levy",
                        "slug": "Omer-Levy",
                        "structuredName": {
                            "firstName": "Omer",
                            "lastName": "Levy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Omer Levy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759422"
                        ],
                        "name": "Veselin Stoyanov",
                        "slug": "Veselin-Stoyanov",
                        "structuredName": {
                            "firstName": "Veselin",
                            "lastName": "Stoyanov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Veselin Stoyanov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1982950"
                        ],
                        "name": "Luke Zettlemoyer",
                        "slug": "Luke-Zettlemoyer",
                        "structuredName": {
                            "firstName": "Luke",
                            "lastName": "Zettlemoyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luke Zettlemoyer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 204960716,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
            "isKey": false,
            "numCitedBy": 2422,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance."
            },
            "slug": "BART:-Denoising-Sequence-to-Sequence-Pre-training-Lewis-Liu",
            "title": {
                "fragments": [],
                "text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "BART is presented, a denoising autoencoder for pretraining sequence-to-sequence models, which matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39798499"
                        ],
                        "name": "Yang Liu",
                        "slug": "Yang-Liu",
                        "structuredName": {
                            "firstName": "Yang",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yang Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747893"
                        ],
                        "name": "Mirella Lapata",
                        "slug": "Mirella-Lapata",
                        "structuredName": {
                            "firstName": "Mirella",
                            "lastName": "Lapata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mirella Lapata"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 12
                            }
                        ],
                        "text": "BERTSUMABS (Liu & Lapata, 2019) fine-tunes a BERT encoder that is pre-trained with an autoencoding objective, concatenating with a randomly initialized decoder."
                    },
                    "intents": []
                }
            ],
            "corpusId": 201304248,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "63748e59f4e106cbda6b65939b77589f40e48fcb",
            "isKey": false,
            "numCitedBy": 642,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Bidirectional Encoder Representations from Transformers (BERT) represents the latest incarnation of pretrained language models which have recently advanced a wide range of natural language processing tasks. In this paper, we showcase how BERT can be usefully applied in text summarization and propose a general framework for both extractive and abstractive models. We introduce a novel document-level encoder based on BERT which is able to express the semantics of a document and obtain representations for its sentences. Our extractive model is built on top of this encoder by stacking several inter-sentence Transformer layers. For abstractive summarization, we propose a new fine-tuning schedule which adopts different optimizers for the encoder and the decoder as a means of alleviating the mismatch between the two (the former is pretrained while the latter is not). We also demonstrate that a two-staged fine-tuning approach can further boost the quality of the generated summaries. Experiments on three datasets show that our model achieves state-of-the-art results across the board in both extractive and abstractive settings."
            },
            "slug": "Text-Summarization-with-Pretrained-Encoders-Liu-Lapata",
            "title": {
                "fragments": [],
                "text": "Text Summarization with Pretrained Encoders"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper introduces a novel document-level encoder based on BERT which is able to express the semantics of a document and obtain representations for its sentences and proposes a new fine-tuning schedule which adopts different optimizers for the encoder and the decoder as a means of alleviating the mismatch between the two."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40348417"
                        ],
                        "name": "Ashish Vaswani",
                        "slug": "Ashish-Vaswani",
                        "structuredName": {
                            "firstName": "Ashish",
                            "lastName": "Vaswani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ashish Vaswani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1846258"
                        ],
                        "name": "Noam M. Shazeer",
                        "slug": "Noam-M.-Shazeer",
                        "structuredName": {
                            "firstName": "Noam",
                            "lastName": "Shazeer",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noam M. Shazeer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3877127"
                        ],
                        "name": "Niki Parmar",
                        "slug": "Niki-Parmar",
                        "structuredName": {
                            "firstName": "Niki",
                            "lastName": "Parmar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Niki Parmar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39328010"
                        ],
                        "name": "Jakob Uszkoreit",
                        "slug": "Jakob-Uszkoreit",
                        "structuredName": {
                            "firstName": "Jakob",
                            "lastName": "Uszkoreit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jakob Uszkoreit"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145024664"
                        ],
                        "name": "Llion Jones",
                        "slug": "Llion-Jones",
                        "structuredName": {
                            "firstName": "Llion",
                            "lastName": "Jones",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Llion Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "19177000"
                        ],
                        "name": "Aidan N. Gomez",
                        "slug": "Aidan-N.-Gomez",
                        "structuredName": {
                            "firstName": "Aidan",
                            "lastName": "Gomez",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aidan N. Gomez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40527594"
                        ],
                        "name": "Lukasz Kaiser",
                        "slug": "Lukasz-Kaiser",
                        "structuredName": {
                            "firstName": "Lukasz",
                            "lastName": "Kaiser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lukasz Kaiser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3443442"
                        ],
                        "name": "Illia Polosukhin",
                        "slug": "Illia-Polosukhin",
                        "structuredName": {
                            "firstName": "Illia",
                            "lastName": "Polosukhin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Illia Polosukhin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 185
                            }
                        ],
                        "text": ", 2018) randomly masks some tokens in a text sequence, and then independently recovers the masked tokens by conditioning on the encoding vectors obtained by a bidirectional Transformer (Vaswani et al., 2017)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 27
                            }
                        ],
                        "text": "Then L stacked Transformer (Vaswani et al., 2017) blocks compute the encoding vectors via: H = Transformerl(H l\u22121), l \u2208 [1, L] (1)"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 280,
                                "start": 260
                            }
                        ],
                        "text": "For example, the masked language modeling task used by BERT (Devlin et al., 2018) randomly masks some tokens in a text sequence, and then independently recovers the masked tokens by conditioning on the encoding vectors obtained by a bidirectional Transformer (Vaswani et al., 2017)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 28
                            }
                        ],
                        "text": "Then L stacked Transformer (Vaswani et al., 2017) blocks compute\nthe encoding vectors via:\nHl = Transformerl(H l\u22121), l \u2208 [1, L] (1)\nwhere L is the number of layers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13756489,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
            "isKey": true,
            "numCitedBy": 35148,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
            },
            "slug": "Attention-is-All-you-Need-Vaswani-Shazeer",
            "title": {
                "fragments": [],
                "text": "Attention is All you Need"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely is proposed, which generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2362534"
                        ],
                        "name": "Zhenzhong Lan",
                        "slug": "Zhenzhong-Lan",
                        "structuredName": {
                            "firstName": "Zhenzhong",
                            "lastName": "Lan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhenzhong Lan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46221498"
                        ],
                        "name": "Mingda Chen",
                        "slug": "Mingda-Chen",
                        "structuredName": {
                            "firstName": "Mingda",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mingda Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7685850"
                        ],
                        "name": "Sebastian Goodman",
                        "slug": "Sebastian-Goodman",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Goodman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sebastian Goodman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700980"
                        ],
                        "name": "Kevin Gimpel",
                        "slug": "Kevin-Gimpel",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Gimpel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Gimpel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48267618"
                        ],
                        "name": "Piyush Sharma",
                        "slug": "Piyush-Sharma",
                        "structuredName": {
                            "firstName": "Piyush",
                            "lastName": "Sharma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piyush Sharma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737285"
                        ],
                        "name": "Radu Soricut",
                        "slug": "Radu-Soricut",
                        "structuredName": {
                            "firstName": "Radu",
                            "lastName": "Soricut",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radu Soricut"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 63
                            }
                        ],
                        "text": "2019; Liu et al., 2019; Yang et al., 2019; Lewis et al., 2019; Lan et al., 2019; Raffel et al., 2019; Chi et al., 2020)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 202888986,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7a064df1aeada7e69e5173f7d4c8606f4470365b",
            "isKey": false,
            "numCitedBy": 2706,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at this https URL."
            },
            "slug": "ALBERT:-A-Lite-BERT-for-Self-supervised-Learning-of-Lan-Chen",
            "title": {
                "fragments": [],
                "text": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work presents two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT, and uses a self-supervised loss that focuses on modeling inter-sentence coherence."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2402716"
                        ],
                        "name": "Colin Raffel",
                        "slug": "Colin-Raffel",
                        "structuredName": {
                            "firstName": "Colin",
                            "lastName": "Raffel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Colin Raffel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1846258"
                        ],
                        "name": "Noam M. Shazeer",
                        "slug": "Noam-M.-Shazeer",
                        "structuredName": {
                            "firstName": "Noam",
                            "lastName": "Shazeer",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noam M. Shazeer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145625142"
                        ],
                        "name": "Adam Roberts",
                        "slug": "Adam-Roberts",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Roberts",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam Roberts"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3844009"
                        ],
                        "name": "Katherine Lee",
                        "slug": "Katherine-Lee",
                        "structuredName": {
                            "firstName": "Katherine",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Katherine Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46617804"
                        ],
                        "name": "Sharan Narang",
                        "slug": "Sharan-Narang",
                        "structuredName": {
                            "firstName": "Sharan",
                            "lastName": "Narang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sharan Narang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1380243217"
                        ],
                        "name": "Michael Matena",
                        "slug": "Michael-Matena",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Matena",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Matena"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2389316"
                        ],
                        "name": "Yanqi Zhou",
                        "slug": "Yanqi-Zhou",
                        "structuredName": {
                            "firstName": "Yanqi",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yanqi Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157338362"
                        ],
                        "name": "Wei Li",
                        "slug": "Wei-Li",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35025299"
                        ],
                        "name": "Peter J. Liu",
                        "slug": "Peter-J.-Liu",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Liu",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter J. Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 15
                            }
                        ],
                        "text": ", 2019) and T5 (Raffel et al., 2019) pre-train encoderdecoder Transformers with masked LM, which relies on the autoregressive pre-training."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 33
                            }
                        ],
                        "text": "MASS (Song et al., 2019) and T5 (Raffel et al., 2019) pre-train encoderdecoder Transformers with masked LM, which relies on the autoregressive pre-training."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 81
                            }
                        ],
                        "text": "2019; Liu et al., 2019; Yang et al., 2019; Lewis et al., 2019; Lan et al., 2019; Raffel et al., 2019; Chi et al., 2020)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 122
                            }
                        ],
                        "text": "The second type of pretraining uses autoregressive modeling (Radford et al., 2018; Lewis et al., 2019; Yang et al., 2019; Raffel et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 36
                            }
                        ],
                        "text": "We also add relative position bias (Raffel et al., 2019) to attention scores."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 204838007,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3cfb319689f06bf04c2e28399361f414ca32c4b3",
            "isKey": true,
            "numCitedBy": 3764,
            "numCiting": 139,
            "paperAbstract": {
                "fragments": [],
                "text": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code."
            },
            "slug": "Exploring-the-Limits-of-Transfer-Learning-with-a-Raffel-Shazeer",
            "title": {
                "fragments": [],
                "text": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks and achieves state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38909097"
                        ],
                        "name": "Alec Radford",
                        "slug": "Alec-Radford",
                        "structuredName": {
                            "firstName": "Alec",
                            "lastName": "Radford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alec Radford"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144958935"
                        ],
                        "name": "Karthik Narasimhan",
                        "slug": "Karthik-Narasimhan",
                        "structuredName": {
                            "firstName": "Karthik",
                            "lastName": "Narasimhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karthik Narasimhan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 185
                            }
                        ],
                        "text": "Language model (LM) pre-training on large-scale text corpora has substantially advanced the state of the art across a variety of natural language processing tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018; Dong et al.,\n* Contribution during internship at Microsoft."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 61
                            }
                        ],
                        "text": "The second type of pretraining uses autoregressive modeling (Radford et al., 2018; Lewis et al., 2019; Yang et al., 2019; Raffel et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 49313245,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035",
            "isKey": false,
            "numCitedBy": 3533,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": "Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classi\ufb01cation. Although large unlabeled text corpora are abundant, labeled data for learning these speci\ufb01c tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative \ufb01ne-tuning on each speci\ufb01c task. In contrast to previous approaches, we make use of task-aware input transformations during \ufb01ne-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures speci\ufb01cally crafted for each task, signi\ufb01cantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9% on commonsense reasoning (Stories Cloze Test), 5.7% on question answering (RACE), and 1.5% on textual entailment (MultiNLI)."
            },
            "slug": "Improving-Language-Understanding-by-Generative-Radford-Narasimhan",
            "title": {
                "fragments": [],
                "text": "Improving Language Understanding by Generative Pre-Training"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The general task-agnostic model outperforms discriminatively trained models that use architectures speci\ufb01cally crafted for each task, improving upon the state of the art in 9 out of the 12 tasks studied."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2143397386"
                        ],
                        "name": "Yao Zhao",
                        "slug": "Yao-Zhao",
                        "structuredName": {
                            "firstName": "Yao",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yao Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144278395"
                        ],
                        "name": "Xiaochuan Ni",
                        "slug": "Xiaochuan-Ni",
                        "structuredName": {
                            "firstName": "Xiaochuan",
                            "lastName": "Ni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaochuan Ni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111071878"
                        ],
                        "name": "Yuanyuan Ding",
                        "slug": "Yuanyuan-Ding",
                        "structuredName": {
                            "firstName": "Yuanyuan",
                            "lastName": "Ding",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuanyuan Ding"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2259786"
                        ],
                        "name": "Qifa Ke",
                        "slug": "Qifa-Ke",
                        "structuredName": {
                            "firstName": "Qifa",
                            "lastName": "Ke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qifa Ke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 172
                            }
                        ],
                        "text": ", 2019) is based on pretrained models, while the other three methods are sequenceto-sequence models enhanced with manual features (Du & Cardie, 2018), gated self-attention (Zhao et al., 2018), and reinforcement learning (Zhang & Bansal, 2019)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 104
                            }
                        ],
                        "text": "The first block follows the data split in (Du & Cardie, 2018), while the second block is the same as in (Zhao et al., 2018)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 219
                            }
                        ],
                        "text": "Among the compared results, UNILM (Dong et al., 2019) is based on pretrained models, while the other three methods are sequenceto-sequence models enhanced with manual features (Du & Cardie, 2018), gated self-attention (Zhao et al., 2018), and reinforcement learning (Zhang & Bansal, 2019)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 53081407,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3ed3310558a40388d24c77a43d7b3f13eb6e3d3c",
            "isKey": false,
            "numCitedBy": 167,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Question generation, the task of automatically creating questions that can be answered by a certain span of text within a given passage, is important for question-answering and conversational systems in digital assistants such as Alexa, Cortana, Google Assistant and Siri. Recent sequence to sequence neural models have outperformed previous rule-based systems. Existing models mainly focused on using one or two sentences as the input. Long text has posed challenges for sequence to sequence neural models in question generation \u2013 worse performances were reported if using the whole paragraph (with multiple sentences) as the input. In reality, however, it often requires the whole paragraph as context in order to generate high quality questions. In this paper, we propose a maxout pointer mechanism with gated self-attention encoder to address the challenges of processing long text inputs for question generation. With sentence-level inputs, our model outperforms previous approaches with either sentence-level or paragraph-level inputs. Furthermore, our model can effectively utilize paragraphs as inputs, pushing the state-of-the-art result from 13.9 to 16.3 (BLEU_4)."
            },
            "slug": "Paragraph-level-Neural-Question-Generation-with-and-Zhao-Ni",
            "title": {
                "fragments": [],
                "text": "Paragraph-level Neural Question Generation with Maxout Pointer and Gated Self-attention Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A maxout pointer mechanism with gated self-attention encoder to address the challenges of processing long text inputs for question generation, which outperforms previous approaches with either sentence-level or paragraph-level inputs."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144906624"
                        ],
                        "name": "Alex Wang",
                        "slug": "Alex-Wang",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50286460"
                        ],
                        "name": "Amanpreet Singh",
                        "slug": "Amanpreet-Singh",
                        "structuredName": {
                            "firstName": "Amanpreet",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Amanpreet Singh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38614754"
                        ],
                        "name": "Julian Michael",
                        "slug": "Julian-Michael",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "Michael",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Julian Michael"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145783676"
                        ],
                        "name": "Felix Hill",
                        "slug": "Felix-Hill",
                        "structuredName": {
                            "firstName": "Felix",
                            "lastName": "Hill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Felix Hill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39455775"
                        ],
                        "name": "Omer Levy",
                        "slug": "Omer-Levy",
                        "structuredName": {
                            "firstName": "Omer",
                            "lastName": "Levy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Omer Levy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3644767"
                        ],
                        "name": "Samuel R. Bowman",
                        "slug": "Samuel-R.-Bowman",
                        "structuredName": {
                            "firstName": "Samuel",
                            "lastName": "Bowman",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samuel R. Bowman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 64
                            }
                        ],
                        "text": "The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019) contains various tasks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5034059,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "93b8da28d006415866bf48f9a6e06b5242129195",
            "isKey": false,
            "numCitedBy": 2634,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Human ability to understand language is general, flexible, and robust. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions."
            },
            "slug": "GLUE:-A-Multi-Task-Benchmark-and-Analysis-Platform-Wang-Singh",
            "title": {
                "fragments": [],
                "text": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models, which favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks."
            },
            "venue": {
                "fragments": [],
                "text": "BlackboxNLP@EMNLP"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39139825"
                        ],
                        "name": "Matthew E. Peters",
                        "slug": "Matthew-E.-Peters",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Peters",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew E. Peters"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50043859"
                        ],
                        "name": "Mark Neumann",
                        "slug": "Mark-Neumann",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Neumann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Neumann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2136562"
                        ],
                        "name": "Mohit Iyyer",
                        "slug": "Mohit-Iyyer",
                        "structuredName": {
                            "firstName": "Mohit",
                            "lastName": "Iyyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mohit Iyyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40642935"
                        ],
                        "name": "Matt Gardner",
                        "slug": "Matt-Gardner",
                        "structuredName": {
                            "firstName": "Matt",
                            "lastName": "Gardner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matt Gardner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143997772"
                        ],
                        "name": "Christopher Clark",
                        "slug": "Christopher-Clark",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2544107"
                        ],
                        "name": "Kenton Lee",
                        "slug": "Kenton-Lee",
                        "structuredName": {
                            "firstName": "Kenton",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenton Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1982950"
                        ],
                        "name": "Luke Zettlemoyer",
                        "slug": "Luke-Zettlemoyer",
                        "structuredName": {
                            "firstName": "Luke",
                            "lastName": "Zettlemoyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luke Zettlemoyer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 342,
                                "start": 163
                            }
                        ],
                        "text": "Language model (LM) pre-training on large-scale text corpora has substantially advanced the state of the art across a variety of natural language processing tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018; Dong et al., 2019; Liu et al., 2019; Yang et al., 2019; Lewis et al., 2019; Lan et al., 2019; Raffel et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 164
                            }
                        ],
                        "text": "Language model (LM) pre-training on large-scale text corpora has substantially advanced the state of the art across a variety of natural language processing tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018; Dong et al.,\n* Contribution during internship at Microsoft."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3626819,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3febb2bed8865945e7fddc99efd791887bb7e14f",
            "isKey": false,
            "numCitedBy": 7987,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals."
            },
            "slug": "Deep-Contextualized-Word-Representations-Peters-Neumann",
            "title": {
                "fragments": [],
                "text": "Deep Contextualized Word Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A new type of deep contextualized word representation is introduced that models both complex characteristics of word use and how these uses vary across linguistic contexts, allowing downstream models to mix different types of semi-supervision signals."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48607963"
                        ],
                        "name": "Yonghui Wu",
                        "slug": "Yonghui-Wu",
                        "structuredName": {
                            "firstName": "Yonghui",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yonghui Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144927151"
                        ],
                        "name": "M. Schuster",
                        "slug": "M.-Schuster",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Schuster",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Schuster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2545358"
                        ],
                        "name": "Z. Chen",
                        "slug": "Z.-Chen",
                        "structuredName": {
                            "firstName": "Z.",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144739074"
                        ],
                        "name": "Mohammad Norouzi",
                        "slug": "Mohammad-Norouzi",
                        "structuredName": {
                            "firstName": "Mohammad",
                            "lastName": "Norouzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mohammad Norouzi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3153147"
                        ],
                        "name": "Wolfgang Macherey",
                        "slug": "Wolfgang-Macherey",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Macherey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wolfgang Macherey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2048712"
                        ],
                        "name": "M. Krikun",
                        "slug": "M.-Krikun",
                        "structuredName": {
                            "firstName": "Maxim",
                            "lastName": "Krikun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Krikun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145144022"
                        ],
                        "name": "Yuan Cao",
                        "slug": "Yuan-Cao",
                        "structuredName": {
                            "firstName": "Yuan",
                            "lastName": "Cao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuan Cao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145312180"
                        ],
                        "name": "Qin Gao",
                        "slug": "Qin-Gao",
                        "structuredName": {
                            "firstName": "Qin",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qin Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "113439369"
                        ],
                        "name": "Klaus Macherey",
                        "slug": "Klaus-Macherey",
                        "structuredName": {
                            "firstName": "Klaus",
                            "lastName": "Macherey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Klaus Macherey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2367620"
                        ],
                        "name": "J. Klingner",
                        "slug": "J.-Klingner",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Klingner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Klingner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145825976"
                        ],
                        "name": "Apurva Shah",
                        "slug": "Apurva-Shah",
                        "structuredName": {
                            "firstName": "Apurva",
                            "lastName": "Shah",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Apurva Shah"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145657834"
                        ],
                        "name": "Melvin Johnson",
                        "slug": "Melvin-Johnson",
                        "structuredName": {
                            "firstName": "Melvin",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Melvin Johnson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109059862"
                        ],
                        "name": "Xiaobing Liu",
                        "slug": "Xiaobing-Liu",
                        "structuredName": {
                            "firstName": "Xiaobing",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaobing Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40527594"
                        ],
                        "name": "Lukasz Kaiser",
                        "slug": "Lukasz-Kaiser",
                        "structuredName": {
                            "firstName": "Lukasz",
                            "lastName": "Kaiser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lukasz Kaiser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2776283"
                        ],
                        "name": "Stephan Gouws",
                        "slug": "Stephan-Gouws",
                        "structuredName": {
                            "firstName": "Stephan",
                            "lastName": "Gouws",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephan Gouws"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2739610"
                        ],
                        "name": "Y. Kato",
                        "slug": "Y.-Kato",
                        "structuredName": {
                            "firstName": "Yoshikiyo",
                            "lastName": "Kato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Kato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765329"
                        ],
                        "name": "Taku Kudo",
                        "slug": "Taku-Kudo",
                        "structuredName": {
                            "firstName": "Taku",
                            "lastName": "Kudo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Taku Kudo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1754386"
                        ],
                        "name": "H. Kazawa",
                        "slug": "H.-Kazawa",
                        "structuredName": {
                            "firstName": "Hideto",
                            "lastName": "Kazawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kazawa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144077726"
                        ],
                        "name": "K. Stevens",
                        "slug": "K.-Stevens",
                        "structuredName": {
                            "firstName": "Keith",
                            "lastName": "Stevens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Stevens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753079661"
                        ],
                        "name": "George Kurian",
                        "slug": "George-Kurian",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Kurian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "George Kurian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056800684"
                        ],
                        "name": "Nishant Patil",
                        "slug": "Nishant-Patil",
                        "structuredName": {
                            "firstName": "Nishant",
                            "lastName": "Patil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nishant Patil"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49337181"
                        ],
                        "name": "Wei Wang",
                        "slug": "Wei-Wang",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39660914"
                        ],
                        "name": "C. Young",
                        "slug": "C.-Young",
                        "structuredName": {
                            "firstName": "Cliff",
                            "lastName": "Young",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Young"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2119125158"
                        ],
                        "name": "Jason R. Smith",
                        "slug": "Jason-R.-Smith",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Smith",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason R. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2909504"
                        ],
                        "name": "Jason Riesa",
                        "slug": "Jason-Riesa",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Riesa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason Riesa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "29951847"
                        ],
                        "name": "Alex Rudnick",
                        "slug": "Alex-Rudnick",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Rudnick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Rudnick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689108"
                        ],
                        "name": "Oriol Vinyals",
                        "slug": "Oriol-Vinyals",
                        "structuredName": {
                            "firstName": "Oriol",
                            "lastName": "Vinyals",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oriol Vinyals"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32131713"
                        ],
                        "name": "G. Corrado",
                        "slug": "G.-Corrado",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Corrado",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Corrado"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48342565"
                        ],
                        "name": "Macduff Hughes",
                        "slug": "Macduff-Hughes",
                        "structuredName": {
                            "firstName": "Macduff",
                            "lastName": "Hughes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Macduff Hughes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49959210"
                        ],
                        "name": "J. Dean",
                        "slug": "J.-Dean",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Dean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dean"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 63
                            }
                        ],
                        "text": "2skylion007.github.io/OpenWebTextCorpus\nthe uncased WordPiece (Wu et al., 2016) tokenization used in (Devlin et al., 2018)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3603249,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dbde7dfa6cae81df8ac19ef500c42db96c3d1edd",
            "isKey": false,
            "numCitedBy": 4645,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (\"wordpieces\") for both input and output. This method provides a good balance between the flexibility of \"character\"-delimited models and the efficiency of \"word\"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google's phrase-based production system."
            },
            "slug": "Google's-Neural-Machine-Translation-System:-the-Gap-Wu-Schuster",
            "title": {
                "fragments": [],
                "text": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "GNMT, Google's Neural Machine Translation system, is presented, which attempts to address many of the weaknesses of conventional phrase-based translation systems and provides a good balance between the flexibility of \"character\"-delimited models and the efficiency of \"word\"-delicited models."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "13070498"
                        ],
                        "name": "A. See",
                        "slug": "A.-See",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "See",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. See"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35025299"
                        ],
                        "name": "Peter J. Liu",
                        "slug": "Peter-J.-Liu",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Liu",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter J. Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 160
                            }
                        ],
                        "text": "We evaluate the pre-trained PMLM on two abstractive summarization datasets, i.e., XSum (Narayan et al., 2018), and the non-anonymized version of CNN/DailyMail (See et al., 2017)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 7
                            }
                        ],
                        "text": "PTRNET (See et al., 2017) is a sequence-tosequence model with pointer networks."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 57
                            }
                        ],
                        "text": ", 2018), and the non-anonymized version of CNN/DailyMail (See et al., 2017)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 8
                            }
                        ],
                        "text": "PTRNET (See et al., 2017) is a sequence-to-\nsequence model with pointer networks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8314118,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "668db48c6a79826456341680ee1175dfc4cced71",
            "isKey": true,
            "numCitedBy": 2464,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points."
            },
            "slug": "Get-To-The-Point:-Summarization-with-Networks-See-Liu",
            "title": {
                "fragments": [],
                "text": "Get To The Point: Summarization with Pointer-Generator Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways, using a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "24590005"
                        ],
                        "name": "Alex Perelygin",
                        "slug": "Alex-Perelygin",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Perelygin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Perelygin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110402830"
                        ],
                        "name": "Jean Wu",
                        "slug": "Jean-Wu",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1964541"
                        ],
                        "name": "Jason Chuang",
                        "slug": "Jason-Chuang",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Chuang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason Chuang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144922861"
                        ],
                        "name": "Christopher Potts",
                        "slug": "Christopher-Potts",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Potts",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher Potts"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 252,
                                "start": 234
                            }
                        ],
                        "text": "As shown in Table 6, we compare the PMLM-based variants against previous models on question answering (SQuAD; Rajpurkar et al. 2016; 2018), natural language inference (MNLI; Williams et al. 2018), and sentiment classification (SST-2; Socher et al. 2013)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 36
                            }
                        ],
                        "text": "2018), and sentiment classification (SST-2; Socher et al. 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 144
                            }
                        ],
                        "text": "There are two single-sentence classification tasks, i.e., linguistic\nacceptability (CoLA; Warstadt et al. 2018), and sentiment analysis (SST-2; Socher et al. 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 990233,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "687bac2d3320083eb4530bf18bb8f8f721477600",
            "isKey": false,
            "numCitedBy": 5366,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases."
            },
            "slug": "Recursive-Deep-Models-for-Semantic-Compositionality-Socher-Perelygin",
            "title": {
                "fragments": [],
                "text": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A Sentiment Treebank that includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality, and introduces the Recursive Neural Tensor Network."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40895509"
                        ],
                        "name": "Trieu H. Trinh",
                        "slug": "Trieu-H.-Trinh",
                        "structuredName": {
                            "firstName": "Trieu",
                            "lastName": "Trinh",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trieu H. Trinh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 137
                            }
                        ],
                        "text": "We use 160GB text corpora from English Wikipedia1, BookCorpus (Zhu et al., 2015), OpenWebText2, CC-News (Liu et al., 2019), and Stories (Trinh & Le, 2018)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 47015717,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d7b6753a2d4a2b286c396854063bde3a91b75535",
            "isKey": false,
            "numCitedBy": 238,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Commonsense reasoning is a long-standing challenge for deep learning. For example, it is difficult to use neural networks to tackle the Winograd Schema dataset (Levesque et al., 2011). In this paper, we present a simple method for commonsense reasoning with neural networks, using unsupervised learning. Key to our method is the use of language models, trained on a massive amount of unlabled data, to score multiple choice questions posed by commonsense reasoning tests. On both Pronoun Disambiguation and Winograd Schema challenges, our models outperform previous state-of-the-art methods by a large margin, without using expensive annotated knowledge bases or hand-engineered features. We train an array of large RNN language models that operate at word or character level on LM-1-Billion, CommonCrawl, SQuAD, Gutenberg Books, and a customized corpus for this task and show that diversity of training data plays an important role in test performance. Further analysis also shows that our system successfully discovers important features of the context that decide the correct answer, indicating a good grasp of commonsense knowledge."
            },
            "slug": "A-Simple-Method-for-Commonsense-Reasoning-Trinh-Le",
            "title": {
                "fragments": [],
                "text": "A Simple Method for Commonsense Reasoning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Key to this method is the use of language models, trained on a massive amount of unlabled data, to score multiple choice questions posed by commonsense reasoning tests, which outperform previous state-of-the-art methods by a large margin."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144863691"
                        ],
                        "name": "Mandar Joshi",
                        "slug": "Mandar-Joshi",
                        "structuredName": {
                            "firstName": "Mandar",
                            "lastName": "Joshi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mandar Joshi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50536468"
                        ],
                        "name": "Danqi Chen",
                        "slug": "Danqi-Chen",
                        "structuredName": {
                            "firstName": "Danqi",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Danqi Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11323179"
                        ],
                        "name": "Yinhan Liu",
                        "slug": "Yinhan-Liu",
                        "structuredName": {
                            "firstName": "Yinhan",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yinhan Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780531"
                        ],
                        "name": "Daniel S. Weld",
                        "slug": "Daniel-S.-Weld",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Weld",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel S. Weld"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1982950"
                        ],
                        "name": "Luke Zettlemoyer",
                        "slug": "Luke-Zettlemoyer",
                        "structuredName": {
                            "firstName": "Luke",
                            "lastName": "Zettlemoyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luke Zettlemoyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39455775"
                        ],
                        "name": "Omer Levy",
                        "slug": "Omer-Levy",
                        "structuredName": {
                            "firstName": "Omer",
                            "lastName": "Levy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Omer Levy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 198229624,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "81f5810fbbab9b7203b9556f4ce3c741875407bc",
            "isKey": false,
            "numCitedBy": 879,
            "numCiting": 120,
            "paperAbstract": {
                "fragments": [],
                "text": "We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our approach extends BERT by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it. SpanBERT consistently outperforms BERT and our better-tuned baselines, with substantial gains on span selection tasks such as question answering and coreference resolution. In particular, with the same training data and model size as BERTlarge, our single model obtains 94.6% and 88.7% F1 on SQuAD 1.1 and 2.0 respectively. We also achieve a new state of the art on the OntoNotes coreference resolution task (79.6% F1), strong performance on the TACRED relation extraction benchmark, and even gains on GLUE.1"
            },
            "slug": "SpanBERT:-Improving-Pre-training-by-Representing-Joshi-Chen",
            "title": {
                "fragments": [],
                "text": "SpanBERT: Improving Pre-training by Representing and Predicting Spans"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "The approach extends BERT by masking contiguous random spans, rather than random tokens, and training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it."
            },
            "venue": {
                "fragments": [],
                "text": "TACL"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46724030"
                        ],
                        "name": "Daniel Matthew Cer",
                        "slug": "Daniel-Matthew-Cer",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Cer",
                            "middleNames": [
                                "Matthew"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Matthew Cer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700007"
                        ],
                        "name": "Mona T. Diab",
                        "slug": "Mona-T.-Diab",
                        "structuredName": {
                            "firstName": "Mona",
                            "lastName": "Diab",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mona T. Diab"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733049"
                        ],
                        "name": "Eneko Agirre",
                        "slug": "Eneko-Agirre",
                        "structuredName": {
                            "firstName": "Eneko",
                            "lastName": "Agirre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eneko Agirre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1405529953"
                        ],
                        "name": "I. Lopez-Gazpio",
                        "slug": "I.-Lopez-Gazpio",
                        "structuredName": {
                            "firstName": "I\u00f1igo",
                            "lastName": "Lopez-Gazpio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Lopez-Gazpio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702974"
                        ],
                        "name": "Lucia Specia",
                        "slug": "Lucia-Specia",
                        "structuredName": {
                            "firstName": "Lucia",
                            "lastName": "Specia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lucia Specia"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 26
                            }
                        ],
                        "text": "The text similarity (STS; Cer et al. 2017) task is formulated as a regression problem."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4421747,
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "id": "a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096",
            "isKey": false,
            "numCitedBy": 934,
            "numCiting": 96,
            "paperAbstract": {
                "fragments": [],
                "text": "Semantic Textual Similarity (STS) measures the meaning similarity of sentences. Applications include machine translation (MT), summarization, generation, question answering (QA), short answer grading, semantic search, dialog and conversational systems. The STS shared task is a venue for assessing the current state-of-the-art. The 2017 task focuses on multilingual and cross-lingual pairs with one sub-track exploring MT quality estimation (MTQE) data. The task obtained strong participation from 31 teams, with 17 participating in all language tracks. We summarize performance and review a selection of well performing methods. Analysis highlights common errors, providing insight into the limitations of existing models. To support ongoing work on semantic representations, the STS Benchmark is introduced as a new shared training and evaluation set carefully selected from the corpus of English STS shared task data (2012-2017)."
            },
            "slug": "SemEval-2017-Task-1:-Semantic-Textual-Similarity-Cer-Diab",
            "title": {
                "fragments": [],
                "text": "SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The STS Benchmark is introduced as a new shared training and evaluation set carefully selected from the corpus of English STS shared task data (2012-2017), providing insight into the limitations of existing models."
            },
            "venue": {
                "fragments": [],
                "text": "SemEval@ACL"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11323179"
                        ],
                        "name": "Yinhan Liu",
                        "slug": "Yinhan-Liu",
                        "structuredName": {
                            "firstName": "Yinhan",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yinhan Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40511414"
                        ],
                        "name": "Myle Ott",
                        "slug": "Myle-Ott",
                        "structuredName": {
                            "firstName": "Myle",
                            "lastName": "Ott",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Myle Ott"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39589154"
                        ],
                        "name": "Naman Goyal",
                        "slug": "Naman-Goyal",
                        "structuredName": {
                            "firstName": "Naman",
                            "lastName": "Goyal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Naman Goyal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3048577"
                        ],
                        "name": "Jingfei Du",
                        "slug": "Jingfei-Du",
                        "structuredName": {
                            "firstName": "Jingfei",
                            "lastName": "Du",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jingfei Du"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144863691"
                        ],
                        "name": "Mandar Joshi",
                        "slug": "Mandar-Joshi",
                        "structuredName": {
                            "firstName": "Mandar",
                            "lastName": "Joshi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mandar Joshi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50536468"
                        ],
                        "name": "Danqi Chen",
                        "slug": "Danqi-Chen",
                        "structuredName": {
                            "firstName": "Danqi",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Danqi Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39455775"
                        ],
                        "name": "Omer Levy",
                        "slug": "Omer-Levy",
                        "structuredName": {
                            "firstName": "Omer",
                            "lastName": "Levy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Omer Levy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35084211"
                        ],
                        "name": "M. Lewis",
                        "slug": "M.-Lewis",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Lewis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lewis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1982950"
                        ],
                        "name": "Luke Zettlemoyer",
                        "slug": "Luke-Zettlemoyer",
                        "structuredName": {
                            "firstName": "Luke",
                            "lastName": "Zettlemoyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luke Zettlemoyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759422"
                        ],
                        "name": "Veselin Stoyanov",
                        "slug": "Veselin-Stoyanov",
                        "structuredName": {
                            "firstName": "Veselin",
                            "lastName": "Stoyanov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Veselin Stoyanov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 198953378,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de",
            "isKey": false,
            "numCitedBy": 7266,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code."
            },
            "slug": "RoBERTa:-A-Robustly-Optimized-BERT-Pretraining-Liu-Ott",
            "title": {
                "fragments": [],
                "text": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is found that BERT was significantly undertrained, and can match or exceed the performance of every model published after it, and the best model achieves state-of-the-art results on GLUE, RACE and SQuAD."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2574060"
                        ],
                        "name": "Christian Szegedy",
                        "slug": "Christian-Szegedy",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Szegedy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Szegedy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2657155"
                        ],
                        "name": "Vincent Vanhoucke",
                        "slug": "Vincent-Vanhoucke",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Vanhoucke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vincent Vanhoucke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054165706"
                        ],
                        "name": "S. Ioffe",
                        "slug": "S.-Ioffe",
                        "structuredName": {
                            "firstName": "Sergey",
                            "lastName": "Ioffe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ioffe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789737"
                        ],
                        "name": "Jonathon Shlens",
                        "slug": "Jonathon-Shlens",
                        "structuredName": {
                            "firstName": "Jonathon",
                            "lastName": "Shlens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathon Shlens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3282833"
                        ],
                        "name": "Z. Wojna",
                        "slug": "Z.-Wojna",
                        "structuredName": {
                            "firstName": "Zbigniew",
                            "lastName": "Wojna",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Wojna"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 206593880,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "23ffaa0fe06eae05817f527a47ac3291077f9e58",
            "isKey": false,
            "numCitedBy": 15542,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2% top-1 and 5:6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5% top-5 error and 17:3% top-1 error on the validation set and 3:6% top-5 error on the official test set."
            },
            "slug": "Rethinking-the-Inception-Architecture-for-Computer-Szegedy-Vanhoucke",
            "title": {
                "fragments": [],
                "text": "Rethinking the Inception Architecture for Computer Vision"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work is exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "81840293"
                        ],
                        "name": "Adina Williams",
                        "slug": "Adina-Williams",
                        "structuredName": {
                            "firstName": "Adina",
                            "lastName": "Williams",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adina Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10666396"
                        ],
                        "name": "Nikita Nangia",
                        "slug": "Nikita-Nangia",
                        "structuredName": {
                            "firstName": "Nikita",
                            "lastName": "Nangia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nikita Nangia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3644767"
                        ],
                        "name": "Samuel R. Bowman",
                        "slug": "Samuel-R.-Bowman",
                        "structuredName": {
                            "firstName": "Samuel",
                            "lastName": "Bowman",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samuel R. Bowman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 174
                            }
                        ],
                        "text": "As shown in Table 6, we compare the PMLM-based variants against previous models on question answering (SQuAD; Rajpurkar et al. 2016; 2018), natural language inference (MNLI; Williams et al. 2018), and sentiment classification (SST-2; Socher et al. 2013)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 162
                            }
                        ],
                        "text": "\u2026classification tasks, including natural language inference (RTE, MNLI; Dagan et al. 2006; Bar-Haim et al. 2006; Giampiccolo et al. 2007; Bentivogli et al. 2009; Williams et al. 2018), question answering (QNLI; Rajpurkar et al. 2016), and paraphrase detection (QQP, MRPC; Dolan & Brockett 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 88
                            }
                        ],
                        "text": "The other tasks are pairwise classification tasks, including natural language inference (RTE, MNLI; Dagan et al. 2006; Bar-Haim et al. 2006; Giampiccolo et al. 2007; Bentivogli et al. 2009; Williams et al. 2018), question answering (QNLI; Rajpurkar et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 40
                            }
                        ],
                        "text": "2016; 2018), natural language inference (MNLI; Williams et al. 2018), and sentiment classification (SST-2; Socher et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3432876,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5ded2b8c64491b4a67f6d39ce473d4b9347a672e",
            "isKey": true,
            "numCitedBy": 2036,
            "numCiting": 67,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. At 433k examples, this resource is one of the largest corpora available for natural language inference (a.k.a. recognizing textual entailment), improving upon available resources in both its coverage and difficulty. MultiNLI accomplishes this by offering data from ten distinct genres of written and spoken English, making it possible to evaluate systems on nearly the full complexity of the language, while supplying an explicit setting for evaluating cross-genre domain adaptation. In addition, an evaluation using existing machine learning models designed for the Stanford NLI corpus shows that it represents a substantially more difficult task than does that corpus, despite the two showing similar levels of inter-annotator agreement."
            },
            "slug": "A-Broad-Coverage-Challenge-Corpus-for-Sentence-Williams-Nangia",
            "title": {
                "fragments": [],
                "text": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "The Multi-Genre Natural Language Inference corpus is introduced, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding and shows that it represents a substantially more difficult task than does the Stanford NLI corpus."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143790499"
                        ],
                        "name": "Shashi Narayan",
                        "slug": "Shashi-Narayan",
                        "structuredName": {
                            "firstName": "Shashi",
                            "lastName": "Narayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shashi Narayan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40146204"
                        ],
                        "name": "Shay B. Cohen",
                        "slug": "Shay-B.-Cohen",
                        "structuredName": {
                            "firstName": "Shay",
                            "lastName": "Cohen",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shay B. Cohen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747893"
                        ],
                        "name": "Mirella Lapata",
                        "slug": "Mirella-Lapata",
                        "structuredName": {
                            "firstName": "Mirella",
                            "lastName": "Lapata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mirella Lapata"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 88
                            }
                        ],
                        "text": "We evaluate the pre-trained PMLM on two abstractive summarization datasets, i.e., XSum (Narayan et al., 2018), and the non-anonymized version of CNN/DailyMail (See et al., 2017)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 7
                            }
                        ],
                        "text": ", XSum (Narayan et al., 2018), and the non-anonymized version of CNN/DailyMail (See et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 215768182,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "305b2cf37e5dece81e95c92883d5a6e28ac93b22",
            "isKey": false,
            "numCitedBy": 484,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce \u201cextreme summarization\u201d, a new single-document summarization task which does not favor extractive strategies and calls for an abstractive modeling approach. The idea is to create a short, one-sentence news summary answering the question \u201cWhat is the article about?\u201d. We collect a real-world, large-scale dataset for this task by harvesting online articles from the British Broadcasting Corporation (BBC). We propose a novel abstractive model which is conditioned on the article\u2019s topics and based entirely on convolutional neural networks. We demonstrate experimentally that this architecture captures long-range dependencies in a document and recognizes pertinent content, outperforming an oracle extractive system and state-of-the-art abstractive approaches when evaluated automatically and by humans."
            },
            "slug": "Don\u2019t-Give-Me-the-Details,-Just-the-Summary!-Neural-Narayan-Cohen",
            "title": {
                "fragments": [],
                "text": "Don\u2019t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A novel abstractive model is proposed which is conditioned on the article\u2019s topics and based entirely on convolutional neural networks, outperforming an oracle extractive system and state-of-the-art abstractive approaches when evaluated automatically and by humans."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "13728923"
                        ],
                        "name": "X. Du",
                        "slug": "X.-Du",
                        "structuredName": {
                            "firstName": "X.",
                            "lastName": "Du",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Du"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748501"
                        ],
                        "name": "Claire Cardie",
                        "slug": "Claire-Cardie",
                        "structuredName": {
                            "firstName": "Claire",
                            "lastName": "Cardie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Claire Cardie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 47
                            }
                        ],
                        "text": "We perform evaluations on question generation (Du & Cardie, 2018), the task of automatically producing relevant questions that ask for the given answer and context."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 177
                            }
                        ],
                        "text": "Among the compared results, UNILM (Dong et al., 2019) is based on pretrained models, while the other three methods are sequenceto-sequence models enhanced with manual features (Du & Cardie, 2018), gated self-attention (Zhao et al., 2018), and reinforcement learning (Zhang & Bansal, 2019)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 21702856,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d8b6e965b771e3d0707dd8caa57224a0dfbb886e",
            "isKey": false,
            "numCitedBy": 109,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the task of generating from Wikipedia articles question-answer pairs that cover content beyond a single sentence. We propose a neural network approach that incorporates coreference knowledge via a novel gating mechanism. As compared to models that only take into account sentence-level information (Heilman and Smith, 2010; Du et al., 2017; Zhou et al., 2017), we find that the linguistic knowledge introduced by the coreference representation aids question generation significantly, producing models that outperform the current state-of-the-art. We apply our system (composed of an answer span extraction system and the passage-level QG system) to the 10,000 top ranking Wikipedia articles and create a corpus of over one million question-answer pairs. We provide qualitative analysis for the this large-scale generated corpus from Wikipedia."
            },
            "slug": "Harvesting-Paragraph-level-Question-Answer-Pairs-Du-Cardie",
            "title": {
                "fragments": [],
                "text": "Harvesting Paragraph-level Question-Answer Pairs from Wikipedia"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is found that the linguistic knowledge introduced by the coreference representation aids question generation significantly, producing models that outperform the current state-of-the-art."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7670835"
                        ],
                        "name": "Shiyue Zhang",
                        "slug": "Shiyue-Zhang",
                        "structuredName": {
                            "firstName": "Shiyue",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shiyue Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143977268"
                        ],
                        "name": "Mohit Bansal",
                        "slug": "Mohit-Bansal",
                        "structuredName": {
                            "firstName": "Mohit",
                            "lastName": "Bansal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mohit Bansal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 202572810,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8f23c219233fb67eedb1e4333c8458edb1c5cd68",
            "isKey": false,
            "numCitedBy": 80,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": "Text-based Question Generation (QG) aims at generating natural and relevant questions that can be answered by a given answer in some context. Existing QG models suffer from a \u201csemantic drift\u201d problem, i.e., the semantics of the model-generated question drifts away from the given context and answer. In this paper, we first propose two semantics-enhanced rewards obtained from downstream question paraphrasing and question answering tasks to regularize the QG model to generate semantically valid questions. Second, since the traditional evaluation metrics (e.g., BLEU) often fall short in evaluating the quality of generated questions, we propose a QA-based evaluation method which measures the QG model\u2019s ability to mimic human annotators in generating QA training data. Experiments show that our method achieves the new state-of-the-art performance w.r.t. traditional metrics, and also performs best on our QA-based evaluation metrics. Further, we investigate how to use our QG model to augment QA datasets and enable semi-supervised QA. We propose two ways to generate synthetic QA pairs: generate new questions from existing articles or collect QA pairs from new articles. We also propose two empirically effective strategies, a data filter and mixing mini-batch training, to properly use the QG-generated data for QA. Experiments show that our method improves over both BiDAF and BERT QA baselines, even without introducing new articles."
            },
            "slug": "Addressing-Semantic-Drift-in-Question-Generation-Zhang-Bansal",
            "title": {
                "fragments": [],
                "text": "Addressing Semantic Drift in Question Generation for Semi-Supervised Question Answering"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper proposes two semantics-enhanced rewards obtained from downstream question paraphrasing and question answering tasks to regularize the QG model to generate semantically valid questions, and proposes a QA-based evaluation method which measures the model\u2019s ability to mimic human annotators in generating QA training data."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726807"
                        ],
                        "name": "Diederik P. Kingma",
                        "slug": "Diederik-P.-Kingma",
                        "structuredName": {
                            "firstName": "Diederik",
                            "lastName": "Kingma",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Diederik P. Kingma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2503659"
                        ],
                        "name": "Jimmy Ba",
                        "slug": "Jimmy-Ba",
                        "structuredName": {
                            "firstName": "Jimmy",
                            "lastName": "Ba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jimmy Ba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 14
                            }
                        ],
                        "text": "We used Adam (Kingma & Ba, 2015) with \u03b21 = 0.9, \u03b22 = 0.98, and = 1e-6 for optimization."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 21
                            }
                        ],
                        "text": "The learning rate of Adam (\u03b21 = 0.9, \u03b22 = 0.999) was set to 1e-4, with linear schedule and warmup over the first 10K steps."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6628106,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
            "isKey": true,
            "numCitedBy": 90054,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm."
            },
            "slug": "Adam:-A-Method-for-Stochastic-Optimization-Kingma-Ba",
            "title": {
                "fragments": [],
                "text": "Adam: A Method for Stochastic Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This work introduces Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments, and provides a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1844940337"
                        ],
                        "name": "Yukun Zhu",
                        "slug": "Yukun-Zhu",
                        "structuredName": {
                            "firstName": "Yukun",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yukun Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3450996"
                        ],
                        "name": "Ryan Kiros",
                        "slug": "Ryan-Kiros",
                        "structuredName": {
                            "firstName": "Ryan",
                            "lastName": "Kiros",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ryan Kiros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804104"
                        ],
                        "name": "R. Zemel",
                        "slug": "R.-Zemel",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Zemel",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zemel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2422559"
                        ],
                        "name": "R. Urtasun",
                        "slug": "R.-Urtasun",
                        "structuredName": {
                            "firstName": "Raquel",
                            "lastName": "Urtasun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Urtasun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37895334"
                        ],
                        "name": "S. Fidler",
                        "slug": "S.-Fidler",
                        "structuredName": {
                            "firstName": "Sanja",
                            "lastName": "Fidler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Fidler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "s, we report the major results using similar pre-training datasets and optimization hyperparameters as in RoBERTa BASE (Liu et al.,2019). We use 160GB text corpora from English Wikipedia, BookCorpus (Zhu et al., 2015), OpenWebText1, CC-News (Liu et al.,2019), and 1skylion007.github.io/OpenWebTextCorpus Stories (Trinh &amp; Le,2018). We follow the preprocess and the uncased WordPiece (Wu et al.,2016) tokenization u"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6866988,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0e6824e137847be0599bb0032e37042ed2ef5045",
            "isKey": false,
            "numCitedBy": 1418,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story. This paper aims to align books to their movie releases in order to provide rich descriptive explanations for visual content that go semantically far beyond the captions available in the current datasets. To align movies and books we propose a neural sentence embedding that is trained in an unsupervised way from a large corpus of books, as well as a video-text neural embedding for computing similarities between movie clips and sentences in the book. We propose a context-aware CNN to combine information from multiple sources. We demonstrate good quantitative performance for movie/book alignment and show several qualitative examples that showcase the diversity of tasks our model can be used for."
            },
            "slug": "Aligning-Books-and-Movies:-Towards-Story-Like-by-Zhu-Kiros",
            "title": {
                "fragments": [],
                "text": "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "To align movies and books, a neural sentence embedding that is trained in an unsupervised way from a large corpus of books, as well as a video-text neural embedding for computing similarities between movie clips and sentences in the book are proposed."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2706258"
                        ],
                        "name": "Pranav Rajpurkar",
                        "slug": "Pranav-Rajpurkar",
                        "structuredName": {
                            "firstName": "Pranav",
                            "lastName": "Rajpurkar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pranav Rajpurkar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3422908"
                        ],
                        "name": "Robin Jia",
                        "slug": "Robin-Jia",
                        "structuredName": {
                            "firstName": "Robin",
                            "lastName": "Jia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robin Jia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145419642"
                        ],
                        "name": "Percy Liang",
                        "slug": "Percy-Liang",
                        "structuredName": {
                            "firstName": "Percy",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Percy Liang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 110
                            }
                        ],
                        "text": "As shown in Table 6, we compare the PMLM-based variants against previous models on question answering (SQuAD; Rajpurkar et al. 2016; 2018), natural language inference (MNLI; Williams et al. 2018), and sentiment classification (SST-2; Socher et al. 2013)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 3
                            }
                        ],
                        "text": "0 (Rajpurkar et al., 2018)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 47018994,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4d1c856275744c0284312a3a50efb6ca9dc4cd4c",
            "isKey": false,
            "numCitedBy": 1398,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuADRUn, a new dataset that combines the existing Stanford Question Answering Dataset (SQuAD) with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuADRUn, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuADRUn is a challenging natural language understanding task for existing models: a strong neural system that gets 86% F1 on SQuAD achieves only 66% F1 on SQuADRUn. We release SQuADRUn to the community as the successor to SQuAD."
            },
            "slug": "Know-What-You-Don\u2019t-Know:-Unanswerable-Questions-Rajpurkar-Jia",
            "title": {
                "fragments": [],
                "text": "Know What You Don\u2019t Know: Unanswerable Questions for SQuAD"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "SQuadRUn is a new dataset that combines the existing Stanford Question Answering Dataset (SQuAD) with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46236380"
                        ],
                        "name": "Alex Warstadt",
                        "slug": "Alex-Warstadt",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Warstadt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Warstadt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50286460"
                        ],
                        "name": "Amanpreet Singh",
                        "slug": "Amanpreet-Singh",
                        "structuredName": {
                            "firstName": "Amanpreet",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Amanpreet Singh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3644767"
                        ],
                        "name": "Samuel R. Bowman",
                        "slug": "Samuel-R.-Bowman",
                        "structuredName": {
                            "firstName": "Samuel",
                            "lastName": "Bowman",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samuel R. Bowman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 14
                            }
                        ],
                        "text": "acceptability (CoLA; Warstadt et al. 2018), and sentiment analysis (SST-2; Socher et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 90
                            }
                        ],
                        "text": "There are two single-sentence classification tasks, i.e., linguistic\nacceptability (CoLA; Warstadt et al. 2018), and sentiment analysis (SST-2; Socher et al. 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 44072099,
            "fieldsOfStudy": [
                "Computer Science",
                "Linguistics"
            ],
            "id": "cb0f3ee1e98faf92429d601cdcd76c69c1e484eb",
            "isKey": false,
            "numCitedBy": 545,
            "numCiting": 111,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract This paper investigates the ability of artificial neural networks to judge the grammatical acceptability of a sentence, with the goal of testing their linguistic competence. We introduce the Corpus of Linguistic Acceptability (CoLA), a set of 10,657 English sentences labeled as grammatical or ungrammatical from published linguistics literature. As baselines, we train several recurrent neural network models on acceptability classification, and find that our models outperform unsupervised models by Lau et al. (2016) on CoLA. Error-analysis on specific grammatical phenomena reveals that both Lau et al.\u2019s models and ours learn systematic generalizations like subject-verb-object order. However, all models we test perform far below human level on a wide range of grammatical constructions."
            },
            "slug": "Neural-Network-Acceptability-Judgments-Warstadt-Singh",
            "title": {
                "fragments": [],
                "text": "Neural Network Acceptability Judgments"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "This paper introduces the Corpus of Linguistic Acceptability (CoLA), a set of 10,657 English sentences labeled as grammatical or ungrammatical from published linguistics literature, and trains several recurrent neural network models on acceptability classification, and finds that the authors' models outperform unsupervised models by Lau et al. (2016) on CoLA."
            },
            "venue": {
                "fragments": [],
                "text": "Transactions of the Association for Computational Linguistics"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2486762"
                        ],
                        "name": "L. Bentivogli",
                        "slug": "L.-Bentivogli",
                        "structuredName": {
                            "firstName": "Luisa",
                            "lastName": "Bentivogli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bentivogli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48323507"
                        ],
                        "name": "Peter Clark",
                        "slug": "Peter-Clark",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7465342"
                        ],
                        "name": "Ido Dagan",
                        "slug": "Ido-Dagan",
                        "structuredName": {
                            "firstName": "Ido",
                            "lastName": "Dagan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ido Dagan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2380885"
                        ],
                        "name": "Danilo Giampiccolo",
                        "slug": "Danilo-Giampiccolo",
                        "structuredName": {
                            "firstName": "Danilo",
                            "lastName": "Giampiccolo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Danilo Giampiccolo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 88
                            }
                        ],
                        "text": "The other tasks are pairwise classification tasks, including natural language inference (RTE, MNLI; Dagan et al. 2006; Bar-Haim et al. 2006; Giampiccolo et al. 2007; Bentivogli et al. 2009; Williams et al. 2018), question answering (QNLI; Rajpurkar et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 147
                            }
                        ],
                        "text": "\u2026pairwise classification tasks, including natural language inference (RTE, MNLI; Dagan et al. 2006; Bar-Haim et al. 2006; Giampiccolo et al. 2007; Bentivogli et al. 2009; Williams et al. 2018), question answering (QNLI; Rajpurkar et al. 2016), and paraphrase detection (QQP, MRPC; Dolan &\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 858065,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "db8885a0037fe47d973ade79d696586453710233",
            "isKey": false,
            "numCitedBy": 439,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents the Sixth Recognizing Textual Entailment (RTE-6) challenge. This year a major innovation was introduced, as the traditional Main Task was replaced by a new task, similar to the RTE-5 Search Pilot, in which Textual Entailment is performed on a real corpus in the Update Summarization scenario. A subtask was also proposed, aimed at detecting novel information. To continue the effort of testing RTE in NLP applications, a KBP Validation Pilot Task was set up, in which RTE systems had to validate the output of systems participating in the KBP Slot Filling Task. Eighteen teams participated in the Main Task (48 submitted runs) and 9 in the Novelty Detection Subtask (22 submitted runs). As for the Pilot, 10 runs were submitted by 3 participants. Finally, the exploratory effort started in RTE-5 to perform resource evaluation through ablation tests was not only reiterated in RTE-6, but also extended to tools."
            },
            "slug": "The-Sixth-PASCAL-Recognizing-Textual-Entailment-Bentivogli-Clark",
            "title": {
                "fragments": [],
                "text": "The Sixth PASCAL Recognizing Textual Entailment Challenge"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "This paper presents the Sixth Recognizing Textual Entailment (RTE-6) challenge, as the traditional Main Task was replaced by a new task, similar to the RTE-5 Search Pilot, in which TextualEntailment is performed on a real corpus in the Update Summarization scenario."
            },
            "venue": {
                "fragments": [],
                "text": "TAC"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110878863"
                        ],
                        "name": "S. Banerjee",
                        "slug": "S.-Banerjee",
                        "structuredName": {
                            "firstName": "Satanjeev",
                            "lastName": "Banerjee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Banerjee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784914"
                        ],
                        "name": "A. Lavie",
                        "slug": "A.-Lavie",
                        "structuredName": {
                            "firstName": "Alon",
                            "lastName": "Lavie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lavie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 68
                            }
                        ],
                        "text": "As shown in Table 5, we report BLEU (Papineni et al., 2002), METEOR(Banerjee & Lavie, 2005), and ROUGE (Lin, 2004) scores on two different data splits."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7164502,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0cd18e4400ff75b2f8b58d60ddb9b0bc12f489e7",
            "isKey": false,
            "numCitedBy": 2986,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe METEOR, an automatic metric for machine translation evaluation that is based on a generalized concept of unigram matching between the machineproduced translation and human-produced reference translations. Unigrams can be matched based on their surface forms, stemmed forms, and meanings; furthermore, METEOR can be easily extended to include more advanced matching strategies. Once all generalized unigram matches between the two strings have been found, METEOR computes a score for this matching using a combination of unigram-precision, unigram-recall, and a measure of fragmentation that is designed to directly capture how well-ordered the matched words in the machine translation are in relation to the reference. We evaluate METEOR by measuring the correlation between the metric scores and human judgments of translation quality. We compute the Pearson R correlation value between its scores and human quality assessments of the LDC TIDES 2003 Arabic-to-English and Chinese-to-English datasets. We perform segment-bysegment correlation, and show that METEOR gets an R correlation value of 0.347 on the Arabic data and 0.331 on the Chinese data. This is shown to be an improvement on using simply unigramprecision, unigram-recall and their harmonic F1 combination. We also perform experiments to show the relative contributions of the various mapping modules."
            },
            "slug": "METEOR:-An-Automatic-Metric-for-MT-Evaluation-with-Banerjee-Lavie",
            "title": {
                "fragments": [],
                "text": "METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "METEOR is described, an automatic metric for machine translation evaluation that is based on a generalized concept of unigram matching between the machineproduced translation and human-produced reference translations and can be easily extended to include more advanced matching strategies."
            },
            "venue": {
                "fragments": [],
                "text": "IEEvaluation@ACL"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2706258"
                        ],
                        "name": "Pranav Rajpurkar",
                        "slug": "Pranav-Rajpurkar",
                        "structuredName": {
                            "firstName": "Pranav",
                            "lastName": "Rajpurkar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pranav Rajpurkar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2151810148"
                        ],
                        "name": "Jian Zhang",
                        "slug": "Jian-Zhang",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2787620"
                        ],
                        "name": "Konstantin Lopyrev",
                        "slug": "Konstantin-Lopyrev",
                        "structuredName": {
                            "firstName": "Konstantin",
                            "lastName": "Lopyrev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Konstantin Lopyrev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145419642"
                        ],
                        "name": "Percy Liang",
                        "slug": "Percy-Liang",
                        "structuredName": {
                            "firstName": "Percy",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Percy Liang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 3
                            }
                        ],
                        "text": "1 (Rajpurkar et al., 2016) and v2."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 26
                            }
                        ],
                        "text": "2018), question answering (QNLI; Rajpurkar et al. 2016), and paraphrase detection (QQP, MRPC; Dolan & Brockett 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 102
                            }
                        ],
                        "text": "As shown in Table 6, we compare the PMLM-based variants against previous models on question answering (SQuAD; Rajpurkar et al. 2016; 2018), natural language inference (MNLI; Williams et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 232,
                                "start": 211
                            }
                        ],
                        "text": "\u2026classification tasks, including natural language inference (RTE, MNLI; Dagan et al. 2006; Bar-Haim et al. 2006; Giampiccolo et al. 2007; Bentivogli et al. 2009; Williams et al. 2018), question answering (QNLI; Rajpurkar et al. 2016), and paraphrase detection (QQP, MRPC; Dolan & Brockett 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 110
                            }
                        ],
                        "text": "As shown in Table 6, we compare the PMLM-based variants against previous models on question answering (SQuAD; Rajpurkar et al. 2016; 2018), natural language inference (MNLI; Williams et al. 2018), and sentiment classification (SST-2; Socher et al. 2013)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11816014,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "05dd7254b632376973f3a1b4d39485da17814df5",
            "isKey": true,
            "numCitedBy": 4263,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research. \nThe dataset is freely available at this https URL"
            },
            "slug": "SQuAD:-100,000+-Questions-for-Machine-Comprehension-Rajpurkar-Zhang",
            "title": {
                "fragments": [],
                "text": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"
            },
            "tldr": {
                "abstractSimilarityScore": 35,
                "text": "A strong logistic regression model is built, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%)."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2486762"
                        ],
                        "name": "L. Bentivogli",
                        "slug": "L.-Bentivogli",
                        "structuredName": {
                            "firstName": "Luisa",
                            "lastName": "Bentivogli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bentivogli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48323507"
                        ],
                        "name": "Peter Clark",
                        "slug": "Peter-Clark",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7465342"
                        ],
                        "name": "Ido Dagan",
                        "slug": "Ido-Dagan",
                        "structuredName": {
                            "firstName": "Ido",
                            "lastName": "Dagan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ido Dagan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2380885"
                        ],
                        "name": "Danilo Giampiccolo",
                        "slug": "Danilo-Giampiccolo",
                        "structuredName": {
                            "firstName": "Danilo",
                            "lastName": "Giampiccolo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Danilo Giampiccolo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The other tasks are pairwise classification tasks, including natural language inference (RTE, MNLI; Dagan et al. 2006; Bar-Haim et al. 2006; Giampiccolo et al. 2007; Bentivogli et al. 2009; Williams et al. 2018), question answering (QNLI; Rajpurkar et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5791809,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0f8468de03ee9f12d693237bec87916311bf1c24",
            "isKey": false,
            "numCitedBy": 390,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents the Seventh Recognizing Textual Entailment (RTE-7) challenge. This year\u2019s challenge replicated the exercise proposed in RTE-6, consisting of a Main Task, in which Textual Entailment is performed on a real corpus in the Update Summarization scenario; a Main subtask aimed at detecting novel information; and a KBP Validation Task, in which RTE systems had to validate the output of systems participating in the KBP Slot Filling Task. Thirteen teams participated in the Main Task (submitting 33 runs) and 5 in the Novelty Detection Subtask (submitting 13 runs). The KBP Validation Task was undertaken by 2 participants which submitted 5 runs. The ablation test experiment, introduced in RTE-5 to evaluate the impact of knowledge resources used by the systems participating in the Main Task and extended also to tools in RTE-6, was also repeated in RTE-7."
            },
            "slug": "The-Seventh-PASCAL-Recognizing-Textual-Entailment-Bentivogli-Clark",
            "title": {
                "fragments": [],
                "text": "The Seventh PASCAL Recognizing Textual Entailment Challenge"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "This paper presents the Seventh Recognizing Textual Entailment (RTE-7) challenge, which replicated the exercise proposed in RTE-6, consisting of a Main Task, a Main subtask aimed at detecting novel information; and a KBP Validation Task, in which RTE systems had to validate the output of systems participating in the KBP Slot Filling Task."
            },
            "venue": {
                "fragments": [],
                "text": "TAC"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "83415753"
                        ],
                        "name": "W. Dolan",
                        "slug": "W.-Dolan",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Dolan",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Dolan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3125776"
                        ],
                        "name": "Chris Brockett",
                        "slug": "Chris-Brockett",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Brockett",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Brockett"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 299,
                                "start": 294
                            }
                        ],
                        "text": "The other tasks are pairwise classification tasks, including natural language inference (RTE, MNLI; Dagan et al. 2006; Bar-Haim et al. 2006; Giampiccolo et al. 2007; Bentivogli et al. 2009; Williams et al. 2018), question answering (QNLI; Rajpurkar et al. 2016), and paraphrase detection (QQP, MRPC; Dolan & Brockett 2005)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 293,
                                "start": 272
                            }
                        ],
                        "text": "\u2026classification tasks, including natural language inference (RTE, MNLI; Dagan et al. 2006; Bar-Haim et al. 2006; Giampiccolo et al. 2007; Bentivogli et al. 2009; Williams et al. 2018), question answering (QNLI; Rajpurkar et al. 2016), and paraphrase detection (QQP, MRPC; Dolan & Brockett 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16639476,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "475354f10798f110d34792b6d88f31d6d5cb099e",
            "isKey": false,
            "numCitedBy": 834,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "An obstacle to research in automatic paraphrase identification and generation is the lack of large-scale, publiclyavailable labeled corpora of sentential paraphrases. This paper describes the creation of the recently-released Microsoft Research Paraphrase Corpus, which contains 5801 sentence pairs, each hand-labeled with a binary judgment as to whether the pair constitutes a paraphrase. The corpus was created using heuristic extraction techniques in conjunction with an SVM-based classifier to select likely sentence-level paraphrases from a large corpus of topicclustered news data. These pairs were then submitted to human judges, who confirmed that 67% were in fact semantically equivalent. In addition to describing the corpus itself, we explore a number of issues that arose in defining guidelines for the human raters."
            },
            "slug": "Automatically-Constructing-a-Corpus-of-Sentential-Dolan-Brockett",
            "title": {
                "fragments": [],
                "text": "Automatically Constructing a Corpus of Sentential Paraphrases"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The creation of the recently-released Microsoft Research Paraphrase Corpus, which contains 5801 sentence pairs, each hand-labeled with a binary judgment as to whether the pair constitutes a paraphrase, is described."
            },
            "venue": {
                "fragments": [],
                "text": "IJCNLP"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781574"
                        ],
                        "name": "Chin-Yew Lin",
                        "slug": "Chin-Yew-Lin",
                        "structuredName": {
                            "firstName": "Chin-Yew",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chin-Yew Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 964287,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "60b05f32c32519a809f21642ef1eb3eaf3848008",
            "isKey": false,
            "numCitedBy": 6943,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans. This paper introduces four different ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S included in the ROUGE summarization evaluation package and their evaluations. Three of them have been used in the Document Understanding Conference (DUC) 2004, a large-scale summarization evaluation sponsored by NIST."
            },
            "slug": "ROUGE:-A-Package-for-Automatic-Evaluation-of-Lin",
            "title": {
                "fragments": [],
                "text": "ROUGE: A Package for Automatic Evaluation of Summaries"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Four different RouGE measures are introduced: ROUGE-N, ROUge-L, R OUGE-W, and ROUAGE-S included in the Rouge summarization evaluation package and their evaluations."
            },
            "venue": {
                "fragments": [],
                "text": "ACL 2004"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3323275"
                        ],
                        "name": "Kishore Papineni",
                        "slug": "Kishore-Papineni",
                        "structuredName": {
                            "firstName": "Kishore",
                            "lastName": "Papineni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kishore Papineni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781292"
                        ],
                        "name": "S. Roukos",
                        "slug": "S.-Roukos",
                        "structuredName": {
                            "firstName": "Salim",
                            "lastName": "Roukos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roukos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144582029"
                        ],
                        "name": "T. Ward",
                        "slug": "T.-Ward",
                        "structuredName": {
                            "firstName": "Todd",
                            "lastName": "Ward",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ward"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2587983"
                        ],
                        "name": "Wei-Jing Zhu",
                        "slug": "Wei-Jing-Zhu",
                        "structuredName": {
                            "firstName": "Wei-Jing",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei-Jing Zhu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 37
                            }
                        ],
                        "text": "As shown in Table 5, we report BLEU (Papineni et al., 2002), METEOR(Banerjee & Lavie, 2005), and ROUGE (Lin, 2004) scores on two different data splits."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11080756,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d7da009f457917aa381619facfa5ffae9329a6e9",
            "isKey": false,
            "numCitedBy": 16616,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations."
            },
            "slug": "Bleu:-a-Method-for-Automatic-Evaluation-of-Machine-Papineni-Roukos",
            "title": {
                "fragments": [],
                "text": "Bleu: a Method for Automatic Evaluation of Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work proposes a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403835575"
                        ],
                        "name": "Roy Bar-Haim",
                        "slug": "Roy-Bar-Haim",
                        "structuredName": {
                            "firstName": "Roy",
                            "lastName": "Bar-Haim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roy Bar-Haim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7465342"
                        ],
                        "name": "Ido Dagan",
                        "slug": "Ido-Dagan",
                        "structuredName": {
                            "firstName": "Ido",
                            "lastName": "Dagan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ido Dagan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66648221"
                        ],
                        "name": "Bill Dolan",
                        "slug": "Bill-Dolan",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Dolan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bill Dolan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36405930"
                        ],
                        "name": "L. Ferro",
                        "slug": "L.-Ferro",
                        "structuredName": {
                            "firstName": "Lisa",
                            "lastName": "Ferro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Ferro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2380885"
                        ],
                        "name": "Danilo Giampiccolo",
                        "slug": "Danilo-Giampiccolo",
                        "structuredName": {
                            "firstName": "Danilo",
                            "lastName": "Giampiccolo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Danilo Giampiccolo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712352"
                        ],
                        "name": "B. Magnini",
                        "slug": "B.-Magnini",
                        "structuredName": {
                            "firstName": "Bernardo",
                            "lastName": "Magnini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Magnini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711977"
                        ],
                        "name": "Idan Szpektor",
                        "slug": "Idan-Szpektor",
                        "structuredName": {
                            "firstName": "Idan",
                            "lastName": "Szpektor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Idan Szpektor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 88
                            }
                        ],
                        "text": "The other tasks are pairwise classification tasks, including natural language inference (RTE, MNLI; Dagan et al. 2006; Bar-Haim et al. 2006; Giampiccolo et al. 2007; Bentivogli et al. 2009; Williams et al. 2018), question answering (QNLI; Rajpurkar et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 119
                            }
                        ],
                        "text": "The other tasks are pairwise classification tasks, including natural language inference (RTE, MNLI; Dagan et al. 2006; Bar-Haim et al. 2006; Giampiccolo et al. 2007; Bentivogli et al. 2009; Williams et al. 2018), question answering (QNLI; Rajpurkar et al. 2016), and paraphrase detection (QQP, MRPC;\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13385138,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "136326377c122560768db674e35f5bcd6de3bc40",
            "isKey": false,
            "numCitedBy": 408,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the Second PASCAL Recognising Textual Entailment Challenge (RTE-2). 1 We describe the RTE2 dataset and overview the submissions for the challenge. One of the main goals for this year\u2019s dataset was to provide more \u201crealistic\u201d text-hypothesis examples, based mostly on outputs of actual systems. The 23 submissions for the challenge present diverse approaches and research directions, and the best results achieved this year are considerably higher than last year\u2019s state of the art."
            },
            "slug": "The-Second-PASCAL-Recognising-Textual-Entailment-Bar-Haim-Dagan",
            "title": {
                "fragments": [],
                "text": "The Second PASCAL Recognising Textual Entailment Challenge"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The RTE2 dataset is described and the submissions for the challenge are overviewed, to provide more \u201crealistic\u201d text-hypothesis examples, based mostly on outputs of actual systems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7465342"
                        ],
                        "name": "Ido Dagan",
                        "slug": "Ido-Dagan",
                        "structuredName": {
                            "firstName": "Ido",
                            "lastName": "Dagan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ido Dagan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2807469"
                        ],
                        "name": "Oren Glickman",
                        "slug": "Oren-Glickman",
                        "structuredName": {
                            "firstName": "Oren",
                            "lastName": "Glickman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oren Glickman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712352"
                        ],
                        "name": "B. Magnini",
                        "slug": "B.-Magnini",
                        "structuredName": {
                            "firstName": "Bernardo",
                            "lastName": "Magnini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Magnini"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 168
                            }
                        ],
                        "text": "As shown in Table 6, we compare the PMLM-based variants against previous models on question answering (SQuAD; Rajpurkar et al. 2016; 2018), natural language inference (MNLI; Williams et al. 2018), and sentiment classification (SST-2; Socher et al. 2013)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 94
                            }
                        ],
                        "text": "The other tasks are pairwise classification tasks, including natural language inference (RTE, MNLI; Dagan et al. 2006; Bar-Haim et al. 2006; Giampiccolo et al. 2007; Bentivogli et al. 2009; Williams et al. 2018), question answering (QNLI; Rajpurkar et al. 2016), and paraphrase detection (QQP, MRPC; Dolan & Brockett 2005)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 88
                            }
                        ],
                        "text": "The other tasks are pairwise classification tasks, including natural language inference (RTE, MNLI; Dagan et al. 2006; Bar-Haim et al. 2006; Giampiccolo et al. 2007; Bentivogli et al. 2009; Williams et al. 2018), question answering (QNLI; Rajpurkar et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 100
                            }
                        ],
                        "text": "The other tasks are pairwise classification tasks, including natural language inference (RTE, MNLI; Dagan et al. 2006; Bar-Haim et al. 2006; Giampiccolo et al. 2007; Bentivogli et al. 2009; Williams et al. 2018), question answering (QNLI; Rajpurkar et al. 2016), and paraphrase detection (QQP, MRPC;\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8587959,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "de794d50713ea5f91a7c9da3d72041e2f5ef8452",
            "isKey": true,
            "numCitedBy": 1762,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents the Third PASCAL Recognising Textual Entailment Challenge (RTE-3), providing an overview of the dataset creating methodology and the submitted systems. In creating this year's dataset, a number of longer texts were introduced to make the challenge more oriented to realistic scenarios. Additionally, a pool of resources was offered so that the participants could share common tools. A pilot task was also set up, aimed at differentiating unknown entailments from identified contradictions and providing justifications for overall system decisions. 26 participants submitted 44 runs, using different approaches and generally presenting new entailment models and achieving higher scores than in the previous challenges."
            },
            "slug": "The-PASCAL-Recognising-Textual-Entailment-Challenge-Dagan-Glickman",
            "title": {
                "fragments": [],
                "text": "The PASCAL Recognising Textual Entailment Challenge"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper presents the Third PASCAL Recognising Textual Entailment Challenge (RTE-3), providing an overview of the dataset creating methodology and the submitted systems."
            },
            "venue": {
                "fragments": [],
                "text": "MLCW"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47324198"
                        ],
                        "name": "W. L. Taylor",
                        "slug": "W.-L.-Taylor",
                        "structuredName": {
                            "firstName": "Wilson",
                            "lastName": "Taylor",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. L. Taylor"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 85
                            }
                        ],
                        "text": "2018) task to pre-train a Transformer network, which is also known as the cloze task (Taylor, 1953)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 142
                            }
                        ],
                        "text": "We use the masked language modeling (MLM; Devlin et al. 2018) task to pre-train a Transformer network, which is also known as the cloze task (Taylor, 1953)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206666846,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "766ce989b8b8b984f7a4691fd8c9af4bdb2b74cd",
            "isKey": false,
            "numCitedBy": 2036,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Here is the first comprehensive statement of a research method and its theory which were introduced briefly during a workshop at the 1953 AEJ convention. Included are findings from three pilot studies and two experiments in which \u201ccloze procedure\u201d results are compared with those of two readability formulas."
            },
            "slug": "\u201cCloze-Procedure\u201d:-A-New-Tool-for-Measuring-Taylor",
            "title": {
                "fragments": [],
                "text": "\u201cCloze Procedure\u201d: A New Tool for Measuring Readability"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "This is the first comprehensive statement of a research method and its theory and findings from three pilot studies and two experiments in which \u201ccloze procedure\u201d results are compared with those of two readability formulas."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1953
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2380885"
                        ],
                        "name": "Danilo Giampiccolo",
                        "slug": "Danilo-Giampiccolo",
                        "structuredName": {
                            "firstName": "Danilo",
                            "lastName": "Giampiccolo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Danilo Giampiccolo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712352"
                        ],
                        "name": "B. Magnini",
                        "slug": "B.-Magnini",
                        "structuredName": {
                            "firstName": "Bernardo",
                            "lastName": "Magnini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Magnini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7465342"
                        ],
                        "name": "Ido Dagan",
                        "slug": "Ido-Dagan",
                        "structuredName": {
                            "firstName": "Ido",
                            "lastName": "Dagan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ido Dagan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "83415753"
                        ],
                        "name": "W. Dolan",
                        "slug": "W.-Dolan",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Dolan",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Dolan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 88
                            }
                        ],
                        "text": "The other tasks are pairwise classification tasks, including natural language inference (RTE, MNLI; Dagan et al. 2006; Bar-Haim et al. 2006; Giampiccolo et al. 2007; Bentivogli et al. 2009; Williams et al. 2018), question answering (QNLI; Rajpurkar et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 141
                            }
                        ],
                        "text": "The other tasks are pairwise classification tasks, including natural language inference (RTE, MNLI; Dagan et al. 2006; Bar-Haim et al. 2006; Giampiccolo et al. 2007; Bentivogli et al. 2009; Williams et al. 2018), question answering (QNLI; Rajpurkar et al. 2016), and paraphrase detection (QQP, MRPC;\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 195352006,
            "fieldsOfStudy": [
                "Philosophy"
            ],
            "id": "b2815bc4c9e4260227cd7ca0c9d68d41c4c2f58b",
            "isKey": false,
            "numCitedBy": 474,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Third-PASCAL-Recognizing-Textual-Entailment-Giampiccolo-Magnini",
            "title": {
                "fragments": [],
                "text": "The Third PASCAL Recognizing Textual Entailment Challenge"
            },
            "venue": {
                "fragments": [],
                "text": "ACL-PASCAL@ACL"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 174
                            }
                        ],
                        "text": "As shown in Table 6, we compare the PMLM-based variants against previous models on question answering (SQuAD; Rajpurkar et al. 2016; 2018), natural language inference (MNLI; Williams et al. 2018), and sentiment classification (SST-2; Socher et al. 2013)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 162
                            }
                        ],
                        "text": "\u2026classification tasks, including natural language inference (RTE, MNLI; Dagan et al. 2006; Bar-Haim et al. 2006; Giampiccolo et al. 2007; Bentivogli et al. 2009; Williams et al. 2018), question answering (QNLI; Rajpurkar et al. 2016), and paraphrase detection (QQP, MRPC; Dolan & Brockett 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A broadcoverage challenge corpus for sentence understanding UNILMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training through inference"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
            },
            "year": 2018
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A simple method for common - sense reasoning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fine-Tuning Table 9 reports the hyperparameters used for fine-tuning UNILMv2BASE over SQuAD v1.10 (Rajpurkar et al., 2016"
            },
            "venue": {
                "fragments": [],
                "text": "(Rajpurkar et al., 2018), and the GLUE benchmark (Wang et al.,"
            },
            "year": 2019
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 342,
                                "start": 163
                            }
                        ],
                        "text": "Language model (LM) pre-training on large-scale text corpora has substantially advanced the state of the art across a variety of natural language processing tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018; Dong et al., 2019; Liu et al., 2019; Yang et al., 2019; Lewis et al., 2019; Lan et al., 2019; Raffel et al., 2019)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 63
                            }
                        ],
                        "text": "2019; Liu et al., 2019; Yang et al., 2019; Lewis et al., 2019; Lan et al., 2019; Raffel et al., 2019; Chi et al., 2020)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "ALBERT: A lite bert for selfsupervised learning of language"
            },
            "venue": {
                "fragments": [],
                "text": "representations. ArXiv,"
            },
            "year": 2019
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 137
                            }
                        ],
                        "text": "We use 160GB text corpora from English Wikipedia1, BookCorpus (Zhu et al., 2015), OpenWebText2, CC-News (Liu et al., 2019), and Stories (Trinh & Le, 2018)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A simple method for commonsense"
            },
            "venue": {
                "fragments": [],
                "text": "reasoning. ArXiv,"
            },
            "year": 2018
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A method for stochastic optimization"
            },
            "venue": {
                "fragments": [],
                "text": "3rd International Conference on Learning Representations"
            },
            "year": 2015
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 22,
            "methodology": 23,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 47,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/UniLMv2:-Pseudo-Masked-Language-Models-for-Unified-Bao-Dong/f64e1d6bc13aae99aab5449fc9ae742a9ba7761e?sort=total-citations"
}