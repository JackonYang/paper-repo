{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40622051"
                        ],
                        "name": "M. Aharon",
                        "slug": "M.-Aharon",
                        "structuredName": {
                            "firstName": "Michal",
                            "lastName": "Aharon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Aharon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753908"
                        ],
                        "name": "Michael Elad",
                        "slug": "Michael-Elad",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Elad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Elad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143610924"
                        ],
                        "name": "A. Bruckstein",
                        "slug": "A.-Bruckstein",
                        "structuredName": {
                            "firstName": "Alfred",
                            "lastName": "Bruckstein",
                            "middleNames": [
                                "Marcel"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bruckstein"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 61
                            }
                        ],
                        "text": "A more detailed description of those methods can be found in [10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7477309,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "83b522f4bfa5db7f7d34f839475af7d078107634",
            "isKey": false,
            "numCitedBy": 7336,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "In recent years there has been a growing interest in the study of sparse representation of signals. Using an overcomplete dictionary that contains prototype signal-atoms, signals are described by sparse linear combinations of these atoms. Applications that use sparse representation are many and include compression, regularization in inverse problems, feature extraction, and more. Recent activity in this field has concentrated mainly on the study of pursuit algorithms that decompose signals with respect to a given dictionary. Designing dictionaries to better fit the above model can be done by either selecting one from a prespecified set of linear transforms or adapting the dictionary to a set of training signals. Both of these techniques have been considered, but this topic is largely still open. In this paper we propose a novel algorithm for adapting dictionaries in order to achieve sparse signal representations. Given a set of training signals, we seek the dictionary that leads to the best representation for each member in this set, under strict sparsity constraints. We present a new method-the K-SVD algorithm-generalizing the K-means clustering process. K-SVD is an iterative method that alternates between sparse coding of the examples based on the current dictionary and a process of updating the dictionary atoms to better fit the data. The update of the dictionary columns is combined with an update of the sparse representations, thereby accelerating convergence. The K-SVD algorithm is flexible and can work with any pursuit method (e.g., basis pursuit, FOCUSS, or matching pursuit). We analyze this algorithm and demonstrate its results both on synthetic tests and in applications on real image data"
            },
            "slug": "$rm-K$-SVD:-An-Algorithm-for-Designing-Overcomplete-Aharon-Elad",
            "title": {
                "fragments": [],
                "text": "$rm K$-SVD: An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A novel algorithm for adapting dictionaries in order to achieve sparse signal representations, the K-SVD algorithm, an iterative method that alternates between sparse coding of the examples based on the current dictionary and a process of updating the dictionary atoms to better fit the data."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Signal Processing"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1395421758"
                        ],
                        "name": "K. Kreutz-Delgado",
                        "slug": "K.-Kreutz-Delgado",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Kreutz-Delgado",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kreutz-Delgado"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31052092"
                        ],
                        "name": "J. Murray",
                        "slug": "J.-Murray",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Murray",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Murray"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144876925"
                        ],
                        "name": "B. Rao",
                        "slug": "B.-Rao",
                        "structuredName": {
                            "firstName": "Bhaskar",
                            "lastName": "Rao",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Rao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2691592"
                        ],
                        "name": "K. Engan",
                        "slug": "K.-Engan",
                        "structuredName": {
                            "firstName": "Kjersti",
                            "lastName": "Engan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Engan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103917250"
                        ],
                        "name": "Te-Won Lee",
                        "slug": "Te-Won-Lee",
                        "structuredName": {
                            "firstName": "Te-Won",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Te-Won Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 21
                            }
                        ],
                        "text": "[20] B.D. Rao and K. Kreutz-Delgado."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 8
                            }
                        ],
                        "text": "[37] K. Kreutz-Delgado, J.F. Murray, B.D. Rao, K. Engan, T. Lee, and T.J. Sejnowski."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 24
                            }
                        ],
                        "text": "[42] J.F. Murray and K. Kreutz-Delgado."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 24
                            }
                        ],
                        "text": "Simulations reported in [37], [41], [42], [44] on synthetic and real image data seem to provide encouraging results."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [37], [41], [42], [44] a probabilistic point of view is adopted, very similar to the ML methods discussed above."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 56
                            }
                        ],
                        "text": "[21] B.D. Rao, K. Engan, S.F. Cotter, J. Palmer, and K. Kreutz-Delgado."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 92
                            }
                        ],
                        "text": "This technique and closely related ones have been referred to as approximated ML techniques [37]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 67
                            }
                        ],
                        "text": "We also executed the MAP-based algorithm of Rao and Kreutz-Delgado [37]2."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 8
                            }
                        ],
                        "text": "[44] K. Kreutz-Delgado and B.D. Rao."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 15
                            }
                        ],
                        "text": "The authors of [37] have generously shared their software with us."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "As in previously reported works [37], [45], we first try the K-SVD algorithm on synthetic signals, to test whether this algorithm recovers the original dictionary that generated the data, and to compare its results with other reported algorithms."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 168
                            }
                        ],
                        "text": "This idea of iterative refinement, mentioned before as a generalization of the K-Means algorithm, was later used again by other researchers, with some variations [36], [37], [40], [41], [42]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 32
                            }
                        ],
                        "text": "[41] K. Engan, B.D. Rao, and K. Kreutz-Delgado."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 71
                            }
                        ],
                        "text": "This connection has previously been mentioned in several reports [36], [37], [38]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 21
                            }
                        ],
                        "text": "[19] B.D. Rao and K. Kreutz-Delgado."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1218517,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "306de9c553695822ae9e6de044b6856baf0cce7d",
            "isKey": true,
            "numCitedBy": 835,
            "numCiting": 88,
            "paperAbstract": {
                "fragments": [],
                "text": "Algorithms for data-driven learning of domain-specific overcomplete dictionaries are developed to obtain maximum likelihood and maximum a posteriori dictionary estimates based on the use of Bayesian models with concave/Schur-concave (CSC) negative log priors. Such priors are appropriate for obtaining sparse representations of environmental signals within an appropriately chosen (environmentally matched) dictionary. The elements of the dictionary can be interpreted as concepts, features, or words capable of succinct expression of events encountered in the environment (the source of the measured signals). This is a generalization of vector quantization in that one is interested in a description involving a few dictionary entries (the proverbial 25 words or less), but not necessarily as succinct as one entry. To learn an environmentally adapted dictionary capable of concise expression of signals generated by the environment, we develop algorithms that iterate between a representative set of sparse representations found by variants of FOCUSS and an update of the dictionary using these sparse representations. Experiments were performed using synthetic data and natural images. For complete dictionaries, we demonstrate that our algorithms have improved performance over other independent component analysis (ICA) methods, measured in terms of signal-to-noise ratios of separated sources. In the overcomplete case, we show that the true underlying dictionary and sparse sources can be accurately recovered. In tests with natural images, learned overcomplete dictionaries are shown to have higher coding efficiency than complete dictionaries; that is, images encoded with an overcomplete dictionary have both higher compression (fewer bits per pixel) and higher accuracy (lower mean square error)."
            },
            "slug": "Dictionary-Learning-Algorithms-for-Sparse-Kreutz-Delgado-Murray",
            "title": {
                "fragments": [],
                "text": "Dictionary Learning Algorithms for Sparse Representation"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "Algorithms for data-driven learning of domain-specific overcomplete dictionaries are developed to obtain maximum likelihood and maximum a posteriori dictionary estimates based on the use of Bayesian models with concave/Schur-concave negative log priors, showing improved performance over other independent component analysis methods."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1395421758"
                        ],
                        "name": "K. Kreutz-Delgado",
                        "slug": "K.-Kreutz-Delgado",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Kreutz-Delgado",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kreutz-Delgado"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144876925"
                        ],
                        "name": "B. Rao",
                        "slug": "B.-Rao",
                        "structuredName": {
                            "firstName": "Bhaskar",
                            "lastName": "Rao",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Rao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "Simulations reported in [37], [41], [42], [44] on synthetic and real image data seem to provide encouraging results."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 21
                            }
                        ],
                        "text": "In [37], [41], [42], [44] a probabilistic point of view is adopted, very similar to the ML methods discussed above."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16705932,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5a09a81393b824bf7b2efe38e2049c3dc9941293",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "Algorithms for data-driven learning of domain-specific over complete dictionaries are developed to obtain maximum likelihood and maximum a posteriori dictionary estimates based on the use of Bayesian models with concave/Schur- concave negative log-priors. Such priors are appropriate for obtaining sparse representations of environmental signals within an appropriately chosen dictionary. The elements of the dictionary can be interpreted as 'concepts,' features or 'words' capable of succinct expression of events encountered in the environment. This is a generalization of vector quantization in that one is interested in a description involving a few dictionary entries, but not necessarily as succinct as one entry. To learn an environmentally-adapted dictionary capable of concise expression of signals generated by the environment, we develop algorithms that iterate between a representative set of sparse representations found by variants of FOCUSS, an affine scaling transformation (ACT)-like sparse signal representation algorithm recently developed at UCSD, and an update of the dictionary using these sparse representations."
            },
            "slug": "FOCUSS-based-dictionary-learning-algorithms-Kreutz-Delgado-Rao",
            "title": {
                "fragments": [],
                "text": "FOCUSS-based dictionary learning algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "To learn an environmentally-adapted dictionary capable of concise expression of signals generated by the environment, this work develops algorithms that iterate between a representative set of sparse representations found by variants of FOCUSS, an affine scaling transformation (ACT)-like sparse signal representation algorithm recently developed at UCSD, and an update of the dictionary using these sparse representations."
            },
            "venue": {
                "fragments": [],
                "text": "SPIE Optics + Photonics"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753908"
                        ],
                        "name": "Michael Elad",
                        "slug": "Michael-Elad",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Elad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Elad"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 187
                            }
                        ],
                        "text": "Extensive study of these algorithms in recent years has established that if the sought solution, x, is sparse enough, these techniques recover it well in the exact case [16], [26], [27], [28], [29], [30]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5724741,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "17e7cca7e795d8ba1fa9d2c88bf2675c2d6ddfe8",
            "isKey": false,
            "numCitedBy": 2842,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Given a dictionary D = {dk} of vectors dk, we seek to represent a signal S as a linear combination S = \u2211k \u03b3(k)dk, with scalar coefficients \u03b3(k). In particular, we aim for the sparsest representation possible. In general, this requires a combinatorial optimization process. Previous work considered the special case where D is an overcomplete system consisting of exactly two orthobases and has shown that, under a condition of mutual incoherence of the two bases, and assuming that S has a sufficiently sparse representation, this representation is unique and can be found by solving a convex optimization problem: specifically, minimizing the \u21131 norm of the coefficients \u03b3\u0331. In this article, we obtain parallel results in a more general setting, where the dictionary D can arise from two or several bases, frames, or even less structured systems. We sketch three applications: separating linear features from planar ones in 3D data, noncooperative multiuser encoding, and identification of over-complete independent component models."
            },
            "slug": "Optimally-sparse-representation-in-general-via-\u21131-Donoho-Elad",
            "title": {
                "fragments": [],
                "text": "Optimally sparse representation in general (nonorthogonal) dictionaries via \u21131 minimization"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This article obtains parallel results in a more general setting, where the dictionary D can arise from two or several bases, frames, or even less structured systems, and sketches three applications: separating linear features from planar ones in 3D data, noncooperative multiuser encoding, and identification of over-complete independent component models."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753908"
                        ],
                        "name": "Michael Elad",
                        "slug": "Michael-Elad",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Elad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Elad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755101"
                        ],
                        "name": "V. Temlyakov",
                        "slug": "V.-Temlyakov",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Temlyakov",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Temlyakov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 91
                            }
                        ],
                        "text": "Further work considered the approximated versions and has shown stability in recovery of x [31], [32]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14813938,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7236864d8c2f62defea559465462c43a4b4b6b47",
            "isKey": false,
            "numCitedBy": 2166,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "Overcomplete representations are attracting interest in signal processing theory, particularly due to their potential to generate sparse representations of signals. However, in general, the problem of finding sparse representations must be unstable in the presence of noise. This paper establishes the possibility of stable recovery under a combination of sufficient sparsity and favorable structure of the overcomplete system. Considering an ideal underlying signal that has a sufficiently sparse representation, it is assumed that only a noisy version of it can be observed. Assuming further that the overcomplete system is incoherent, it is shown that the optimally sparse approximation to the noisy data differs from the optimally sparse decomposition of the ideal noiseless signal by at most a constant multiple of the noise level. As this optimal-sparsity method requires heavy (combinatorial) computational effort, approximation algorithms are considered. It is shown that similar stability is also available using the basis and the matching pursuit algorithms. Furthermore, it is shown that these methods result in sparse approximation of the noisy data that contains only terms also appearing in the unique sparsest representation of the ideal noiseless sparse signal."
            },
            "slug": "Stable-recovery-of-sparse-overcomplete-in-the-of-Donoho-Elad",
            "title": {
                "fragments": [],
                "text": "Stable recovery of sparse overcomplete representations in the presence of noise"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper establishes the possibility of stable recovery under a combination of sufficient sparsity and favorable structure of the overcomplete system and shows that similar stability is also available using the basis and the matching pursuit algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Information Theory"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054912981"
                        ],
                        "name": "S. Lesage",
                        "slug": "S.-Lesage",
                        "structuredName": {
                            "firstName": "Sylvain",
                            "lastName": "Lesage",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lesage"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731535"
                        ],
                        "name": "R. Gribonval",
                        "slug": "R.-Gribonval",
                        "structuredName": {
                            "firstName": "R\u00e9mi",
                            "lastName": "Gribonval",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Gribonval"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9457270"
                        ],
                        "name": "F. Bimbot",
                        "slug": "F.-Bimbot",
                        "structuredName": {
                            "firstName": "Fr\u00e9d\u00e9ric",
                            "lastName": "Bimbot",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Bimbot"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2434483"
                        ],
                        "name": "L. Benaroya",
                        "slug": "L.-Benaroya",
                        "structuredName": {
                            "firstName": "Laurent",
                            "lastName": "Benaroya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Benaroya"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 33
                            }
                        ],
                        "text": "The very recent work reported in [45] considers a dictionary composed as a union of orthonormal bases"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 38
                            }
                        ],
                        "text": "As in previously reported works [37], [45], we first try the K-SVD algorithm on synthetic signals, to test whether this algorithm recovers the original dictionary that generated the data, and to compare its results with other reported algorithms."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 48
                            }
                        ],
                        "text": "Interestingly, experimental results reported in [45] show weak performance compared to previous methods."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 63
                            }
                        ],
                        "text": "Note the resemblance between this error and the one defined in [45]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 75
                            }
                        ],
                        "text": "Compared to previously-mentioned training algorithms, the work reported in [45] is different in two important ways: Beyond the evident difference of using a structured dictionary rather than a free one, a second major difference is in the proposed sequential update of the dictionary."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 14749584,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d9ef403969035e022b1b61c7dc513ffe189f031",
            "isKey": true,
            "numCitedBy": 143,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a new method to learn overcomplete dictionaries for sparse coding structured as unions of orthonormal bases. The interest of such a structure is manifold. Indeed, it seems that many signals or images can be modeled as the superimposition of several layers with sparse decompositions in as many bases. Moreover, in such dictionaries, the efficient block coordinate relaxation (BCR) algorithm can be used to compute sparse decompositions. We show that it is possible to design an iterative learning algorithm that produces a dictionary with the required structure. Each step is based on the coefficients estimation, using a variant of BCR, followed by the update of one chosen basis, using singular value decomposition. We assess experimentally how well the learning algorithm recovers dictionaries that may or may not have the required structure, and to what extent the noise level is a disturbing factor."
            },
            "slug": "Learning-unions-of-orthonormal-bases-with-singular-Lesage-Gribonval",
            "title": {
                "fragments": [],
                "text": "Learning unions of orthonormal bases with thresholded singular value decomposition"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that it is possible to design an iterative learning algorithm that produces a dictionary with the required structure, and how well the learning algorithm recovers dictionaries that may or may not have the necessary structure is assessed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3069792"
                        ],
                        "name": "M. Lewicki",
                        "slug": "M.-Lewicki",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lewicki",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lewicki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 93
                            }
                        ],
                        "text": "A different approach to handle the integration in (7) was suggested by Lewicki and Sejnowski [25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 74
                            }
                        ],
                        "text": "[37] K. Kreutz-Delgado, J.F. Murray, B.D. Rao, K. Engan, T. Lee, and T.J. Sejnowski."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 202
                            }
                        ],
                        "text": "The above method is then similar to ICA in that the algorithm can be interpreted as trying to maximize the mutual information between the inputs (samples) and the outputs (the coefficients) [24], [22], [25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 27
                            }
                        ],
                        "text": "[25] M.S. Lewicki and T.J. Sejnowski."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 24
                            }
                        ],
                        "text": "[43] A.J. Bell and T.J. Sejnowski."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 164
                            }
                        ],
                        "text": "Both the BP and the FOCUSS can be motivated based on Maximum A Posteriori (MAP) estimation, and indeed several works used this reasoning directly [22], [23], [24], [25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "The methods reported in [22], [23], [24], [25] use probabilistic reasoning in the construction of D."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6254191,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "42d906c733f273109c0ed716a5ef6e2a379beb26",
            "isKey": true,
            "numCitedBy": 1255,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "In an overcomplete basis, the number of basis vectors is greater than the dimensionality of the input, and the representation of an input is not a unique combination of basis vectors. Overcomplete representations have been advocated because they have greater robustness in the presence of noise, can be sparser, and can have greater flexibility in matching structure in the data. Overcomplete codes have also been proposed as a model of some of the response properties of neurons in primary visual cortex. Previous work has focused on finding the best representation of a signal using a fixed overcomplete basis (or dictionary). We present an algorithm for learning an overcomplete basis by viewing it as probabilistic model of the observed data. We show that overcomplete bases can yield a better approximation of the underlying statistical distribution of the data and can thus lead to greater coding efficiency. This can be viewed as a generalization of the technique of independent component analysis and provides a method for Bayesian reconstruction of signals in the presence of noise and for blind source separation when there are more sources than mixtures."
            },
            "slug": "Learning-Overcomplete-Representations-Lewicki-Sejnowski",
            "title": {
                "fragments": [],
                "text": "Learning Overcomplete Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "It is shown that overcomplete bases can yield a better approximation of the underlying statistical distribution of the data and can thus lead to greater coding efficiency and provide a method for Bayesian reconstruction of signals in the presence of noise and for blind source separation when there are more sources than mixtures."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788107"
                        ],
                        "name": "J. Tropp",
                        "slug": "J.-Tropp",
                        "structuredName": {
                            "firstName": "Joel",
                            "lastName": "Tropp",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tropp"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 169
                            }
                        ],
                        "text": "Extensive study of these algorithms in recent years has established that if the sought solution, x, is sparse enough, these techniques recover it well in the exact case [16], [26], [27], [28], [29], [30]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 124
                            }
                        ],
                        "text": "The simplest ones are the Matching Pursuit (MP) [12] and the Orthogonal Matching Pursuit (OMP) algorithms [13], [14], [15], [16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 675692,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "46ee0292a71fa86a9e2f9d691da5f0f1cf281f83",
            "isKey": false,
            "numCitedBy": 3451,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "This article presents new results on using a greedy algorithm, orthogonal matching pursuit (OMP), to solve the sparse approximation problem over redundant dictionaries. It provides a sufficient condition under which both OMP and Donoho's basis pursuit (BP) paradigm can recover the optimal representation of an exactly sparse signal. It leverages this theory to show that both OMP and BP succeed for every sparse input signal from a wide class of dictionaries. These quasi-incoherent dictionaries offer a natural generalization of incoherent dictionaries, and the cumulative coherence function is introduced to quantify the level of incoherence. This analysis unifies all the recent results on BP and extends them to OMP. Furthermore, the paper develops a sufficient condition under which OMP can identify atoms from an optimal approximation of a nonsparse signal. From there, it argues that OMP is an approximation algorithm for the sparse problem over a quasi-incoherent dictionary. That is, for every input signal, OMP calculates a sparse approximant whose error is only a small factor worse than the minimal error that can be attained with the same number of terms."
            },
            "slug": "Greed-is-good:-algorithmic-results-for-sparse-Tropp",
            "title": {
                "fragments": [],
                "text": "Greed is good: algorithmic results for sparse approximation"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "This article presents new results on using a greedy algorithm, orthogonal matching pursuit (OMP), to solve the sparse approximation problem over redundant dictionaries and develops a sufficient condition under which OMP can identify atoms from an optimal approximation of a nonsparse signal."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Information Theory"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31052092"
                        ],
                        "name": "J. Murray",
                        "slug": "J.-Murray",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Murray",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Murray"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1395421758"
                        ],
                        "name": "K. Kreutz-Delgado",
                        "slug": "K.-Kreutz-Delgado",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Kreutz-Delgado",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kreutz-Delgado"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 36
                            }
                        ],
                        "text": "Simulations reported in [37], [41], [42], [44] on synthetic and real image data seem to provide encouraging results."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 186
                            }
                        ],
                        "text": "This idea of iterative refinement, mentioned before as a generalization of the K-Means algorithm, was later used again by other researchers, with some variations [36], [37], [40], [41], [42]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 15
                            }
                        ],
                        "text": "In [37], [41], [42], [44] a probabilistic point of view is adopted, very similar to the ML methods discussed above."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10472643,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4ed0700119ed281d210897117863fa290d383cd0",
            "isKey": false,
            "numCitedBy": 79,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop an improved algorithm for solving blind sparse linear inverse problems where both the dictionary (possibly overcomplete) and the sources are unknown. The algorithm is derived in the Bayesian framework by the maximum a posteriori method, with the choice of prior distribution restricted to the class of concave/Schur-concave functions, which has been shown previously to be a sufficient condition for sparse solutions. This formulation leads to a constrained and regularized minimization problem which can be solved in part using the FOCUSS (focal underdetermined system solver) algorithm for vector selection. We introduce three key improvements in the algorithm: an efficient way of adjusting the regularization parameter; column normalization that restricts the learned dictionary; reinitialization to escape from local optima. Experiments were performed using synthetic data with matrix sizes up to 64/spl times/128; the algorithm solves the blind identification problem, recovering both the dictionary and the sparse sources. The improved algorithm is much more accurate than the original FOCUSS-dictionary learning algorithm when using large matrices. We also test our algorithm on natural images, and show that a learned overcomplete representation can encode the data more efficiently than a complete basis at the same level of accuracy."
            },
            "slug": "An-improved-FOCUSS-based-learning-algorithm-for-Murray-Kreutz-Delgado",
            "title": {
                "fragments": [],
                "text": "An improved FOCUSS-based learning algorithm for solving sparse linear inverse problems"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "An improved algorithm for solving blind sparse linear inverse problems where both the dictionary and the sources are unknown is developed, and it is shown that a learned overcomplete representation can encode the data more efficiently than a complete basis at the same level of accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "Conference Record of Thirty-Fifth Asilomar Conference on Signals, Systems and Computers (Cat.No.01CH37256)"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145720668"
                        ],
                        "name": "J. Fuchs",
                        "slug": "J.-Fuchs",
                        "structuredName": {
                            "firstName": "Jean-Jacques",
                            "lastName": "Fuchs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Fuchs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 199
                            }
                        ],
                        "text": "Extensive study of these algorithms in recent years has established that if the sought solution, x, is sparse enough, these techniques recover it well in the exact case [16], [26], [27], [28], [29], [30]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18432970,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "6cf9b027aa09d042dd13fd9ae848d902240b3d34",
            "isKey": false,
            "numCitedBy": 571,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The purpose of this contribution is to generalize some recent results on sparse representations of signals in redundant bases. The question that is considered is the following: given a matrix A of dimension (n,m) with m>n and a vector b=Ax, find a sufficient condition for b to have a unique sparsest representation x as a linear combination of columns of A. Answers to this question are known when A is the concatenation of two unitary matrices and either an extensive combinatorial search is performed or a linear program is solved. We consider arbitrary A matrices and give a sufficient condition for the unique sparsest solution to be the unique solution to both a linear program or a parametrized quadratic program. The proof is elementary and the possibility of using a quadratic program opens perspectives to the case where b=Ax+e with e a vector of noise or modeling errors."
            },
            "slug": "On-sparse-representations-in-arbitrary-redundant-Fuchs",
            "title": {
                "fragments": [],
                "text": "On sparse representations in arbitrary redundant bases"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "The purpose of this contribution is to generalize some recent results on sparse representations of signals in redundant bases and give a sufficient condition for the unique sparsest solution to be the unique solution to both a linear program or a parametrized quadratic program."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Information Theory"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727797"
                        ],
                        "name": "S. Chen",
                        "slug": "S.-Chen",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Chen",
                            "middleNames": [
                                "Saobing"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145621255"
                        ],
                        "name": "M. Saunders",
                        "slug": "M.-Saunders",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Saunders",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Saunders"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 63
                            }
                        ],
                        "text": "A second well known pursuit approach is the Basis Pursuit (BP) [17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2429822,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9af121fbed84c3484ab86df8f17f1f198ed790a0",
            "isKey": false,
            "numCitedBy": 9740,
            "numCiting": 81,
            "paperAbstract": {
                "fragments": [],
                "text": "The time-frequency and time-scale communities have recently developed a large number of overcomplete waveform dictionaries --- stationary wavelets, wavelet packets, cosine packets, chirplets, and warplets, to name a few. Decomposition into overcomplete systems is not unique, and several methods for decomposition have been proposed, including the method of frames (MOF), Matching pursuit (MP), and, for special dictionaries, the best orthogonal basis (BOB). \nBasis Pursuit (BP) is a principle for decomposing a signal into an \"optimal\" superposition of dictionary elements, where optimal means having the smallest l1 norm of coefficients among all such decompositions. We give examples exhibiting several advantages over MOF, MP, and BOB, including better sparsity and superresolution. BP has interesting relations to ideas in areas as diverse as ill-posed problems, in abstract harmonic analysis, total variation denoising, and multiscale edge denoising. \nBP in highly overcomplete dictionaries leads to large-scale optimization problems. With signals of length 8192 and a wavelet packet dictionary, one gets an equivalent linear program of size 8192 by 212,992. Such problems can be attacked successfully only because of recent advances in linear programming by interior-point methods. We obtain reasonable success with a primal-dual logarithmic barrier method and conjugate-gradient solver."
            },
            "slug": "Atomic-Decomposition-by-Basis-Pursuit-Chen-Donoho",
            "title": {
                "fragments": [],
                "text": "Atomic Decomposition by Basis Pursuit"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Basis Pursuit (BP) is a principle for decomposing a signal into an \"optimal\" superposition of dictionary elements, where optimal means having the smallest l1 norm of coefficients among all such decompositions."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Sci. Comput."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2769197"
                        ],
                        "name": "S. Sardy",
                        "slug": "S.-Sardy",
                        "structuredName": {
                            "firstName": "Sylvain",
                            "lastName": "Sardy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sardy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153092438"
                        ],
                        "name": "A. Bruce",
                        "slug": "A.-Bruce",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Bruce",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bruce"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682823"
                        ],
                        "name": "P. Tseng",
                        "slug": "P.-Tseng",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Tseng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Tseng"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 81
                            }
                        ],
                        "text": "The coefficients are found using the Block Coordinate Relaxation (BCR) algorithm [46]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 117809956,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c2c8d9018fa96735a1fb6330a8aee7caa9fe6900",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "An important class of nonparametric signal processing methods entail forming a set of predictors from an overcomplete set of basis functions associated with a fast transform (e.g., wavelet packets). In these methods, the number of basis functions can far exceed the number of sample values in the signal, leading to an ill-posed prediction problem. The \\Basis Pursuit\" denoising method of Chen, Donoho, and Saunders regularizes the prediction problem by adding an L 1 penalty term on the coeecients for the basis functions. Use of an L 1 penalty instead of L 2 has signiicant beneets, including higher resolution of signals close in time/frequency and a more parsimonious representation. The L 1 penalty, however, poses a challenging optimization problem that was solved by Chen, Donoho and Saunders using a novel application of interior point algorithms (IP). In this paper, we investigate an alternative optimization approach based on \\block coordinate relaxation\" (BCR). We show that the BCR algorithm is globally convergent, and empirically, the BCR algorithm is faster than the IP algorithm for a variety of signal denoising problems."
            },
            "slug": "Block-coordinate-relaxation-methods-for-signal-with-Sardy-Bruce",
            "title": {
                "fragments": [],
                "text": "Block coordinate relaxation methods for nonparametric signal denoising with wavelet dictionaries"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is shown that the B CR algorithm is globally convergent, and empirically, the BCR algorithm is faster than the IP algorithm for a variety of signal denoising problems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145394553"
                        ],
                        "name": "Jean-Luc Starck",
                        "slug": "Jean-Luc-Starck",
                        "structuredName": {
                            "firstName": "Jean-Luc",
                            "lastName": "Starck",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean-Luc Starck"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753908"
                        ],
                        "name": "Michael Elad",
                        "slug": "Michael-Elad",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Elad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Elad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 160
                            }
                        ],
                        "text": "Sparsity and overcompleteness have been successfully used for dynamic range compression in images [6], separation of texture and cartoon content in images [7], [8], inpainting [9], and more."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 878806,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7b4282bb330f4a20db52e12bd6ce754f73cf4216",
            "isKey": false,
            "numCitedBy": 977,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "The separation of image content into semantic parts plays a vital role in applications such as compression, enhancement, restoration, and more. In recent years, several pioneering works suggested such a separation be based on variational formulation and others using independent component analysis and sparsity. This paper presents a novel method for separating images into texture and piecewise smooth (cartoon) parts, exploiting both the variational and the sparsity mechanisms. The method combines the basis pursuit denoising (BPDN) algorithm and the total-variation (TV) regularization scheme. The basic idea presented in this paper is the use of two appropriate dictionaries, one for the representation of textures and the other for the natural scene parts assumed to be piecewise smooth. Both dictionaries are chosen such that they lead to sparse representations over one type of image-content (either texture or piecewise smooth). The use of the BPDN with the two amalgamed dictionaries leads to the desired separation, along with noise removal as a by-product. As the need to choose proper dictionaries is generally hard, a TV regularization is employed to better direct the separation process and reduce ringing artifacts. We present a highly efficient numerical scheme to solve the combined optimization problem posed by our model and to show several experimental results that validate the algorithm's performance."
            },
            "slug": "Image-decomposition-via-the-combination-of-sparse-a-Starck-Elad",
            "title": {
                "fragments": [],
                "text": "Image decomposition via the combination of sparse representations and a variational approach"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A novel method for separating images into texture and piecewise smooth (cartoon) parts, exploiting both the variational and the sparsity mechanisms is presented, combining the basis pursuit denoising (BPDN) algorithm and the total-variation (TV) regularization scheme."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Image Processing"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708655"
                        ],
                        "name": "B. Olshausen",
                        "slug": "B.-Olshausen",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Olshausen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Olshausen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49649079"
                        ],
                        "name": "D. Field",
                        "slug": "D.-Field",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Field",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Field"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 164
                            }
                        ],
                        "text": "This difficulty has been handled by constraining the \"2-norm of each basis element, so that the output variance of the coefficients is kept at an appropriate level [24]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 190
                            }
                        ],
                        "text": "The above method is then similar to ICA in that the algorithm can be interpreted as trying to maximize the mutual information between the inputs (samples) and the outputs (the coefficients) [24], [22], [25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 129
                            }
                        ],
                        "text": "The prior distribution of the representation vector x is assumed to be such that the entries of x are zero-mean iid, with Cauchy [24] or Laplace distributions [22], [23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 126
                            }
                        ],
                        "text": "(6)\nThe prior distribution of the representation vector x is assumed to be such that the entries of x are zero-mean iid, with Cauchy [24] or Laplace distributions [22], [23]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 165
                            }
                        ],
                        "text": "It includes two main steps in each iteration: (i) calculate the coefficients xi using a simple gradient descent procedure; and then (ii) update the dictionary using [24]"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 158
                            }
                        ],
                        "text": "Both the BP and the FOCUSS can be motivated based on Maximum A Posteriori (MAP) estimation, and indeed several works used this reasoning directly [22], [23], [24], [25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 26
                            }
                        ],
                        "text": "Whereas the work in [23], [24], [22] applies a steepest descent to evaluate xi, those are evaluated much more efficiently with either OMP or FOCUSS."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 36
                            }
                        ],
                        "text": "The methods reported in [22], [23], [24], [25] use probabilistic reasoning in the construction of D."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14208692,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "2805537bec87a6177037b18f9a3a9d3f1038867b",
            "isKey": true,
            "numCitedBy": 3574,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Sparse-coding-with-an-overcomplete-basis-set:-A-by-Olshausen-Field",
            "title": {
                "fragments": [],
                "text": "Sparse coding with an overcomplete basis set: A strategy employed by V1?"
            },
            "venue": {
                "fragments": [],
                "text": "Vision Research"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788107"
                        ],
                        "name": "J. Tropp",
                        "slug": "J.-Tropp",
                        "structuredName": {
                            "firstName": "Joel",
                            "lastName": "Tropp",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tropp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788107"
                        ],
                        "name": "J. Tropp",
                        "slug": "J.-Tropp",
                        "structuredName": {
                            "firstName": "Joel",
                            "lastName": "Tropp",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tropp"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 97
                            }
                        ],
                        "text": "Further work considered the approximated versions and has shown stability in recovery of x [31], [32]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1454355,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff71f4a25b0a4c97b20ff49ced0e94cdcf6784e9",
            "isKey": false,
            "numCitedBy": 195,
            "numCiting": 95,
            "paperAbstract": {
                "fragments": [],
                "text": "Subset selection and sparse approximation problems request a good approximation of an input signal using a linear combination of elementary signals, yet they stipulate that the approximation may only involve a few of the elementary signals. This class of problems arises throughout electrical engineering, applied mathematics and statistics, but small theoretical progress has been made over the last fifty years. Subset selection and sparse approximation both admit natural convex relaxations, but the literature contains few results on the behavior of these relaxations for general input signals. This report demonstrates that the solution of the convex program frequently coincides with the solution of the original approximation problem. The proofs depend essentially on geometric properties of the ensemble of elementary signals. The results are powerful because sparse approximation problems are combinatorial, while convex programs can be solved in polynomial time with standard software. Comparable new results for a greedy algorithm, Orthogonal Matching Pursuit, are also stated. This report should have a major practical impact because the theory applies immediately to many real-world signal processing problems."
            },
            "slug": "JUST-RELAX:-CONVEX-PROGRAMMING-METHODS-FOR-SUBSET-Tropp-Tropp",
            "title": {
                "fragments": [],
                "text": "JUST RELAX: CONVEX PROGRAMMING METHODS FOR SUBSET SELECTION AND SPARSE APPROXIMATION"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is demonstrated that the solution of the convex program frequently coincides with the solutionof the original approximation problem, and comparable new results for a greedy algorithm, Orthogonal Matching Pursuit, are stated."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2085407484"
                        ],
                        "name": "H\u00f8gskolen i Stavanger",
                        "slug": "H\u00f8gskolen-i-Stavanger",
                        "structuredName": {
                            "firstName": "H\u00f8gskolen",
                            "lastName": "Stavanger",
                            "middleNames": [
                                "i"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H\u00f8gskolen i Stavanger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 30
                            }
                        ],
                        "text": "Simulations reported in [37], [41], [42], [44] on synthetic and real image data seem to provide encouraging results."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 180
                            }
                        ],
                        "text": "This idea of iterative refinement, mentioned before as a generalization of the K-Means algorithm, was later used again by other researchers, with some variations [36], [37], [40], [41], [42]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 9
                            }
                        ],
                        "text": "In [37], [41], [42], [44] a probabilistic point of view is adopted, very similar to the ML methods discussed above."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1743446,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3fa9a10e3dbd7ad5c287175c70c0e9337dc63b47",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "The equation b = Ax + n where the columns of A form an overcomplete set, i.e. the system is underdetermined, and with a sparsity constraint on x can be important to solve in many applications. It can be used as a convenient signal representation model useful for compression, and it can also be a model for the true underlying system that produced the available dataset b. It is hard enough to solve the equation for a sparse solution when A is known. An even harder problem is to try to nd both the A and the x that produced the data set b, which is the only available data. This paper shows that a frame design algorithm, Method of Optimal Directions (MOD), proposed by Engan et al. [1], used with a noise robust version of FOCUSS we proposed in [2] works well for reconstructing the true A from the dataset b. The MOD algorithm has already produced good results on designing frames for compression of ElectroCardioGram (ECG) signals [3, 4], and the results in this paper provides complimentary evidence of its good properties."
            },
            "slug": "FRAME-DESIGN-USING-FOCUSS-WITH-METHOD-OF-OPTIMAL-Stavanger",
            "title": {
                "fragments": [],
                "text": "FRAME DESIGN USING FOCUSS WITH METHOD OF OPTIMAL DIRECTIONS (MOD)"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper shows that a frame design algorithm, Method of Optimal Directions (MOD), proposed by Engan et al, used with a noise robust version of FOCUSS works well for reconstructing the true A from the dataset b."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49810022"
                        ],
                        "name": "G. Davis",
                        "slug": "G.-Davis",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Davis",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Davis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52070751"
                        ],
                        "name": "S. Mallat",
                        "slug": "S.-Mallat",
                        "structuredName": {
                            "firstName": "Stphane",
                            "lastName": "Mallat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mallat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2359063"
                        ],
                        "name": "M. Avellaneda",
                        "slug": "M.-Avellaneda",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Avellaneda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Avellaneda"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 80
                            }
                        ],
                        "text": "Exact determination of sparsest representations proves to be an NP-hard problem [11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 121066701,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "39a853ced2928eea53702618ecb923865389757c",
            "isKey": false,
            "numCitedBy": 1140,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of optimally approximating a function with a linear expansion over a redundant dictionary of waveforms is NP-hard. The greedy matching pursuit algorithm and its orthogonalized variant produce suboptimal function expansions by iteratively choosing dictionary waveforms that best match the function\u2019s structures. A matching pursuit provides a means of quickly computing compact, adaptive function approximations.Numerical experiments show that the approximation errors from matching pursuits initially decrease rapidly, but the asymptotic decay rate of the errors is slow. We explain this behavior by showing that matching pursuits are chaotic, ergodic maps. The statistical properties of the approximation errors of a pursuit can be obtained from the invariant measure of the pursuit. We characterize these measures using group symmetries of dictionaries and by constructing a stochastic differential equation model.We derive a notion of the coherence of a signal with respect to a dictionary from our characterization of the approximation errors of a pursuit. The dictionary elements slected during the initial iterations of a pursuit correspond to a function\u2019s coherent structures. The tail of the expansion, on the other hand, corresponds to a noise which is characterized by the invariant measure of the pursuit map.When using a suitable dictionary, the expansion of a function into its coherent structures yields a compact approximation. We demonstrate a denoising algorithm based on coherent function expansions."
            },
            "slug": "Adaptive-greedy-approximations-Davis-Mallat",
            "title": {
                "fragments": [],
                "text": "Adaptive greedy approximations"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A notion of the coherence of a signal with respect to a dictionary is derived from the characterization of the approximation errors of a pursuit from their statistical properties, which can be obtained from the invariant measure of the pursuit."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746242"
                        ],
                        "name": "S. Mallat",
                        "slug": "S.-Mallat",
                        "structuredName": {
                            "firstName": "St\u00e9phane",
                            "lastName": "Mallat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mallat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109019649"
                        ],
                        "name": "Zhifeng Zhang",
                        "slug": "Zhifeng-Zhang",
                        "structuredName": {
                            "firstName": "Zhifeng",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhifeng Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 48
                            }
                        ],
                        "text": "The simplest ones are the Matching Pursuit (MP) [12] and the Orthogonal Matching Pursuit (OMP) algorithms [13], [14], [15], [16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14427335,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2210a7157565422261b03cf2cdf4e91b583df5a0",
            "isKey": false,
            "numCitedBy": 8852,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors introduce an algorithm, called matching pursuit, that decomposes any signal into a linear expansion of waveforms that are selected from a redundant dictionary of functions. These waveforms are chosen in order to best match the signal structures. Matching pursuits are general procedures to compute adaptive signal representations. With a dictionary of Gabor functions a matching pursuit defines an adaptive time-frequency transform. They derive a signal energy distribution in the time-frequency plane, which does not include interference terms, unlike Wigner and Cohen class distributions. A matching pursuit isolates the signal structures that are coherent with respect to a given dictionary. An application to pattern extraction from noisy signals is described. They compare a matching pursuit decomposition with a signal expansion over an optimized wavepacket orthonormal basis, selected with the algorithm of Coifman and Wickerhauser see (IEEE Trans. Informat. Theory, vol. 38, Mar. 1992). >"
            },
            "slug": "Matching-pursuits-with-time-frequency-dictionaries-Mallat-Zhang",
            "title": {
                "fragments": [],
                "text": "Matching pursuits with time-frequency dictionaries"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "The authors introduce an algorithm, called matching pursuit, that decomposes any signal into a linear expansion of waveforms that are selected from a redundant dictionary of functions, chosen in order to best match the signal structures."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Signal Process."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49810022"
                        ],
                        "name": "G. Davis",
                        "slug": "G.-Davis",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Davis",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Davis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746242"
                        ],
                        "name": "S. Mallat",
                        "slug": "S.-Mallat",
                        "structuredName": {
                            "firstName": "St\u00e9phane",
                            "lastName": "Mallat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mallat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109019649"
                        ],
                        "name": "Zhifeng Zhang",
                        "slug": "Zhifeng-Zhang",
                        "structuredName": {
                            "firstName": "Zhifeng",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhifeng Zhang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 112
                            }
                        ],
                        "text": "The simplest ones are the Matching Pursuit (MP) [12] and the Orthogonal Matching Pursuit (OMP) algorithms [13], [14], [15], [16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 120546315,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2acd8d8f8d2df5bb3f5350e3b009436544a5f907",
            "isKey": false,
            "numCitedBy": 354,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Computing the optimal expansion of a signal in a redundant dictionary of waveforms is an NP-hard problem. We introduce a greedy algorithm, called a matching pursuit, which computes a suboptimal expansion. The dictionary waveforms that best match a signal's structures are chosen iteratively. An orthogonalized version of the matching pursuit is also developed. Matching pursuits are general procedures for computing adaptive signal representations. With a dictionary of Gabor functions, a matching pursuit defines an adaptive time-frequency transform. Matching pursuits are chaotic maps whose attractors define a generic noise with respect to the dictionary. We derive an algorithm that isolates the coherent structures of a signal and describe an application to pattern extraction from noisy signals."
            },
            "slug": "Adaptive-time-frequency-decompositions-Davis-Mallat",
            "title": {
                "fragments": [],
                "text": "Adaptive time-frequency decompositions"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "An algorithm is derived that isolates the coherent structures of a signal and describes an application to pattern extraction from noisy signals, using a greedy algorithm called a matching pursuit, which computes a suboptimal expansion."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144876925"
                        ],
                        "name": "B. Rao",
                        "slug": "B.-Rao",
                        "structuredName": {
                            "firstName": "Bhaskar",
                            "lastName": "Rao",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Rao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2691592"
                        ],
                        "name": "K. Engan",
                        "slug": "K.-Engan",
                        "structuredName": {
                            "firstName": "Kjersti",
                            "lastName": "Engan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Engan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752706"
                        ],
                        "name": "S. Cotter",
                        "slug": "S.-Cotter",
                        "structuredName": {
                            "firstName": "Shane",
                            "lastName": "Cotter",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Cotter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144464115"
                        ],
                        "name": "J. Palmer",
                        "slug": "J.-Palmer",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Palmer",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Palmer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1395421758"
                        ],
                        "name": "K. Kreutz-Delgado",
                        "slug": "K.-Kreutz-Delgado",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Kreutz-Delgado",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kreutz-Delgado"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 147
                            }
                        ],
                        "text": "The Focal Under-determined System Solver (FOCUSS) is very similar, using the \"p-norm with p \u2264 1, as a replacement to the \"0-norm [18], [19], [20], [21]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11880855,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "d40c7f467b5e60a6363f70add2540f8a7cec6fa3",
            "isKey": false,
            "numCitedBy": 311,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop robust methods for subset selection based on the minimization of diversity measures. A Bayesian framework is used to account for noise in the data and a maximum a posteriori (MAP) estimation procedure leads to an iterative procedure which is a regularized version of the focal underdetermined system solver (FOCUSS) algorithm. The convergence of the regularized FOCUSS algorithm is established and it is shown that the stable fixed points of the algorithm are sparse. We investigate three different criteria for choosing the regularization parameter: quality of fit; sparsity criterion; L-curve. The L-curve method, as applied to the problem of subset selection, is found not to be robust, and we propose a novel modified L-curve procedure that solves this problem. Each of the regularized FOCUSS algorithms is evaluated through simulation of a detection problem, and the results are compared with those obtained using a sequential forward selection algorithm termed orthogonal matching pursuit (OMP). In each case, the regularized FOCUSS algorithm is shown to be superior to the OMP in noisy environments."
            },
            "slug": "Subset-selection-in-noise-based-on-diversity-Rao-Engan",
            "title": {
                "fragments": [],
                "text": "Subset selection in noise based on diversity measure minimization"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "A Bayesian framework is used to account for noise in the data and a maximum a posteriori (MAP) estimation procedure leads to an iterative procedure which is a regularized version of the focal underdetermined system solver (FOCUSS) algorithm that is superior to the OMP in noisy environments."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Signal Process."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111466"
                        ],
                        "name": "I. Gorodnitsky",
                        "slug": "I.-Gorodnitsky",
                        "structuredName": {
                            "firstName": "Irina",
                            "lastName": "Gorodnitsky",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Gorodnitsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144876925"
                        ],
                        "name": "B. Rao",
                        "slug": "B.-Rao",
                        "structuredName": {
                            "firstName": "Bhaskar",
                            "lastName": "Rao",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Rao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 129
                            }
                        ],
                        "text": "The Focal Under-determined System Solver (FOCUSS) is very similar, using the \"p-norm with p \u2264 1, as a replacement to the \"0-norm [18], [19], [20], [21]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8087541,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "20394faa89898ec65b20d2e1008ff8a63cbabfcc",
            "isKey": false,
            "numCitedBy": 1763,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a nonparametric algorithm for finding localized energy solutions from limited data. The problem we address is underdetermined, and no prior knowledge of the shape of the region on which the solution is nonzero is assumed. Termed the FOcal Underdetermined System Solver (FOCUSS), the algorithm has two integral parts: a low-resolution initial estimate of the real signal and the iteration process that refines the initial estimate to the final localized energy solution. The iterations are based on weighted norm minimization of the dependent variable with the weights being a function of the preceding iterative solutions. The algorithm is presented as a general estimation tool usable across different applications. A detailed analysis laying the theoretical foundation for the algorithm is given and includes proofs of global and local convergence and a derivation of the rate of convergence. A view of the algorithm as a novel optimization method which combines desirable characteristics of both classical optimization and learning-based algorithms is provided. Mathematical results on conditions for uniqueness of sparse solutions are also given. Applications of the algorithm are illustrated on problems in direction-of-arrival (DOA) estimation and neuromagnetic imaging."
            },
            "slug": "Sparse-signal-reconstruction-from-limited-data-a-Gorodnitsky-Rao",
            "title": {
                "fragments": [],
                "text": "Sparse signal reconstruction from limited data using FOCUSS: a re-weighted minimum norm algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A view of the algorithm as a novel optimization method which combines desirable characteristics of both classical optimization and learning-based algorithms is provided and Mathematical results on conditions for uniqueness of sparse solutions are also given."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Signal Process."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753908"
                        ],
                        "name": "Michael Elad",
                        "slug": "Michael-Elad",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Elad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Elad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143610924"
                        ],
                        "name": "A. Bruckstein",
                        "slug": "A.-Bruckstein",
                        "structuredName": {
                            "firstName": "Alfred",
                            "lastName": "Bruckstein",
                            "middleNames": [
                                "Marcel"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bruckstein"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 181
                            }
                        ],
                        "text": "Extensive study of these algorithms in recent years has established that if the sought solution, x, is sparse enough, these techniques recover it well in the exact case [16], [26], [27], [28], [29], [30]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 991948,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "115221749c9505a8a305ab6df676d9b71ed1438c",
            "isKey": false,
            "numCitedBy": 628,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "An elementary proof of a basic uncertainty principle concerning pairs of representations of R/sup N/ vectors in different orthonormal bases is provided. The result, slightly stronger than stated before, has a direct impact on the uniqueness property of the sparse representation of such vectors using pairs of orthonormal bases as overcomplete dictionaries. The main contribution in this paper is the improvement of an important result due to Donoho and Huo (2001) concerning the replacement of the l/sub 0/ optimization problem by a linear programming (LP) minimization when searching for the unique sparse representation."
            },
            "slug": "A-generalized-uncertainty-principle-and-sparse-in-Elad-Bruckstein",
            "title": {
                "fragments": [],
                "text": "A generalized uncertainty principle and sparse representation in pairs of bases"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The main contribution in this paper is the improvement of an important result due to Donoho and Huo (2001) concerning the replacement of the l/sub 0/ optimization problem by a linear programming minimization when searching for the unique sparse representation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3069792"
                        ],
                        "name": "M. Lewicki",
                        "slug": "M.-Lewicki",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lewicki",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lewicki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708655"
                        ],
                        "name": "B. Olshausen",
                        "slug": "B.-Olshausen",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Olshausen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Olshausen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 63
                            }
                        ],
                        "text": "For the Laplace distribution this approach is equivalent to BP [22]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 196
                            }
                        ],
                        "text": "The above method is then similar to ICA in that the algorithm can be interpreted as trying to maximize the mutual information between the inputs (samples) and the outputs (the coefficients) [24], [22], [25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 159
                            }
                        ],
                        "text": "The prior distribution of the representation vector x is assumed to be such that the entries of x are zero-mean iid, with Cauchy [24] or Laplace distributions [22], [23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 146
                            }
                        ],
                        "text": "Both the BP and the FOCUSS can be motivated based on Maximum A Posteriori (MAP) estimation, and indeed several works used this reasoning directly [22], [23], [24], [25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "Whereas the work in [23], [24], [22] applies a steepest descent to evaluate xi, those are evaluated much more efficiently with either OMP or FOCUSS."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 24
                            }
                        ],
                        "text": "The methods reported in [22], [23], [24], [25] use probabilistic reasoning in the construction of D."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6482128,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a87e0d75a8c17e464cf8e95a0466533e14b97c5e",
            "isKey": true,
            "numCitedBy": 346,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": "We apply a Bayesian method for inferring an optimal basis to the problem of finding efficient image codes for natural scenes. The basis functions learned by the algorithm are oriented and localized in both space and frequency, bearing a resemblance to two-dimensional Gabor functions, and increasing the number of basis functions results in a greater sampling density in position, orientation, and scale. These properties also resemble the spatial receptive fields of neurons in the primary visual cortex of mammals, suggesting that the receptive-field structure of these neurons can be accounted for by a general efficient coding principle. The probabilistic framework provides a method for comparing the coding efficiency of different bases objectively by calculating their probability given the observed data or by measuring the entropy of the basis function coefficients. The learned bases are shown to have better coding efficiency than traditional Fourier and wavelet bases. This framework also provides a Bayesian solution to the problems of image denoising and filling in of missing pixels. We demonstrate that the results obtained by applying the learned bases to these problems are improved over those obtained with traditional techniques."
            },
            "slug": "PROBABILISTIC-FRAMEWORK-FOR-THE-ADAPTATION-AND-OF-Lewicki-Olshausen",
            "title": {
                "fragments": [],
                "text": "PROBABILISTIC FRAMEWORK FOR THE ADAPTATION AND COMPARISON OF IMAGE CODES"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The learned bases are shown to have better coding efficiency than traditional Fourier and wavelet bases and to provide a Bayesian solution to the problems of image denoising and filling in of missing pixels."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123257999"
                        ],
                        "name": "B. Rao",
                        "slug": "B.-Rao",
                        "structuredName": {
                            "firstName": "B.D.",
                            "lastName": "Rao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Rao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1395421758"
                        ],
                        "name": "K. Kreutz-Delgado",
                        "slug": "K.-Kreutz-Delgado",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Kreutz-Delgado",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kreutz-Delgado"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 141
                            }
                        ],
                        "text": "The Focal Under-determined System Solver (FOCUSS) is very similar, using the \"p-norm with p \u2264 1, as a replacement to the \"0-norm [18], [19], [20], [21]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 23303572,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f44e85948ef6bdc9645581173b1349d5dc569c9c",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel methodology is employed to develop algorithms for computing sparse solutions to linear inverse problems, starting from suitably defined diversity measures whose minimization promotes sparsity. These measures include p-norm-like (/spl Lscr//sub (p/spl les/1)/) diversity measures, and the Gaussian and Shannon entropies. The algorithm development methodology uses a factored representation of the gradient, and involves successive relaxation of the Lagrangian necessary condition. The general nature of the methodology provides a systematic approach for deriving a class of algorithms called FOCUSS (FOCal Underdetermined System Solver), and a natural mechanism for extending them."
            },
            "slug": "Deriving-algorithms-for-computing-sparse-solutions-Rao-Kreutz-Delgado",
            "title": {
                "fragments": [],
                "text": "Deriving algorithms for computing sparse solutions to linear inverse problems"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "A novel methodology is employed to develop algorithms for computing sparse solutions to linear inverse problems, starting from suitably defined diversity measures whose minimization promotes sparsity, using a factored representation of the gradient."
            },
            "venue": {
                "fragments": [],
                "text": "Conference Record of the Thirty-First Asilomar Conference on Signals, Systems and Computers (Cat. No.97CB36136)"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144839080"
                        ],
                        "name": "X. Huo",
                        "slug": "X.-Huo",
                        "structuredName": {
                            "firstName": "Xiaoming",
                            "lastName": "Huo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Huo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 175
                            }
                        ],
                        "text": "Extensive study of these algorithms in recent years has established that if the sought solution, x, is sparse enough, these techniques recover it well in the exact case [16], [26], [27], [28], [29], [30]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9500527,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "64a4094ccbbb7f00491b25ac9089b7b6a58be721",
            "isKey": false,
            "numCitedBy": 1994,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Suppose a discrete-time signal S(t), 0/spl les/t<N, is a superposition of atoms taken from a combined time-frequency dictionary made of spike sequences 1/sub {t=/spl tau/}/ and sinusoids exp{2/spl pi/iwt/N}//spl radic/N. Can one recover, from knowledge of S alone, the precise collection of atoms going to make up S? Because every discrete-time signal can be represented as a superposition of spikes alone, or as a superposition of sinusoids alone, there is no unique way of writing S as a sum of spikes and sinusoids in general. We prove that if S is representable as a highly sparse superposition of atoms from this time-frequency dictionary, then there is only one such highly sparse representation of S, and it can be obtained by solving the convex optimization problem of minimizing the l/sup 1/ norm of the coefficients among all decompositions. Here \"highly sparse\" means that N/sub t/+N/sub w/</spl radic/N/2 where N/sub t/ is the number of time atoms, N/sub w/ is the number of frequency atoms, and N is the length of the discrete-time signal. Underlying this result is a general l/sup 1/ uncertainty principle which says that if two bases are mutually incoherent, no nonzero signal can have a sparse representation in both bases simultaneously. For the above setting, the bases are sinusoids and spikes, and mutual incoherence is measured in terms of the largest inner product between different basis elements. The uncertainty principle holds for a variety of interesting basis pairs, not just sinusoids and spikes. The results have idealized applications to band-limited approximation with gross errors, to error-correcting encryption, and to separation of uncoordinated sources. Related phenomena hold for functions of a real variable, with basis pairs such as sinusoids and wavelets, and for functions of two variables, with basis pairs such as wavelets and ridgelets. In these settings, if a function f is representable by a sufficiently sparse superposition of terms taken from both bases, then there is only one such sparse representation; it may be obtained by minimum l/sup 1/ norm atomic decomposition. The condition \"sufficiently sparse\" becomes a multiscale condition; for example, that the number of wavelets at level j plus the number of sinusoids in the jth dyadic frequency band are together less than a constant times 2/sup j/2/."
            },
            "slug": "Uncertainty-principles-and-ideal-atomic-Donoho-Huo",
            "title": {
                "fragments": [],
                "text": "Uncertainty principles and ideal atomic decomposition"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "It is proved that if S is representable as a highly sparse superposition of atoms from this time-frequency dictionary, then there is only one such highly sparse representation of S, and it can be obtained by solving the convex optimization problem of minimizing the l/sup 1/ norm of the coefficients among all decompositions."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 168
                            }
                        ],
                        "text": "The recent front of activity revisits those questions within a probabilistic setting, obtaining more realistic assessments on pursuit algorithm performance and success [33], [34], [35]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8510060,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "3424286d6d39de51080ddd683646565545d015e2",
            "isKey": false,
            "numCitedBy": 2296,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider linear equations y = \u03a6x where y is a given vector in \u211dn and \u03a6 is a given n \u00d7 m matrix with n < m \u2264 \u03c4n, and we wish to solve for x \u2208 \u211dm. We suppose that the columns of \u03a6 are normalized to the unit \ud835\udcc12\u2010norm, and we place uniform measure on such \u03a6. We prove the existence of \u03c1 = \u03c1(\u03c4) > 0 so that for large n and for all \u03a6's except a negligible fraction, the following property holds: For every y having a representation y = \u03a6x0 by a coefficient vector x0 \u2208 \u211dm with fewer than \u03c1 \u00b7 n nonzeros, the solution x1 of the \ud835\udcc11\u2010minimization problem $${\\rm min} \\|x\\|_{1} \\;\\;{subject \\; to}\\;\\; \\Phi x = y$$ is unique and equal to x0. In contrast, heuristic attempts to sparsely solve such systems\u2014greedy algorithms and thresholding\u2014perform poorly in this challenging setting. The techniques include the use of random proportional embeddings and almost\u2010spherical sections in Banach space theory, and deviation bounds for the eigenvalues of random Wishart matrices. \u00a9 2006 Wiley Periodicals, Inc."
            },
            "slug": "For-most-large-underdetermined-systems-of-linear-is-Donoho",
            "title": {
                "fragments": [],
                "text": "For most large underdetermined systems of linear equations the minimal \ud835\udcc11\u2010norm solution is also the sparsest solution"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "The techniques include the use of random proportional embeddings and almost\u2010spherical sections in Banach space theory, and deviation bounds for the eigenvalues of random Wishart matrices."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145394553"
                        ],
                        "name": "Jean-Luc Starck",
                        "slug": "Jean-Luc-Starck",
                        "structuredName": {
                            "firstName": "Jean-Luc",
                            "lastName": "Starck",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean-Luc Starck"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120002676"
                        ],
                        "name": "Mikael Elad",
                        "slug": "Mikael-Elad",
                        "structuredName": {
                            "firstName": "Mikael",
                            "lastName": "Elad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mikael Elad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 155
                            }
                        ],
                        "text": "Sparsity and overcompleteness have been successfully used for dynamic range compression in images [6], separation of texture and cartoon content in images [7], [8], inpainting [9], and more."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18014884,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "bd5ecf9f8988f6c2bd974058102f9ffef6b39f78",
            "isKey": false,
            "numCitedBy": 54,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a novel method for separating images into texture and piecewise smooth parts. The proposed approach is based on a combination of the Basis Pursuit Denoising (BPDN) algorithm and the Total-Variation (TV) regularization scheme. The basic idea promoted in this paper is the use of two appropriate dictionaries, one for the representation of textures, and the other for the natural scene parts. Each dictionary is designed for sparse representation of a particular type of image-content (either texture or piecewise smooth). The use of BPDN with the two augmented dictionaries leads to the desired separation, along with noise removal as a by-product. As the need to choose a proper dictionary for natural scene is very hard, a TV regularization is employed to better direct the separation process. Experimental results validate the algorithm's performance."
            },
            "slug": "Image-decomposition:-separation-of-texture-from-Starck-Elad",
            "title": {
                "fragments": [],
                "text": "Image decomposition: separation of texture from piecewise smooth content"
            },
            "venue": {
                "fragments": [],
                "text": "SPIE Optics + Photonics"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187218"
                        ],
                        "name": "A. J. Bell",
                        "slug": "A.-J.-Bell",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Bell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Bell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 113
                            }
                        ],
                        "text": "There is an interesting relation between the above method and the Independent Component Analysis (ICA) algorithm [43]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1701422,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d7d0e8c4791700defd4b0df82a26b50055346e0",
            "isKey": false,
            "numCitedBy": 8758,
            "numCiting": 121,
            "paperAbstract": {
                "fragments": [],
                "text": "We derive a new self-organizing learning algorithm that maximizes the information transferred in a network of nonlinear units. The algorithm does not assume any knowledge of the input distributions, and is defined here for the zero-noise limit. Under these conditions, information maximization has extra properties not found in the linear case (Linsker 1989). The nonlinearities in the transfer function are able to pick up higher-order moments of the input distributions and perform something akin to true redundancy reduction between units in the output representation. This enables the network to separate statistically independent components in the inputs: a higher-order generalization of principal components analysis. We apply the network to the source separation (or cocktail party) problem, successfully separating unknown mixtures of up to 10 speakers. We also show that a variant on the network architecture is able to perform blind deconvolution (cancellation of unknown echoes and reverberation in a speech signal). Finally, we derive dependencies of information transfer on time delays. We suggest that information maximization provides a unifying framework for problems in \"blind\" signal processing."
            },
            "slug": "An-Information-Maximization-Approach-to-Blind-and-Bell-Sejnowski",
            "title": {
                "fragments": [],
                "text": "An Information-Maximization Approach to Blind Separation and Blind Deconvolution"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is suggested that information maximization provides a unifying framework for problems in \"blind\" signal processing and dependencies of information transfer on time delays are derived."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 174
                            }
                        ],
                        "text": "The recent front of activity revisits those questions within a probabilistic setting, obtaining more realistic assessments on pursuit algorithm performance and success [33], [34], [35]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12929381,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "36ab8bd64210ac5c4f7d326ed2c0a5745e91320f",
            "isKey": false,
            "numCitedBy": 1067,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider inexact linear equations y \u2248 \u03a6x where y is a given vector in \u211dn, \u03a6 is a given n \u00d7 m matrix, and we wish to find x0,\u03f5 as sparse as possible while obeying \u2016y \u2212 \u03a6x0,\u03f5\u20162 \u2264 \u03f5. In general, this requires combinatorial optimization and so is considered intractable. On the other hand, the \ud835\udcc11\u2010minimization problem $${\\rm min} \\; \\|x\\|_{1}\\;\\;\\; {\\rm subject \\; to}\\;\\;\\; \\|y - \\Phi{x}\\|_{2} \\leq \\epsilon$$ is convex and is considered tractable. We show that for most \u03a6, if the optimally sparse approximation x0,\u03f5 is sufficiently sparse, then the solution x1,\u03f5 of the \ud835\udcc11\u2010minimization problem is a good approximation to x0,\u03f5."
            },
            "slug": "For-most-large-underdetermined-systems-of-the-the-Donoho",
            "title": {
                "fragments": [],
                "text": "For most large underdetermined systems of equations, the minimal \ud835\udcc11\u2010norm near\u2010solution approximates the sparsest near\u2010solution"
            },
            "tldr": {
                "abstractSimilarityScore": 34,
                "text": "It is shown that for most \u03a6, if the optimally sparse approximation x0,\u03f5 is sufficiently sparse, then the solution x1, \u03f5 of the \ud835\udcc11\u2010minimization problem is a good approximation to x0 ,\u03f5."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2691592"
                        ],
                        "name": "K. Engan",
                        "slug": "K.-Engan",
                        "structuredName": {
                            "firstName": "Kjersti",
                            "lastName": "Engan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Engan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3046011"
                        ],
                        "name": "S. O. Aase",
                        "slug": "S.-O.-Aase",
                        "structuredName": {
                            "firstName": "Sven",
                            "lastName": "Aase",
                            "middleNames": [
                                "Ole"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. O. Aase"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31557075"
                        ],
                        "name": "J. H. Hus\u00f8y",
                        "slug": "J.-H.-Hus\u00f8y",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hus\u00f8y",
                            "middleNames": [
                                "H\u00e5kon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H. Hus\u00f8y"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 162
                            }
                        ],
                        "text": "This idea of iterative refinement, mentioned before as a generalization of the K-Means algorithm, was later used again by other researchers, with some variations [36], [37], [40], [41], [42]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 65
                            }
                        ],
                        "text": "This connection has previously been mentioned in several reports [36], [37], [38]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 8624950,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "86b84cdbf11b4ebf562633a6868471487076de67",
            "isKey": false,
            "numCitedBy": 267,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Multi-frame-compression:-theory-and-design-Engan-Aase",
            "title": {
                "fragments": [],
                "text": "Multi-frame compression: theory and design"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Process."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708655"
                        ],
                        "name": "B. Olshausen",
                        "slug": "B.-Olshausen",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Olshausen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Olshausen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49649079"
                        ],
                        "name": "D. Field",
                        "slug": "D.-Field",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Field",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Field"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 127
                            }
                        ],
                        "text": "= \u222b P (yi|x,D) \u00b7 P (x)dx = Const \u00b7 \u222b exp {\n1 2\u03c32\n\u2016Dx \u2212 yi\u20162 } \u00b7 exp {\u03bb\u2016x\u20161} dx (7)\nThis integration over x is difficult to evaluate, and indeed, Olshausen and Field [23] handled this by replacing it with the extremal value of P (yi,x|D)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 48
                            }
                        ],
                        "text": "The same measure was practiced by previous work [23]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 132
                            }
                        ],
                        "text": "Thus, while the MOD method assumes known coefficients at each iteration, and derives the best possible dictionary, the ML method by Olshausen and Field only gets closer to this best current solution, and then turns to calculate the coefficients."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 29
                            }
                        ],
                        "text": "[23] B.A. Olshausen and D.J. Field."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 165
                            }
                        ],
                        "text": "The prior distribution of the representation vector x is assumed to be such that the entries of x are zero-mean iid, with Cauchy [24] or Laplace distributions [22], [23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 82
                            }
                        ],
                        "text": "This integration over x is difficult to evaluate, and indeed, Olshausen and Field [23] handled this by replacing it with the extremal value of P (yi,x|D)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 170
                            }
                        ],
                        "text": "Taking\nthe derivative of (10) with respect to D we obtain the relation (Y \u2212 DX)XT = 0, leading to\nD(n+1) = YX(n) T \u00b7 (X(n)X(n)T )\u22121 (11)\nMOD is closely related to the work by Olshausen and Field, with improvements both in the sparse coding and the dictionary update stages."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 68
                            }
                        ],
                        "text": "When no prior is chosen, the update formula is the very one used by Olshausen and Field, as in (9)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 29
                            }
                        ],
                        "text": "[24] B.A. Olshausen and B.J. Field."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 152
                            }
                        ],
                        "text": "Both the BP and the FOCUSS can be motivated based on Maximum A Posteriori (MAP) estimation, and indeed several works used this reasoning directly [22], [23], [24], [25]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 20
                            }
                        ],
                        "text": "Whereas the work in [23], [24], [22] applies a steepest descent to evaluate xi, those are evaluated much more efficiently with either OMP or FOCUSS."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 30
                            }
                        ],
                        "text": "The methods reported in [22], [23], [24], [25] use probabilistic reasoning in the construction of D."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9526302,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e309e441a38ccee6456bd02e0f1e894e44180d53",
            "isKey": true,
            "numCitedBy": 618,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Natural images contain characteristic statistical regularities that set them apart from purely random images. Understanding what these regularities are can enable natural images to be coded more efficiently. In this paper, we describe some of the forms of structure that are contained in natural images, and we show how these are related to the response properties of neurons at early stages of the visual system. Many of the important forms of structure require higher-order (i.e. more than linear, pairwise) statistics to characterize, which makes models based on linear Hebbian learning, or principal components analysis, inappropriate for finding efficient codes for natural images. We suggest that a good objective for an efficient coding of natural scenes is to maximize the sparseness of the representation, and we show that a network that learns sparse codes of natural scenes succeeds in developing localized, oriented, bandpass receptive fields similar to those in the mammalian striate cortex."
            },
            "slug": "Natural-image-statistics-and-efficient-coding.-Olshausen-Field",
            "title": {
                "fragments": [],
                "text": "Natural image statistics and efficient coding."
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is suggested that a good objective for an efficient coding of natural Scenes is to maximize the sparseness of the representation, and it is shown that a network that learns sparse codes of natural scenes succeeds in developing localized, oriented, bandpass receptive fields similar to those in the mammalian striate cortex."
            },
            "venue": {
                "fragments": [],
                "text": "Network"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34817070"
                        ],
                        "name": "Y. C. Pati",
                        "slug": "Y.-C.-Pati",
                        "structuredName": {
                            "firstName": "Yagyensh",
                            "lastName": "Pati",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. C. Pati"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2368882"
                        ],
                        "name": "R. Rezaiifar",
                        "slug": "R.-Rezaiifar",
                        "structuredName": {
                            "firstName": "Ramin",
                            "lastName": "Rezaiifar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rezaiifar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2426392"
                        ],
                        "name": "P. Krishnaprasad",
                        "slug": "P.-Krishnaprasad",
                        "structuredName": {
                            "firstName": "Perinkulam",
                            "lastName": "Krishnaprasad",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Krishnaprasad"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 118
                            }
                        ],
                        "text": "The simplest ones are the Matching Pursuit (MP) [12] and the Orthogonal Matching Pursuit (OMP) algorithms [13], [14], [15], [16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16513805,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "2071f3ee9ec4d17250b00626d55e47bf75ae2726",
            "isKey": false,
            "numCitedBy": 4153,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a recursive algorithm to compute representations of functions with respect to nonorthogonal and possibly overcomplete dictionaries of elementary building blocks e.g. affine (wavelet) frames. We propose a modification to the matching pursuit algorithm of Mallat and Zhang (1992) that maintains full backward orthogonality of the residual (error) at every step and thereby leads to improved convergence. We refer to this modified algorithm as orthogonal matching pursuit (OMP). It is shown that all additional computation required for the OMP algorithm may be performed recursively.<<ETX>>"
            },
            "slug": "Orthogonal-matching-pursuit:-recursive-function-to-Pati-Rezaiifar",
            "title": {
                "fragments": [],
                "text": "Orthogonal matching pursuit: recursive function approximation with applications to wavelet decomposition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A modification to the matching pursuit algorithm of Mallat and Zhang (1992) that maintains full backward orthogonality of the residual at every step and thereby leads to improved convergence is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 27th Asilomar Conference on Signals, Systems and Computers"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780112"
                        ],
                        "name": "R. Coifman",
                        "slug": "R.-Coifman",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Coifman",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Coifman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 168
                            }
                        ],
                        "text": "In denoising, wavelet methods and shift-invariant variations that exploit overcomplete representation, are among the most effective known algorithms for this task [2], [3], [4], [5]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 19092230,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "d7a7d429de08c85f44fb77500e45a898f271571f",
            "isKey": false,
            "numCitedBy": 593,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "De-Noising with the traditional (orthogonal, maximally-decimated) wavelet transform sometimes exhibits visual artifacts; we attribute some of these \u2013 for example, Gibbs phenomena in the neighborhood of discontinuities \u2013 to the lack of translation invariance of the wavelet basis. One method to suppress such artifacts, termed \u201ccycle spinning\u201d by Coifman, is to \u201caverage out\u201d the translation dependence. For a range of shifts, one shifts the data (right or left as the case may be), De-Noises the shifted data, and then unshifts the de-noised data. Doing this for each of a range of shifts, and averaging the several results so obtained, produces a reconstruction subject to far weaker Gibbs phenomena than thresholding based De-Noising using the traditional orthogonal wavelet transform. Cycle-Spinning over the range of all circulant shifts can be accomplished in order n log2(n) time; it is equivalent to de-noising using the undecimated or stationary wavelet transform. Cycle-spinning exhibits benefits outside of wavelet de-noising, for example in cosine packet denoising, where it helps suppress \u2018clicks\u2019. It also has a counterpart in frequency domain de-noising, where the goal of translation-invariance is replaced by modulation invariance, and the central shift-De-Noise-unshift operation is replaced by modulate-De-Noise-demodulate. We illustrate these concepts with extensive computational examples; all figures presented here are reproducible using the WaveLab software package."
            },
            "slug": "Translation-Invariant-DeNoising-Coifman-Donoho",
            "title": {
                "fragments": [],
                "text": "Translation-Invariant DeNoising"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1410796425"
                        ],
                        "name": "P.",
                        "slug": "P.",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "P.",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P."
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2101896801"
                        ],
                        "name": "SimoncelliyWilliam",
                        "slug": "SimoncelliyWilliam",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "SimoncelliyWilliam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "SimoncelliyWilliam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "134775902"
                        ],
                        "name": "T.",
                        "slug": "T.",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "T.",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T."
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2083812684"
                        ],
                        "name": "FreemanzEdward",
                        "slug": "FreemanzEdward",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "FreemanzEdward",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "FreemanzEdward"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1410812392"
                        ],
                        "name": "H.",
                        "slug": "H.",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "H.",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H."
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2078534133"
                        ],
                        "name": "AdelsonxDavid",
                        "slug": "AdelsonxDavid",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "AdelsonxDavid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "AdelsonxDavid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11447118"
                        ],
                        "name": "J. Heeger",
                        "slug": "J.-Heeger",
                        "structuredName": {
                            "firstName": "Joanna",
                            "lastName": "Heeger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Heeger"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 173
                            }
                        ],
                        "text": "In denoising, wavelet methods and shift-invariant variations that exploit overcomplete representation, are among the most effective known algorithms for this task [2], [3], [4], [5]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11472712,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ceefd65430b990596f9d986c750a347479d4d280",
            "isKey": false,
            "numCitedBy": 153,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Orthogonal wavelet transforms have recently become a popular representation for multi-scale signal and image analysis. One of the major drawbacks of these representations is their lack of translation invariance: the content of wavelet subbands is unstable under translations of the input signal. Wavelet transforms are also unstable with respect to dilations of the input signal, and in two dimensions, rotations of the input signal. We formalize these problems by deening a type of translation invariance that we call \\shiftability\". In the spatial domain, shiftability corresponds to a lack of aliasing; thus, the conditions under which the property holds are speciied by the sampling theorem. Shiftability may also be considered in the context of other domains, particularly orientation and scale. We explore \\jointly shiftable\" transforms that are simultaneously shiftable in more than one domain. Two examples of jointly shiftable transforms are designed and implemented: a one-dimensional transform that is jointly shiftable in position and scale, and a two-dimensional transform that is jointly shiftable in position and orientation. We demonstrate the usefulness of these image representations for scale-space analysis, stereo disparity measurement, and image enhancement."
            },
            "slug": "Shiftable-Multi-scale-TransformsEero-P.-SimoncelliyWilliam",
            "title": {
                "fragments": [],
                "text": "Shiftable Multi-scale TransformsEero"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144876925"
                        ],
                        "name": "B. Rao",
                        "slug": "B.-Rao",
                        "structuredName": {
                            "firstName": "Bhaskar",
                            "lastName": "Rao",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Rao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1395421758"
                        ],
                        "name": "K. Kreutz-Delgado",
                        "slug": "K.-Kreutz-Delgado",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Kreutz-Delgado",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kreutz-Delgado"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 135
                            }
                        ],
                        "text": "The Focal Under-determined System Solver (FOCUSS) is very similar, using the \"p-norm with p \u2264 1, as a replacement to the \"0-norm [18], [19], [20], [21]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8035693,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "88855bc679d6b94e42b297f37b3decf451cd908e",
            "isKey": false,
            "numCitedBy": 538,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "A methodology is developed to derive algorithms for optimal basis selection by minimizing diversity measures proposed by Wickerhauser (1994) and Donoho (1994). These measures include the p-norm-like (l/sub (p/spl les/1)/) diversity measures and the Gaussian and Shannon entropies. The algorithm development methodology uses a factored representation for the gradient and involves successive relaxation of the Lagrangian necessary condition. This yields algorithms that are intimately related to the affine scaling transformation (AST) based methods commonly employed by the interior point approach to nonlinear optimization. The algorithms minimizing the (l/sub (p/spl les/1)/) diversity measures are equivalent to a previously developed class of algorithms called focal underdetermined system solver (FOCUSS). The general nature of the methodology provides a systematic approach for deriving this class of algorithms and a natural mechanism for extending them. It also facilitates a better understanding of the convergence behavior and a strengthening of the convergence results. The Gaussian entropy minimization algorithm is shown to be equivalent to a well-behaved p=0 norm-like optimization algorithm. Computer experiments demonstrate that the p-norm-like and the Gaussian entropy algorithms perform well, converging to sparse solutions. The Shannon entropy algorithm produces solutions that are concentrated but are shown to not converge to a fully sparse solution."
            },
            "slug": "An-affine-scaling-methodology-for-best-basis-Rao-Kreutz-Delgado",
            "title": {
                "fragments": [],
                "text": "An affine scaling methodology for best basis selection"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "A methodology is developed to derive algorithms for optimal basis selection by minimizing diversity measures proposed by Wickerhauser (1994) and Donoho (1994), which include the p-norm-like (l/sub (p/spl les/1)/) diversity measures and the Gaussian and Shannon entropies."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Signal Process."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145394553"
                        ],
                        "name": "Jean-Luc Starck",
                        "slug": "Jean-Luc-Starck",
                        "structuredName": {
                            "firstName": "Jean-Luc",
                            "lastName": "Starck",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean-Luc Starck"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2006869"
                        ],
                        "name": "E. Cand\u00e8s",
                        "slug": "E.-Cand\u00e8s",
                        "structuredName": {
                            "firstName": "Emmanuel",
                            "lastName": "Cand\u00e8s",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Cand\u00e8s"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 178
                            }
                        ],
                        "text": "In denoising, wavelet methods and shift-invariant variations that exploit overcomplete representation, are among the most effective known algorithms for this task [2], [3], [4], [5]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8284003,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "90d788e4da8347335d63c73d11ec789d1ab959ff",
            "isKey": false,
            "numCitedBy": 1682,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Summary form only given, as follows. We present approximate digital implementations of two new mathematical transforms, namely, the ridgelet transform and the curvelet transform. Our implementations offer exact reconstruction, stability against perturbations, ease of implementation, and low computational complexity. We apply these digital transforms to the denoising of some standard images embedded in white noise. In the tests reported here, simple thresholding of the curvelet coefficients is very competitive with 'state of the art' techniques based on wavelets, including thresholding of decimated or undecimated wavelet transforms and also including tree-based Bayesian posterior mean methods. Moreover, the curvelet reconstructions exhibit higher perceptual quality than wavelet-based reconstructions, offering visually sharper images and, in particular, higher quality recovery of edges and of faint linear and curvilinear features."
            },
            "slug": "The-curvelet-transform-for-image-denoising-Starck-Cand\u00e8s",
            "title": {
                "fragments": [],
                "text": "The curvelet transform for image denoising"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "In the tests reported here, simple thresholding of the curvelet coefficients is very competitive with 'state of the art' techniques based on wavelets, including thresholded of decimated or undecimated wavelet transforms and also including tree-based Bayesian posterior mean methods."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 2001 International Conference on Image Processing (Cat. No.01CH37205)"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2691592"
                        ],
                        "name": "K. Engan",
                        "slug": "K.-Engan",
                        "structuredName": {
                            "firstName": "Kjersti",
                            "lastName": "Engan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Engan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3046011"
                        ],
                        "name": "S. O. Aase",
                        "slug": "S.-O.-Aase",
                        "structuredName": {
                            "firstName": "Sven",
                            "lastName": "Aase",
                            "middleNames": [
                                "Ole"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. O. Aase"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31557075"
                        ],
                        "name": "J. H. Hus\u00f8y",
                        "slug": "J.-H.-Hus\u00f8y",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hus\u00f8y",
                            "middleNames": [
                                "H\u00e5kon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H. Hus\u00f8y"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 174
                            }
                        ],
                        "text": "This idea of iterative refinement, mentioned before as a generalization of the K-Means algorithm, was later used again by other researchers, with some variations [36], [37], [40], [41], [42]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 33097614,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "684732677d91a93b115f57e8d671ef7f5f13ee14",
            "isKey": false,
            "numCitedBy": 1228,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "A frame design technique for use with vector selection algorithms, for example matching pursuits (MP), is presented. The design algorithm is iterative and requires a training set of signal vectors. The algorithm, called method of optimal directions (MOD), is an improvement of the algorithm presented by Engan, Aase and Husoy see (Proc. ICASSP '98, Seattle, USA, p.1817-20, 1998). The MOD is applied to speech and electrocardiogram (ECG) signals, and the designed frames are tested on signals outside the training sets. Experiments demonstrate that the approximation capabilities, in terms of mean squared error (MSE), of the optimized frames are significantly better than those obtained using frames designed by the algorithm of Engan et. al. Experiments show typical reduction in MSE by 20-50%."
            },
            "slug": "Method-of-optimal-directions-for-frame-design-Engan-Aase",
            "title": {
                "fragments": [],
                "text": "Method of optimal directions for frame design"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Experiments demonstrate that the approximation capabilities, in terms of mean squared error (MSE), of the optimized frames are significantly better than those obtained using frames designed by the algorithm of Engan et."
            },
            "venue": {
                "fragments": [],
                "text": "1999 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings. ICASSP99 (Cat. No.99CH36258)"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2006869"
                        ],
                        "name": "E. Cand\u00e8s",
                        "slug": "E.-Cand\u00e8s",
                        "structuredName": {
                            "firstName": "Emmanuel",
                            "lastName": "Cand\u00e8s",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Cand\u00e8s"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735291"
                        ],
                        "name": "J. Romberg",
                        "slug": "J.-Romberg",
                        "structuredName": {
                            "firstName": "Justin",
                            "lastName": "Romberg",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Romberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 180
                            }
                        ],
                        "text": "The recent front of activity revisits those questions within a probabilistic setting, obtaining more realistic assessments on pursuit algorithm performance and success [33], [34], [35]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6798024,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "927de77d0002851c150be9fb0ea2204880135f3f",
            "isKey": false,
            "numCitedBy": 542,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "AbstractIn this paper we develop a robust uncertainty principle for\n finite signals in ${\\Bbb C}^N$ which states that, for nearly all choices\n $T, \\Omega\\subset\\{0,\\ldots,N-1\\}$ such that $|T| + |\\Omega| \\asymp (\\log N)^{-1/2} \\cdot N,$ there is no signal $f$ supported on $T$ whose discrete Fourier transform $\\hat{f}$ is supported on $\\Omega.$ In fact, we can make the above uncertainty principle quantitative in the sense that if $f$ is supported on $T,$ then only a small percentage of the energy (less than half, say) of $\\hat{f}$ is concentrated on $\\Omega.$ As an application of this robust uncertainty principle (QRUP), we consider the problem of decomposing a signal into a sparse superposition of spikes and complex sinusoids $f(s) = \\sum_{t\\in T}\\alpha_1(t)\\delta(s-t) + \\sum_{\\omega\\in\\Omega}\\alpha_2(\\omega)e^{i2\\pi \\omega s/N}/\\sqrt{N}.$ We show that if a generic signal $f$ has a decomposition $(\\alpha_1,\\alpha_2)$ using spike and frequency locations in $T$ and $\\Omega,$ respectively, and obeying $|T| + |\\Omega| \\leq {\\rm Const}\\cdot (\\log N)^{-1/2}\\cdot N,$ then $(\\alpha_1, \\alpha_2)$ is the unique sparsest possible decomposition (all other decompositions have more nonzero terms). In addition, if $|T| + |\\Omega| \\leq {\\rm Const}\\cdot (\\log N)^{-1}\\cdot N,$ then the sparsest $(\\alpha_1,\\alpha_2)$ can be found by solving a convex optimization problem. Underlying our results is a new probabilistic approach which insists on finding the correct uncertainty relation, or the optimally sparse solution for nearly all subsets but not necessarily all of them, and allows us to considerably sharpen previously known results [9], [10]. In fact, we show that the fraction of sets $(T, \\Omega)$ for which the above properties do not hold can be upper bounded by quantities like $N^{-\\alpha}$ for large values of $\\alpha.$ The QRUP (and the application to finding sparse representations) can be extended to general pairs of orthogonal bases $\\Phi_1,\\Phi_2 \\mbox{of} {\\Bbb C}^N.$ For nearly all choices ${\\Gamma_1},{\\Gamma_2}\\subset\\{0,\\ldots,N-1\\}$ obeying $|{\\Gamma_1}| + |{\\Gamma_2}| \\asymp \\mu(\\Phi_1,\\Phi_2)^{-2} \\cdot (\\log N)^{-m},$ where $m\\leq 6,$ there is no signal $f$ such that $\\Phi_1 f$ is supported on ${\\Gamma_1}$ and $\\Phi_2 f$ is supported on ${\\Gamma_2}$ where $\\mu(\\Phi_1,\\Phi_2)$ is the mutual coherence between $\\Phi_1$ and $\\Phi_2.$"
            },
            "slug": "Quantitative-Robust-Uncertainty-Principles-and-Cand\u00e8s-Romberg",
            "title": {
                "fragments": [],
                "text": "Quantitative Robust Uncertainty Principles and Optimally Sparse Decompositions"
            },
            "venue": {
                "fragments": [],
                "text": "Found. Comput. Math."
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753908"
                        ],
                        "name": "Michael Elad",
                        "slug": "Michael-Elad",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Elad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Elad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145394554"
                        ],
                        "name": "J. Starck",
                        "slug": "J.-Starck",
                        "structuredName": {
                            "firstName": "J-L.",
                            "lastName": "Starck",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Starck"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2196023"
                        ],
                        "name": "P. Querre",
                        "slug": "P.-Querre",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Querre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Querre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 176
                            }
                        ],
                        "text": "Sparsity and overcompleteness have been successfully used for dynamic range compression in images [6], separation of texture and cartoon content in images [7], [8], inpainting [9], and more."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5671048,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c77338d031d41c3c5ae027e64a64242555b3ec2d",
            "isKey": false,
            "numCitedBy": 914,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Simultaneous-cartoon-and-texture-image-inpainting-Elad-Starck",
            "title": {
                "fragments": [],
                "text": "Simultaneous cartoon and texture image inpainting using morphological component analysis (MCA)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2608771"
                        ],
                        "name": "A. Gersho",
                        "slug": "A.-Gersho",
                        "structuredName": {
                            "firstName": "Allen",
                            "lastName": "Gersho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gersho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144790332"
                        ],
                        "name": "R. Gray",
                        "slug": "R.-Gray",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Gray",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Gray"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 77
                            }
                        ],
                        "text": "Since the K-Means algorithm (also known as generalized Lloyd algorithm - GLA [39]) is the most commonly used procedure for training in the vector quantization setting, it is natural to consider generalizations of this algorithm when turning to the problem of dictionary training."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 92
                            }
                        ],
                        "text": "The K-Means algorithm is an iterative method used for designing the optimal codebook for VQ [39]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 155
                            }
                        ],
                        "text": "Note that we have deliberately chosen not to discuss stopping rules for the above-described algorithm, since those vary a lot but are quite easy to handle [39]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 75
                            }
                        ],
                        "text": "5 Since the K-Means algorithm (also known as generalized Lloyd algorithm - GLA [39]) is the most commonly used procedure for training in the vector quantization setting, it is natural to consider generalizations of this algorithm when turning to the problem of dictionary training."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 130
                            }
                        ],
                        "text": "There is a variant of the vector quantization (VQ) coding method, called Gain-Shape VQ, where this coefficient is allowed to vary [39]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 118950728,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c564aa7639a08c280423489e52b6e32055c9aa7f",
            "isKey": true,
            "numCitedBy": 7028,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1 Introduction.- 1.1 Signals, Coding, and Compression.- 1.2 Optimality.- 1.3 How to Use this Book.- 1.4 Related Reading.- I Basic Tools.- 2 Random Processes and Linear Systems.- 2.1 Introduction.- 2.2 Probability.- 2.3 Random Variables and Vectors.- 2.4 Random Processes.- 2.5 Expectation.- 2.6 Linear Systems.- 2.7 Stationary and Ergodic Properties.- 2.8 Useful Processes.- 2.9 Problems.- 3 Sampling.- 3.1 Introduction.- 3.2 Periodic Sampling.- 3.3 Noise in Sampling.- 3.4 Practical Sampling Schemes.- 3.5 Sampling Jitter.- 3.6 Multidimensional Sampling.- 3.7 Problems.- 4 Linear Prediction.- 4.1 Introduction.- 4.2 Elementary Estimation Theory.- 4.3 Finite-Memory Linear Prediction.- 4.4 Forward and Backward Prediction.- 4.5 The Levinson-Durbin Algorithm.- 4.6 Linear Predictor Design from Empirical Data.- 4.7 Minimum Delay Property.- 4.8 Predictability and Determinism.- 4.9 Infinite Memory Linear Prediction.- 4.10 Simulation of Random Processes.- 4.11 Problems.- II Scalar Coding.- 5 Scalar Quantization I.- 5.1 Introduction.- 5.2 Structure of a Quantizer.- 5.3 Measuring Quantizer Performance.- 5.4 The Uniform Quantizer.- 5.5 Nonuniform Quantization and Companding.- 5.6 High Resolution: General Case.- 5.7 Problems.- 6 Scalar Quantization II.- 6.1 Introduction.- 6.2 Conditions for Optimality.- 6.3 High Resolution Optimal Companding.- 6.4 Quantizer Design Algorithms.- 6.5 Implementation.- 6.6 Problems.- 7 Predictive Quantization.- 7.1 Introduction.- 7.2 Difference Quantization.- 7.3 Closed-Loop Predictive Quantization.- 7.4 Delta Modulation.- 7.5 Problems.- 8 Bit Allocation and Transform Coding.- 8.1 Introduction.- 8.2 The Problem of Bit Allocation.- 8.3 Optimal Bit Allocation Results.- 8.4 Integer Constrained Allocation Techniques.- 8.5 Transform Coding.- 8.6 Karhunen-Loeve Transform.- 8.7 Performance Gain of Transform Coding.- 8.8 Other Transforms.- 8.9 Sub-band Coding.- 8.10 Problems.- 9 Entropy Coding.- 9.1 Introduction.- 9.2 Variable-Length Scalar Noiseless Coding.- 9.3 Prefix Codes.- 9.4 Huffman Coding.- 9.5 Vector Entropy Coding.- 9.6 Arithmetic Coding.- 9.7 Universal and Adaptive Entropy Coding.- 9.8 Ziv-Lempel Coding.- 9.9 Quantization and Entropy Coding.- 9.10 Problems.- III Vector Coding.- 10 Vector Quantization I.- 10.1 Introduction.- 10.2 Structural Properties and Characterization.- 10.3 Measuring Vector Quantizer Performance.- 10.4 Nearest Neighbor Quantizers.- 10.5 Lattice Vector Quantizers.- 10.6 High Resolution Distortion Approximations.- 10.7 Problems.- 11 Vector Quantization II.- 11.1 Introduction.- 11.2 Optimality Conditions for VQ.- 11.3 Vector Quantizer Design.- 11.4 Design Examples.- 11.5 Problems.- 12 Constrained Vector Quantization.- 12.1 Introduction.- 12.2 Complexity and Storage Limitations.- 12.3 Structurally Constrained VQ.- 12.4 Tree-Structured VQ.- 12.5 Classified VQ.- 12.6 Transform VQ.- 12.7 Product Code Techniques.- 12.8 Partitioned VQ.- 12.9 Mean-Removed VQ.- 12.10 Shape-Gain VQ.- 12.11 Multistage VQ.- 12.12 Constrained Storage VQ.- 12.13 Hierarchical and Multiresolution VQ.- 12.14 Nonlinear Interpolative VQ.- 12.15 Lattice Codebook VQ.- 12.16 Fast Nearest Neighbor Encoding.- 12.17 Problems.- 13 Predictive Vector Quantization.- 13.1 Introduction.- 13.2 Predictive Vector Quantization.- 13.3 Vector Linear Prediction.- 13.4 Predictor Design from Empirical Data.- 13.5 Nonlinear Vector Prediction.- 13.6 Design Examples.- 13.7 Problems.- 14 Finite-State Vector Quantization.- 14.1 Recursive Vector Quantizers.- 14.2 Finite-State Vector Quantizers.- 14.3 Labeled-States and Labeled-Transitions.- 14.4 Encoder/Decoder Design.- 14.5 Next-State Function Design.- 14.6 Design Examples.- 14.7 Problems.- 15 Tree and Trellis Encoding.- 15.1 Delayed Decision Encoder.- 15.2 Tree and Trellis Coding.- 15.3 Decoder Design.- 15.4 Predictive Trellis Encoders.- 15.5 Other Design Techniques.- 15.6 Problems.- 16 Adaptive Vector Quantization.- 16.1 Introduction.- 16.2 Mean Adaptation.- 16.3 Gain-Adaptive Vector Quantization.- 16.4 Switched Codebook Adaptation.- 16.5 Adaptive Bit Allocation.- 16.6 Address VQ.- 16.7 Progressive Code Vector Updating.- 16.8 Adaptive Codebook Generation.- 16.9 Vector Excitation Coding.- 16.10 Problems.- 17 Variable Rate Vector Quantization.- 17.1 Variable Rate Coding.- 17.2 Variable Dimension VQ.- 17.3 Alternative Approaches to Variable Rate VQ.- 17.4 Pruned Tree-Structured VQ.- 17.5 The Generalized BFOS Algorithm.- 17.6 Pruned Tree-Structured VQ.- 17.7 Entropy Coded VQ.- 17.8 Greedy Tree Growing.- 17.9 Design Examples.- 17.10 Bit Allocation Revisited.- 17.11 Design Algorithms.- 17.12 Problems."
            },
            "slug": "Vector-quantization-and-signal-compression-Gersho-Gray",
            "title": {
                "fragments": [],
                "text": "Vector quantization and signal compression"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The author explains the design and implementation of the Levinson-Durbin Algorithm, which automates the very labor-intensive and therefore time-heavy and expensive process of designing and implementing a Quantizer."
            },
            "venue": {
                "fragments": [],
                "text": "The Kluwer international series in engineering and computer science"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118528593"
                        ],
                        "name": "Sheng Chen",
                        "slug": "Sheng-Chen",
                        "structuredName": {
                            "firstName": "Sheng",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sheng Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719377"
                        ],
                        "name": "S. Billings",
                        "slug": "S.-Billings",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Billings",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Billings"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9192861"
                        ],
                        "name": "W. Luo",
                        "slug": "W.-Luo",
                        "structuredName": {
                            "firstName": "Wan",
                            "lastName": "Luo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Luo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 106
                            }
                        ],
                        "text": "The simplest ones are the Matching Pursuit (MP) [12] and the Orthogonal Matching Pursuit (OMP) algorithms [13], [14], [15], [16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7567970,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bce84d14172b25f3844efc0b11507cbc93c049d3",
            "isKey": false,
            "numCitedBy": 1532,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Identification algorithms based on the well-known linear least squares methods of gaussian elimination, Cholesky decomposition, classical Gram-Schmidt, modified Gram-Schmidt, Householder transformation, Givens method, and singular value decomposition are reviewed. The classical Gram-Schmidt, modified Gram-Schmidt, and Householder transformation algorithms are then extended to combine structure determination, or which terms to include in the model, and parameter estimation in a very simple and efficient manner for a class of multivariate discrete-time non-linear stochastic systems which are linear in the parameters."
            },
            "slug": "Orthogonal-least-squares-methods-and-their-to-Chen-Billings",
            "title": {
                "fragments": [],
                "text": "Orthogonal least squares methods and their application to non-linear system identification"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "Identification algorithms based on the well-known linear least squares methods of gaussian elimination, Cholesky decomposition, classical Gram-Schmidt, modified Gram- Schmidt, Householder transformation, Givens method, and singular value decomposition are reviewed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709392"
                        ],
                        "name": "D. Donoho",
                        "slug": "D.-Donoho",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Donoho",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Donoho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2364792"
                        ],
                        "name": "I. Johnstone",
                        "slug": "I.-Johnstone",
                        "structuredName": {
                            "firstName": "Iain",
                            "lastName": "Johnstone",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Johnstone"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 163
                            }
                        ],
                        "text": "In denoising, wavelet methods and shift-invariant variations that exploit overcomplete representation, are among the most effective known algorithms for this task [2], [3], [4], [5]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17008380,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a6ff4e21143fbb73192bfb9a474f1f15a2f5016e",
            "isKey": false,
            "numCitedBy": 373,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Nous decrivons une extension de la methode de la \u00abmeilleure base\u00bb qui permet de selectionner, a partir de donnees bruitees, une base orthonormee dans laquelle le debruitage est d'efficacite presque ideale"
            },
            "slug": "Ideal-denoising-in-an-orthonormal-basis-chosen-from-Donoho-Johnstone",
            "title": {
                "fragments": [],
                "text": "Ideal denoising in an orthonormal basis chosen from a library of bases"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722764"
                        ],
                        "name": "M. Marcellin",
                        "slug": "M.-Marcellin",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Marcellin",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marcellin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729125"
                        ],
                        "name": "M. Gormish",
                        "slug": "M.-Gormish",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Gormish",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gormish"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145728646"
                        ],
                        "name": "A. Bilgin",
                        "slug": "A.-Bilgin",
                        "structuredName": {
                            "firstName": "Ali",
                            "lastName": "Bilgin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bilgin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2704683"
                        ],
                        "name": "M. Boliek",
                        "slug": "M.-Boliek",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Boliek",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Boliek"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 132
                            }
                        ],
                        "text": "Indeed, the success of the JPEG2000 coding standard can be attributed to the sparsity of the wavelet coefficients of natural images [1]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1665683,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "19b7acd9b98e064e0704e2cc5760e1199fb4f785",
            "isKey": false,
            "numCitedBy": 430,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "JPEG-2000 is an emerging standard for still image compression. This paper provides a brief history of the JPEG-2000 standardization process, an overview of the standard, and some description of the capabilities provided by the standard. Part I of the JPEG-2000 standard specifies the minimum compliant decoder, while Part II describes optional, value-added extensions. Although the standard specifies only the decoder and bitstream syntax, in this paper we describe JPEG-2000 from the point of view of encoding. We take this approach, as we believe it is more amenable to a compact description more easily understood by most readers."
            },
            "slug": "An-overview-of-JPEG-2000-Marcellin-Gormish",
            "title": {
                "fragments": [],
                "text": "An overview of JPEG-2000"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper describes JPEG-2000 from the point of view of encoding, as it believes it is more amenable to a compact description more easily understood by most readers."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings DCC 2000. Data Compression Conference"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35043531"
                        ],
                        "name": "A. Dempster",
                        "slug": "A.-Dempster",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Dempster",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dempster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7890796"
                        ],
                        "name": "N. Laird",
                        "slug": "N.-Laird",
                        "structuredName": {
                            "firstName": "Nan",
                            "lastName": "Laird",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Laird"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235217"
                        ],
                        "name": "D. Rubin",
                        "slug": "D.-Rubin",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Rubin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rubin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 249,
                                "start": 245
                            }
                        ],
                        "text": "As a side note we remind the reader that based on the expectation maximization procedure, the K-Means can be extended to suggest a fuzzy assignment and a covariance matrix per each cluster, so that the data is modelled as a mixture of Gaussians [47]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4193919,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "d36efb9ad91e00faa334b549ce989bfae7e2907a",
            "isKey": false,
            "numCitedBy": 48406,
            "numCiting": 134,
            "paperAbstract": {
                "fragments": [],
                "text": "Vibratory power unit for vibrating conveyers and screens comprising an asynchronous polyphase motor, at least one pair of associated unbalanced masses disposed on the shaft of said motor, with the first mass of a pair of said unbalanced masses being rigidly fastened to said shaft and with said second mass of said pair being movably arranged relative to said first mass, means for controlling and regulating the conveying rate during conveyer operation by varying the rotational speed of said motor between predetermined minimum and maximum values, said second mass being movably outwardly by centrifugal force against the pressure of spring means, said spring means being prestressed in such a manner that said second mass is, at rotational motor speeds lower than said minimum speed, held in its initial position, and at motor speeds between said lower and upper values in positions which are radially offset with respect to the axis of said motor to an extent depending on the value of said rotational motor speed."
            },
            "slug": "Maximum-likelihood-from-incomplete-data-via-the-EM-Dempster-Laird",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788107"
                        ],
                        "name": "J. Tropp",
                        "slug": "J.-Tropp",
                        "structuredName": {
                            "firstName": "Joel",
                            "lastName": "Tropp",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tropp"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 77
                            }
                        ],
                        "text": "This connection has previously been mentioned in several reports [36], [37], [38]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 120495185,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "26ffce3a8bc1c069c3c9819b2ca6690bb0023fc6",
            "isKey": false,
            "numCitedBy": 146,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Topics-in-sparse-approximation-Tropp",
            "title": {
                "fragments": [],
                "text": "Topics in sparse approximation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 8
                            }
                        ],
                        "text": "We analyze this algorithm and demonstrate its results on both synthetic tests and in applications on real image data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Translation invariant denoising. Wavelets and Statistics"
            },
            "venue": {
                "fragments": [],
                "text": "Lecture Notes in Statistics"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 98
                            }
                        ],
                        "text": "Sparsity and overcompleteness have been successfully used for dynamic range compression in images [6], separation of texture and cartoon content in images [7], [8], inpainting [9], and more."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dynamic range compression : a new method based on wavelet transform"
            },
            "venue": {
                "fragments": [],
                "text": "Astronomical Data Analysis Software and Systems Conference,"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 193
                            }
                        ],
                        "text": "Extensive study of these algorithms in recent years has established that if the sought solution, x, is sparse enough, these techniques recover it well in the exact case [16], [26], [27], [28], [29], [30]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sparse decompositions in unions of bases"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Information Theory, 49(12):3320\u20133325,"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Aase , and J . H . Hus \u03c6 y . Multi - frame compression : Theory and design"
            },
            "venue": {
                "fragments": [],
                "text": "EURASIP Signal Processing"
            },
            "year": 2000
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 27,
            "methodology": 28,
            "result": 4
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 49,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/K-SVD-:-An-Algorithm-for-Designing-of-Overcomplete-Aharon-Elad/f6e0fb4c77906bc23fe59a8f848ce62ba9687181?sort=total-citations"
}