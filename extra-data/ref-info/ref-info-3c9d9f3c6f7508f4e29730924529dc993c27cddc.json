{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695463"
                        ],
                        "name": "D. Marcu",
                        "slug": "D.-Marcu",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Marcu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Marcu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722360"
                        ],
                        "name": "Hal Daum\u00e9",
                        "slug": "Hal-Daum\u00e9",
                        "structuredName": {
                            "firstName": "Hal",
                            "lastName": "Daum\u00e9",
                            "middleNames": [],
                            "suffix": "III"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hal Daum\u00e9"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6067840,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e1d77997b84c56a4d1865f4386a04c820fd1491a",
            "isKey": false,
            "numCitedBy": 116,
            "numCiting": 230,
            "paperAbstract": {
                "fragments": [],
                "text": "Natural language processing is replete with problems whose outputs are highly complex and structured. The current state-of-the-art in machine learning is not yet sufficiently general to be applied to general problems in NLP. In this thesis, I present Searn (for \u201csearch-learn\u201d), an approach to learning for structured outputs that is applicable to the wide variety of problems encountered in natural language (and, hopefully, to problems in other domains, such as vision and biology). To demonstrate Searn\u2019s general applicability, I present applications in such diverse areas as automatic document summarization and entity detection and tracking. In these applications, Searn is empirically shown to achieve state-of-the-art performance. \nSearn is based on an integration of learning and search. This contrasts with standard approaches that define a model, learn parameters for that model, and then use the model and the learned parameters to produce new outputs. In most NLP problems, the \u201cproduce new outputs\u201d step includes an intractable computation. One must therefore employ a heuristic search function for the production step. Instead of shying away from search, Searn attacks it head on and considers structured prediction to be defined by a search problem. The corresponding learning problem is then made natural: learn parameters so that search succeeds. \nThe two application domains I study most closely in this thesis are entity detection and tracking (EDT) and automatic document summarization. EDT is the problem of finding all references to people, places and organizations in a document and identifying their relationships. Summarization is the task of producing a short summary for either a single document or for a collection of documents. These problems exhibit complex structure that cannot be captured and exploited using previously proposed structured prediction algorithms. By applying Searn to these problems, I am able to learn models that benefit from complex, non-local features of both the input and the output. Such features would not be available to structured prediction algorithm that require model tractability. These improvements lead to state-of-the-art performance on standardized data sets with low computational overhead. \nSearn operates by transforming structured prediction problems into a collection of classification problems, to which any standard binary classifier may be applied (for instance, a support vector machine or decision tree). In fact, Searn represents a family of structured prediction algorithms depending on the classifier and search space used. From a theoretical perspective, Searn satisfies a strong fundamental performance guarantee: given a good classification algorithm, Searn yields a good structured prediction algorithm. Such theoretical results are possible for other structured prediction only when the underlying model is tractable. For Searn, I am able to state strong results that are independent of the size or tractability of the search space. This provides theoretical justification for integrating search with learning."
            },
            "slug": "Practical-structured-learning-techniques-for-Marcu-Daum\u00e9",
            "title": {
                "fragments": [],
                "text": "Practical structured learning techniques for natural language processing"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This thesis presents Searn (for \u201csearch-learn\u201d), an approach to learning for structured outputs that is applicable to the wide variety of problems encountered in natural language (and, hopefully, to problems in other domains, such as vision and biology)."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722360"
                        ],
                        "name": "Hal Daum\u00e9",
                        "slug": "Hal-Daum\u00e9",
                        "structuredName": {
                            "firstName": "Hal",
                            "lastName": "Daum\u00e9",
                            "middleNames": [],
                            "suffix": "III"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hal Daum\u00e9"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695463"
                        ],
                        "name": "D. Marcu",
                        "slug": "D.-Marcu",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Marcu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Marcu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 781,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a5c48c673b0d3152010e3374cac189314a13df10",
            "isKey": false,
            "numCitedBy": 279,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Mappings to structured output spaces (strings, trees, partitions, etc.) are typically learned using extensions of classification algorithms to simple graphical structures (eg., linear chains) in which search and parameter estimation can be performed exactly. Unfortunately, in many complex problems, it is rare that exact search or parameter estimation is tractable. Instead of learning exact models and searching via heuristic means, we embrace this difficulty and treat the structured output problem in terms of approximate search. We present a framework for learning as search optimization, and two parameter updates with convergence the-orems and bounds. Empirical evidence shows that our integrated approach to learning and decoding can outperform exact models at smaller computational cost."
            },
            "slug": "Learning-as-search-optimization:-approximate-large-Daum\u00e9-Marcu",
            "title": {
                "fragments": [],
                "text": "Learning as search optimization: approximate large margin methods for structured prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work presents a framework for learning as search optimization, and two parameter updates with convergence the-orems and bounds, and shows that this integrated approach to learning and decoding can outperform exact models at smaller computational cost."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39839719"
                        ],
                        "name": "Libin Shen",
                        "slug": "Libin-Shen",
                        "structuredName": {
                            "firstName": "Libin",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Libin Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152299194"
                        ],
                        "name": "G. Satta",
                        "slug": "G.-Satta",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Satta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Satta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714374"
                        ],
                        "name": "A. Joshi",
                        "slug": "A.-Joshi",
                        "structuredName": {
                            "firstName": "Aravind",
                            "lastName": "Joshi",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Joshi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 52
                            }
                        ],
                        "text": "One answer, akin to the learned search order method (Shen et al. 2007) would be to simply allow"
                    },
                    "intents": []
                }
            ],
            "corpusId": 15876808,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6503a3d9fb204c2a08ecfcfe6ba5b815fc65a030",
            "isKey": false,
            "numCitedBy": 200,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose guided learning, a new learning framework for bidirectional sequence classification. The tasks of learning the order of inference and training the local classifier are dynamically incorporated into a single Perceptron like learning algorithm. We apply this novel learning algorithm to POS tagging. It obtains an error rate of 2.67% on the standard PTB test set, which represents 3.3% relative error reduction over the previous best result on the same data set, while using fewer features."
            },
            "slug": "Guided-Learning-for-Bidirectional-Sequence-Shen-Satta",
            "title": {
                "fragments": [],
                "text": "Guided Learning for Bidirectional Sequence Classification"
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685978"
                        ],
                        "name": "B. Taskar",
                        "slug": "B.-Taskar",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Taskar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Taskar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2482074"
                        ],
                        "name": "Vassil Chatalbashev",
                        "slug": "Vassil-Chatalbashev",
                        "structuredName": {
                            "firstName": "Vassil",
                            "lastName": "Chatalbashev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vassil Chatalbashev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730156"
                        ],
                        "name": "Carlos Guestrin",
                        "slug": "Carlos-Guestrin",
                        "structuredName": {
                            "firstName": "Carlos",
                            "lastName": "Guestrin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carlos Guestrin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 55
                            }
                        ],
                        "text": "\u201d We use the max-margin Markov network (M3N) formalism (Taskar et al. 2005)"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 41
                            }
                        ],
                        "text": "to compute a loss-augmented minimization (Taskar et al. 2005)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 201978,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8d56b2a75aa5624660b60787e1f38ee2c70d493a",
            "isKey": false,
            "numCitedBy": 558,
            "numCiting": 124,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider large margin estimation in a broad range of prediction models where inference involves solving combinatorial optimization problems, for example, weighted graph-cuts or matchings. Our goal is to learn parameters such that inference using the model reproduces correct answers on the training data. Our method relies on the expressive power of convex optimization problems to compactly capture inference or solution optimality in structured prediction models. Directly embedding this structure within the learning formulation produces concise convex problems for efficient estimation of very complex and diverse models. We describe experimental results on a matching task, disulfide connectivity prediction, showing significant improvements over state-of-the-art methods."
            },
            "slug": "Learning-structured-prediction-models:-a-large-Taskar-Chatalbashev",
            "title": {
                "fragments": [],
                "text": "Learning structured prediction models: a large margin approach"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "This work considers large margin estimation in a broad range of prediction models where inference involves solving combinatorial optimization problems, for example, weighted graph-cuts or matchings, and relies on the expressive power of convex optimization problems to compactly capture inference or solution optimality in structured prediction models."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765700"
                        ],
                        "name": "Ioannis Tsochantaridis",
                        "slug": "Ioannis-Tsochantaridis",
                        "structuredName": {
                            "firstName": "Ioannis",
                            "lastName": "Tsochantaridis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ioannis Tsochantaridis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143936663"
                        ],
                        "name": "Thomas Hofmann",
                        "slug": "Thomas-Hofmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hofmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Hofmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783941"
                        ],
                        "name": "Y. Altun",
                        "slug": "Y.-Altun",
                        "structuredName": {
                            "firstName": "Yasemin",
                            "lastName": "Altun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Altun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 70
                            }
                        ],
                        "text": "A 300-sentence subset of the training data set was previously used by (Tsochantaridis et al. 2005) for evaluating the SVMstruct framework in the context of sequence labeling."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 45
                            }
                        ],
                        "text": "There are, of course, alternative frameworks (see, e.g., Weston et al. 2002; McAllester et al. 2004; Altun et al. 2004; McDonald et al. 2004; Tsochantaridis et al. 2005), but these are common examples."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17671150,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dc97e7dbb821a4edfb5151bff4352655eedca9ee",
            "isKey": false,
            "numCitedBy": 2247,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning general functional dependencies between arbitrary input and output spaces is one of the key challenges in computational intelligence. While recent progress in machine learning has mainly focused on designing flexible and powerful input representations, this paper addresses the complementary issue of designing classification algorithms that can deal with more complex outputs, such as trees, sequences, or sets. More generally, we consider problems involving multiple dependent output variables, structured output spaces, and classification problems with class attributes. In order to accomplish this, we propose to appropriately generalize the well-known notion of a separation margin and derive a corresponding maximum-margin formulation. While this leads to a quadratic program with a potentially prohibitive, i.e. exponential, number of constraints, we present a cutting plane algorithm that solves the optimization problem in polynomial time for a large class of problems. The proposed method has important applications in areas such as computational biology, natural language processing, information retrieval/extraction, and optical character recognition. Experiments from various domains involving different types of output spaces emphasize the breadth and generality of our approach."
            },
            "slug": "Large-Margin-Methods-for-Structured-and-Output-Tsochantaridis-Joachims",
            "title": {
                "fragments": [],
                "text": "Large Margin Methods for Structured and Interdependent Output Variables"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper proposes to appropriately generalize the well-known notion of a separation margin and derive a corresponding maximum-margin formulation and presents a cutting plane algorithm that solves the optimization problem in polynomial time for a large class of problems."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783941"
                        ],
                        "name": "Y. Altun",
                        "slug": "Y.-Altun",
                        "structuredName": {
                            "firstName": "Yasemin",
                            "lastName": "Altun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Altun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143936663"
                        ],
                        "name": "Thomas Hofmann",
                        "slug": "Thomas-Hofmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hofmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Hofmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 45
                            }
                        ],
                        "text": "There are, of course, alternative frameworks (see, e.g., Weston et al. 2002; McAllester et al. 2004; Altun et al. 2004; McDonald et al. 2004; Tsochantaridis et al. 2005), but these are common examples."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8403013,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a2182e5a37f5fc04ce23bd2f4d6b5070382c8c5e",
            "isKey": false,
            "numCitedBy": 79,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Many real-world classification tasks involve the prediction of multiple, inter-dependent class labels. A prototypical case of this sort deals with prediction of a sequence of labels for a sequence of observations. Such problems arise naturally in the context of annotating and segmenting observation sequences. This paper generalizes Gaussian Process classification to predict multiple labels by taking dependencies between neighboring labels into account. Our approach is motivated by the desire to retain rigorous probabilistic semantics, while overcoming limitations of parametric methods like Conditional Random Fields, which exhibit conceptual and computational difficulties in high-dimensional input spaces. Experiments on named entity recognition and pitch accent prediction tasks demonstrate the competitiveness of our approach."
            },
            "slug": "Gaussian-process-classification-for-segmenting-and-Altun-Hofmann",
            "title": {
                "fragments": [],
                "text": "Gaussian process classification for segmenting and annotating sequences"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "Gaussian Process classification is generalized to predict multiple labels by taking dependencies between neighboring labels into account, motivated by the desire to retain rigorous probabilistic semantics, while overcoming limitations of parametric methods like Conditional Random Fields."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143946906"
                        ],
                        "name": "Yoshimasa Tsuruoka",
                        "slug": "Yoshimasa-Tsuruoka",
                        "structuredName": {
                            "firstName": "Yoshimasa",
                            "lastName": "Tsuruoka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshimasa Tsuruoka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737901"
                        ],
                        "name": "Junichi Tsujii",
                        "slug": "Junichi-Tsujii",
                        "structuredName": {
                            "firstName": "Junichi",
                            "lastName": "Tsujii",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Junichi Tsujii"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 182
                            }
                        ],
                        "text": "\u201d For certain sequence labeling tasks, it has been observed that ordering the search based on the entropy of the label distribution of certain words can lead to improved performance (Tsuruoka and Tsujii 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5882669,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e85ea7311e8fcc6c0a603ca39a74b99109e185e9",
            "isKey": false,
            "numCitedBy": 278,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a bidirectional inference algorithm for sequence labeling problems such as part-of-speech tagging, named entity recognition and text chunking. The algorithm can enumerate all possible decomposition structures and find the highest probability sequence together with the corresponding decomposition structure in polynomial time. We also present an efficient decoding algorithm based on the easiest-first strategy, which gives comparably good performance to full bidirectional inference with significantly lower computational cost. Experimental results of part-of-speech tagging and text chunking show that the proposed bidirectional inference methods consistently outperform unidirectional inference methods and bidirectional MEMMs give comparable performance to that achieved by state-of-the-art learning algorithms including kernel support vector machines."
            },
            "slug": "Bidirectional-Inference-with-the-Easiest-First-for-Tsuruoka-Tsujii",
            "title": {
                "fragments": [],
                "text": "Bidirectional Inference with the Easiest-First Strategy for Tagging Sequence Data"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "This paper presents a bidirectional inference algorithm for sequence labeling problems such as part-of-speech tagging, named entity recognition and text chunking that can enumerate all possible decomposition structures and find the highest probability sequence together with the corresponding decomposition structure in polynomial time."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38070424"
                        ],
                        "name": "R. Ando",
                        "slug": "R.-Ando",
                        "structuredName": {
                            "firstName": "Rie",
                            "lastName": "Ando",
                            "middleNames": [
                                "Kubota"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ando"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117881943"
                        ],
                        "name": "Tong Zhang",
                        "slug": "Tong-Zhang",
                        "structuredName": {
                            "firstName": "Tong",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tong Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 69
                            }
                        ],
                        "text": "The best reported results to date using the full data set are due to (Ando and Zhang 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 62
                            }
                        ],
                        "text": "This feature set is fairly standard in the literature, though (Ando and Zhang 2005) report significantly improved results using a much larger set of features."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 13650160,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "944e1a7b2c5c62e952418d7684e3cade89c76f87",
            "isKey": false,
            "numCitedBy": 1414,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the most important issues in machine learning is whether one can improve the performance of a supervised learning algorithm by including unlabeled data. Methods that use both labeled and unlabeled data are generally referred to as semi-supervised learning. Although a number of such methods are proposed, at the current stage, we still don't have a complete understanding of their effectiveness. This paper investigates a closely related problem, which leads to a novel approach to semi-supervised learning. Specifically we consider learning predictive structures on hypothesis spaces (that is, what kind of classifiers have good predictive power) from multiple learning tasks. We present a general framework in which the structural learning problem can be formulated and analyzed theoretically, and relate it to learning with unlabeled data. Under this framework, algorithms for structural learning will be proposed, and computational issues will be investigated. Experiments will be given to demonstrate the effectiveness of the proposed algorithms in the semi-supervised learning setting."
            },
            "slug": "A-Framework-for-Learning-Predictive-Structures-from-Ando-Zhang",
            "title": {
                "fragments": [],
                "text": "A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper presents a general framework in which the structural learning problem can be formulated and analyzed theoretically, and relate it to learning with unlabeled data, and algorithms for structural learning will be proposed, and computational issues will be investigated."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143957226"
                        ],
                        "name": "Ryan T. McDonald",
                        "slug": "Ryan-T.-McDonald",
                        "structuredName": {
                            "firstName": "Ryan",
                            "lastName": "McDonald",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ryan T. McDonald"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3014198,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f56405a2e323af6dcbbc0a94c08ecc9f94740242",
            "isKey": false,
            "numCitedBy": 175,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a model for sentence compression that uses a discriminative largemargin learning framework coupled with a novel feature set defined on compressed bigrams as well as deep syntactic representations provided by auxiliary dependency and phrase-structure parsers. The parsers are trained out-of-domain and contain a significant amount of noise. We argue that the discriminative nature of the learning algorithm allows the model to learn weights relative to any noise in the feature set to optimize compression accuracy directly. This differs from current state-of-the-art models (Knight and Marcu, 2000) that treat noisy parse trees, for both compressed and uncompressed sentences, as gold standard when calculating model parameters."
            },
            "slug": "Discriminative-Sentence-Compression-with-Soft-McDonald",
            "title": {
                "fragments": [],
                "text": "Discriminative Sentence Compression with Soft Syntactic Evidence"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "A model for sentence compression that uses a discriminative largemargin learning framework coupled with a novel feature set defined on compressed bigrams as well as deep syntactic representations provided by auxiliary dependency and phrase-structure parsers is presented."
            },
            "venue": {
                "fragments": [],
                "text": "EACL"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37210858"
                        ],
                        "name": "Charles Sutton",
                        "slug": "Charles-Sutton",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Sutton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles Sutton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2090805570"
                        ],
                        "name": "Michael Sindelar",
                        "slug": "Michael-Sindelar",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Sindelar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Sindelar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "n transitions. In the results shown later in this section, some of the algorithms use a slightly di\ufb00erent feature set. In particular, the CRF-based model uses similar, but not identical features; see [50] for details. 6.1.4 Joint Chunking and Tagging In the preceding sections, we considered the single sequence labeling task: to each element in a sequence, a single label is assigned. In this section, w"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13020788,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "643e3a133916e2affb0181e1e35c910c123b7f9d",
            "isKey": true,
            "numCitedBy": 28,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Discriminatively-trained probabilistic models are very popular in NLP because of the latitude they afford in designing features. But training involves complex trade-offs among weights, whichcanbedangerous: afew highlyindicative features can swamp the contribution of many individually weaker features, causing their weights to be undertrained. Such a model is less robust, for the highly-indicative features may be noisy or missing in the test data. To ameliorate this weight undertraining, we propose a new training method, called feature bagging, in which separate models are trained on subsets of the original features, and combined using a mixture model or a product of experts. We evaluate feature bagging on linear-chain conditional random fields for two natural-language tasks. On both tasks, the feature-bagged CRF performs better than simply training a single CRF on all the features."
            },
            "slug": "Feature-Bagging:-Preventing-Weight-Undertraining-in-Sutton-Sindelar",
            "title": {
                "fragments": [],
                "text": "Feature Bagging: Preventing Weight Undertraining in Structured Discriminative Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work proposes a new training method, called feature bagging, in which separate models are trained on subsets of the original features, and combined using a mixture model or a product of experts, which performs better than simply training a single CRF on all the features."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794100"
                        ],
                        "name": "Brian Roark",
                        "slug": "Brian-Roark",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Roark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brian Roark"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "orrect output. As in the standard perceptron, this often leads to a learned model that generalizes poorly. As before, one solution to this problem is weight averaging [18]. The incremental perceptron [9] is a variant on the structured perceptron that deals with the issue that the argmax may not be analytically available. The idea ofthe incrementalperceptronis to replacethe argmax with a beam search a"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10366378,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "41828fc3dab24784f95e6976e8aaa73f68e1840e",
            "isKey": false,
            "numCitedBy": 448,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes an incremental parsing approach where parameters are estimated using a variant of the perceptron algorithm. A beam-search algorithm is used during both training and decoding phases of the method. The perceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn treebank. We demonstrate that training a perceptron model to combine with the generative model during search provides a 2.1 percent F-measure improvement over the generative model alone, to 88.8 percent."
            },
            "slug": "Incremental-Parsing-with-the-Perceptron-Algorithm-Collins-Roark",
            "title": {
                "fragments": [],
                "text": "Incremental Parsing with the Perceptron Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is demonstrated that training a perceptron model to combine with the generative model during search provides a 2.1 percent F-measure improvement over the Generative model alone, to 88.8 percent."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2624289"
                        ],
                        "name": "A. Beygelzimer",
                        "slug": "A.-Beygelzimer",
                        "structuredName": {
                            "firstName": "Alina",
                            "lastName": "Beygelzimer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Beygelzimer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2714811"
                        ],
                        "name": "Varsha Dani",
                        "slug": "Varsha-Dani",
                        "structuredName": {
                            "firstName": "Varsha",
                            "lastName": "Dani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Varsha Dani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1806117"
                        ],
                        "name": "Thomas P. Hayes",
                        "slug": "Thomas-P.-Hayes",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hayes",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas P. Hayes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144162125"
                        ],
                        "name": "J. Langford",
                        "slug": "J.-Langford",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Langford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Langford"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735228"
                        ],
                        "name": "B. Zadrozny",
                        "slug": "B.-Zadrozny",
                        "structuredName": {
                            "firstName": "Bianca",
                            "lastName": "Zadrozny",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Zadrozny"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 70
                            }
                        ],
                        "text": "A reduction of cost sensitive classification to binary classification (Beygelzimer et al. 2005) reduces the requirement to an importance-weighted binary learning algorithm."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6478578,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1521e475bc6d8ed07c95a46ec099377a8584ba7d",
            "isKey": false,
            "numCitedBy": 83,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a reduction-based model for analyzing supervised learning tasks. We use this model to devise a new reduction from multi-class cost-sensitive classification to binary classification with the following guarantee: If the learned binary classifier has error rate at most \u03b5 then the cost-sensitive classifier has cost at most 2\u03b5 times the expected sum of costs of all possible lables. Since cost-sensitive classification can embed any bounded loss finite choice supervised learning task, this result shows that any such task can be solved using a binary classification oracle. Finally, we present experimental results showing that our new reduction outperforms existing algorithms for multi-class cost-sensitive learning."
            },
            "slug": "Error-limiting-reductions-between-classification-Beygelzimer-Dani",
            "title": {
                "fragments": [],
                "text": "Error limiting reductions between classification tasks"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "This work introduces a reduction-based model for analyzing supervised learning tasks and designs a new reduction from multi-class cost-sensitive classification to binary classification with the following guarantee: if the learned binary classifier has error rate at most \u03b5 then the cost- sensitive classifiers has cost at most 2\u03b5 times the expected sum of costs of all possible lables."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145107462"
                        ],
                        "name": "Stuart J. Russell",
                        "slug": "Stuart-J.-Russell",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Russell",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stuart J. Russell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2784519"
                        ],
                        "name": "Peter Norvig",
                        "slug": "Peter-Norvig",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Norvig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Norvig"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 30
                            }
                        ],
                        "text": "In a general search algorithm (Russell and Norvig 1995), one maintains a queue of active states and expands a single state in each search step."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 53142908,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3524cdf7cf8344e7eb74886f71fcbb5c6732c337",
            "isKey": false,
            "numCitedBy": 26735,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "The long-anticipated revision of this #1 selling book offers the most comprehensive, state of the art introduction to the theory and practice of artificial intelligence for modern applications. Intelligent Agents. Solving Problems by Searching. Informed Search Methods. Game Playing. Agents that Reason Logically. First-order Logic. Building a Knowledge Base. Inference in First-Order Logic. Logical Reasoning Systems. Practical Planning. Planning and Acting. Uncertainty. Probabilistic Reasoning Systems. Making Simple Decisions. Making Complex Decisions. Learning from Observations. Learning with Neural Networks. Reinforcement Learning. Knowledge in Learning. Agents that Communicate. Practical Communication in English. Perception. Robotics. For computer professionals, linguists, and cognitive scientists interested in artificial intelligence."
            },
            "slug": "Artificial-Intelligence:-A-Modern-Approach-Russell-Norvig",
            "title": {
                "fragments": [],
                "text": "Artificial Intelligence: A Modern Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The long-anticipated revision of this #1 selling book offers the most comprehensive, state of the art introduction to the theory and practice of artificial intelligence for modern applications."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765329"
                        ],
                        "name": "Taku Kudo",
                        "slug": "Taku-Kudo",
                        "structuredName": {
                            "firstName": "Taku",
                            "lastName": "Kudo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Taku Kudo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681502"
                        ],
                        "name": "Yuji Matsumoto",
                        "slug": "Yuji-Matsumoto",
                        "structuredName": {
                            "firstName": "Yuji",
                            "lastName": "Matsumoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuji Matsumoto"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "d by enforcement of membership in Y\u00a7 constraints [45]. The second variety is typi\ufb01ed by maximum entropy Markov models [37], though the basic idea of MEMMs has also been applied more generally to SVMs [27,28,20]. In this variety, the elements in the prediction vector are made sequentially, with the nth element conditional on outputs n\u2212k...n\u22121 for a kth order model. In the purely independent classi\ufb01er setting"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6165849,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6fe1db3f50a528b60c2cfc8f60c33073e07e031d",
            "isKey": false,
            "numCitedBy": 230,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Kernel-based learning (e.g., Support Vector Machines) has been successfully applied to many hard problems in Natural Language Processing (NLP). In NLP, although feature combinations are crucial to improving performance, they are heuristically selected. Kernel methods change this situation. The merit of the kernel methods is that effective feature combination is implicitly expanded without loss of generality and increasing the computational costs. Kernel-based text analysis shows an excellent performance in terms in accuracy; however, these methods are usually too slow to apply to large-scale text analysis. In this paper, we extend a Basket Mining algorithm to convert a kernel-based classifier into a simple and fast linear classifier. Experimental results on English BaseNP Chunking, Japanese Word Segmentation and Japanese Dependency Parsing show that our new classifiers are about 30 to 300 times faster than the standard kernel-based classifiers."
            },
            "slug": "Fast-Methods-for-Kernel-Based-Text-Analysis-Kudo-Matsumoto",
            "title": {
                "fragments": [],
                "text": "Fast Methods for Kernel-Based Text Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A Basket Mining algorithm is extended to convert a kernel-based classifier into a simple and fast linear classifier, showing results that show that these new classifiers are about 30 to 300 times faster than the standard kernel- based classifiers."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2075292388"
                        ],
                        "name": "P. Liang",
                        "slug": "P.-Liang",
                        "structuredName": {
                            "firstName": "Percy",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Liang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398531195"
                        ],
                        "name": "A. Bouchard-C\u00f4t\u00e9",
                        "slug": "A.-Bouchard-C\u00f4t\u00e9",
                        "structuredName": {
                            "firstName": "Alexandre",
                            "lastName": "Bouchard-C\u00f4t\u00e9",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bouchard-C\u00f4t\u00e9"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38666915"
                        ],
                        "name": "D. Klein",
                        "slug": "D.-Klein",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Klein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Klein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685978"
                        ],
                        "name": "B. Taskar",
                        "slug": "B.-Taskar",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Taskar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Taskar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 107
                            }
                        ],
                        "text": "Similar recent algorithms such a decision-tree-based parsing [55] and perceptron-based machine translation [32] can also be seen as running a (slightly modified) first iteration of Searn."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1391785,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "10d21ca7728cb3dd15731accedda9ea711d8a0f4",
            "isKey": false,
            "numCitedBy": 317,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a perceptron-style discriminative approach to machine translation in which large feature sets can be exploited. Unlike discriminative reranking approaches, our system can take advantage of learned features in all stages of decoding. We first discuss several challenges to error-driven discriminative approaches. In particular, we explore different ways of updating parameters given a training example. We find that making frequent but smaller updates is preferable to making fewer but larger updates. Then, we discuss an array of features and show both how they quantitatively increase BLEU score and how they qualitatively interact on specific examples. One particular feature we investigate is a novel way to introduce learning into the initial phrase extraction process, which has previously been entirely heuristic."
            },
            "slug": "An-End-to-End-Discriminative-Approach-to-Machine-Liang-Bouchard-C\u00f4t\u00e9",
            "title": {
                "fragments": [],
                "text": "An End-to-End Discriminative Approach to Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "A perceptron-style discriminative approach to machine translation in which large feature sets can be exploited and a novel way to introduce learning into the initial phrase extraction process, which has previously been entirely heuristic."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35153517"
                        ],
                        "name": "D. Lewis",
                        "slug": "D.-Lewis",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lewis",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lewis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Using a weighted Hamming loss cannot completely alleviate this problem, for essentially the same reasons that a weighted zero-one loss cannot optimize F1 measure in binary classification, though one can often achieve an approximation ( Lewis 2001 ; M usicant et al.2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 761952,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4a7d40ba2e553ab481526b2b6e6df5bd2cd9952f",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Given the large training set available for batch filtering, choosing a supervised learning algorithm that would make effective use of this data was critical. The support vector machine approach (SVM) to training linear classifiers has outperformed competing approaches in a number of recent text categorization studies, particularly for categories with substantial numbers of positive training examples. SVMs require little or no feature selection, since they avoid overfitting by optimizing a margin-based criterion rather than one based on number of features. This minimizes the complexity of the software and processing. Finally, Thorsten Joachims has made publicly available an efficient implementation of SVMs, SVM_Light [Joachims 1999]: http://www.joachims.org/svm_light/"
            },
            "slug": "Applying-Support-Vector-Machines-to-the-TREC-2001-Lewis",
            "title": {
                "fragments": [],
                "text": "Applying Support Vector Machines to the TREC-2001 Batch Filtering and Routing Tasks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The support vector machine approach (SVM) to training linear classifiers has outperformed competing approaches in a number of recent text categorization studies, particularly for categories with substantial numbers of positive training examples."
            },
            "venue": {
                "fragments": [],
                "text": "TREC"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "13693897"
                        ],
                        "name": "Nathan D. Ratliff",
                        "slug": "Nathan-D.-Ratliff",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Ratliff",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nathan D. Ratliff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756566"
                        ],
                        "name": "J. Bagnell",
                        "slug": "J.-Bagnell",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Bagnell",
                            "middleNames": [
                                "Andrew"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bagnell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8195063"
                        ],
                        "name": "Martin A. Zinkevich",
                        "slug": "Martin-A.-Zinkevich",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Zinkevich",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martin A. Zinkevich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1044953,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "117a50fbdfd473e43e550c6103733e6cb4aecb4c",
            "isKey": false,
            "numCitedBy": 628,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Imitation learning of sequential, goal-directed behavior by standard supervised techniques is often difficult. We frame learning such behaviors as a maximum margin structured prediction problem over a space of policies. In this approach, we learn mappings from features to cost so an optimal policy in an MDP with these cost mimics the expert's behavior. Further, we demonstrate a simple, provably efficient approach to structured maximum margin learning, based on the subgradient method, that leverages existing fast algorithms for inference. Although the technique is general, it is particularly relevant in problems where A* and dynamic programming approaches make learning policies tractable in problems beyond the limitations of a QP formulation. We demonstrate our approach applied to route planning for outdoor mobile robots, where the behavior a designer wishes a planner to execute is often clear, while specifying cost functions that engender this behavior is a much more difficult task."
            },
            "slug": "Maximum-margin-planning-Ratliff-Bagnell",
            "title": {
                "fragments": [],
                "text": "Maximum margin planning"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work learns mappings from features to cost so an optimal policy in an MDP with these cost mimics the expert's behavior, and demonstrates a simple, provably efficient approach to structured maximum margin learning, based on the subgradient method, that leverages existing fast algorithms for inference."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744313"
                        ],
                        "name": "L. Ramshaw",
                        "slug": "L.-Ramshaw",
                        "structuredName": {
                            "firstName": "Lance",
                            "lastName": "Ramshaw",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Ramshaw"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734174"
                        ],
                        "name": "M. Marcus",
                        "slug": "M.-Marcus",
                        "structuredName": {
                            "firstName": "Mitchell",
                            "lastName": "Marcus",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marcus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 725590,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d9c71db75046473f0e3d3229950d7c84c09afd5e",
            "isKey": false,
            "numCitedBy": 1530,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Transformation-based learning, a technique introduced by Eric Brill (1993b), has been shown to do part-of-speech tagging with fairly high accuracy. This same method can be applied at a higher level of textual interpretation for locating chunks in the tagged text, including non-recursive \u201cbaseNP\u201d chunks. For this purpose, it is convenient to view chunking as a tagging problem by encoding the chunk structure in new tags attached to each word. In automatic tests using Treebank-derived data, this technique achieved recall and precision rates of roughly 93% for baseNP chunks (trained on 950K words) and 88% for somewhat more complex chunks that partition the sentence (trained on 200K words). Working in this new application and with larger template and training sets has also required some interesting adaptations to the transformation-based learning approach."
            },
            "slug": "Text-Chunking-using-Transformation-Based-Learning-Ramshaw-Marcus",
            "title": {
                "fragments": [],
                "text": "Text Chunking using Transformation-Based Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work has shown that the transformation-based learning approach can be applied at a higher level of textual interpretation for locating chunks in the tagged text, including non-recursive \u201cbaseNP\u201d chunks."
            },
            "venue": {
                "fragments": [],
                "text": "VLC@ACL"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474158"
                        ],
                        "name": "Vasin Punyakanok",
                        "slug": "Vasin-Punyakanok",
                        "structuredName": {
                            "firstName": "Vasin",
                            "lastName": "Punyakanok",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vasin Punyakanok"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144590225"
                        ],
                        "name": "D. Roth",
                        "slug": "D.-Roth",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Roth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 84
                            }
                        ],
                        "text": "This has not thus far hindered its applicability to problems like sequence labeling [43], parsing and semantic role labeling [44], but does seem to be an overly strict condition."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 157
                            }
                        ],
                        "text": "In the first variety, the structure in the problem is ignored, and a single classifier is trained to predict each element in the output vector independently [43] or with dependence created by enforcement of membership in Y\u00a7 constraints [45]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 193,
                                "start": 189
                            }
                        ],
                        "text": "In the context of structured prediction algorithms, Searn lies somewhere between global learning algorithms, such as M(3)Ns and CRFs, and local learning algorithms, such as those described [43]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 110
                            }
                        ],
                        "text": "In fact, some results suggest that one need not actually consider the structure at all for some such problems [43,45]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14509422,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3fab92869cfab684b3ffb1c16a771e9c3b774acd",
            "isKey": true,
            "numCitedBy": 220,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints. In particular, we develop two general approaches for an important subproblem - identifying phrase structure. The first is a Markovian approach that extends standard HMMs to allow the use of a rich observation structure and of general classifiers to model state-observation dependencies. The second is an extension of constraint satisfaction formalisms. We develop efficient combination algorithms under both models and study them experimentally in the context of shallow parsing."
            },
            "slug": "The-Use-of-Classifiers-in-Sequential-Inference-Punyakanok-Roth",
            "title": {
                "fragments": [],
                "text": "The Use of Classifiers in Sequential Inference"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A Markovian approach is developed that extends standard HMMs to allow the use of a rich observation structure and of general classifiers to model state-observation dependencies and an extension of constraint satisfaction formalisms are extended."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685978"
                        ],
                        "name": "B. Taskar",
                        "slug": "B.-Taskar",
                        "structuredName": {
                            "firstName": "Ben",
                            "lastName": "Taskar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Taskar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730156"
                        ],
                        "name": "Carlos Guestrin",
                        "slug": "Carlos-Guestrin",
                        "structuredName": {
                            "firstName": "Carlos",
                            "lastName": "Guestrin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carlos Guestrin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 49
                            }
                        ],
                        "text": "The first, \u201csmall,\u201d is the problem considered by [52]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 60
                            }
                        ],
                        "text": "The data set we consider is identical to that considered by [52] and includes 6600 sequences (words) collected from 150 subjects."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 7
                            }
                        ],
                        "text": "Later, [52] presented state-of-the-art results on this task using max-margin Markov networks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 131
                            }
                        ],
                        "text": "The maximum margin Markov network (M(3)N) formalism considers the structured prediction problem as a quadratic programming problem [52,51], following the formalism for the support vector machine for binary classification."
                    },
                    "intents": []
                }
            ],
            "corpusId": 201720,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0c450531e1121cfb657be5195e310217a4675397",
            "isKey": true,
            "numCitedBy": 1477,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "In typical classification tasks, we seek a function which assigns a label to a single object. Kernel-based approaches, such as support vector machines (SVMs), which maximize the margin of confidence of the classifier, are the method of choice for many such tasks. Their popularity stems both from the ability to use high-dimensional feature spaces, and from their strong theoretical guarantees. However, many real-world tasks involve sequential, spatial, or structured data, where multiple labels must be assigned. Existing kernel-based methods ignore structure in the problem, assigning labels independently to each object, losing much useful information. Conversely, probabilistic graphical models, such as Markov networks, can represent correlations between labels, by exploiting problem structure, but cannot handle high-dimensional feature spaces, and lack strong theoretical generalization guarantees. In this paper, we present a new framework that combines the advantages of both approaches: Maximum margin Markov (M3) networks incorporate both kernels, which efficiently deal with high-dimensional features, and the ability to capture correlations in structured data. We present an efficient algorithm for learning M3 networks based on a compact quadratic program formulation. We provide a new theoretical bound for generalization in structured domains. Experiments on the task of handwritten character recognition and collective hypertext classification demonstrate very significant gains over previous approaches."
            },
            "slug": "Max-Margin-Markov-Networks-Taskar-Guestrin",
            "title": {
                "fragments": [],
                "text": "Max-Margin Markov Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "Maximum margin Markov (M3) networks incorporate both kernels, which efficiently deal with high-dimensional features, and the ability to capture correlations in structured data, and a new theoretical bound for generalization in structured domains is provided."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143957226"
                        ],
                        "name": "Ryan T. McDonald",
                        "slug": "Ryan-T.-McDonald",
                        "structuredName": {
                            "firstName": "Ryan",
                            "lastName": "McDonald",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ryan T. McDonald"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145366908"
                        ],
                        "name": "Fernando C Pereira",
                        "slug": "Fernando-C-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando C Pereira"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 162
                            }
                        ],
                        "text": "The advantage of F1 measure over Hamming loss seen most easily in problems where the majority of words are \u201cnot chunks\u201d\u2014for instance, in gene name identification (McDonald and Pereira 2005)\u2014 Hamming loss often prefers a system that identifies no chunks to one that identifies some correctly and other incorrectly."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7380336,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "747618bde9d3155c0d7ccc024732044bce907bd6",
            "isKey": false,
            "numCitedBy": 249,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "BackgroundWe present a model for tagging gene and protein mentions from text using the probabilistic sequence tagging framework of conditional random fields (CRFs). Conditional random fields model the probability P(t|o) of a tag sequence given an observation sequence directly, and have previously been employed successfully for other tagging tasks. The mechanics of CRFs and their relationship to maximum entropy are discussed in detail.ResultsWe employ a diverse feature set containing standard orthographic features combined with expert features in the form of gene and biological term lexicons to achieve a precision of 86.4% and recall of 78.7%. An analysis of the contribution of the various features of the model is provided."
            },
            "slug": "Identifying-gene-and-protein-mentions-in-text-using-McDonald-Pereira",
            "title": {
                "fragments": [],
                "text": "Identifying gene and protein mentions in text using conditional random fields"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A diverse feature set containing standard orthographic features combined with expert features in the form of gene and biological term lexicons is employed to achieve a precision of 86.4% and recall of 78.7% for tagging gene and protein mentions from text using the probabilistic sequence tagging framework of conditional random fields."
            },
            "venue": {
                "fragments": [],
                "text": "BMC Bioinformatics"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 112
                            }
                        ],
                        "text": "The structured perceptron is an extension of the standard perceptron (Rosenblatt 1958) to structured prediction (Collins 2002)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 10888973,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5a7958b418bceb48a315384568091ab1898b1640",
            "isKey": false,
            "numCitedBy": 2272,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe new algorithms for training tagging models, as an alternative to maximum-entropy models or conditional random fields (CRFs). The algorithms rely on Viterbi decoding of training examples, combined with simple additive updates. We describe theory justifying the algorithms through a modification of the proof of convergence of the perceptron algorithm for classification problems. We give experimental results on part-of-speech tagging and base noun phrase chunking, in both cases showing improvements over results for a maximum-entropy tagger."
            },
            "slug": "Discriminative-Training-Methods-for-Hidden-Markov-Collins",
            "title": {
                "fragments": [],
                "text": "Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Experimental results on part-of-speech tagging and base noun phrase chunking are given, in both cases showing improvements over results for a maximum-entropy tagger."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474158"
                        ],
                        "name": "Vasin Punyakanok",
                        "slug": "Vasin-Punyakanok",
                        "structuredName": {
                            "firstName": "Vasin",
                            "lastName": "Punyakanok",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vasin Punyakanok"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144590225"
                        ],
                        "name": "D. Roth",
                        "slug": "D.-Roth",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Roth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144105277"
                        ],
                        "name": "Wen-tau Yih",
                        "slug": "Wen-tau-Yih",
                        "structuredName": {
                            "firstName": "Wen-tau",
                            "lastName": "Yih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wen-tau Yih"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2680974"
                        ],
                        "name": "Dav Zimak",
                        "slug": "Dav-Zimak",
                        "structuredName": {
                            "firstName": "Dav",
                            "lastName": "Zimak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dav Zimak"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 240,
                                "start": 236
                            }
                        ],
                        "text": "In the first variety, the structure in the problem is ignored, and a single classifier is trained to predict each element in the output vector independently [43] or with dependence created by enforcement of membership in Y\u00a7 constraints [45]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 110
                            }
                        ],
                        "text": "In fact, some results suggest that one need not actually consider the structure at all for some such problems [43,45]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1622820,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "64141d2e7a3cab0be864bbe2d5c7e0212fb17d27",
            "isKey": false,
            "numCitedBy": 117,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We study learning structured output in a discriminative framework where values of the output variables are estimated by local classifiers. In this framework, complex dependencies among the output variables are captured by constraints and dictate which global labels can be inferred. We compare two strategies, learning independent classifiers and inference based training, by observing their behaviors in different conditions. Experiments and theoretical justification lead to the conclusion that using inference based learning is superior when the local classifiers are difficult to learn but may require many examples before any discernible difference can be observed."
            },
            "slug": "Learning-and-Inference-over-Constrained-Output-Punyakanok-Roth",
            "title": {
                "fragments": [],
                "text": "Learning and Inference over Constrained Output"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Experiments and theoretical justification lead to the conclusion that using inference based learning is superior when the local classifiers are difficult to learn but may require many examples before any discernible difference can be observed."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "113414328"
                        ],
                        "name": "Fernando Pereira",
                        "slug": "Fernando-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando Pereira"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 113
                            }
                        ],
                        "text": "Conditional random fields are an extension of logistic regression (maximum entropy models) to structured outputs (Lafferty et al. 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 74
                            }
                        ],
                        "text": "However, to give a flavor, the popular conditional random field algorithm (Lafferty et al. 2001) is viewed along these dimensions as follows."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 219683473,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f4ba954b0412773d047dc41231c733de0c1f4926",
            "isKey": false,
            "numCitedBy": 13411,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "We present conditional random fields , a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data."
            },
            "slug": "Conditional-Random-Fields:-Probabilistic-Models-for-Lafferty-McCallum",
            "title": {
                "fragments": [],
                "text": "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work presents iterative parameter estimation algorithms for conditional random fields and compares the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37210858"
                        ],
                        "name": "Charles Sutton",
                        "slug": "Charles-Sutton",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Sutton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles Sutton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2274348"
                        ],
                        "name": "Khashayar Rohanimanesh",
                        "slug": "Khashayar-Rohanimanesh",
                        "structuredName": {
                            "firstName": "Khashayar",
                            "lastName": "Rohanimanesh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Khashayar Rohanimanesh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 77
                            }
                        ],
                        "text": "A canonical example of this task is joint POS tagging and syntactic chunking (Sutton et al. 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6038991,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0329663498462521483612649c0dffc85d9d9419",
            "isKey": false,
            "numCitedBy": 901,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "In sequence modeling, we often wish to represent complex interaction between labels, such as when performing multiple, cascaded labeling tasks on the same sequence, or when long-range dependencies exist. We present dynamic conditional random fields (DCRFs), a generalization of linear-chain conditional random fields (CRFs) in which each time slice contains a set of state variables and edges---a distributed state representation as in dynamic Bayesian networks (DBNs)---and parameters are tied across slices. Since exact inference can be intractable in such models, we perform approximate inference using several schedules for belief propagation, including tree-based reparameterization (TRP). On a natural-language chunking task, we show that a DCRF performs better than a series of linear-chain CRFs, achieving comparable performance using only half the training data."
            },
            "slug": "Dynamic-conditional-random-fields:-factorized-for-Sutton-McCallum",
            "title": {
                "fragments": [],
                "text": "Dynamic conditional random fields: factorized probabilistic models for labeling and segmenting sequence data"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "On a natural-language chunking task, it is shown that a DCRF performs better than a series of linear-chain CRFs, achieving comparable performance using only half the training data."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153160559"
                        ],
                        "name": "Joseph P. Turian",
                        "slug": "Joseph-P.-Turian",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Turian",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joseph P. Turian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144599392"
                        ],
                        "name": "I. D. Melamed",
                        "slug": "I.-D.-Melamed",
                        "structuredName": {
                            "firstName": "I.",
                            "lastName": "Melamed",
                            "middleNames": [
                                "Dan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. D. Melamed"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 61
                            }
                        ],
                        "text": "Similar recent algorithms such a decision-tree-based parsing [55] and perceptron-based machine translation [32] can also be seen as running a (slightly modified) first iteration of Searn."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5886536,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a28aeb725556736d3954d17e599b93367ff8dd3f",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "The present work advances the accuracy and training speed of discriminative parsing. Our discriminative parsing method has no generative component, yet surpasses a generative baseline on constituent parsing, and does so with minimal linguistic cleverness. Our model can incorporate arbitrary features of the input and parse state, and performs feature selection incrementally over an exponential feature space during training. We demonstrate the flexibility of our approach by testing it with several parsing strategies and various feature sets. Our implementation is freely available at: http://nlp.cs.nyu.edu/parser/."
            },
            "slug": "Advances-in-Discriminative-Parsing-Turian-Melamed",
            "title": {
                "fragments": [],
                "text": "Advances in Discriminative Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "This work has no generative component, yet surpasses a generative baseline on constituent parsing, and does so with minimal linguistic cleverness, and performs feature selection incrementally over an exponential feature space during training."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152971314"
                        ],
                        "name": "Kevin Knight",
                        "slug": "Kevin-Knight",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Knight",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Knight"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695463"
                        ],
                        "name": "D. Marcu",
                        "slug": "D.-Marcu",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Marcu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Marcu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 9
                            }
                        ],
                        "text": "pression (Knight and Marcu 2002; McDonald 2006) and document compression (Daum\u00e9"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7793213,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fa07fa673d8c908e91d22c4566572a72548ccee9",
            "isKey": false,
            "numCitedBy": 520,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Summarization-beyond-sentence-extraction:-A-to-Knight-Marcu",
            "title": {
                "fragments": [],
                "text": "Summarization beyond sentence extraction: A probabilistic approach to sentence compression"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50056360"
                        ],
                        "name": "William W. Cohen",
                        "slug": "William-W.-Cohen",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Cohen",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William W. Cohen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145177235"
                        ],
                        "name": "Vitor R. Carvalho",
                        "slug": "Vitor-R.-Carvalho",
                        "structuredName": {
                            "firstName": "Vitor",
                            "lastName": "Carvalho",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vitor R. Carvalho"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 10
                            }
                        ],
                        "text": "Recently, [7] has described an algorithm termed stacked sequential learning that attempts to remove this bias from MEMMs in a similar fashion to Searn."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9594939,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "89c2b3bfcc309ce16c85d2ab0c8cac5295400715",
            "isKey": false,
            "numCitedBy": 126,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a new sequential learning scheme called \"stacked sequential learning\". Stacked sequential learning is a meta-learning algorithm, in which an arbitrary base learner is augmented so as to make it aware of the labels of nearby examples. We evaluate the method on several \"sequential partitioning problems\", which are characterized by long runs of identical labels. We demonstrate that on these problems, sequential stacking consistently improves the performance of nonsequential base learners; that sequential stacking often improves performance of learners (such as CRFs) that are designed specifically for sequential tasks; and that a sequentially stacked maximum-entropy learner generally outperforms CRFs."
            },
            "slug": "Stacked-Sequential-Learning-Cohen-Carvalho",
            "title": {
                "fragments": [],
                "text": "Stacked Sequential Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is demonstrated that on several \"sequential partitioning problems\", sequential stacking consistently improves the performance of nonsequential base learners; that sequential stacking often improves performance of learners (such as CRFs) that are designed specifically for sequential tasks; and that a sequentially stacked maximum-entropy learner generally outperforms CRFs."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735228"
                        ],
                        "name": "B. Zadrozny",
                        "slug": "B.-Zadrozny",
                        "structuredName": {
                            "firstName": "Bianca",
                            "lastName": "Zadrozny",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Zadrozny"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144162125"
                        ],
                        "name": "J. Langford",
                        "slug": "J.-Langford",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Langford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Langford"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145629164"
                        ],
                        "name": "N. Abe",
                        "slug": "N.-Abe",
                        "structuredName": {
                            "firstName": "Naoki",
                            "lastName": "Abe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Abe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 18
                            }
                        ],
                        "text": "2005) and costing (Zadrozny et al. 2003) to reduce from cost-sensitive classification to binary classification."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9929910,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "757628481a4bddecd0b7bc63ecb5946f31a20f37",
            "isKey": false,
            "numCitedBy": 669,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose and evaluate a family of methods for converting classifier learning algorithms and classification theory into cost-sensitive algorithms and theory. The proposed conversion is based on cost-proportionate weighting of the training examples, which can be realized either by feeding the weights to the classification algorithm (as often done in boosting), or by careful subsampling. We give some theoretical performance guarantees on the proposed methods, as well as empirical evidence that they are practical alternatives to existing approaches. In particular, we propose costing, a method based on cost-proportionate rejection sampling and ensemble aggregation, which achieves excellent predictive performance on two publicly available datasets, while drastically reducing the computation required by other methods."
            },
            "slug": "Cost-sensitive-learning-by-cost-proportionate-Zadrozny-Langford",
            "title": {
                "fragments": [],
                "text": "Cost-sensitive learning by cost-proportionate example weighting"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Costing is proposed, a method based on cost-proportionate rejection sampling and ensemble aggregation, which achieves excellent predictive performance on two publicly available datasets, while drastically reducing the computation required by other methods."
            },
            "venue": {
                "fragments": [],
                "text": "Third IEEE International Conference on Data Mining"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2474158"
                        ],
                        "name": "Vasin Punyakanok",
                        "slug": "Vasin-Punyakanok",
                        "structuredName": {
                            "firstName": "Vasin",
                            "lastName": "Punyakanok",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vasin Punyakanok"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144590225"
                        ],
                        "name": "D. Roth",
                        "slug": "D.-Roth",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Roth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Roth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144105277"
                        ],
                        "name": "Wen-tau Yih",
                        "slug": "Wen-tau-Yih",
                        "structuredName": {
                            "firstName": "Wen-tau",
                            "lastName": "Yih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wen-tau Yih"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 125
                            }
                        ],
                        "text": "This has not thus far hindered its applicability to problems like sequence labeling [43], parsing and semantic role labeling [44], but does seem to be an overly strict condition."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13340546,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "31274eabb84407e3bc2c1d14b804cb7bcc068111",
            "isKey": false,
            "numCitedBy": 133,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We provide an experimental study of the role of syntactic parsing in semantic role labeling. Our conclusions demonstrate that syntactic parse information is clearly most relevant in the very first stage - the pruning stage. In addition, the quality of the pruning stage cannot be determined solely based on its recall and precision. Instead it depends on the characteristics of the output candidates that make downstream problems easier or harder. Motivated by this observation, we suggest an effective and simple approach of combining different semantic role labeling systems through joint inference, which significantly improves the performance."
            },
            "slug": "The-Necessity-of-Syntactic-Parsing-for-Semantic-Punyakanok-Roth",
            "title": {
                "fragments": [],
                "text": "The Necessity of Syntactic Parsing for Semantic Role Labeling"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "It is demonstrated that syntactic parse information is clearly most relevant in the very first stage - the pruning stage, and an effective and simple approach of combining different semantic role labeling systems through joint inference is suggested, which significantly improves the performance."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "24036451"
                        ],
                        "name": "J. Turner",
                        "slug": "J.-Turner",
                        "structuredName": {
                            "firstName": "Jenine",
                            "lastName": "Turner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Turner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749837"
                        ],
                        "name": "Eugene Charniak",
                        "slug": "Eugene-Charniak",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Charniak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eugene Charniak"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 190
                            }
                        ],
                        "text": "Unfortunately, their training hinges on the existence of \u3008 sentence, compression \u3009 pairs, where the compression is obtained from the sentence by only dropping words and phrases (the work of [56] is an exception)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15519576,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a89ac9902ccb889d096ec74f980fe8fc00940390",
            "isKey": false,
            "numCitedBy": 169,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "In Statistics-Based Summarization - Step One: Sentence Compression, Knight and Marcu (Knight and Marcu, 2000) (KM Knight and Marcu use a corpus of 1035 training sentences. More data is not easily available, so in addition to improving the original K&M noisy-channel model, we create unsupervised and semi-supervised models of the task. Finally, we point out problems with modeling the task in this way. They suggest areas for future research."
            },
            "slug": "Supervised-and-Unsupervised-Learning-for-Sentence-Turner-Charniak",
            "title": {
                "fragments": [],
                "text": "Supervised and Unsupervised Learning for Sentence Compression"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "In addition to improving the original K&M noisy-channel model, unsupervised and semi-supervised models of the task are created to point out problems with modeling the task in this way."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144162125"
                        ],
                        "name": "J. Langford",
                        "slug": "J.-Langford",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Langford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Langford"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735228"
                        ],
                        "name": "B. Zadrozny",
                        "slug": "B.-Zadrozny",
                        "structuredName": {
                            "firstName": "Bianca",
                            "lastName": "Zadrozny",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Zadrozny"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 74
                            }
                        ],
                        "text": "The lnT factor is not essential and can be removed using other approaches (Bagnell et al. 2003; Langford and Zadrozny 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1064489,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bc1955ec9542acf180ee67f33e72a6881977bd5e",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We prove a quantitative connection between the expected sum of rewards of a policy and binary classification performance on created subproblems. This connection holds without any unobservable assumptions (no assumption of independence, small mixing time, fully observable states, or even hidden states) and the resulting statement is independent of the number of states or actions. The statement is critically dependent on the size of the rewards and prediction performance of the created classifiers.We also provide some general guidelines for obtaining good classification performance on the created subproblems. In particular, we discuss possible methods for generating training examples for a classifier learning algorithm."
            },
            "slug": "Relating-reinforcement-learning-performance-to-Langford-Zadrozny",
            "title": {
                "fragments": [],
                "text": "Relating reinforcement learning performance to classification performance"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "It is proved a quantitative connection between the expected sum of rewards of a policy and binary classification performance on created subproblems and the resulting statement is independent of the number of states or actions."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36924412"
                        ],
                        "name": "J. Gim\u00e9nez",
                        "slug": "J.-Gim\u00e9nez",
                        "structuredName": {
                            "firstName": "Jes\u00fas",
                            "lastName": "Gim\u00e9nez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gim\u00e9nez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3049328"
                        ],
                        "name": "Llu\u00eds M\u00e0rquez i Villodre",
                        "slug": "Llu\u00eds-M\u00e0rquez-i-Villodre",
                        "structuredName": {
                            "firstName": "Llu\u00eds",
                            "lastName": "Villodre",
                            "middleNames": [
                                "M\u00e0rquez",
                                "i"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Llu\u00eds M\u00e0rquez i Villodre"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 102
                            }
                        ],
                        "text": "Similarly, a vanilla logistic regression classifier trained on the large NER data set outperforms the SVMstruct and the CRF on the small data sets."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 137
                            }
                        ],
                        "text": "On the small handwriting task, the two best performing systems are M3Ns with quadratic kernels (87.0% accuracy) and Searn with quadratic SVMs (87.6% accuracy)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 83
                            }
                        ],
                        "text": "2000), though the basic idea of MEMMs has also been applied more generally to SVMs (Kudo and Matsumoto 2001, 2003; Gim\u00e9nez and M\u00e0rquez 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 141
                            }
                        ],
                        "text": "The second variety is typified by maximum entropy Markov models [37], though the basic idea of MEMMs has also been applied more generally to SVMs [27,28,20]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 75
                            }
                        ],
                        "text": "On the NER task, Searn with a perceptron classifier performs comparably to SVMstruct and CRFs (at around 95.9% accuracy)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 94
                            }
                        ],
                        "text": "A 300-sentence subset of the training data set was previously used by [54] for evaluating the SVMstruct framework in the context of sequence labeling."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 93
                            }
                        ],
                        "text": "In this table, we compare raw classification algorithms (perceptron, logistic regression and SVMs) to alternative structured prediction algorithms (structured perceptron, CRFs, SVMstructs and M3Ns) to Searn with three baseline classifiers (perceptron, logistic regression and SVMs)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 125
                            }
                        ],
                        "text": "Although this comparison is not completely fair\u2014both algorithms should get access to the same data\u2014 if the algorithm (like the SVMstruct or the M3N) cannot scale to the large data set, then something is missing."
                    },
                    "intents": []
                }
            ],
            "corpusId": 10242516,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4b56b3008918d07165d765285808a11edd971091",
            "isKey": true,
            "numCitedBy": 422,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents the SVMTool, a simple, flexible, effective and efficient part\u2013of\u2013speech tagger based on Support Vector Machines. The SVMTool offers a fairly good balance among these properties which make it really practical for current NLP applications. It is very easy to use and easily configurable so as to perfectly fit the needs of a number of different applications. Results are also very competitive, achieving an accuracy of 97.16% for English on the Wall Street Journal corpus. It has been also successfully applied to Spanish exhibiting a similar performance. A first release of the SVMTool Perl prototype is now freely available for public use. A most efficient C++ version is coming very soon."
            },
            "slug": "SVMTool:-A-general-POS-Tagger-Generator-Based-on-Gim\u00e9nez-Villodre",
            "title": {
                "fragments": [],
                "text": "SVMTool: A general POS Tagger Generator Based on Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The SVMTool offers a fairly good balance among these properties which make it really practical for current NLP applications, and it is very easy to use and easily configurable so as to perfectly fit the needs of a number of different applications."
            },
            "venue": {
                "fragments": [],
                "text": "LREC"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730609"
                        ],
                        "name": "O. Chapelle",
                        "slug": "O.-Chapelle",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Chapelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chapelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1766703"
                        ],
                        "name": "A. Elisseeff",
                        "slug": "A.-Elisseeff",
                        "structuredName": {
                            "firstName": "Andr\u00e9",
                            "lastName": "Elisseeff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Elisseeff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 2
                            }
                        ],
                        "text": ", [58,36,1,39,54]), but these are common examples."
                    },
                    "intents": []
                }
            ],
            "corpusId": 813046,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "359300ac4a78687d60e18681925709b404b8fa54",
            "isKey": false,
            "numCitedBy": 183,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the learning problem of finding a dependency between a general class of objects and another, possibly different, general class of objects. The objects can be for example: vectors, images, strings, trees or graphs. Such a task is made possible by employing similarity measures in both input and output spaces using kernel functions, thus embedding the objects into vector spaces. We experimentally validate our approach on several tasks: mapping strings to strings, pattern recognition, and reconstruction from partial images."
            },
            "slug": "Kernel-Dependency-Estimation-Weston-Chapelle",
            "title": {
                "fragments": [],
                "text": "Kernel Dependency Estimation"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This work considers the learning problem of finding a dependency between a general class of objects and another, possibly different, generalclass of objects, made possible by employing similarity measures in both input and output spaces using kernel functions, thus embedding the objects into vector spaces."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722360"
                        ],
                        "name": "Hal Daum\u00e9",
                        "slug": "Hal-Daum\u00e9",
                        "structuredName": {
                            "firstName": "Hal",
                            "lastName": "Daum\u00e9",
                            "middleNames": [],
                            "suffix": "III"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hal Daum\u00e9"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695463"
                        ],
                        "name": "D. Marcu",
                        "slug": "D.-Marcu",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Marcu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Marcu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Moreover, for search problems with a large number of internal decisions (such as entity detection and tracking [ 15 ]), aborting search at the rst error is far from optimal."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In many applications, such as entity detection and tracking [ 15 ], this is not true."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3115914,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9c6653ec2e01d379a77adb61d209020d75c9505b",
            "isKey": false,
            "numCitedBy": 101,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Entity detection and tracking (EDT) is the task of identifying textual mentions of real-world entities in documents, extending the named entity detection and coreference resolution task by considering mentions other than names (pronouns, definite descriptions, etc.). Like NE tagging and coreference resolution, most solutions to the EDT task separate out the mention detection aspect from the coreference aspect. By doing so, these solutions are limited to using only local features for learning. In contrast, by modeling both aspects of the EDT task simultaneously, we are able to learn using highly complex, non-local features. We develop a new joint EDT model and explore the utility of many features, demonstrating their effectiveness on this task."
            },
            "slug": "A-Large-Scale-Exploration-of-Effective-Global-for-a-Daum\u00e9-Marcu",
            "title": {
                "fragments": [],
                "text": "A Large-Scale Exploration of Effective Global Features for a Joint Entity Detection and Tracking Model"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A new joint EDT model is developed and the utility of many features are explored, demonstrating their effectiveness on this task."
            },
            "venue": {
                "fragments": [],
                "text": "HLT"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5885617,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2479a5cf6cefefb83166c612564787414e47131f",
            "isKey": false,
            "numCitedBy": 811,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce and analyze a new algorithm for linear classification which combines Rosenblatt's perceptron algorithm with Helmbold and Warmuth's leave-one-out method. Like Vapnik's maximal-margin classifier, our algorithm takes advantage of data that are linearly separable with large margins. Compared to Vapnik's algorithm, however, ours is much simpler to implement, and much more efficient in terms of computation time. We also show that our algorithm can be efficiently used in very high dimensional spaces using kernel functions. We performed some experiments using our algorithm, and some variants of it, for classifying images of handwritten digits. The performance of our algorithm is close to, but not as good as, the performance of maximal-margin classifiers on the same problem, while saving significantly on computation time and programming effort."
            },
            "slug": "Large-Margin-Classification-Using-the-Perceptron-Freund-Schapire",
            "title": {
                "fragments": [],
                "text": "Large Margin Classification Using the Perceptron Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A new algorithm for linear classification which combines Rosenblatt's perceptron algorithm with Helmbold and Warmuth's leave-one-out method is introduced, which is much simpler to implement, and much more efficient in terms of computation time."
            },
            "venue": {
                "fragments": [],
                "text": "COLT' 98"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We begin by generating dependency parse trees of each sentence in the document collection (an example such tree is shown in Figure 2) using an in-house implementation of Model II ( Collins, 2003 ) and hand-written head rules."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7901127,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3fc44ff7f37ec5585310666c183c65e0a0bb2446",
            "isKey": false,
            "numCitedBy": 2062,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "This article describes three statistical models for natural language parsing. The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature. To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies. We analyze various characteristics of the models through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples. Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models."
            },
            "slug": "Head-Driven-Statistical-Models-for-Natural-Language-Collins",
            "title": {
                "fragments": [],
                "text": "Head-Driven Statistical Models for Natural Language Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "Three statistical models for natural language parsing are described, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree."
            },
            "venue": {
                "fragments": [],
                "text": "Computational Linguistics"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693407"
                        ],
                        "name": "K. Crammer",
                        "slug": "K.-Crammer",
                        "structuredName": {
                            "firstName": "Koby",
                            "lastName": "Crammer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Crammer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10151608,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "cfc6d0c8260594ebc5dd20ee558d29b1014ed41a",
            "isKey": false,
            "numCitedBy": 2190,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we describe the algorithmic implementation of multiclass kernel-based vector machines. Our starting point is a generalized notion of the margin to multiclass problems. Using this notion we cast multiclass categorization problems as a constrained optimization problem with a quadratic objective function. Unlike most of previous approaches which typically decompose a multiclass problem into multiple independent binary classification tasks, our notion of margin yields a direct method for training multiclass predictors. By using the dual of the optimization problem we are able to incorporate kernels with a compact set of constraints and decompose the dual problem into multiple optimization problems of reduced size. We describe an efficient fixed-point algorithm for solving the reduced optimization problems and prove its convergence. We then discuss technical details that yield significant running time improvements for large datasets. Finally, we describe various experiments with our approach comparing it to previously studied kernel-based methods. Our experiments indicate that for multiclass problems we attain state-of-the-art accuracy."
            },
            "slug": "On-the-Algorithmic-Implementation-of-Multiclass-Crammer-Singer",
            "title": {
                "fragments": [],
                "text": "On the Algorithmic Implementation of Multiclass Kernel-based Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "This paper describes the algorithmic implementation of multiclass kernel-based vector machines using a generalized notion of the margin to multiclass problems, and describes an efficient fixed-point algorithm for solving the reduced optimization problems and proves its convergence."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770124"
                        ],
                        "name": "Sunita Sarawagi",
                        "slug": "Sunita-Sarawagi",
                        "structuredName": {
                            "firstName": "Sunita",
                            "lastName": "Sarawagi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sunita Sarawagi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50056360"
                        ],
                        "name": "William W. Cohen",
                        "slug": "William-W.-Cohen",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Cohen",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William W. Cohen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 165
                            }
                        ],
                        "text": "In our experiments, we focus exclusively on chunk-at-a-time decoding, as it is more expressive (feature-wise) and has been seen to perform better in other scenarios (Sarawagi and Cohen 2004)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14036493,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "135ace829b6ad2ec9db040d8e5fd137034e83665",
            "isKey": false,
            "numCitedBy": 710,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe semi-Markov conditional random fields (semi-CRFs), a conditionally trained version of semi-Markov chains. Intuitively, a semi-CRF on an input sequence x outputs a \"segmentation\" of x, in which labels are assigned to segments (i.e., subsequences) of x rather than to individual elements xi of x. Importantly, features for semi-CRFs can measure properties of segments, and transitions within a segment can be non-Markovian. In spite of this additional power, exact learning and inference algorithms for semi-CRFs are polynomial-time\u2014often only a small constant factor slower than conventional CRFs. In experiments on five named entity recognition problems, semi-CRFs generally outperform conventional CRFs."
            },
            "slug": "Semi-Markov-Conditional-Random-Fields-for-Sarawagi-Cohen",
            "title": {
                "fragments": [],
                "text": "Semi-Markov Conditional Random Fields for Information Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Intuitively, a semi-CRF on an input sequence x outputs a \"segmentation\" of x, in which labels are assigned to segments rather than to individual elements of xi, and transitions within a segment can be non-Markovian."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145757665"
                        ],
                        "name": "Fei Sha",
                        "slug": "Fei-Sha",
                        "structuredName": {
                            "firstName": "Fei",
                            "lastName": "Sha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fei Sha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796044"
                        ],
                        "name": "L. Saul",
                        "slug": "L.-Saul",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Saul",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saul"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6759386,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "685f806fb9ea12ab2795ada01b57a414a9d3e45d",
            "isKey": false,
            "numCitedBy": 212,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the problem of parameter estimation in continuous density hidden Markov models (CD-HMMs) for automatic speech recognition (ASR). As in support vector machines, we propose a learning algorithm based on the goal of margin maximization. Unlike earlier work on max-margin Markov networks, our approach is specifically geared to the modeling of real-valued observations (such as acoustic feature vectors) using Gaussian mixture models. Unlike previous discriminative frameworks for ASR, such as maximum mutual information and minimum classification error, our framework leads to a convex optimization, without any spurious local minima. The objective function for large margin training of CD-HMMs is defined over a parameter space of positive semidefinite matrices. Its optimization can be performed efficiently with simple gradient-based methods that scale well to large problems. We obtain competitive results for phonetic recognition on the TIMIT speech corpus."
            },
            "slug": "Large-Margin-Hidden-Markov-Models-for-Automatic-Sha-Saul",
            "title": {
                "fragments": [],
                "text": "Large Margin Hidden Markov Models for Automatic Speech Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "This work proposes a learning algorithm based on the goal of margin maximization in continuous density hidden Markov models for automatic speech recognition (ASR) using Gaussian mixture models, and obtains competitive results for phonetic recognition on the TIMIT speech corpus."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758106"
                        ],
                        "name": "Dayne Freitag",
                        "slug": "Dayne-Freitag",
                        "structuredName": {
                            "firstName": "Dayne",
                            "lastName": "Freitag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dayne Freitag"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145366908"
                        ],
                        "name": "Fernando C Pereira",
                        "slug": "Fernando-C-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando C Pereira"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 64
                            }
                        ],
                        "text": "The second variety is typified by maximum entropy Markov models (McCallum et al. 2000), though the basic idea of MEMMs has also been applied more generally to SVMs (Kudo and Matsumoto 2001, 2003; Gim\u00e9nez and M\u00e0rquez 2004)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 775373,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bece46ed303f8eaef2affae2cba4e0aef51fe636",
            "isKey": false,
            "numCitedBy": 1551,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Hidden Markov models (HMMs) are a powerful probabilistic tool for modeling sequential data, and have been applied with success to many text-related tasks, such as part-of-speech tagging, text segmentation and information extraction. In these cases, the observations are usually modeled as multinomial distributions over a discrete vocabulary, and the HMM parameters are set to maximize the likelihood of the observations. This paper presents a new Markovian sequence model, closely related to HMMs, that allows observations to be represented as arbitrary overlapping features (such as word, capitalization, formatting, part-of-speech), and defines the conditional probability of state sequences given observation sequences. It does this by using the maximum entropy framework to fit a set of exponential models that represent the probability of a state given an observation and the previous state. We present positive experimental results on the segmentation of FAQ\u2019s."
            },
            "slug": "Maximum-Entropy-Markov-Models-for-Information-and-McCallum-Freitag",
            "title": {
                "fragments": [],
                "text": "Maximum Entropy Markov Models for Information Extraction and Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A new Markovian sequence model is presented that allows observations to be represented as arbitrary overlapping features (such as word, capitalization, formatting, part-of-speech), and defines the conditional probability of state sequences given observation sequences."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722360"
                        ],
                        "name": "Hal Daum\u00e9",
                        "slug": "Hal-Daum\u00e9",
                        "structuredName": {
                            "firstName": "Hal",
                            "lastName": "Daum\u00e9",
                            "middleNames": [],
                            "suffix": "III"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hal Daum\u00e9"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695463"
                        ],
                        "name": "D. Marcu",
                        "slug": "D.-Marcu",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Marcu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Marcu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ructured prediction techniques. This summarization problem, even in its simpli\ufb01ed form, is far too complex to be amenable to other methods.) For comparison, we present results from the BayeSum system [14,16], which achieved the highest score according to human evaluations of responsiveness in DUC 05. This system, as submitted to DUC 05, was trained on DUC 2003 data; the results for this con\ufb01guration are "
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 6241932,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cc8fe65096cc0971aebe45c50c64a173b94a36d5",
            "isKey": false,
            "numCitedBy": 263,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We present BAYESUM (for \"Bayesian summarization\"), a model for sentence extraction in query-focused summarization. BAYESUM leverages the common case in which multiple documents are relevant to a single query. Using these documents as reinforcement for query terms, BAYESUM is not afflicted by the paucity of information in short queries. We show that approximate inference in BAYESUM is possible on large data sets and results in a state-of-the-art summarization system. Furthermore, we show how BAYESUM can be understood as a justified query expansion technique in the language modeling for IR framework."
            },
            "slug": "Bayesian-Query-Focused-Summarization-Daum\u00e9-Marcu",
            "title": {
                "fragments": [],
                "text": "Bayesian Query-Focused Summarization"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that approximate inference in BAYESUM is possible on large data sets and results in a state-of-the-art summarization system, and how B Bayesian summarization can be understood as a justified query expansion technique in the language modeling for IR framework."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50655248"
                        ],
                        "name": "Ulrich Germann",
                        "slug": "Ulrich-Germann",
                        "structuredName": {
                            "firstName": "Ulrich",
                            "lastName": "Germann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ulrich Germann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21755122"
                        ],
                        "name": "Michael E. Jahr",
                        "slug": "Michael-E.-Jahr",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jahr",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael E. Jahr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152971314"
                        ],
                        "name": "Kevin Knight",
                        "slug": "Kevin-Knight",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Knight",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Knight"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695463"
                        ],
                        "name": "D. Marcu",
                        "slug": "D.-Marcu",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Marcu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Marcu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109821889"
                        ],
                        "name": "Kenji Yamada",
                        "slug": "Kenji-Yamada",
                        "structuredName": {
                            "firstName": "Kenji",
                            "lastName": "Yamada",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenji Yamada"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15700548,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c057bb2bfd42f316edd0972c43a78bd8acf6e92",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Fast-and-optimal-decoding-for-machine-translation-Germann-Jahr",
            "title": {
                "fragments": [],
                "text": "Fast and optimal decoding for machine translation"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2023469"
                        ],
                        "name": "D. Bikel",
                        "slug": "D.-Bikel",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Bikel",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Bikel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 177
                            }
                        ],
                        "text": "Even in comparatively simple problems like sequence labeling and parsing\u2014which are only O(n) or O(n3)\u2014it is often still computationally prohibitive to perform exhaustive search (Bikel 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 862713,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0606291dae96446e812ea8f09d9fbdc6acc3ec37",
            "isKey": false,
            "numCitedBy": 303,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "This article documents a large set of heretofore unpublished details Collins used in his parser, such that, along with Collins' (1999) thesis, this article contains all information necessary to duplicate Collins' benchmark results. Indeed, these as-yet-unpublished details account for an 11 relative increase in error from an implementation including all details to a clean-room implementation of Collins' model. We also show a cleaner and equally well-performing method for the handling of punctuation and conjunction and reveal certain other probabilistic oddities about Collins' parser. We not only analyze the effect of the unpublished details, but also reanalyze the effect of certain well-known details, revealing that bilexical dependencies are barely used by the model and that head choice is not nearly as important to overall parsing performance as once thought. Finally, we perform experiments that show that the true discriminative power of lexicalization appears to lie in the fact that unlexicalized syntactic structures are generated conditioning on the headword and its part of speech."
            },
            "slug": "Intricacies-of-Collins'-Parsing-Model-Bikel",
            "title": {
                "fragments": [],
                "text": "Intricacies of Collins' Parsing Model"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "A large set of heretofore unpublished details Collins used in his parser are documents, such that, along with Collins' (1999) thesis, this article contains all information necessary to duplicate Collins' benchmark results."
            },
            "venue": {
                "fragments": [],
                "text": "Computational Linguistics"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144299726"
                        ],
                        "name": "Thomas G. Dietterich",
                        "slug": "Thomas-G.-Dietterich",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dietterich",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas G. Dietterich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Structured learning problems have been addressed by algorithms such as conditional random elds (Laert y et al., 2001), max-margin Markov networks (Taskar et al., 2003) and constrained perceptrons (Punyakanok & Roth, 2001); see ( Dietterich, 2002 ) for a review."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 38476,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "23c3953fb45536c9129e86ac7a23098bd9f1381d",
            "isKey": false,
            "numCitedBy": 674,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Statistical learning problems in many fields involve sequential data. This paper formalizes the principal learning tasks and describes the methods that have been developed within the machine learning research community for addressing these problems. These methods include sliding window methods, recurrent sliding windows, hidden Markov models, conditional random fields, and graph transformer networks. The paper also discusses some open research issues."
            },
            "slug": "Machine-Learning-for-Sequential-Data:-A-Review-Dietterich",
            "title": {
                "fragments": [],
                "text": "Machine Learning for Sequential Data: A Review"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This paper formalizes the principal learning tasks and describes the methods that have been developed within the machine learning research community for addressing these problems, including sliding window methods, recurrent sliding windows, hidden Markov models, conditional random fields, and graph transformer networks."
            },
            "venue": {
                "fragments": [],
                "text": "SSPR/SPR"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51083130"
                        ],
                        "name": "F. Rosenblatt",
                        "slug": "F.-Rosenblatt",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Rosenblatt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Rosenblatt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 69
                            }
                        ],
                        "text": "The structured perceptron is an extension of the standard perceptron [46] to structured prediction [8]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12781225,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "5d11aad09f65431b5d3cb1d85328743c9e53ba96",
            "isKey": false,
            "numCitedBy": 9074,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "The first of these questions is in the province of sensory physiology, and is the only one for which appreciable understanding has been achieved. This article will be concerned primarily with the second and third questions, which are still subject to a vast amount of speculation, and where the few relevant facts currently supplied by neurophysiology have not yet been integrated into an acceptable theory. With regard to the second question, two alternative positions have been maintained. The first suggests that storage of sensory information is in the form of coded representations or images, with some sort of one-to-one mapping between the sensory stimulus"
            },
            "slug": "The-perceptron:-a-probabilistic-model-for-storage-Rosenblatt",
            "title": {
                "fragments": [],
                "text": "The perceptron: a probabilistic model for information storage and organization in the brain."
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This article will be concerned primarily with the second and third questions, which are still subject to a vast amount of speculation, and where the few relevant facts currently supplied by neurophysiology have not yet been integrated into an acceptable theory."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological review"
            },
            "year": 1958
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 119
                            }
                        ],
                        "text": "Single Monte-Carlo sampling: draw a single path and use the corresponding cost, with tied randomization as per Pegasus (Ng and Jordan 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 111
                            }
                        ],
                        "text": "Single Monte-Carlo sampling: draw a single path and use the corresponding cost, with tied randomization as per Pegasus [42]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 11691568,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d51fe3976ab4f4dc60745433c8638a2ecc3bf123",
            "isKey": false,
            "numCitedBy": 483,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a new approach to the problem of searching a space of policies for a Markov decision process (MDP) or a partially observable Markov decision process (POMDP), given a model. Our approach is based on the following observation: Any (PO)MDP can be transformed into an \"equivalent\" POMDP in which all state transitions (given the current state and action) are deterministic. This reduces the general problem of policy search to one in which we need only consider POMDPs with deterministic transitions. We give a natural way of estimating the value of all policies in these transformed POMDPs. Policy search is then simply performed by searching for a policy with high estimated value. We also establish conditions under which our value estimates will be good, recovering theoretical results similar to those of Kearns, Mansour and Ng [7], but with \"sample complexity\" bounds that have only a polynomial rather than exponential dependence on the horizon time. Our method applies to arbitrary POMDPs, including ones with infinite state and action spaces. We also present empirical results for our approach on a small discrete problem, and on a complex continuous state/continuous action problem involving learning to ride a bicycle."
            },
            "slug": "PEGASUS:-A-policy-search-method-for-large-MDPs-and-Ng-Jordan",
            "title": {
                "fragments": [],
                "text": "PEGASUS: A policy search method for large MDPs and POMDPs"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "This work proposes a new approach to the problem of searching a space of policies for a Markov decision process (MDP) or a partially observable Markov decisions process (POMDP), given a model, based on the following observation: Any (PO)MDP can be transformed into an \"equivalent\" POMDP in which all state transitions are deterministic."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50655248"
                        ],
                        "name": "Ulrich Germann",
                        "slug": "Ulrich-Germann",
                        "structuredName": {
                            "firstName": "Ulrich",
                            "lastName": "Germann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ulrich Germann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21755122"
                        ],
                        "name": "Michael E. Jahr",
                        "slug": "Michael-E.-Jahr",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jahr",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael E. Jahr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152971314"
                        ],
                        "name": "Kevin Knight",
                        "slug": "Kevin-Knight",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Knight",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Knight"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695463"
                        ],
                        "name": "D. Marcu",
                        "slug": "D.-Marcu",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Marcu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Marcu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109821889"
                        ],
                        "name": "Kenji Yamada",
                        "slug": "Kenji-Yamada",
                        "structuredName": {
                            "firstName": "Kenji",
                            "lastName": "Yamada",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenji Yamada"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 119
                            }
                        ],
                        "text": "These search algorithms are unlikely to be provably optimal (since this would imply that one is efficiently solving an NP-hard problem), but the hope is that they perform well on problems that are observed in the real world, as opposed to \u201cworst case\u201d inputs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 135
                            }
                        ],
                        "text": "For example, In natural language processing most statistical word-based and phrase-based models of translation are known to be NP-hard (Germann et al. 2003); syntactic translations models based on synchronous context free grammars are sometimes polynomial, but with an exponent that is too large in practice, such as n11 (Huang et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 117
                            }
                        ],
                        "text": "Further, there exist problems for which the optimal policy is computable in constant time and for which Eq (3) is an NP-hard computation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 144
                            }
                        ],
                        "text": "For another sort of example, in computational biology, most models for phylogeny [17] and protein secondary structure prediction [10] result in NP-hard search problems."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 212
                            }
                        ],
                        "text": "In the limit as the Markov order approaches the length of the longest sequence, Tmax, the computation for the minimal cost path (with or without the added complexity of augmenting the cost with the loss) becomes NP-hard."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 127
                            }
                        ],
                        "text": "For example, In natural language processing most statistical word-based and phrase-based models of translation are known to be NP-hard [19]; syntactic translations models based on synchronous context free grammars are sometimes polynomial, but with an exponent that is too large in practice, such as n11 [21]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 90111,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dd5514876b7e1c09b6d2f931d90bb34aa3501441",
            "isKey": false,
            "numCitedBy": 281,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "A good decoding algorithm is critical to the success of any statistical machine translation system. The decoder's job is to find the translation that is most likely according to set of previously learned parameters (and a formula for combining them). Since the space of possible translations is extremely large, typical decoding algorithms are only able to examine a portion of it, thus risking to miss good solutions. In this paper, we compare the speed and output quality of a traditional stack-based decoding algorithm with two new decoders: a fast greedy decoder and a slow but optimal decoder that treats decoding as an integer-programming optimization problem."
            },
            "slug": "Fast-Decoding-and-Optimal-Decoding-for-Machine-Germann-Jahr",
            "title": {
                "fragments": [],
                "text": "Fast Decoding and Optimal Decoding for Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper compares the speed and output quality of a traditional stack-based decoding algorithm with two new decoders: a fast greedy decoder and a slow but optimal decoder that treats decoding as an integer-programming optimization problem."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765329"
                        ],
                        "name": "Taku Kudo",
                        "slug": "Taku-Kudo",
                        "structuredName": {
                            "firstName": "Taku",
                            "lastName": "Kudo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Taku Kudo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681502"
                        ],
                        "name": "Yuji Matsumoto",
                        "slug": "Yuji-Matsumoto",
                        "structuredName": {
                            "firstName": "Yuji",
                            "lastName": "Matsumoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuji Matsumoto"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 102
                            }
                        ],
                        "text": "Similarly, a vanilla logistic regression classifier trained on the large NER data set outperforms the SVMstruct and the CRF on the small data sets."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 137
                            }
                        ],
                        "text": "On the small handwriting task, the two best performing systems are M3Ns with quadratic kernels (87.0% accuracy) and Searn with quadratic SVMs (87.6% accuracy)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 146
                            }
                        ],
                        "text": "The second variety is typified by maximum entropy Markov models [37], though the basic idea of MEMMs has also been applied more generally to SVMs [27,28,20]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 75
                            }
                        ],
                        "text": "On the NER task, Searn with a perceptron classifier performs comparably to SVMstruct and CRFs (at around 95.9% accuracy)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 94
                            }
                        ],
                        "text": "A 300-sentence subset of the training data set was previously used by [54] for evaluating the SVMstruct framework in the context of sequence labeling."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 93
                            }
                        ],
                        "text": "In this table, we compare raw classification algorithms (perceptron, logistic regression and SVMs) to alternative structured prediction algorithms (structured perceptron, CRFs, SVMstructs and M3Ns) to Searn with three baseline classifiers (perceptron, logistic regression and SVMs)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 125
                            }
                        ],
                        "text": "Although this comparison is not completely fair\u2014both algorithms should get access to the same data\u2014 if the algorithm (like the SVMstruct or the M3N) cannot scale to the large data set, then something is missing."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3446853,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6ffea7929f0e4bbee9e98755eb3d8fc09e89cf4e",
            "isKey": true,
            "numCitedBy": 586,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "We apply Support Vector Machines (SVMs) to identify English base phrases (chunks). SVMs are known to achieve high generalization performance even with input data of high dimensional feature spaces. Furthermore, by the Kernel principle, SVMs can carry out training with smaller computational overhead independent of their dimensionality. We apply weighted voting of 8 SVMs-based systems trained with distinct chunk representations. Experimental results show that our approach achieves higher accuracy than previous approaches."
            },
            "slug": "Chunking-with-Support-Vector-Machines-Kudo-Matsumoto",
            "title": {
                "fragments": [],
                "text": "Chunking with Support Vector Machines"
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2378691"
                        ],
                        "name": "Shiren Ye",
                        "slug": "Shiren-Ye",
                        "structuredName": {
                            "firstName": "Shiren",
                            "lastName": "Ye",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shiren Ye"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50682544"
                        ],
                        "name": "Long Qiu",
                        "slug": "Long-Qiu",
                        "structuredName": {
                            "firstName": "Long",
                            "lastName": "Qiu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Long Qiu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144078686"
                        ],
                        "name": "Tat-Seng Chua",
                        "slug": "Tat-Seng-Chua",
                        "structuredName": {
                            "firstName": "Tat-Seng",
                            "lastName": "Chua",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tat-Seng Chua"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37596605"
                        ],
                        "name": "Min-Yen Kan",
                        "slug": "Min-Yen-Kan",
                        "structuredName": {
                            "firstName": "Min-Yen",
                            "lastName": "Kan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Min-Yen Kan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14069123,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4e95782aabb9cd083ea2c03d57135576978cc8ac",
            "isKey": false,
            "numCitedBy": 50,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "The primary goal of our participation in DUC 2005 is two-fold. One is to benchmark the performance of a method of computing sentence semantic similarity. The other is to test the effectiveness of a new redundancy minimization formula inspired by Maximal Marginal Relevance (MMR). By using only these two features and eschewing other heuristics, our system performed competitively, achieving the top automated ROUGE scores among participants this year. This is a revised version of our notebook paper."
            },
            "slug": "NUS-at-DUC-2005:-Understanding-Documents-via-Links-Ye-Qiu",
            "title": {
                "fragments": [],
                "text": "NUS at DUC 2005: Understanding Documents via Concept Links"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This work uses a new redundancy minimization formula inspired by Maximal Marginal Relevance to benchmark the performance of a method of computing sentence semantic similarity and achieves the top automated ROUGE scores among participants this year."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756566"
                        ],
                        "name": "J. Bagnell",
                        "slug": "J.-Bagnell",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Bagnell",
                            "middleNames": [
                                "Andrew"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bagnell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144695232"
                        ],
                        "name": "S. Kakade",
                        "slug": "S.-Kakade",
                        "structuredName": {
                            "firstName": "Sham",
                            "lastName": "Kakade",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kakade"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753432"
                        ],
                        "name": "J. Schneider",
                        "slug": "J.-Schneider",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Schneider",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schneider"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 74
                            }
                        ],
                        "text": "The lnT factor is not essential and can be removed using other approaches (Bagnell et al. 2003; Langford and Zadrozny 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3225383,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d925fca2d6859c43e4c91a53fca96528de0c28dc",
            "isKey": false,
            "numCitedBy": 166,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the policy search approach to reinforcement learning. We show that if a \"baseline distribution\" is given (indicating roughly how often we expect a good policy to visit each state), then we can derive a policy search algorithm that terminates in a finite number of steps, and for which we can provide non-trivial performance guarantees. We also demonstrate this algorithm on several grid-world POMDPs, a planar biped walking robot, and a double-pole balancing problem."
            },
            "slug": "Policy-Search-by-Dynamic-Programming-Bagnell-Kakade",
            "title": {
                "fragments": [],
                "text": "Policy Search by Dynamic Programming"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "If a \"baseline distribution\" is given (indicating roughly how often the authors expect a good policy to visit each state), then a policy search algorithm is derived that terminates in a finite number of steps, and for which the author can provide non-trivial performance guarantees."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791069"
                        ],
                        "name": "D. Musicant",
                        "slug": "D.-Musicant",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Musicant",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Musicant"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107978833"
                        ],
                        "name": "Vipin Kumar",
                        "slug": "Vipin-Kumar",
                        "structuredName": {
                            "firstName": "Vipin",
                            "lastName": "Kumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vipin Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7460490"
                        ],
                        "name": "Aysel Ozgur",
                        "slug": "Aysel-Ozgur",
                        "structuredName": {
                            "firstName": "Aysel",
                            "lastName": "Ozgur",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aysel Ozgur"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 268,
                                "start": 234
                            }
                        ],
                        "text": "Using a weighted Hamming loss cannot completely alleviate this problem, for essentially the same reasons that a weighted zero-one loss cannot optimize F1 measure in binary classification, though one can often achieve an approximation (Lewis 2001; Musicant et al. 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14398042,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c6e8c365ee78cbbb2e6278b5329710063a297ea",
            "isKey": false,
            "numCitedBy": 96,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Support vector machines (SVMs) are regularly used for classification of unbalanced data by weighting more heavily the error contribution from the rare class. This heuristic technique is often used to learn classifiers with high F-measure, although this particular application of SVMs has not been rigorously examined. We provide significant and new theoretical results that support this popular heuristic. Specifically, we demonstrate that with the right parameter settings SVMs approximately optimize F-measure in the same way that SVMs have already been known to approximately optimize accuracy. This finding has a number of theoretical and practical implications for using SVMs in F-measure optimization."
            },
            "slug": "Optimizing-F-Measure-with-Support-Vector-Machines-Musicant-Kumar",
            "title": {
                "fragments": [],
                "text": "Optimizing F-Measure with Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "It is demonstrated that with the right parameter settings SVMs approximately optimize F-measure in the same way that SVMs have already been known to approximately optimize accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "FLAIRS Conference"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722360"
                        ],
                        "name": "Hal Daum\u00e9",
                        "slug": "Hal-Daum\u00e9",
                        "structuredName": {
                            "firstName": "Hal",
                            "lastName": "Daum\u00e9",
                            "middleNames": [],
                            "suffix": "III"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hal Daum\u00e9"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695463"
                        ],
                        "name": "D. Marcu",
                        "slug": "D.-Marcu",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Marcu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Marcu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": " vine-growth model. document collection until a pre-de\ufb01ned word limit is reached. [53] and [33] describe representative examples. Recent work in sentence compression [26, 38] and document compression [13] attempts to take small steps beyond sentence extraction. Compression models can be seen as techniques for extracting sentences then dropping extraneous information. They are more powerful than simple"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 189898,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b4d4304597b6a6c56e71846f68d5eb64385837d3",
            "isKey": false,
            "numCitedBy": 105,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a document compression system that uses a hierarchical noisy-channel model of text production. Our compression system first automatically derives the syntactic structure of each sentence and the overall discourse structure of the text given as input. The system then uses a statistical hierarchical model of text production in order to drop non-important syntactic and discourse constituents so as to generate coherent, grammatical document compressions of arbitrary length. The system outperforms both a baseline and a sentence-based compression system that operates by simplifying sequentially all sentences in a text. Our results support the claim that discourse knowledge plays an important role in document summarization."
            },
            "slug": "A-Noisy-Channel-Model-for-Document-Compression-Daum\u00e9-Marcu",
            "title": {
                "fragments": [],
                "text": "A Noisy-Channel Model for Document Compression"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The system outperforms both a baseline and a sentence-based compression system that operates by simplifying sequentially all sentences in a text and supports the claim that discourse knowledge plays an important role in document summarization."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20730631"
                        ],
                        "name": "R. Kassel",
                        "slug": "R.-Kassel",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Kassel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kassel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 63
                            }
                        ],
                        "text": "The handwriting recognition task we consider was introduced by (Kassel 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 37004361,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d3b97beda33f190487eaeb411ee81627f2efcff6",
            "isKey": false,
            "numCitedBy": 79,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Speech and handwriting are manifestations of a common need for linguistic communication. The similar nature of speech and handwriting recognition problems suggests that a largely shared solution may be possible. Recent advances in speech recognition can be partly attributed to changes in the research paradigm. These changes include using large corpora of common training and testing data, adopting statistical modeling over rule-based approaches, and ensuring meaningful comparisons between candidate technologies. The resulting improvements in system performance and robustness permit the study of increasingly difficult recognition tasks. \nThe primary goal of my thesis is to compare handwriting representations for on-line, printed, alphanumeric character recognition without striving to construct the highest-performance system. My studies are based on a carefully collected body of data containing some 87,000 characters from 150 writers. Material was selected automatically to ensure compact coverage of significant letter sequences. Subjects were instructed and prompted so as to minimally influence the writing they produced. A time-aligned transcription was entered for all of this data. I conducted an authentication study to understand better the classification difficulty of this writing. Only 81.7% of testing characters were identified correctly. \nI examined a number of potential representations for handwriting classification including bitmaps, projections, transforms, chain codes, and point-sampling, paying particular attention to pen motion as an information source. All experiments were based on Gaussian mixture models because of their flexibility. The best representation features Cartesian coordinates of 10 equally-spaced samples along the pen trajectory. Without the benefit of relative size information, this representation resulted in 77.2% correct character classification on testing data. \nFinally, I adapted the scSUMMIT segment-based speech recognition system developed at MIT to handwriting. Segmentation is based primarily on pen-lifts, but strokes are divided to account for connected character pairs. The parameter described above is computed for each segment and the resulting graph passed to the recognition engine for classification and search. This system was able to correctly recognize 65.1% of the test-set characters. Incorporating a bigram character grammar with perplexity 11.3 improved this performance to 76.4%. (Copies available exclusively from MIT Libraries, Rm. 14-0551, Cambridge, MA 02139-4307. Ph. 617-253-5668; Fax 617-253-1690.)"
            },
            "slug": "A-comparison-of-approaches-to-on-line-handwritten-Kassel",
            "title": {
                "fragments": [],
                "text": "A comparison of approaches to on-line handwritten character recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The primary goal of this thesis is to compare handwriting representations for on-line, printed, alphanumeric character recognition without striving to construct the highest-performance system."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "These difculties have been partially dealt with by successive algorithms (Tsochantaridis et al., 2005; Taskar et al., 2005;  Joachims, 2005 )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1115550,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "175c1bb60ee46dac56d942ef8c7339977b4ebb0e",
            "isKey": false,
            "numCitedBy": 845,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a Support Vector Method for optimizing multivariate nonlinear performance measures like the F1-score. Taking a multivariate prediction approach, we give an algorithm with which such multivariate SVMs can be trained in polynomial time for large classes of potentially non-linear performance measures, in particular ROCArea and all measures that can be computed from the contingency table. The conventional classification SVM arises as a special case of our method."
            },
            "slug": "A-support-vector-method-for-multivariate-measures-Joachims",
            "title": {
                "fragments": [],
                "text": "A support vector method for multivariate performance measures"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An algorithm with which such multivariate SVMs can be trained in polynomial time for large classes of potentially non-linear performance measures, in particular ROCArea and all measures that can be computed from the contingency table are given."
            },
            "venue": {
                "fragments": [],
                "text": "K\u00fcnstliche Intell."
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721860"
                        ],
                        "name": "M. Wainwright",
                        "slug": "M.-Wainwright",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Wainwright",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Wainwright"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "3 In fact, [ 57 ] has provided evidence that when using approximation algorithms"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16451367,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f3d831a9447ae7a8f9aa5c5beef8a28dbfacf352",
            "isKey": false,
            "numCitedBy": 129,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Consider the problem of joint parameter estimation and prediction in a Markov random field: that is, the model parameters are estimated on the basis of an initial set of data, and then the fitted model is used to perform prediction (e.g., smoothing, denoising, interpolation) on a new noisy observation. Working under the restriction of limited computation, we analyze a joint method in which the same convex variational relaxation is used to construct an M-estimator for fitting parameters, and to perform approximate marginalization for the prediction step. The key result of this paper is that in the computation-limited setting, using an inconsistent parameter estimator (i.e., an estimator that returns the \"wrong\" model even in the infinite data limit) is provably beneficial, since the resulting errors can partially compensate for errors made by using an approximate prediction technique. En route to this result, we analyze the asymptotic properties of M-estimators based on convex variational relaxations, and establish a Lipschitz stability property that holds for a broad class of convex variational methods. This stability result provides additional incentive, apart from the obvious benefit of unique global optima, for using message-passing methods based on convex variational relaxations. We show that joint estimation/prediction based on the reweighted sum-product algorithm substantially outperforms a commonly used heuristic based on ordinary sum-product."
            },
            "slug": "Estimating-the-\"Wrong\"-Graphical-Model:-Benefits-in-Wainwright",
            "title": {
                "fragments": [],
                "text": "Estimating the \"Wrong\" Graphical Model: Benefits in the Computation-Limited Setting"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The key result of this paper is that in the computation-limited setting, using an inconsistent parameter estimator is provably beneficial, since the resulting errors can partially compensate for errors made by using an approximate prediction technique."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145689002"
                        ],
                        "name": "David A. McAllester",
                        "slug": "David-A.-McAllester",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "McAllester",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David A. McAllester"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145366908"
                        ],
                        "name": "Fernando C Pereira",
                        "slug": "Fernando-C-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": [
                                "C"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando C Pereira"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 45
                            }
                        ],
                        "text": "There are, of course, alternative frameworks (see, e.g., Weston et al. 2002; McAllester et al. 2004; Altun et al. 2004; McDonald et al. 2004; Tsochantaridis et al. 2005), but these are common examples."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 197521,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "94f5431413f2ad94a42d5fd5fd4ab30134f044d3",
            "isKey": false,
            "numCitedBy": 75,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Case-factor-diagrams-for-structured-probabilistic-McAllester-Collins",
            "title": {
                "fragments": [],
                "text": "Case-factor diagrams for structured probabilistic modeling"
            },
            "venue": {
                "fragments": [],
                "text": "J. Comput. Syst. Sci."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722360"
                        ],
                        "name": "Hal Daum\u00e9",
                        "slug": "Hal-Daum\u00e9",
                        "structuredName": {
                            "firstName": "Hal",
                            "lastName": "Daum\u00e9",
                            "middleNames": [],
                            "suffix": "III"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hal Daum\u00e9"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695463"
                        ],
                        "name": "D. Marcu",
                        "slug": "D.-Marcu",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Marcu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Marcu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 61
                            }
                        ],
                        "text": ") For comparison, we present results from the BayeSum system [14,16], which achieved the highest score according to human evaluations of responsiveness in DUC 05."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 12051268,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eff5625ea1fd51a6675f86827f719b68a9e1591a",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe our entry into the Document Understanding Conference competition for evaluating query-focused multidocument summarization systems. Our system is based on a Bayesian QueryFocused Summarization model, similar to the system we entered into the MSE competition. This paper begins by describing the (few) differences between our DUC system and our MSE system and describes our placement in the competition. The remainder of this paper argues in favor of performing extrinsic evaluation of summarization systems, and suggests a method for doing so."
            },
            "slug": "Bayesian-Summarization-at-DUC-and-a-Suggestion-for-Daum\u00e9-Marcu",
            "title": {
                "fragments": [],
                "text": "Bayesian Summarization at DUC and a Suggestion for Extrinsic Evaluation"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Their system is based on a Bayesian QueryFocused Summarization model, similar to the system the authors entered into the MSE competition, and their placement in the competition is described."
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2101497"
                        ],
                        "name": "L. Foulds",
                        "slug": "L.-Foulds",
                        "structuredName": {
                            "firstName": "Les",
                            "lastName": "Foulds",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Foulds"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102750343"
                        ],
                        "name": "R. Graham",
                        "slug": "R.-Graham",
                        "structuredName": {
                            "firstName": "Ron",
                            "lastName": "Graham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Graham"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 81
                            }
                        ],
                        "text": "For another sort of example, in computational biology, most models for phylogeny [17] and protein secondary structure prediction [10] result in NP-hard search problems."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 123072101,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "04dbac7624026a74aa9dff920b52a228975b437a",
            "isKey": false,
            "numCitedBy": 420,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-steiner-problem-in-phylogeny-is-NP-complete-Foulds-Graham",
            "title": {
                "fragments": [],
                "text": "The steiner problem in phylogeny is NP-complete"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144768480"
                        ],
                        "name": "Liang Huang",
                        "slug": "Liang-Huang",
                        "structuredName": {
                            "firstName": "Liang",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liang Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40479003"
                        ],
                        "name": "Hao Zhang",
                        "slug": "Hao-Zhang",
                        "structuredName": {
                            "firstName": "Hao",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hao Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793218"
                        ],
                        "name": "D. Gildea",
                        "slug": "D.-Gildea",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Gildea",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gildea"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 170
                            }
                        ],
                        "text": "2003); syntactic translations models based on synchronous context free grammars are sometimes polynomial, but with an exponent that is too large in practice, such as n11 (Huang et al. 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6166308,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "220611a719c9fd800ce9ee4af936814d73e43ef4",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "We adapt the \"hook\" trick for speeding up bilexical parsing to the decoding problem for machine translation models that are based on combining a synchronous context free grammar as the translation model with an n-gram language model. This dynamic programming technique yields lower complexity algorithms than have previously been described for an important class of translation models."
            },
            "slug": "Machine-Translation-as-Lexicalized-Parsing-with-Huang-Zhang",
            "title": {
                "fragments": [],
                "text": "Machine Translation as Lexicalized Parsing with Hooks"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "The \"hook\" trick for speeding up bilexical parsing is adapted to the decoding problem for machine translation models that are based on combining a synchronous context free grammar as the translation model with an n-gram language model."
            },
            "venue": {
                "fragments": [],
                "text": "IWPT"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781574"
                        ],
                        "name": "Chin-Yew Lin",
                        "slug": "Chin-Yew-Lin",
                        "structuredName": {
                            "firstName": "Chin-Yew",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chin-Yew Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144547315"
                        ],
                        "name": "E. Hovy",
                        "slug": "E.-Hovy",
                        "structuredName": {
                            "firstName": "Eduard",
                            "lastName": "Hovy",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Hovy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10648347,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "04fa53cfb61af83163258b5c4cbe0883a3fb43e2",
            "isKey": false,
            "numCitedBy": 181,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "NeATS is a multi-document summarization system that attempts to extract relevant or interesting portions from a set of documents about some topic and present them in coherent order. NeATS is among the best performers in the large scale summarization evaluation DUC-01."
            },
            "slug": "From-Single-to-Multi-document-Summarization-:-A-and-Lin-Hovy",
            "title": {
                "fragments": [],
                "text": "From Single to Multi-document Summarization : A Prototype System and its Evaluation"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "NeATS is a multi-document summarization system that attempts to extract relevant or interesting portions from a set of documents about some topic and present them in coherent order."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781574"
                        ],
                        "name": "Chin-Yew Lin",
                        "slug": "Chin-Yew-Lin",
                        "structuredName": {
                            "firstName": "Chin-Yew",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chin-Yew Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144547315"
                        ],
                        "name": "E. Hovy",
                        "slug": "E.-Hovy",
                        "structuredName": {
                            "firstName": "Eduard",
                            "lastName": "Hovy",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Hovy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 178
                            }
                        ],
                        "text": "That is, if our goal is a 100 word summary and we have already created a 50 word summary, then we execute beam search (beam size 20) for the remaining 50 words that maximize the Rouge score."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 47
                            }
                        ],
                        "text": "The current popular choice for metric is Rouge [34], which (roughly speaking) computes n-gram overlap between a system summary and a set of human written summaries."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 56
                            }
                        ],
                        "text": "That is, the oracle sentence extraction system yields a Rouge score of 0.0362, compared to the score achieved by the Searn system of 0.0345."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 57
                            }
                        ],
                        "text": "In the experiments described in this chapter, we use the \u201cRouge 2\u201d metric, which uses evenly weighted bigram scores."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 63
                            }
                        ],
                        "text": "6.2.3 Initial Policy Computing the best label completion under Rouge metric for the vine-growth model is intractable."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 125
                            }
                        ],
                        "text": "The last aspect of the results to notice is how the Searn-based models compare to the best DUC 2005 system, which achieved a Rouge score of 0.0725."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 10
                            }
                        ],
                        "text": "We report Rouge scores for summaries of length 100 and length 250."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 24
                            }
                        ],
                        "text": "In various experiments, Rouge has been seen to correspond with human judgment of summary quality."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 120
                            }
                        ],
                        "text": "First, oracle systems that perform the summarization task with knowledge of the true output, attempting to maximize the Rouge score."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 107
                            }
                        ],
                        "text": "Finally, we present the results for the baseline system and for the best DUC 2005 system (according to the Rouge 2 metric)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 16292125,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c63bb976dc0d3a897f3b0920170a4c573ef904c6",
            "isKey": true,
            "numCitedBy": 1628,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Following the recent adoption by the machine translation community of automatic evaluation using the BLEU/NIST scoring process, we conduct an in-depth study of a similar idea for evaluating summaries. The results show that automatic evaluation using unigram co-occurrences between summary pairs correlates surprising well with human evaluations, based on various statistical metrics; while direct application of the BLEU evaluation procedure does not always give good results."
            },
            "slug": "Automatic-Evaluation-of-Summaries-Using-N-gram-Lin-Hovy",
            "title": {
                "fragments": [],
                "text": "Automatic Evaluation of Summaries Using N-gram Co-occurrence Statistics"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The results show that automatic evaluation using unigram co-occurrences between summary pairs correlates surprising well with human evaluations, based on various statistical metrics; while direct application of the BLEU evaluation procedure does not always give good results."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2472298"
                        ],
                        "name": "Chih-Chung Chang",
                        "slug": "Chih-Chung-Chang",
                        "structuredName": {
                            "firstName": "Chih-Chung",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Chung Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711460"
                        ],
                        "name": "Chih-Jen Lin",
                        "slug": "Chih-Jen-Lin",
                        "structuredName": {
                            "firstName": "Chih-Jen",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Jen Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 961425,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "273dfbcb68080251f5e9ff38b4413d7bd84b10a1",
            "isKey": false,
            "numCitedBy": 40078,
            "numCiting": 77,
            "paperAbstract": {
                "fragments": [],
                "text": "LIBSVM is a library for Support Vector Machines (SVMs). We have been actively developing this package since the year 2000. The goal is to help users to easily apply SVM to their applications. LIBSVM has gained wide popularity in machine learning and many other areas. In this article, we present all implementation details of LIBSVM. Issues such as solving SVM optimization problems theoretical convergence multiclass classification probability estimates and parameter selection are discussed in detail."
            },
            "slug": "LIBSVM:-A-library-for-support-vector-machines-Chang-Lin",
            "title": {
                "fragments": [],
                "text": "LIBSVM: A library for support vector machines"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Issues such as solving SVM optimization problems theoretical convergence multiclass classification probability estimates and parameter selection are discussed in detail."
            },
            "venue": {
                "fragments": [],
                "text": "TIST"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717359"
                        ],
                        "name": "P. Crescenzi",
                        "slug": "P.-Crescenzi",
                        "structuredName": {
                            "firstName": "Pierluigi",
                            "lastName": "Crescenzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Crescenzi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144851009"
                        ],
                        "name": "Deborah Goldman",
                        "slug": "Deborah-Goldman",
                        "structuredName": {
                            "firstName": "Deborah",
                            "lastName": "Goldman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Deborah Goldman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144102674"
                        ],
                        "name": "C. Papadimitriou",
                        "slug": "C.-Papadimitriou",
                        "structuredName": {
                            "firstName": "Christos",
                            "lastName": "Papadimitriou",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Papadimitriou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1769170"
                        ],
                        "name": "A. Piccolboni",
                        "slug": "A.-Piccolboni",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Piccolboni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Piccolboni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748179"
                        ],
                        "name": "M. Yannakakis",
                        "slug": "M.-Yannakakis",
                        "structuredName": {
                            "firstName": "Mihalis",
                            "lastName": "Yannakakis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Yannakakis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18223154,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "a56ef182b5d7484ed4bd3602cfe7829ed53636f6",
            "isKey": false,
            "numCitedBy": 51,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "forefront of today\u2019s science (often referred to dramatically as \u201cbreaking the genetic code\u201d or \u201cthe last phase of the We ahow that the protein folding problem in the two-dimensional Mendelian revolution\u201d). This mapping cm be rou&ly &H-P model io NP-complete."
            },
            "slug": "On-the-complexity-of-protein-folding-(extended-Crescenzi-Goldman",
            "title": {
                "fragments": [],
                "text": "On the complexity of protein folding (extended abstract)"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper presents a mapping of the protein folding problem in the two-dimensional Mendelian revolution on the basis of a H-P model and shows that the model is NP-complete."
            },
            "venue": {
                "fragments": [],
                "text": "STOC '98"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781574"
                        ],
                        "name": "Chin-Yew Lin",
                        "slug": "Chin-Yew-Lin",
                        "structuredName": {
                            "firstName": "Chin-Yew",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chin-Yew Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144547315"
                        ],
                        "name": "E. Hovy",
                        "slug": "E.-Hovy",
                        "structuredName": {
                            "firstName": "Eduard",
                            "lastName": "Hovy",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Hovy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8448956,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9c1a028299cc2b7f4cbdbe6e19abfc844a6ead6b",
            "isKey": false,
            "numCitedBy": 212,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "NeATS is a multi-document summarization system that attempts to extract relevant or interesting portions from a set of documents about some topic and present them in coherent order. NeATS is among the best performers in the large scale summarization evaluation DUC 2001."
            },
            "slug": "From-Single-to-Multi-document-Summarization-Lin-Hovy",
            "title": {
                "fragments": [],
                "text": "From Single to Multi-document Summarization"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "NeATS is a multi-document summarization system that attempts to extract relevant or interesting portions from a set of documents about some topic and present them in coherent order."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144695232"
                        ],
                        "name": "S. Kakade",
                        "slug": "S.-Kakade",
                        "structuredName": {
                            "firstName": "Sham",
                            "lastName": "Kakade",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kakade"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725303"
                        ],
                        "name": "Y. Teh",
                        "slug": "Y.-Teh",
                        "structuredName": {
                            "firstName": "Yee",
                            "lastName": "Teh",
                            "middleNames": [
                                "Whye"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Teh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9330607"
                        ],
                        "name": "S. Roweis",
                        "slug": "S.-Roweis",
                        "structuredName": {
                            "firstName": "Sam",
                            "lastName": "Roweis",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Roweis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 65
                            }
                        ],
                        "text": "This is not unsimilar to the \u201calternative objective\u201d proposed by (Kakade et al. 2002) for CRFs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 36932963,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "16477724d72072898c9c9d7fb75e21c6ee54ff38",
            "isKey": false,
            "numCitedBy": 80,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A new red poinsettia cultivar particularly distinguished by its earlier blooming and at a lower temperature than the plant that it most nearly resembles, namely Poinsettia V-14, U.S. Plant Pat. No. 4,384; and by its having the unique growth and habit characteristics of the Gutbier's Family of poinsettia plants, including easy multiple branching, full bracts, and small cyathia clusters."
            },
            "slug": "An-Alternate-Objective-Function-for-Markovian-Kakade-Teh",
            "title": {
                "fragments": [],
                "text": "An Alternate Objective Function for Markovian Fields"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "A new red poinsettia cultivar particularly distinguished by its earlier blooming and at a lower temperature than the plant that it most nearly resembles, namely Poinsettias V-14, U.S. Plant Pat."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2480901"
                        ],
                        "name": "Simone Teufel",
                        "slug": "Simone-Teufel",
                        "structuredName": {
                            "firstName": "Simone",
                            "lastName": "Teufel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simone Teufel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11846745,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "343219e299d980873c72f37c21841a0ea54b2c1d",
            "isKey": false,
            "numCitedBy": 269,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "A gate network which permits two levels of logic to be performed by interconnected gate networks which are controlled by the same phase."
            },
            "slug": "Sentence-extraction-as-a-classification-task-Teufel",
            "title": {
                "fragments": [],
                "text": "Sentence extraction as a classification task"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A gate network which permits two levels of logic to be performed by interconnected gate networks which are controlled by the same phase."
            },
            "venue": {
                "fragments": [],
                "text": "Workshop On Intelligent Scalable Text Summarization"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717359"
                        ],
                        "name": "P. Crescenzi",
                        "slug": "P.-Crescenzi",
                        "structuredName": {
                            "firstName": "Pierluigi",
                            "lastName": "Crescenzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Crescenzi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144851009"
                        ],
                        "name": "Deborah Goldman",
                        "slug": "Deborah-Goldman",
                        "structuredName": {
                            "firstName": "Deborah",
                            "lastName": "Goldman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Deborah Goldman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144102674"
                        ],
                        "name": "C. Papadimitriou",
                        "slug": "C.-Papadimitriou",
                        "structuredName": {
                            "firstName": "Christos",
                            "lastName": "Papadimitriou",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Papadimitriou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1769170"
                        ],
                        "name": "A. Piccolboni",
                        "slug": "A.-Piccolboni",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Piccolboni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Piccolboni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748179"
                        ],
                        "name": "M. Yannakakis",
                        "slug": "M.-Yannakakis",
                        "structuredName": {
                            "firstName": "Mihalis",
                            "lastName": "Yannakakis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Yannakakis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 149
                            }
                        ],
                        "text": "For another sort of example, in computational biology, most models for phylogeny (Foulds and Graham 1982) and protein secondary structure prediction (Crescenzi et al. 1998) result in NP-hard search problems."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6575153,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "492cbadcc1bb2875209004e6ad93f391d2e6cf9b",
            "isKey": false,
            "numCitedBy": 459,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We show that the protein folding problem in the two-dimensional H-P model is NP-complete."
            },
            "slug": "On-the-Complexity-of-Protein-Folding-Crescenzi-Goldman",
            "title": {
                "fragments": [],
                "text": "On the Complexity of Protein Folding"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "It is shown that the protein folding problem in the two-dimensional H-P model is NP-complete."
            },
            "venue": {
                "fragments": [],
                "text": "J. Comput. Biol."
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 93
                            }
                        ],
                        "text": "For all Searn-based models, we use the the following settings of the tunable parameters (see [12] for a comparison of different settings)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 57
                            }
                        ],
                        "text": "An empirical comparison of these options is performed in [12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 119
                            }
                        ],
                        "text": "The exact model we use for the document summarization task is a novel \u201cvine-growth\u201d model, described in more detail in [12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 140
                            }
                        ],
                        "text": "Other orders are possible (such as allowing any arbitrary position to be labeled at any time, effectively mimicing belief propagation); see [12] for more experimental results under alternative orderings."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Practical structured learning for natural language processing"
            },
            "venue": {
                "fragments": [],
                "text": "PhD thesis,"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144695232"
                        ],
                        "name": "S. Kakade",
                        "slug": "S.-Kakade",
                        "structuredName": {
                            "firstName": "Sham",
                            "lastName": "Kakade",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kakade"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144162125"
                        ],
                        "name": "J. Langford",
                        "slug": "J.-Langford",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Langford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Langford"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 95
                            }
                        ],
                        "text": "This positive contribution can be understood in the framework of conservative policy iteration (Kakade and Langford 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 36
                            }
                        ],
                        "text": "1 for conservative policy iteration (Kakade and Langford 2002)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 31442909,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "523b4ce1c2a1336962444abc1dec215756c2f3e6",
            "isKey": false,
            "numCitedBy": 662,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Approximately-Optimal-Approximate-Reinforcement-Kakade-Langford",
            "title": {
                "fragments": [],
                "text": "Approximately Optimal Approximate Reinforcement Learning"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bayesian query-focused summarization. In Proceedings of the conference of the association for computational linguistics (ACL), Sydney, Australia"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 141
                            }
                        ],
                        "text": "4 We note in passing that directly optimizing F1 may not be the best approach, from the perspective of integrating information in a pipeline [35]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Doing named entity recognition? Don\u2019t optimize for F1"
            },
            "venue": {
                "fragments": [],
                "text": "Post on the NLPers Blog,"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Chunking with support vector machines. In Proceedings of the conference of the North American chapter of the association for computational linguistics (NAACL)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 40
                            }
                        ],
                        "text": "to parse backwards rather than forwards (Turian and Melamed 2006)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 61
                            }
                        ],
                        "text": "Similar recent algorithms such a decision-tree-based parsing (Turian and Melamed 2006) and perceptron-based machine translation (Liang et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Advances in discriminative parsing. In Proceedings of the joint international conference on computational linguistics and association of computational linguistics (COLING/ACL)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Doing named entity recognition? Don't optimize for F1. Post on the NLPers Blog"
            },
            "venue": {
                "fragments": [],
                "text": "Doing named entity recognition? Don't optimize for F1. Post on the NLPers Blog"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[53] and [33] describe representative examples."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sentence extraction as a classification"
            },
            "venue": {
                "fragments": [],
                "text": "Workshop on Intelligent and Scalable Text Summarization,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 39
                            }
                        ],
                        "text": "For data, we use the DUC 2005 data set (Dang 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fifth document understanding conference (DUC-2005)"
            },
            "venue": {
                "fragments": [],
                "text": "Ann Arbor,"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Doing named entity recognition? Don't optimize for F 1 . Post on the NLPers Blog"
            },
            "venue": {
                "fragments": [],
                "text": "Doing named entity recognition? Don't optimize for F 1 . Post on the NLPers Blog"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "American Chapter of the Association for Computational Linguistics and Human Language Technology (NAACL/HLT)"
            },
            "venue": {
                "fragments": [],
                "text": "American Chapter of the Association for Computational Linguistics and Human Language Technology (NAACL/HLT)"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mach Learn"
            },
            "venue": {
                "fragments": [],
                "text": "Mach Learn"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 45
                            }
                        ],
                        "text": "There are, of course, alternative frameworks (see, e.g., Weston et al. 2002; McAllester et al. 2004; Altun et al. 2004; McDonald et al. 2004; Tsochantaridis et al. 2005), but these are common examples."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Large margin online learning algorithms for scalable structured classification"
            },
            "venue": {
                "fragments": [],
                "text": "In NIPS workshop on learning with structured outputs"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Lower bounds for reductions"
            },
            "venue": {
                "fragments": [],
                "text": "The atomic learning workshop (TTI-C)"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Supervised and unsupervised learning for sentence compression. In Proceedings of the conference of the association for computational linguistics (ACL)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 36
                            }
                        ],
                        "text": "Recent work in sentence compression [26, 38] and document compression [13] attempts to take small steps beyond sentence extraction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Discriminative sentence compression with soft syntactic constraints"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the conference of the European association for computational linguistics"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 10
                            }
                        ],
                        "text": "Theorem 3 (K\u00e4\u00e4ri\u00e4inen 2006) There exists a distribution D over first order binary Markov"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Lower bounds for reductions. In The atomic learning workshop (TTI-C)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Lower bounds for reductions"
            },
            "venue": {
                "fragments": [],
                "text": "The atomic learning workshop ( TTI - C ) , March 2006 ."
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 9
                            }
                        ],
                        "text": "pression (Knight and Marcu 2002; McDonald 2006) and document compression (Daum\u00e9"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Discriminative sentence compression with soft syntactic constraints. In Proceedings of the conference of the European association for computational linguistics (EACL)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Intricacies of Collins' parsing model. Computational Linguistics"
            },
            "venue": {
                "fragments": [],
                "text": "Intricacies of Collins' parsing model. Computational Linguistics"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 11
                            }
                        ],
                        "text": "Theorem 3 ([22]) There exists a distribution D over first order binary Markov problems such that training a binary classifier based on true previous predictions to an error rate of > 0 leads to a Hamming loss given in Eq (15), where T is the length of the sequence."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Lower bounds for reductions. Talk at the Atomic Learning Workshop (TTI-C)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 140
                            }
                        ],
                        "text": "5We note in passing that directly optimizing F1 may not be the best approach, from the perspective of integrating information in a pipeline (Manning 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Doing named entity recognition? Don\u2019t optimize for"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Personal communication , June 2006 ."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 44,
            "methodology": 17,
            "result": 3
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 91,
        "totalPages": 10
    },
    "page_url": "https://www.semanticscholar.org/paper/Search-based-structured-prediction-Daum\u00e9-Langford/3c9d9f3c6f7508f4e29730924529dc993c27cddc?sort=total-citations"
}