{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145797806"
                        ],
                        "name": "G. Ridgeway",
                        "slug": "G.-Ridgeway",
                        "structuredName": {
                            "firstName": "Greg",
                            "lastName": "Ridgeway",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Ridgeway"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710305"
                        ],
                        "name": "D. Madigan",
                        "slug": "D.-Madigan",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Madigan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Madigan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10726446"
                        ],
                        "name": "T. Richardson",
                        "slug": "T.-Richardson",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Richardson",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Richardson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 41
                            }
                        ],
                        "text": "Some of these, such as those of Ridgeway [63] and Freund and Schapire [32], attempt to reduce the regression problem to a classification pr blem."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 32
                            }
                        ],
                        "text": "Some of these, such as those of Ridgeway [63] and Freund and Schapire [32], attempt to reduce the regression problem to a classification problem."
                    },
                    "intents": []
                }
            ],
            "corpusId": 10978542,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "dbd79e3c25d8a730ac1ee490aecf2469c043a2f2",
            "isKey": false,
            "numCitedBy": 102,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Classification problems have dominated research on boosting to date. The application of boosting to regression problems, on the other hand, has received little investigation. In this paper we develop a new boosting method for regression problems. We cast the regression problem as a classification problem and apply an interpretable form of the boosted naive Bayes classifier. This induces a regression model that we show to be expressible as an additive model for which we derive estimators and discuss computational issues. We compare the performance of our boosted naive Bayes regression model with other interpretable multivariate regression procedures."
            },
            "slug": "Boosting-methodology-for-regression-problems-Ridgeway-Madigan",
            "title": {
                "fragments": [],
                "text": "Boosting methodology for regression problems"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper develops a new boosting method for regression problems that casts the regression problem as a classification problem and applies an interpretable form of the boosted naive Bayes classifier, which induces a regression model that is shown to be expressible as an additive model."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144299726"
                        ],
                        "name": "Thomas G. Dietterich",
                        "slug": "Thomas-G.-Dietterich",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dietterich",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas G. Dietterich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12394453,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "45310c3ace403329b1fce03cb973830412bf7307",
            "isKey": false,
            "numCitedBy": 2615,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Bagging and boosting are methods that generate a diverse ensemble of classifiers by manipulating the training data given to a \u201cbase\u201d learning algorithm. Breiman has pointed out that they rely for their effectiveness on the instability of the base learning algorithm. An alternative approach to generating an ensemble is to randomize the internal decisions made by the base algorithm. This general approach has been studied previously by Ali and Pazzani and by Dietterich and Kong. This paper compares the effectiveness of randomization, bagging, and boosting for improving the performance of the decision-tree algorithm C4.5. The experiments show that in situations with little or no classification noise, randomization is competitive with (and perhaps slightly superior to) bagging but not as accurate as boosting. In situations with substantial classification noise, bagging is much better than boosting, and sometimes better than randomization."
            },
            "slug": "An-Experimental-Comparison-of-Three-Methods-for-of-Dietterich",
            "title": {
                "fragments": [],
                "text": "An Experimental Comparison of Three Methods for Constructing Ensembles of Decision Trees: Bagging, Boosting, and Randomization"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The experiments show that in situations with little or no classification noise, randomization is competitive with (and perhaps slightly superior to) bagging but not as accurate as boosting, and sometimes better than randomization."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144518416"
                        ],
                        "name": "Holger Schwenk",
                        "slug": "Holger-Schwenk",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Schwenk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Holger Schwenk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 69
                            }
                        ],
                        "text": "AdaBoost has been tested empirically by many researchers, i nclud ng [4, 18, 21, 40, 49, 59, 73]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5244761,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d7dbd8503a9fe61c8e02465de2fa327e4d89c05",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "\"Boosting\" is a general method for improving the performance of any learning algorithm that consistently generates classifiers which need to perform only slightly better than random guessing. A recently proposed and very promising boosting algorithm is AdaBoost [5]. It has been applied with great success to several benchmark machine learning problems using rather simple learning algorithms [4], and decision trees [1, 2, 6]. In this paper we use AdaBoost to improve the performances of neural networks. We compare training methods based on sampling the training set and weighting the cost function. Our system achieves about 1.4% error on a data base of online handwritten digits from more than 200 writers. Adaptive boosting of a multi-layer network achieved 1.5% error on the UCI Letters and 8.1 % error on the UCI satellite data set."
            },
            "slug": "Training-Methods-for-Adaptive-Boosting-of-Neural-Schwenk-Bengio",
            "title": {
                "fragments": [],
                "text": "Training Methods for Adaptive Boosting of Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper uses AdaBoost to improve the performances of neural networks and compares training methods based on sampling the training set and weighting the cost function."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1836349,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "68c1bfe375dde46777fe1ac8f3636fb651e3f0f8",
            "isKey": false,
            "numCitedBy": 8626,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "In an earlier paper, we introduced a new \"boosting\" algorithm called AdaBoost which, theoretically, can be used to significantly reduce the error of any learning algorithm that con- sistently generates classifiers whose performance is a little better than random guessing. We also introduced the related notion of a \"pseudo-loss\" which is a method for forcing a learning algorithm of multi-label concepts to concentrate on the labels that are hardest to discriminate. In this paper, we describe experiments we carried out to assess how well AdaBoost with and without pseudo-loss, performs on real learning problems. We performed two sets of experiments. The first set compared boosting to Breiman's \"bagging\" method when used to aggregate various classifiers (including decision trees and single attribute- value tests). We compared the performance of the two methods on a collection of machine-learning benchmarks. In the second set of experiments, we studied in more detail the performance of boosting using a nearest-neighbor classifier on an OCR problem."
            },
            "slug": "Experiments-with-a-New-Boosting-Algorithm-Freund-Schapire",
            "title": {
                "fragments": [],
                "text": "Experiments with a New Boosting Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper describes experiments carried out to assess how well AdaBoost with and without pseudo-loss, performs on real learning problems and compared boosting to Breiman's \"bagging\" method when used to aggregate various classifiers."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143857271"
                        ],
                        "name": "Nigel P. Duffy",
                        "slug": "Nigel-P.-Duffy",
                        "structuredName": {
                            "firstName": "Nigel",
                            "lastName": "Duffy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nigel P. Duffy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1772099"
                        ],
                        "name": "D. Helmbold",
                        "slug": "D.-Helmbold",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Helmbold",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Helmbold"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Others, such as those of Friedman [35] and Duffy and Helmbold [24] use the functional gradient descent view of boosting to derive algorithms that directly minimize a loss function appropriate for regression."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11021804,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cc345e4c25ab9b34b6c4e930086bb2e44871ce70",
            "isKey": false,
            "numCitedBy": 160,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we examine ensemble methods for regression that leverage or \u201cboost\u201d base regressors by iteratively calling them on modified samples. The most successful leveraging algorithm for classification is AdaBoost, an algorithm that requires only modest assumptions on the base learning method for its strong theoretical guarantees. We present several gradient descent leveraging algorithms for regression and prove AdaBoost-style bounds on their sample errors using intuitive assumptions on the base learners. We bound the complexity of the regression functions produced in order to derive PAC-style bounds on their generalization errors. Experiments validate our theoretical results."
            },
            "slug": "Boosting-Methods-for-Regression-Duffy-Helmbold",
            "title": {
                "fragments": [],
                "text": "Boosting Methods for Regression"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "This paper examines ensemble methods for regression that leverage or \u201cboost\u201d base regressors by iteratively calling them on modified samples and bound the complexity of the regression functions produced in order to derive PAC-style bounds on their generalization errors."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2050470845"
                        ],
                        "name": "H. Drucker",
                        "slug": "H.-Drucker",
                        "structuredName": {
                            "firstName": "Harris",
                            "lastName": "Drucker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Drucker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 62
                            }
                        ],
                        "text": "Another boosting-based approach to regression was proposed by Drucker [20]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 71
                            }
                        ],
                        "text": "Another boosting-based a pproach to regression was proposed by Drucker [20]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 79
                            }
                        ],
                        "text": "The first experiments with these early boosting algorithms were carried out by Drucker, Schapire and Simard [22] on an OCR task."
                    },
                    "intents": []
                }
            ],
            "corpusId": 16242966,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d8226a52ebc70c8d97ccae10a74e1b0a3908ec1",
            "isKey": false,
            "numCitedBy": 542,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "In the regression context, boosting and bagging are techniques to build a committee of regressors that may be superior to a single regressor. We use regression trees as fundamental building blocks in bagging committee machines and boosting committee machines. Performance is analyzed on three non-linear functions and the Boston housing database. In all cases, boosting is at least equivalent, and in most cases better than bagging in terms of prediction error."
            },
            "slug": "Improving-Regressors-using-Boosting-Techniques-Drucker",
            "title": {
                "fragments": [],
                "text": "Improving Regressors using Boosting Techniques"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This work uses regression trees as fundamental building blocks in bagging committee machines and boosting committee machines to build a committee of regressors that may be superior to a single regressor."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057702866"
                        ],
                        "name": "Eric Bauer",
                        "slug": "Eric-Bauer",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Bauer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eric Bauer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726733"
                        ],
                        "name": "Ron Kohavi",
                        "slug": "Ron-Kohavi",
                        "structuredName": {
                            "firstName": "Ron",
                            "lastName": "Kohavi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ron Kohavi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 69
                            }
                        ],
                        "text": "AdaBoost has been tested empirically by many researchers, i nclud ng [4, 18, 21, 40, 49, 59, 73]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1088806,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fc80cdf18198dc5677ccabe501b6f7209c28483c",
            "isKey": false,
            "numCitedBy": 2466,
            "numCiting": 88,
            "paperAbstract": {
                "fragments": [],
                "text": "Methods for voting classification algorithms, such as Bagging and AdaBoost, have been shown to be very successful in improving the accuracy of certain classifiers for artificial and real-world datasets. We review these algorithms and describe a large empirical study comparing several variants in conjunction with a decision tree inducer (three variants) and a Naive-Bayes inducer. The purpose of the study is to improve our understanding of why and when these algorithms, which use perturbation, reweighting, and combination techniques, affect classification error. We provide a bias and variance decomposition of the error to show how different methods and variants influence these two terms. This allowed us to determine that Bagging reduced variance of unstable methods, while boosting methods (AdaBoost and Arc-x4) reduced both the bias and variance of unstable methods but increased the variance for Naive-Bayes, which was very stable. We observed that Arc-x4 behaves differently than AdaBoost if reweighting is used instead of resampling, indicating a fundamental difference. Voting variants, some of which are introduced in this paper, include: pruning versus no pruning, use of probabilistic estimates, weight perturbations (Wagging), and backfitting of data. We found that Bagging improves when probabilistic estimates in conjunction with no-pruning are used, as well as when the data was backfit. We measure tree sizes and show an interesting positive correlation between the increase in the average tree size in AdaBoost trials and its success in reducing the error. We compare the mean-squared error of voting methods to non-voting methods and show that the voting methods lead to large and significant reductions in the mean-squared errors. Practical problems that arise in implementing boosting algorithms are explored, including numerical instabilities and underflows. We use scatterplots that graphically show how AdaBoost reweights instances, emphasizing not only \u201chard\u201d areas but also outliers and noise."
            },
            "slug": "An-Empirical-Comparison-of-Voting-Classification-Bauer-Kohavi",
            "title": {
                "fragments": [],
                "text": "An Empirical Comparison of Voting Classification Algorithms: Bagging, Boosting, and Variants"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It is found that Bagging improves when probabilistic estimates in conjunction with no-pruning are used, as well as when the data was backfit, and that Arc-x4 behaves differently than AdaBoost if reweighting is used instead of resampling, indicating a fundamental difference."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341779"
                        ],
                        "name": "J. R. Quinlan",
                        "slug": "J.-R.-Quinlan",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Quinlan",
                            "middleNames": [
                                "Ross"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. R. Quinlan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 48
                            }
                        ],
                        "text": "However, in early exper iments, several authors [8, 21, 59] observed empirically that boosting often d oesnot overfit, even when run for thousands of rounds."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 69
                            }
                        ],
                        "text": "AdaBoost has been tested empirically by many researchers, i nclud ng [4, 18, 21, 40, 49, 59, 73]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 937841,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "79ea6a5a68e05065f82acd11a478aa7eac5f6c06",
            "isKey": false,
            "numCitedBy": 1657,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Breiman's bagging and Freund and Schapire's boosting are recent methods for improving the predictive power of classifier learning systems. Both form a set of classifiers that are combined by voting, bagging by generating replicated bootstrap samples of the data, and boosting by adjusting the weights of training instances. This paper reports results of applying both techniques to a system that learns decision trees and testing on a representative collection of datasets. While both approaches substantially improve predictive accuracy, boosting shows the greater benefit. On the other hand, boosting also produces severe degradation on some datasets. A small change to the way that boosting combines the votes of learned classifiers reduces this downside and also leads to slightly better results on most of the datasets considered."
            },
            "slug": "Bagging,-Boosting,-and-C4.5-Quinlan",
            "title": {
                "fragments": [],
                "text": "Bagging, Boosting, and C4.5"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "Results of applying Breiman's bagging and Freund and Schapire's boosting to a system that learns decision trees and testing on a representative collection of datasets show boosting shows the greater benefit."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI, Vol. 1"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Friedman, Hastie and Tibshirani [34] suggested a method for using the output of AdaBoost to make reasonable estimates of such probabilities."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9913392,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6f4493eff2531536a7aeb3fc11d62c30a8f487f6",
            "isKey": false,
            "numCitedBy": 4829,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Boosting is one of the most important recent developments in classification methodology. Boosting works by sequentially applying a classification algorithm to reweighted versions of the training data and then taking a weighted majority vote of the sequence of classifiers thus produced. For many classification algorithms, this simple strategy results in dramatic improvements in performance. We show that this seemingly mysterious phenomenon can be understood in terms of well-known statistical principles, namely additive modeling and maximum likelihood. For the two-class problem, boosting can be viewed as an approximation to additive modeling on the logistic scale using maximum Bernoulli likelihood as a criterion. We develop more direct approximations and show that they exhibit nearly identical results to boosting. Direct multiclass generalizations based on multinomial likelihood are derived that exhibit performance comparable to other recently proposed multiclass generalizations of boosting in most situations, and far superior in some. We suggest a minor modification to boosting that can reduce computation, often by factors of 10 to 50. Finally, we apply these insights to produce an alternative formulation of boosting decision trees. This approach, based on best-first truncated tree induction, often leads to better performance, and can provide interpretable descriptions of the aggregate decision rule. It is also much faster computationally, making it more suitable to large-scale data mining applications."
            },
            "slug": "Special-Invited-Paper-Additive-logistic-regression:-Friedman",
            "title": {
                "fragments": [],
                "text": "Special Invited Paper-Additive logistic regression: A statistical view of boosting"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work shows that this seemingly mysterious phenomenon of boosting can be understood in terms of well-known statistical principles, namely additive modeling and maximum likelihood, and develops more direct approximations and shows that they exhibit nearly identical results to boosting."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 22
                            }
                        ],
                        "text": "A different technique [67], which incorporates Diet terich and Bakiri\u2019s [19] method of error-correcting output codes, achieves similar provable bounds to those of AdaBoost."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2685539,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cc1374bcd952032dabe891114f29092b868e01b8",
            "isKey": false,
            "numCitedBy": 319,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a new technique for solv- ing multiclass learning problems by combining Freund and Schapire's boosting algorithm with the main ideas of Diet- terich and Bakiri's method of error-correcting output codes (ECOC). Boosting is a general method of improving the ac- curacy of a given base or \"weak\" learning algorithm. ECOC is a robust method of solving multiclass learning problems by reducing to a sequence of two-class problems. We show that our new hybrid method has advantages of both: Like ECOC, our method only requires that the base learning al- gorithm work on binary-labeled data. Like boosting, we prove that the method comes with strong theoretical guar- antees on the training and generalization error of the final combined hypothesis assuming only that the base learning algorithm perform slightly better than random guessing. Although previous methods were known for boosting multi- class problems, the new method may be significantly faster and require less programming effort in creating the base learning algorithm. We also compare the new algorithm experimentally to other voting methods."
            },
            "slug": "Using-output-codes-to-boost-multiclass-learning-Schapire",
            "title": {
                "fragments": [],
                "text": "Using output codes to boost multiclass learning problems"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "This paper describes a new technique for multiclass learning problems by combining Freund and Schapire's boosting algorithm with the main ideas of Diet- terich and Bakiri's method of error-correcting output codes (ECOC), and shows that the new hybrid method has advantages of both."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709927"
                        ],
                        "name": "R. Maclin",
                        "slug": "R.-Maclin",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Maclin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Maclin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752379"
                        ],
                        "name": "D. Opitz",
                        "slug": "D.-Opitz",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Opitz",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Opitz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9378257,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3653266b5427295bdd54d6a22bf4caaa8c0b6961",
            "isKey": false,
            "numCitedBy": 286,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "An ensemble consists of a set of independently trained classifiers (such as neural networks or decision trees) whose predictions are combined when classifying novel instances. Previous research has shown that an ensemble as a whole is often more accurate than any of the single classifiers in the ensemble. Bagging (Breiman 1996a) and Boosting (Freund & Schapire 1996) are two relatively new but popular methods for producing ensembles. In this paper we evaluate these methods using both neural networks and decision trees as our classification algorithms. Our results clearly show two important facts. The first is that even though Bagging almost always produces a better classifier than any of its individual component classifiers and is relatively impervious to overfitting, it does not generalize any better than a baseline neural-network ensemble method. The second is that Boosting is a powerful technique that can usually produce better ensembles than Bagging; however, it is more susceptible to noise and can quickly overfit a data set."
            },
            "slug": "An-Empirical-Evaluation-of-Bagging-and-Boosting-Maclin-Opitz",
            "title": {
                "fragments": [],
                "text": "An Empirical Evaluation of Bagging and Boosting"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The results clearly show that even though Bagging almost always produces a better classifier than any of its individual component classifiers and is relatively impervious to overfitting, it does not generalize any better than a baseline neural-network ensemble method."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717932"
                        ],
                        "name": "G. Lebanon",
                        "slug": "G.-Lebanon",
                        "structuredName": {
                            "firstName": "Guy",
                            "lastName": "Lebanon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Lebanon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3156468,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aecad954bbbc5d0d5d02d2f799a50a65ec5e6dde",
            "isKey": false,
            "numCitedBy": 190,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We derive an equivalence between AdaBoost and the dual of a convex optimization problem, showing that the only difference between minimizing the exponential loss used by AdaBoost and maximum likelihood for exponential models is that the latter requires the model to be normalized to form a conditional probability distribution over labels. In addition to establishing a simple and easily understood connection between the two methods, this framework enables us to derive new regularization procedures for boosting that directly correspond to penalized maximum likelihood. Experiments on UCI datasets support our theoretical analysis and give additional insight into the relationship between boosting and logistic regression."
            },
            "slug": "Boosting-and-Maximum-Likelihood-for-Exponential-Lebanon-Lafferty",
            "title": {
                "fragments": [],
                "text": "Boosting and Maximum Likelihood for Exponential Models"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "An equivalence is derived between AdaBoost and the dual of a convex optimization problem, showing that the only difference between minimizing the exponential loss used by Ada boost and maximum likelihood for exponential models is that the latter requires the model to be normalized to form a conditional probability distribution over labels."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 1
                            }
                        ],
                        "text": "The algorithm takes as input a training set I ] ^ where each belongs to some domain or instance space , and each label D is in some label set\n."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2329907,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "14e53403a0055dbe5faaf9f1f3be96ca0e692a4d",
            "isKey": false,
            "numCitedBy": 1918,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe several improvements to Freund and Schapire's AdaBoost boosting algorithm, particularly in a setting in which hypotheses may assign confidences to each of their predictions. We give a simplified analysis of AdaBoost in this setting, and we show how this analysis can be used to find improved parameter settings as well as a refined criterion for training weak hypotheses. We give a specific method for assigning confidences to the predictions of decision trees, a method closely related to one used by Quinlan. This method also suggests a technique for growing decision trees which turns out to be identical to one proposed by Kearns and Mansour. We focus next on how to apply the new boosting algorithms to multiclass classification problems, particularly to the multi-label case in which each example may belong to more than one class. We give two boosting methods for this problem, plus a third method based on output coding. One of these leads to a new method for handling the single-label case which is simpler but as effective as techniques suggested by Freund and Schapire. Finally, we give some experimental results comparing a few of the algorithms discussed in this paper."
            },
            "slug": "Improved-Boosting-Algorithms-Using-Confidence-rated-Schapire-Singer",
            "title": {
                "fragments": [],
                "text": "Improved Boosting Algorithms Using Confidence-rated Predictions"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "Several improvements to Freund and Schapire's AdaBoost boosting algorithm are described, particularly in a setting in which hypotheses may assign confidences to each of their predictions."
            },
            "venue": {
                "fragments": [],
                "text": "COLT' 98"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2050470845"
                        ],
                        "name": "H. Drucker",
                        "slug": "H.-Drucker",
                        "structuredName": {
                            "firstName": "Harris",
                            "lastName": "Drucker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Drucker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 109
                            }
                        ],
                        "text": "The first experiments wit h these early boosting algorithms were carried out by Drucker, Schapire and Simard [22] on an OCR task."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 79
                            }
                        ],
                        "text": "The first experiments with these early boosting algorithms were carried out by Drucker, Schapire and Simard [22] on an OCR task."
                    },
                    "intents": []
                }
            ],
            "corpusId": 33515643,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "843ffb9898cedf899ddcdb9c4bdd10881c122429",
            "isKey": false,
            "numCitedBy": 228,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A boosting algorithm, based on the probably approximately correct (PAC) learning model is used to construct an ensemble of neural networks that significantly improves performance (compared to a single network) in optical character recognition (OCR) problems. The effect of boosting is reported on four handwritten image databases consisting of 12000 digits from segmented ZIP Codes from the United States Postal Service and the following from the National Institute of Standards and Technology: 220000 digits, 45000 upper case letters, and 45000 lower case letters. We use two performance measures: the raw error rate (no rejects) and the reject rate required to achieve a 1% error rate on the patterns not rejected. Boosting improved performance significantly, and, in some cases, dramatically."
            },
            "slug": "Boosting-Performance-in-Neural-Networks-Drucker-Schapire",
            "title": {
                "fragments": [],
                "text": "Boosting Performance in Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "The boosting algorithm is used to construct an ensemble of neural networks that significantly improves performance (compared to a single network) in optical character recognition (OCR) problems and improved performance significantly, and, in some cases, dramatically."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Pattern Recognit. Artif. Intell."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2414086"
                        ],
                        "name": "G. R\u00e4tsch",
                        "slug": "G.-R\u00e4tsch",
                        "structuredName": {
                            "firstName": "Gunnar",
                            "lastName": "R\u00e4tsch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. R\u00e4tsch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794034"
                        ],
                        "name": "Manfred K. Warmuth",
                        "slug": "Manfred-K.-Warmuth",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Warmuth",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred K. Warmuth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2459012"
                        ],
                        "name": "S. Mika",
                        "slug": "S.-Mika",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Mika",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mika"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35205384"
                        ],
                        "name": "T. Onoda",
                        "slug": "T.-Onoda",
                        "structuredName": {
                            "firstName": "Takashi",
                            "lastName": "Onoda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Onoda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3294736"
                        ],
                        "name": "S. Lemm",
                        "slug": "S.-Lemm",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Lemm",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lemm"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 49456,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "24a56a8becf20ed1daf79497b33ff909e1f9811e",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 110,
            "paperAbstract": {
                "fragments": [],
                "text": "Boosting algorithms like AdaBoost and Arc-GV are iterative strategies to minimize a constrained objective function, equivalent to Barrier algorithms. Based on this new understanding it is shown that convergence of Boosting-type algorithms becomes simpler to prove and we outline directions to develop further Boosting schemes. In particular a new Boosting technique for regression \u2013 \"-Boost \u2013 is proposed."
            },
            "slug": "Barrier-Boosting-R\u00e4tsch-Warmuth",
            "title": {
                "fragments": [],
                "text": "Barrier Boosting"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown that convergence of Boosting-type algorithms becomes simpler to prove and directions to develop further Boosting schemes are outlined, in particular a new Boosting technique for regression \u2013 \"-Boost \u2013 is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "COLT"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700597"
                        ],
                        "name": "Jyrki Kivinen",
                        "slug": "Jyrki-Kivinen",
                        "structuredName": {
                            "firstName": "Jyrki",
                            "lastName": "Kivinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jyrki Kivinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794034"
                        ],
                        "name": "Manfred K. Warmuth",
                        "slug": "Manfred-K.-Warmuth",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Warmuth",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred K. Warmuth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Following up on work by Kivinen and Warmuth [43] and Lafferty [47], they derive this algorithm using a unification of logistic regression and boosting based on Bregman distances."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13289060,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c64f76165e66bdce90544b8edff39997fa55c6f3",
            "isKey": false,
            "numCitedBy": 134,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the AdaBoost procedure for boosting weak learners. In AdaBoost, a key step is choosing a new distribution on the training examples based on the old distribution and the mistakes made by the present weak hypothesis. We show how AdaBoost\u2019s choice of the new distribution can be seen as an approximate solution to the following problem: Find a new distribution that is closest to the old distribution subject to the constraint that the new distribution is orthogonal to the vector of mistakes of the current weak hypothesis. The distance (or divergence) between distributions is measured by the relative entropy. Alternatively, we could say that AdaBoost approximately projects the distribution vector onto a hyperplane defined by the mistake vector. We show that this new view of AdaBoost as an entropy projection is dual to the usual view of AdaBoost as minimizing the normalization factors of the updated distributions."
            },
            "slug": "Boosting-as-entropy-projection-Kivinen-Warmuth",
            "title": {
                "fragments": [],
                "text": "Boosting as entropy projection"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "It is shown how AdaBoost\u2019s choice of the new distribution can be seen as an approximate solution to the following problem: Find a new distribution that is closest to the old distribution subject to the constraint that thenew distribution is orthogonal to the vector of mistakes of the current weak hypothesis."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '99"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47690405"
                        ],
                        "name": "P. Moreno",
                        "slug": "P.-Moreno",
                        "structuredName": {
                            "firstName": "Pedro",
                            "lastName": "Moreno",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Moreno"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34614703"
                        ],
                        "name": "B. Logan",
                        "slug": "B.-Logan",
                        "structuredName": {
                            "firstName": "Beth",
                            "lastName": "Logan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Logan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681921"
                        ],
                        "name": "B. Raj",
                        "slug": "B.-Raj",
                        "structuredName": {
                            "firstName": "Bhiksha",
                            "lastName": "Raj",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Raj"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1180240,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cf692d561e57353bdc1014b2a47da2e12298198c",
            "isKey": false,
            "numCitedBy": 54,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present the application of a boosting classification algorithm to confidence scoring. We derive feature vectors from speech recognition lattices and feed them into a boosting classifier . This classifier combines hundreds of very simple \u2018weak learners\u2019 and derives classification rules that can reduce the confidence error rate by up to 34%. We compare our results to those obtained using two other standard classification techniques, Support Vector Machines (SVMs) and Classification and Regression Trees (CART), and show significant improvements. Furthermore, the nature of the boosting algorithm allows us to combine the best single classifier and improve its performance. We present experimental results on real world corpora derived from the Compaq SpeechBot Web index and from the HUB4 DARPA evaluation sets. We believe these results have wide applicability to audio indexing and to acoustic and language modeling adaptation where word confide nce scores can be used in iterative adaptation schemes. In Eurospeech\u20192001"
            },
            "slug": "A-boosting-approach-for-confidence-scoring-Moreno-Logan",
            "title": {
                "fragments": [],
                "text": "A boosting approach for confidence scoring"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "This paper derives feature vectors from speech recognition lattices and feed them into a boosting classifier that combines hundreds of very simple \u2018weak learners\u2019 and derives classification rules that can reduce the confidence error rate by up to 34%."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 28
                            }
                        ],
                        "text": "A year later, Freund [26] developed a much more efficient boosting algorithm which, although optimal in a certain sense, nevertheless suffered like Schapire\u2019s algorithm from certain practical drawbacks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 19728033,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b824cb051ffbdd81b529c4b82379a3af270fb6f7",
            "isKey": false,
            "numCitedBy": 1284,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract We present an algorithm for improving the accuracy of algorithms for learning binary concepts. The improvement is achieved by combining a large number of hypotheses, each of which is generated by training the given learning algorithm on a different set of examples. Our algorithm is based on ideas presented by Schapire and represents an improvement over his results, The analysis of our algorithm provides general upper bounds on the resources required for learning in Valiant\u2032s polynomial PAC learning framework, which are the best general upper bounds known today. We show that the number of hypotheses that are combined by our algorithm is the smallest number possible. Other outcomes of our analysis are results regarding the representational power of threshold circuits, the relation between learnability and compression, and a method for parallelizing PAC learning algorithms. We provide extensions of our algorithms to cases in which the concepts are not binary and to the case where the accuracy of the learning algorithm depends on the distribution of the instances."
            },
            "slug": "Boosting-a-weak-learning-algorithm-by-majority-Freund",
            "title": {
                "fragments": [],
                "text": "Boosting a weak learning algorithm by majority"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "An algorithm for improving the accuracy of algorithms for learning binary concepts by combining a large number of hypotheses, each of which is generated by training the given learning algorithm on a different set of examples, is presented."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '90"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2050470845"
                        ],
                        "name": "H. Drucker",
                        "slug": "H.-Drucker",
                        "structuredName": {
                            "firstName": "Harris",
                            "lastName": "Drucker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Drucker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145115014"
                        ],
                        "name": "Corinna Cortes",
                        "slug": "Corinna-Cortes",
                        "structuredName": {
                            "firstName": "Corinna",
                            "lastName": "Cortes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Corinna Cortes"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "AdaBoost has been tested empirically by many researchers, including [4, 18, 21, 40, 49, 59, 73]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "However, in early experiments, several authors [8, 21, 59] observed empirically that boosting often does not overfit, even when run for thousands of rounds."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1266014,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a1dfeb731fc0c79e04523cd655413c223f6fa102",
            "isKey": false,
            "numCitedBy": 280,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a constructive, incremental learning system for regression problems that models data by means of locally linear experts. In contrast to other approaches, the experts are trained independently and do not compete for data during learning. Only when a prediction for a query is required do the experts cooperate by blending their individual predictions. Each expert is trained by minimizing a penalized local cross validation error using second order methods. In this way, an expert is able to find a local distance metric by adjusting the size and shape of the receptive field in which its predictions are valid, and also to detect relevant input features by adjusting its bias on the importance of individual input dimensions. We derive asymptotic results for our method. In a variety of simulations the properties of the algorithm are demonstrated with respect to interference, learning speed, prediction accuracy, feature detection, and task oriented incremental learning."
            },
            "slug": "Boosting-Decision-Trees-Drucker-Cortes",
            "title": {
                "fragments": [],
                "text": "Boosting Decision Trees"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A constructive, incremental learning system for regression problems that models data by means of locally linear experts that does not compete for data during learning and derives asymptotic results for this method."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 133
                            }
                        ],
                        "text": "In ad dition, the margin theory points to a strong connection between boosting and th e support-vector machines of Vapnik and others [7, 14, 77] which explicitly atte mpt to maximize the minimum margin."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207165665,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2599131a4bc2fa957338732a37c744cfe3e17b24",
            "isKey": false,
            "numCitedBy": 10840,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms."
            },
            "slug": "A-training-algorithm-for-optimal-margin-classifiers-Boser-Guyon",
            "title": {
                "fragments": [],
                "text": "A training algorithm for optimal margin classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented, applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '92"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2187850"
                        ],
                        "name": "Adam J. Grove",
                        "slug": "Adam-J.-Grove",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Grove",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam J. Grove"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714772"
                        ],
                        "name": "Dale Schuurmans",
                        "slug": "Dale-Schuurmans",
                        "structuredName": {
                            "firstName": "Dale",
                            "lastName": "Schuurmans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dale Schuurmans"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1556809,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "639057ad00ddaf8bbed3fa0dbd9663dfeb663d62",
            "isKey": false,
            "numCitedBy": 282,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "The \"minimum margin\" of an ensemble classifier on a given training set is, roughly speaking, the smallest vote it gives to any correct training label. Recent work has shown that the Adaboost algorithm is particularly effective at producing ensembles with large minimum margins, and theory suggests that this may account for its success at reducing generalization error. We note, however, that the problem of finding good margins is closely related to linear programming, and we use this connection to derive and test new \"LPboosting\" algorithms that achieve better minimum margins than Adaboost.However, these algorithms do not always yield better generalization performance. In fact, more often the opposite is true. We report on a series of controlled experiments which show that no simple version of the minimum-margin story can be complete. We conclude that the crucial question as to why boosting works so well in practice, and how to further improve upon it, remains mostly open.Some of our experiments are interesting for another reason: we show that Adaboost sometimes does overfit--eventually. This may take a very long time to occur, however, which is perhaps why this phenomenon has gone largely unnoticed."
            },
            "slug": "Boosting-in-the-Limit:-Maximizing-the-Margin-of-Grove-Schuurmans",
            "title": {
                "fragments": [],
                "text": "Boosting in the Limit: Maximizing the Margin of Learned Ensembles"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The crucial question as to why boosting works so well in practice, and how to further improve upon it, remains mostly open, and it is concluded that no simple version of the minimum-margin story can be complete."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784986"
                        ],
                        "name": "V. Koltchinskii",
                        "slug": "V.-Koltchinskii",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Koltchinskii",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Koltchinskii"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144611153"
                        ],
                        "name": "D. Panchenko",
                        "slug": "D.-Panchenko",
                        "structuredName": {
                            "firstName": "Dmitry",
                            "lastName": "Panchenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Panchenko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145358525"
                        ],
                        "name": "Fernando Lozano",
                        "slug": "Fernando-Lozano",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Lozano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando Lozano"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 54
                            }
                        ],
                        "text": "On the other hand, Koltchinskii, Panchenko and Lozano [44, 45, 46, 58] have recently proved new margin-theoretic bounds that are tight enough to give useful quantitative predictions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11921243,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4db3d20b41628d3db51a2cb646474f54060aaa8c",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present new bounds on the generalization error of a classifier f constructed as a convex combination of base classifiers from the class H. The algorithms of combining simple classifiers into a complex one, such as boosting and bagging, have attracted a lot of attention. We obtain new sharper bounds on the generalization error of combined classifiers that take into account both the empirical distribution of \"classification margins\" and the \"approximate dimension\" of the classifier, which is defined in terms of weights assigned to base classifiers by a voting algorithm. We study the performance of these bounds in several experiments with learning algorithms."
            },
            "slug": "Further-Explanation-of-the-Effectiveness-of-Voting-Koltchinskii-Panchenko",
            "title": {
                "fragments": [],
                "text": "Further Explanation of the Effectiveness of Voting Methods: The Game between Margins and Weights"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "New sharper bounds on the generalization error of combined classifiers that take into account both the empirical distribution of \"classification margins\" and the \"approximate dimension\" of the classifier, which is defined in terms of weights assigned to base classifiers by a voting algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "COLT/EuroCOLT"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143857271"
                        ],
                        "name": "Nigel P. Duffy",
                        "slug": "Nigel-P.-Duffy",
                        "structuredName": {
                            "firstName": "Nigel",
                            "lastName": "Duffy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nigel P. Duffy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1772099"
                        ],
                        "name": "D. Helmbold",
                        "slug": "D.-Helmbold",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Helmbold",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Helmbold"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This view of boosting and its generalization are examined in considerable detail by Duffy and Helmbold [23], Mason et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This is because both algorithms are doing a kind of functional gradient descent, an observation that is spelled out and exploited by Breiman [9], Duffy and Helmbold [23], Mason et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12926428,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c6cd4446aa0398a5d2ed1b2f74bdadfbcb6629cc",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent interpretations of the Adaboost algorithm view it as performing a gradient descent on a potential function. Simply changing the potential function allows one to create new algorithms related to AdaBoost. However, these new algorithms are generally not known to have the formal boosting property. This paper examines the question of which potential functions lead to new algorithms that are boosters. The two main results are general sets of conditions on the potential; one set implies that the resulting algorithm is a booster, while the other implies that the algorithm is not. These conditions are applied to previously studied potential functions, such as those used by LogitBoost and Doom II."
            },
            "slug": "Potential-Boosters-Duffy-Helmbold",
            "title": {
                "fragments": [],
                "text": "Potential Boosters?"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "The question of which potential functions lead to new algorithms that are boosters is examined, and two main results are general sets of conditions on the potential that imply that the resulting algorithm is a booster, while the other implies that the algorithm is not."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145001121"
                        ],
                        "name": "J. C. Jackson",
                        "slug": "J.-C.-Jackson",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Jackson",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. C. Jackson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144557047"
                        ],
                        "name": "M. Craven",
                        "slug": "M.-Craven",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Craven",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Craven"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 852411,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2d249c66b2c3703e78674b5c1ec415c87576b9b5",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new algorithm designed to learn sparse perceptrons over input representations which include high-order features. Our algorithm, which is based on a hypothesis-boosting method, is able to PAC-learn a relatively natural class of target concepts. Moreover, the algorithm appears to work well in practice: on a set of three problem domains, the algorithm produces classifiers that utilize small numbers of features yet exhibit good generalization performance. Perhaps most importantly, our algorithm generates concept descriptions that are easy for humans to understand."
            },
            "slug": "Learning-Sparse-Perceptrons-Jackson-Craven",
            "title": {
                "fragments": [],
                "text": "Learning Sparse Perceptrons"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "A new algorithm designed to learn sparse perceptrons over input representations which include high-order features is introduced, which is based on a hypothesis-boosting method and is able to PAC-learn a relatively natural class of target concepts."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2586148"
                        ],
                        "name": "L. Mason",
                        "slug": "L.-Mason",
                        "structuredName": {
                            "firstName": "Llew",
                            "lastName": "Mason",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Mason"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In other work, Freund and Mason [29] showed how to apply boosting to learn a generalization of decision trees called \u201calternating trees."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3772657,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "29b7eebc893acd2c2596de227333480e7a118af8",
            "isKey": false,
            "numCitedBy": 819,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "The application of boosting procedures to decision tree algorithms has been shown to produce very accurate classi ers. These classiers are in the form of a majority vote over a number of decision trees. Unfortunately, these classi ers are often large, complex and di\u00c6cult to interpret. This paper describes a new type of classi cation rule, the alternating decision tree, which is a generalization of decision trees, voted decision trees and voted decision stumps. At the same time classi ers of this type are relatively easy to interpret. We present a learning algorithm for alternating decision trees that is based on boosting. Experimental results show it is competitive with boosted decision tree algorithms such as C5.0, and generates rules that are usually smaller in size and thus easier to interpret. In addition these rules yield a natural measure of classi cation con dence which can be used to improve the accuracy at the cost of abstaining from predicting examples that are hard to classify."
            },
            "slug": "The-Alternating-Decision-Tree-Learning-Algorithm-Freund-Mason",
            "title": {
                "fragments": [],
                "text": "The Alternating Decision Tree Learning Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A new type of classi cation rule, the alternating decision tree, which is a generalization of decision trees, voted decision trees and voted decision stumps and generates rules that are usually smaller in size and thus easier to interpret."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 39450643,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1679beddda3a183714d380e944fe6bf586c083cd",
            "isKey": false,
            "numCitedBy": 13771,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent boosting paradigm is developed for additive expansions based on any fitting criterion. Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such TreeBoost models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed."
            },
            "slug": "Greedy-function-approximation:-A-gradient-boosting-Friedman",
            "title": {
                "fragments": [],
                "text": "Greedy function approximation: A gradient boosting machine."
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A general gradient descent boosting paradigm is developed for additive expansions based on any fitting criterion, and specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1943813"
                        ],
                        "name": "Erin Allwein",
                        "slug": "Erin-Allwein",
                        "structuredName": {
                            "firstName": "Erin",
                            "lastName": "Allwein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Erin Allwein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 44
                            }
                        ],
                        "text": "In another set of experiments, Schapire and Singer [71] used boosting for text categorization tasks."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 13
                            }
                        ],
                        "text": "Schapire and Singer\u2019s [70] algorithm AdaBoost."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 57
                            }
                        ],
                        "text": "1 in the slightly generalized form given by Schapire and Singer [70]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 58
                            }
                        ],
                        "text": "Schapire and Singer [70] and Allwein, Schapire and Singer [2] give yet another method of combining boosting with errorcorrecting output codes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 10
                            }
                        ],
                        "text": "Cohen and Singer\u2019s system, called SLIPPER, is fast, accurate and produces quite compact rule sets."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 19
                            }
                        ],
                        "text": "Here, Schapire and Singer advocate choosing ?"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 44
                            }
                        ],
                        "text": "M2 (which is a special case of Schapire and Singer\u2019s [70] AdaBoost."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 10
                            }
                        ],
                        "text": "Cohen and Singer [11] showed how to design a base\nlearning algorithm that, when combined with AdaBoost, results in a final classifier consisting of a relatively small set of rules similar to those generated by systems like RIPPER [10], IREP [36] and C4.5rules [60]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 13
                            }
                        ],
                        "text": "Schapire and Singer [70] discuss the choice of ?"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 27
                            }
                        ],
                        "text": "Specifically, Schapire and Singer [70], in generalizing a theorem of Freund and Schapire [32], show that the training error of the final classifier is bounded as follows:$1 , 9 L O ) $1 X CFEHGI -\" : \\ 6 K 6 (2) where henceforth we define : Z : X 6 ?"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 63
                            }
                        ],
                        "text": "The modification of AdaBoost proposed by Collins, Schapire and Singer to handle this loss function is particularly simple."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 106
                            }
                        ],
                        "text": "A different, more direct modification of AdaBoost for logistic loss was proposed by Collins, Schapire and Singer [13]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9790719,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cd74cc5129c45268d4e766d3619e7cb0ead5c8c8",
            "isKey": false,
            "numCitedBy": 1991,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a unifying framework for studying the solution of multiclass categorization problems by reducing them to multiple binary problems that are then solved using a margin-based binary learning algorithm. The proposed framework unifies some of the most popular approaches in which each class is compared against all others, or in which all pairs of classes are compared to each other, or in which output codes with error-correcting properties are used. We propose a general method for combining the classifiers generated on the binary problems, and we prove a general empirical multiclass loss bound given the empirical loss of the individual binary learning algorithms. The scheme and the corresponding bounds apply to many popular classification learning algorithms including support-vector machines, AdaBoost, regression, logistic regression and decision-tree algorithms. We also give a multiclass generalization error analysis for general output codes with AdaBoost as the binary learner. Experimental results with SVM and AdaBoost show that our scheme provides a viable alternative to the most commonly used multiclass algorithms."
            },
            "slug": "Reducing-Multiclass-to-Binary:-A-Unifying-Approach-Allwein-Schapire",
            "title": {
                "fragments": [],
                "text": "Reducing Multiclass to Binary: A Unifying Approach for Margin Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A general method for combining the classifiers generated on the binary problems is proposed, and a general empirical multiclass loss bound is proved given the empirical loss of the individual binary learning algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6644398,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ccf5208521cb8c35f50ee8873df89294b8ed7292",
            "isKey": false,
            "numCitedBy": 13123,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "In the first part of the paper we consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight-update Littlestone?Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show how the resulting learning algorithm can be applied to a variety of problems, including gambling, multiple-outcome prediction, repeated games, and prediction of points in Rn. In the second part of the paper we apply the multiplicative weight-update technique to derive a new boosting algorithm. This boosting algorithm does not require any prior knowledge about the performance of the weak learning algorithm. We also study generalizations of the new boosting algorithm to the problem of learning functions whose range, rather than being binary, is an arbitrary finite set or a bounded segment of the real line."
            },
            "slug": "A-decision-theoretic-generalization-of-on-line-and-Freund-Schapire",
            "title": {
                "fragments": [],
                "text": "A decision-theoretic generalization of on-line learning and an application to boosting"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "The model studied can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting, and it is shown that the multiplicative weight-update Littlestone?Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems."
            },
            "venue": {
                "fragments": [],
                "text": "EuroCOLT"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 53
                            }
                        ],
                        "text": "Following up on work by Kivinen and Warmuth [43] and Lafferty [47], they derive this algorithm using a unification of logistic regression and boosting based on Bregman distances."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 62
                            }
                        ],
                        "text": "Following up on work by Kivinen and Warmuth [43] and Lafferty [47], they derive this algorithm usin g a unification of logistic regression and boosting based on Bregman distances."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 39
                            }
                        ],
                        "text": "See also the later work of Lebanon and Lafferty [48] who showed that logistic regression and boosting are in fact solving the same constrained optimization problem, except that in boosting, certain normalization constraints have been dropped."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6336008,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b03f43c5620bfc8993ea25dee20ce52a203ebcf7",
            "isKey": false,
            "numCitedBy": 91,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a framework for designing incremental learning algorithms derived from generalized entropy functionals. Our approach is based on the use of Bregman divergences together with the associated class of additive models constructed using the Legendre transform. A particular one-parameter family of Bregman divergences is shown to yield a family of loss functions that includes the log-likelihood criterion of logistic regression as a special case, and that closely approximates the exponential loss criterion used in the AdaBoost algorithms of Schapire et a/., as the natural parameter of the family varies. We also show how the quadratic approximation of the gain in Bregman divergence results in a weighted least-squares criterion. This leads to a family of incremental learning algorithms that builds upon and extends the recent interpretation of boosting in terms of additive models proposed by Friedman, Hastie, and Tibshirani."
            },
            "slug": "Additive-models,-boosting,-and-inference-for-Lafferty",
            "title": {
                "fragments": [],
                "text": "Additive models, boosting, and inference for generalized divergences"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "A framework for designing incremental learning algorithms derived from generalized entropy functionals based on the use of Bregman divergences together with the associated class of additive models constructed using the Legendre transform is presented."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '99"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2022386739"
                        ],
                        "name": "Peter Barlett",
                        "slug": "Peter-Barlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Barlett",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Barlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740222"
                        ],
                        "name": "Wee Sun Lee",
                        "slug": "Wee-Sun-Lee",
                        "structuredName": {
                            "firstName": "Wee",
                            "lastName": "Lee",
                            "middleNames": [
                                "Sun"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wee Sun Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 573509,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4d19272112b50547614479a0c409fca66e3b05f7",
            "isKey": false,
            "numCitedBy": 2844,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the surprising recurring phenomena observed in experiments with boosting is that the test error of the generated classifier usually does not increase as its size becomes very large, and often is observed to decrease even after the training error reaches zero. In this paper, we show that this phenomenon is related to the distribution of margins of the training examples with respect to the generated voting classification rule, where the margin of an example is simply the difference between the number of correct votes and the maximum number of votes received by any incorrect label. We show that techniques used in the analysis of Vapnik's support vector classifiers and of neural networks with small weights can be applied to voting methods to relate the margin distribution to the test error. We also show theoretically and experimentally that boosting is especially effective at increasing the margins of the training examples. Finally, we compare our explanation to those based on the bias-variance"
            },
            "slug": "Boosting-the-margin:-A-new-explanation-for-the-of-Schapire-Freund",
            "title": {
                "fragments": [],
                "text": "Boosting the margin: A new explanation for the effectiveness of voting methods"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that techniques used in the analysis of Vapnik's support vector classifiers and of neural networks with small weights can be applied to voting methods to relate the margin distribution to the test error."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2185716,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e7a07c3aaef303850e5a1fcc81bb44f6d2db6696",
            "isKey": false,
            "numCitedBy": 2272,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "This work focuses on algorithms which learn from examples to perform multiclass text and speech categorization tasks. Our approach is based on a new and improved family of boosting algorithms. We describe in detail an implementation, called BoosTexter, of the new boosting algorithms for text categorization tasks. We present results comparing the performance of BoosTexter and a number of other text-categorization algorithms on a variety of tasks. We conclude by describing the application of our system to automatic call-type identification from unconstrained spoken customer responses."
            },
            "slug": "BoosTexter:-A-Boosting-based-System-for-Text-Schapire-Singer",
            "title": {
                "fragments": [],
                "text": "BoosTexter: A Boosting-based System for Text Categorization"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work describes in detail an implementation, called BoosTexter, of the new boosting algorithms for text categorization tasks, and presents results comparing the performance of Boos Texter and a number of other text-categorization algorithms on a variety of tasks."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2586148"
                        ],
                        "name": "L. Mason",
                        "slug": "L.-Mason",
                        "structuredName": {
                            "firstName": "Llew",
                            "lastName": "Mason",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Mason"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47392513"
                        ],
                        "name": "Jonathan Baxter",
                        "slug": "Jonathan-Baxter",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Baxter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Baxter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40073871"
                        ],
                        "name": "Marcus Frean",
                        "slug": "Marcus-Frean",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Frean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcus Frean"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[51, 52] and Friedman [35]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6101385,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "24c499f250196252626c19a56174436b08ff4f78",
            "isKey": false,
            "numCitedBy": 709,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We provide an abstract characterization of boosting algorithms as gradient decsent on cost-functionals in an inner-product function space. We prove convergence of these functional-gradient-descent algorithms under quite weak conditions. Following previous theoretical results bounding the generalization performance of convex combinations of classifiers in terms of general cost functions of the margin, we present a new algorithm (DOOM II) for performing a gradient descent optimization of such cost functions. Experiments on several data sets from the UC Irvine repository demonstrate that DOOM II generally outperforms AdaBoost, especially in high noise situations, and that the overfitting behaviour of AdaBoost is predicted by our cost functions."
            },
            "slug": "Boosting-Algorithms-as-Gradient-Descent-Mason-Baxter",
            "title": {
                "fragments": [],
                "text": "Boosting Algorithms as Gradient Descent"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Following previous theoretical results bounding the generalization performance of convex combinations of classifiers in terms of general cost functions of the margin, a new algorithm (DOOM II) is presented for performing a gradient descent optimization of such cost functions."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784986"
                        ],
                        "name": "V. Koltchinskii",
                        "slug": "V.-Koltchinskii",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Koltchinskii",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Koltchinskii"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144611153"
                        ],
                        "name": "D. Panchenko",
                        "slug": "D.-Panchenko",
                        "structuredName": {
                            "firstName": "Dmitry",
                            "lastName": "Panchenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Panchenko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145358525"
                        ],
                        "name": "Fernando Lozano",
                        "slug": "Fernando-Lozano",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Lozano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fernando Lozano"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 54
                            }
                        ],
                        "text": "On the other hand, Koltchinskii, Panchenko and Lozano [44, 45, 46, 58] have recently proved new margin-theoretic bounds that are tight enough to give useful quantitative predictions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2545964,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "4cb5275c91a2d0459491a1dee5548a51b52649c0",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we develop the method of bounding the generalization error of a classifier in terms of its margin distribution which was introduced in the recent papers of Bartlett and Schapire, Freund, Bartlett and Lee. The theory of Gaussian and empirical processes allow us to prove the margin type inequalities for the most general functional classes, the complexity of the class being measured via the so called Gaussian complexity functions. As a simple application of our results, we obtain the bounds of Schapire, Freund, Bartlett and Lee for the generalization error of boosting. We also substantially improve the results of Bartlett on bounding the generalization error of neural networks in terms of l1-norms of the weights of neurons. Furthermore, under additional assumptions on the complexity of the class of hypotheses we provide some tighter bounds, which in the case of boosting improve the results of Schapire, Freund, Bartlett and Lee."
            },
            "slug": "Some-New-Bounds-on-the-Generalization-Error-of-Koltchinskii-Panchenko",
            "title": {
                "fragments": [],
                "text": "Some New Bounds on the Generalization Error of Combined Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "The method of bounding the generalization error of a classifier in terms of its margin distribution is developed and under additional assumptions on the complexity of the class of hypotheses some tighter bounds are provided, which in the case of boosting improve the results of Schapire, Freund, Bartlett and Lee."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 7
                            }
                        ],
                        "text": "Freund [27] suggested another algorithm , called \u201cBrownBoost,\u201d that takes a more radical approach that de-emphasizes outli ers when it seems clear that they are \u201ctoo hard\u201d to classify correctly."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 975467,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "be9a9efa52c1c0cc708ce3dc79c85433ebd08108",
            "isKey": false,
            "numCitedBy": 276,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a new boosting algorithm. This boosting algorithm is an adaptive version of the boost by majority algorithm and combines bounded goals of the boost by majority algorithm with the adaptivity of AdaBoost.The method used for making boost-by-majority adaptive is to consider the limit in which each of the boosting iterations makes an infinitesimally small contribution to the process as a whole. This limit can be modeled using the differential equations that govern Brownian motion. The new boosting algorithm, named BrownBoost, is based on finding solutions to these differential equations.The paper describes two methods for finding approximate solutions to the differential equations. The first is a method that results in a provably polynomial time algorithm. The second method, based on the Newton-Raphson minimization procedure, is much more efficient in practice but is not known to be polynomial."
            },
            "slug": "An-Adaptive-Version-of-the-Boost-by-Majority-Freund",
            "title": {
                "fragments": [],
                "text": "An Adaptive Version of the Boost by Majority Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The paper describes two methods for finding approximate solutions to the differential equations and a method that results in a provably polynomial time algorithm based on the Newton-Raphson minimization procedure, which is much more efficient in practice but is not known to bePolynomial."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '99"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145163573"
                        ],
                        "name": "A. Singhal",
                        "slug": "A.-Singhal",
                        "structuredName": {
                            "firstName": "Amit",
                            "lastName": "Singhal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Singhal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 49
                            }
                        ],
                        "text": "Boosting has also been applied to text filtering [72] and rout ing [39], \u201cranking\u201d problems [28], learning problems arising in natural langua ge processing [1, 12, 25, 38, 55, 78], image retrieval [74], medical diagnosis [53], a nd customer monitoring and segmentation [56, 57]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7913028,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e5a4f203dcc8ee607a3d41c1f96c5e91f4a66117",
            "isKey": false,
            "numCitedBy": 379,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "We discuss two learning algorithms for text filtering: modified Rocchio and a boosting algorithm called AdaBoost. We show how both algorithms can be adapted to maximize any general utility matrix that associates cost (or gain) for each pair of machine prediction and correct label. We first show that AdaBoost significantly outperforms another highly effective text filtering algorithm. We then compare AdaBoost and Rocchio over three large text filtering tasks. Overall both algorithms are comparable and are quite effective. AdaBoost produces better classifiers than Rocchio when the training collection contains a very large number of relevant documents. However, on these tasks, Rocchio runs much faster than AdaBoost."
            },
            "slug": "Boosting-and-Rocchio-applied-to-text-filtering-Schapire-Singer",
            "title": {
                "fragments": [],
                "text": "Boosting and Rocchio applied to text filtering"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "This paper discusses two learning algorithms for text filtering: modified Rocchio and a boosting algorithm called AdaBoost, and shows how both algorithms can be adapted to maximize any general utility matrix that associates cost for each pair of machine prediction and correct label."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR '98"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2932893"
                        ],
                        "name": "A. Demiriz",
                        "slug": "A.-Demiriz",
                        "structuredName": {
                            "firstName": "Ayhan",
                            "lastName": "Demiriz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Demiriz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145728220"
                        ],
                        "name": "K. Bennett",
                        "slug": "K.-Bennett",
                        "structuredName": {
                            "firstName": "Kristin",
                            "lastName": "Bennett",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Bennett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2541381,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d55e5b86cb98afac27045a6ed53912a84c068071",
            "isKey": false,
            "numCitedBy": 459,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "We examine linear program (LP) approaches to boosting and demonstrate their efficient solution using LPBoost, a column generation based simplex method. We formulate the problem as if all possible weak hypotheses had already been generated. The labels produced by the weak hypotheses become the new feature space of the problem. The boosting task becomes to construct a learning function in the label space that minimizes misclassification error and maximizes the soft margin. We prove that for classification, minimizing the 1-norm soft margin error function directly optimizes a generalization error bound. The equivalent linear program can be efficiently solved using column generation techniques developed for large-scale optimization problems. The resulting LPBoost algorithm can be used to solve any LP boosting formulation by iteratively optimizing the dual misclassification costs in a restricted LP and dynamically generating weak hypotheses to make new LP columns. We provide algorithms for soft margin classification, confidence-rated, and regression boosting problems. Unlike gradient boosting algorithms, which may converge in the limit only, LPBoost converges in a finite number of iterations to a global solution satisfying mathematically well-defined optimality conditions. The optimal solutions of LPBoost are very sparse in contrast with gradient based methods. Computationally, LPBoost is competitive in quality and computational cost to AdaBoost."
            },
            "slug": "Linear-Programming-Boosting-via-Column-Generation-Demiriz-Bennett",
            "title": {
                "fragments": [],
                "text": "Linear Programming Boosting via Column Generation"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It is proved that for classification, minimizing the 1-norm soft margin error function directly optimizes a generalization error bound and is competitive in quality and computational cost to AdaBoost."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744109"
                        ],
                        "name": "S. Salzberg",
                        "slug": "S.-Salzberg",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Salzberg",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Salzberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800681"
                        ],
                        "name": "Alberto Maria Segre",
                        "slug": "Alberto-Maria-Segre",
                        "structuredName": {
                            "firstName": "Alberto",
                            "lastName": "Segre",
                            "middleNames": [
                                "Maria"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alberto Maria Segre"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 2
                            }
                        ],
                        "text": "5 [60] as a base learning algorithm, as well as an algorithm that finds the best \u201cdecision stump\u201d or single-test decision tree."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": "5 decision-tree learning algorithm [60] on the \u201cletter\u201d dataset."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60499165,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7feb0fc888cd55360949554db032d7d1cba9e947",
            "isKey": false,
            "numCitedBy": 7028,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Algorithms for constructing decision trees are among the most well known and widely used of all machine learning methods. Among decision tree algorithms, J. Ross Quinlan's ID3 and its successor, C4.5, are probably the most popular in the machine learning community. These algorithms and variations on them have been the subject of numerous research papers since Quinlan introduced ID3. Until recently, most researchers looking for an introduction to decision trees turned to Quinlan's seminal 1986 Machine Learning journal article [Quinlan, 1986]. In his new book, C4.5: Programs for Machine Learning, Quinlan has put together a definitive, much needed description of his complete system, including the latest developments. As such, this book will be a welcome addition to the library of many researchers and students."
            },
            "slug": "Programs-for-Machine-Learning-Salzberg-Segre",
            "title": {
                "fragments": [],
                "text": "Programs for Machine Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "In his new book, C4.5: Programs for Machine Learning, Quinlan has put together a definitive, much needed description of his complete system, including the latest developments, which will be a welcome addition to the library of many researchers and students."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207651918,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b54c9359e8858842d1b1b744ac5ca573b8031dcc",
            "isKey": false,
            "numCitedBy": 662,
            "numCiting": 103,
            "paperAbstract": {
                "fragments": [],
                "text": "We give a unified account of boosting and logistic regression in which each learning problem is cast in terms of optimization of Bregman distances. The striking similarity of the two problems in this framework allows us to design and analyze algorithms for both simultaneously, and to easily adapt algorithms designed for one problem to the other. For both problems, we give new algorithms and explain their potential advantages over existing methods. These algorithms are iterative and can be divided into two types based on whether the parameters are updated sequentially (one at a time) or in parallel (all at once). We also describe a parameterized family of algorithms that includes both a sequential- and a parallel-update algorithm as special cases, thus showing how the sequential and parallel approaches can themselves be unified. For all of the algorithms, we give convergence proofs using a general formalization of the auxiliary-function proof technique. As one of our sequential-update algorithms is equivalent to AdaBoost, this provides the first general proof of convergence for AdaBoost. We show that all of our algorithms generalize easily to the multiclass case, and we contrast the new algorithms with the iterative scaling algorithm. We conclude with a few experimental results with synthetic data that highlight the behavior of the old and newly proposed algorithms in different settings."
            },
            "slug": "Logistic-Regression,-AdaBoost-and-Bregman-Distances-Collins-Schapire",
            "title": {
                "fragments": [],
                "text": "Logistic Regression, AdaBoost and Bregman Distances"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "A unified account of boosting and logistic regression in which each learning problem is cast in terms of optimization of Bregman distances, and a parameterized family of algorithms that includes both a sequential- and a parallel-update algorithm as special cases are described, thus showing how the sequential and parallel approaches can themselves be unified."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068005320"
                        ],
                        "name": "Raj D. Iyer",
                        "slug": "Raj-D.-Iyer",
                        "structuredName": {
                            "firstName": "Raj",
                            "lastName": "Iyer",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Raj D. Iyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16692650,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "75e85c2e90b0abb17ae6445516a49ac05c1dbf0f",
            "isKey": false,
            "numCitedBy": 2182,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the problem of learning to accurately rank a set of objects by combining a given collection of ranking or preference functions. This problem of combining preferences arises in several applications, such as that of combining the results of different search engines, or the \"collaborative-filtering\" problem of ranking movies for a user based on the movie rankings provided by other users. In this work, we begin by presenting a formal framework for this general problem. We then describe and analyze an efficient algorithm called RankBoost for combining preferences based on the boosting approach to machine learning. We give theoretical results describing the algorithm's behavior both on the training data, and on new test data not seen during training. We also describe an efficient implementation of the algorithm for a particular restricted but common case. We next discuss two experiments we carried out to assess the performance of RankBoost. In the first experiment, we used the algorithm to combine different web search strategies, each of which is a query expansion for a given domain. The second experiment is a collaborative-filtering task for making movie recommendations."
            },
            "slug": "An-Efficient-Boosting-Algorithm-for-Combining-Freund-Iyer",
            "title": {
                "fragments": [],
                "text": "An Efficient Boosting Algorithm for Combining Preferences"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work describes and analyze an efficient algorithm called RankBoost for combining preferences based on the boosting approach to machine learning, and gives theoretical results describing the algorithm's behavior both on the training data, and on new test data not seen during training."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784986"
                        ],
                        "name": "V. Koltchinskii",
                        "slug": "V.-Koltchinskii",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Koltchinskii",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Koltchinskii"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144611153"
                        ],
                        "name": "D. Panchenko",
                        "slug": "D.-Panchenko",
                        "structuredName": {
                            "firstName": "Dmitry",
                            "lastName": "Panchenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Panchenko"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 54
                            }
                        ],
                        "text": "On the other hand, Koltchinskii, Panchenko and Lozano [44, 45, 46, 58] have recently proved new margin-theoretic bounds that are tight enough to give useful quantitative predictions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2307733,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "6388150296152c8173fae995e30c80a86d7cf1f7",
            "isKey": false,
            "numCitedBy": 503,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "We prove new probabilistic upper bounds on generalization error of complex classifiers that are combinations of simple classifiers. Such combinations could be implemented by neural networks or by voting methods of combining the classifiers, such as boosting and bagging. The bounds are in terms of the empirical distribution of the margin of the combined classifier. They are based on the methods of the theory of Gaussian and empirical processes (comparison inequalities, symmetrization method, concentration inequalities) and they improve previous results of Bartlett (1998) on bounding the generalization error of neural networks in terms of 1 -norms of the weights of neurons and of Schapire, Freund, Bartlett and Lee (1998) on bounding the generalization error of boosting. We also obtain rates of convergence in Levy distance of empirical margin distribution to the true margin distribution uniformly over the classes of classifiers and prove the optimality of these rates."
            },
            "slug": "Empirical-margin-distributions-and-bounding-the-of-Koltchinskii-Panchenko",
            "title": {
                "fragments": [],
                "text": "Empirical margin distributions and bounding the generalization error of combined classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "New probabilistic upper bounds on generalization error of complex classifiers that are combinations of simple classifier combinations, based on the methods of the theory of Gaussian and empirical processes are proved."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144299726"
                        ],
                        "name": "Thomas G. Dietterich",
                        "slug": "Thomas-G.-Dietterich",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dietterich",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas G. Dietterich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3242194"
                        ],
                        "name": "Ghulum Bakiri",
                        "slug": "Ghulum-Bakiri",
                        "structuredName": {
                            "firstName": "Ghulum",
                            "lastName": "Bakiri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ghulum Bakiri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 47109072,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d221bbcbd20c7157e4500f942de8ceec490f8936",
            "isKey": false,
            "numCitedBy": 2852,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Multiclass learning problems involve finding a definition for an unknown function f(x) whose range is a discrete set containing k > 2 values (i.e., k \"classes\"). The definition is acquired by studying collections of training examples of the form (xi, f(xi)). Existing approaches to multiclass learning problems include direct application of multiclass algorithms such as the decision-tree algorithms C4.5 and CART, application of binary concept learning algorithms to learn individual binary functions for each of the k classes, and application of binary concept learning algorithms with distributed output representations. This paper compares these three approaches to a new technique in which error-correcting codes are employed as a distributed output representation. We show that these output representations improve the generalization performance of both C4.5 and backpropagation on a wide range of multiclass learning tasks. We also demonstrate that this approach is robust with respect to changes in the size of the training sample, the assignment of distributed representations to particular classes, and the application of overfitting avoidance techniques such as decision-tree pruning. Finally, we show that--like the other methods--the error-correcting code technique can provide reliable class probability estimates. Taken together, these results demonstrate that error-correcting output codes provide a general-purpose method for improving the performance of inductive learning programs on multiclass problems."
            },
            "slug": "Solving-Multiclass-Learning-Problems-via-Output-Dietterich-Bakiri",
            "title": {
                "fragments": [],
                "text": "Solving Multiclass Learning Problems via Error-Correcting Output Codes"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It is demonstrated that error-correcting output codes provide a general-purpose method for improving the performance of inductive learning programs on multiclass problems."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49446819"
                        ],
                        "name": "S. Merler",
                        "slug": "S.-Merler",
                        "structuredName": {
                            "firstName": "Stefano",
                            "lastName": "Merler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Merler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687584"
                        ],
                        "name": "C. Furlanello",
                        "slug": "C.-Furlanello",
                        "structuredName": {
                            "firstName": "Cesare",
                            "lastName": "Furlanello",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Furlanello"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2092194"
                        ],
                        "name": "B. Larcher",
                        "slug": "B.-Larcher",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Larcher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Larcher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2315866"
                        ],
                        "name": "A. Sboner",
                        "slug": "A.-Sboner",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Sboner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sboner"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 223,
                                "start": 219
                            }
                        ],
                        "text": "Boosting has also been applied to text filtering [72] and routing [39], \u201cranking\u201d problems [28], learning problems arising in natural language processing [1, 12, 25, 38, 55, 78], image retrieval [74], medical diagnosis [53], and customer monitoring and segmentation [56, 57]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 37638454,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e80a8576009bcec4a589255a22f0c286e11ddd1b",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper investigates a methodology for effective model selection of cost-sensitive boosting algorithms. In many real situations, e.g. for automated medical diagnosis, it is crucial to tune the classification performance towards the sensitivity and specificity required by the user. To this purpose, for binary classification problems, we have designed a cost-sensitive variant of AdaBoost where (1) the model error function is weighted with separate costs for errors (false negative and false positives) in the two classes, and (2) the weights are updated differently for negatives and positives at each boosting step. Finally, (3) a practical search procedure allows to get into or as close as possible to the sensitivity and specificity constraints without an extensive tabulation of the ROC curve. This off-the-shelf methodology was applied for the automatic diagnosis of melanoma on a set of 152 skin lesions described by geometric and colorimetric features, out-performing, on the same data set, skilled dermatologists and a specialized automatic system based on a multiple classifier combination."
            },
            "slug": "Tuning-Cost-Sensitive-Boosting-and-Its-Application-Merler-Furlanello",
            "title": {
                "fragments": [],
                "text": "Tuning Cost-Sensitive Boosting and Its Application to Melanoma Diagnosis"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A cost-sensitive variant of AdaBoost is designed for binary classification problems, where the model error function is weighted with separate costs for errors in the two classes, and the weights are updated differently for negatives and positives at each boosting step."
            },
            "venue": {
                "fragments": [],
                "text": "Multiple Classifier Systems"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2586148"
                        ],
                        "name": "L. Mason",
                        "slug": "L.-Mason",
                        "structuredName": {
                            "firstName": "Llew",
                            "lastName": "Mason",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Mason"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47392513"
                        ],
                        "name": "Jonathan Baxter",
                        "slug": "Jonathan-Baxter",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Baxter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Baxter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 123
                            }
                        ],
                        "text": "Attempts (not always successful) to use the insights gleane d from the theory of margins have been made by several authors [9, 37, 50]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15213196,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "615161deaf19fefaedf1d2afaba7b5718436539c",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Cumulative training margin distributions for AdaBoost versus our \"Direct Optimization Of Margins\" (DOOM) algorithm. The dark curve is AdaBoost, the light curve is DOOM. DOOM sacrifices significant training error for improved test error (horizontal marks on margin = 0 line)."
            },
            "slug": "Direct-Optimization-of-Margins-Improves-in-Combined-Mason-Bartlett",
            "title": {
                "fragments": [],
                "text": "Direct Optimization of Margins Improves Generalization in Combined Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "Cumulative training margin distributions for AdaBoost versus the \"Direct Optimization Of Margins\" (DOOM) algorithm, which sacrifices significant training error for improved test error."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50056360"
                        ],
                        "name": "William W. Cohen",
                        "slug": "William-W.-Cohen",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Cohen",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William W. Cohen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Cohen and Singer [11] showed how to design a base"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 195625660,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9d06638df32f8feefb95ef5a4769adbb1ae6297d",
            "isKey": false,
            "numCitedBy": 392,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe SLIPPER, a new rule learner that generates rulesets by repeatedly boosting a simple, greedy, rule-builder. Like the rulesets built by other rule learners, the ensemble of rules created by SLIPPER is compact and comprehensible. This is made possible by imposing appropriate constraints on the rule-builder, and by use of a recently-proposed generalization of Adaboost called confidence-rated boosting. In spite of its relative simplicity, SLIPPER is highly scalable, and an effective learner. Experimentally, SLIPPER scales no worse than O(n log n), where n is the number of examples, and on a set of 32 benchmark problems, SLIPPER achieves lower error rates than RIPPER 20 times, and lower error rates than C4.5rules 22 times."
            },
            "slug": "A-simple,-fast,-and-effective-rule-learner-Cohen-Singer",
            "title": {
                "fragments": [],
                "text": "A simple, fast, and effective rule learner"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "SLIPPER, a new rule learner that generates rulesets by repeatedly boosting a simple, greedy, rule-builder, is described, and like the rulesets built by other rule learners, the ensemble of rules created by SLIPPER is compact and comprehensible."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI 1999"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152597562"
                        ],
                        "name": "Gunnar R\u00e4tsch",
                        "slug": "Gunnar-R\u00e4tsch",
                        "structuredName": {
                            "firstName": "Gunnar",
                            "lastName": "R\u00e4tsch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gunnar R\u00e4tsch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35205384"
                        ],
                        "name": "T. Onoda",
                        "slug": "T.-Onoda",
                        "structuredName": {
                            "firstName": "Takashi",
                            "lastName": "Onoda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Onoda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3144723,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "03e8d6373b63bb15e11d3092477c55c74c063b72",
            "isKey": false,
            "numCitedBy": 1258,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently ensemble methods like ADABOOST have been applied successfully in many problems, while seemingly defying the problems of overfitting.ADABOOST rarely overfits in the low noise regime, however, we show that it clearly does so for higher noise levels. Central to the understanding of this fact is the margin distribution. ADABOOST can be viewed as a constraint gradient descent in an error function with respect to the margin. We find that ADABOOST asymptotically achieves a hard margin distribution, i.e. the algorithm concentrates its resources on a few hard-to-learn patterns that are interestingly very similar to Support Vectors. A hard margin is clearly a sub-optimal strategy in the noisy case, and regularization, in our case a \u201cmistrust\u201d in the data, must be introduced in the algorithm to alleviate the distortions that single difficult patterns (e.g. outliers) can cause to the margin distribution. We propose several regularization methods and generalizations of the original ADABOOST algorithm to achieve a soft margin. In particular we suggest (1) regularized ADABOOSTREG where the gradient decent is done directly with respect to the soft margin and (2) regularized linear and quadratic programming (LP/QP-) ADABOOST, where the soft margin is attained by introducing slack variables.Extensive simulations demonstrate that the proposed regularized ADABOOST-type algorithms are useful and yield competitive results for noisy data."
            },
            "slug": "Soft-Margins-for-AdaBoost-R\u00e4tsch-Onoda",
            "title": {
                "fragments": [],
                "text": "Soft Margins for AdaBoost"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It is found that ADABOOST asymptotically achieves a hard margin distribution, i.e. the algorithm concentrates its resources on a few hard-to-learn patterns that are interestingly very similar to Support Vectors."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423230"
                        ],
                        "name": "L. Breiman",
                        "slug": "L.-Breiman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Breiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Breiman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 123349680,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cc394d074c8504671eb37926d14a3df4a07520a0",
            "isKey": false,
            "numCitedBy": 933,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work has shown that combining multiple versions of unstable classifiers such as trees or neural nets results in reduced test set error. One of the more effective is bagging. Here, modified training sets are formed by resampling from the original training set, classifiers constructed using these training sets and then combined by voting. Freund and Schapire propose an algorithm the basis of which is to adaptively resample and combine (hence the acronym arcing) so that the weights in the resampling are increased for those cases most often misclassified and the combining is done by weighted voting. Arcing is more successful than bagging in test set error reduction. We explore two arcing algorithms, compare them to each other and to bagging, and try to understand how arcing works. We introduce the definitions of bias and variance for a classifier as components of the test set error. Unstable classifiers can have low bias on a large range of data sets. Their problem is high variance. Combining multiple versions either through bagging or arcing reduces variance significantly."
            },
            "slug": "Arcing-classifier-(with-discussion-and-a-rejoinder-Breiman",
            "title": {
                "fragments": [],
                "text": "Arcing classifier (with discussion and a rejoinder by the author)"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Two arcing algorithms are explored, compared to each other and to bagging, and the definitions of bias and variance for a classifier as components of the test set error are introduced."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341779"
                        ],
                        "name": "J. R. Quinlan",
                        "slug": "J.-R.-Quinlan",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Quinlan",
                            "middleNames": [
                                "Ross"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. R. Quinlan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5262555,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "807c1f19047f96083e13614f7ce20f2ac98c239a",
            "isKey": false,
            "numCitedBy": 21897,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nClassifier systems play a major role in machine learning and knowledge-based systems, and Ross Quinlan's work on ID3 and C4.5 is widely acknowledged to have made some of the most significant contributions to their development. This book is a complete guide to the C4.5 system as implemented in C for the UNIX environment. It contains a comprehensive guide to the system's use , the source code (about 8,800 lines), and implementation notes. The source code and sample datasets are also available on a 3.5-inch floppy diskette for a Sun workstation. \n \nC4.5 starts with large sets of cases belonging to known classes. The cases, described by any mixture of nominal and numeric properties, are scrutinized for patterns that allow the classes to be reliably discriminated. These patterns are then expressed as models, in the form of decision trees or sets of if-then rules, that can be used to classify new cases, with emphasis on making the models understandable as well as accurate. The system has been applied successfully to tasks involving tens of thousands of cases described by hundreds of properties. The book starts from simple core learning methods and shows how they can be elaborated and extended to deal with typical problems such as missing data and over hitting. Advantages and disadvantages of the C4.5 approach are discussed and illustrated with several case studies. \n \nThis book and software should be of interest to developers of classification-based intelligent systems and to students in machine learning and expert systems courses."
            },
            "slug": "C4.5:-Programs-for-Machine-Learning-Quinlan",
            "title": {
                "fragments": [],
                "text": "C4.5: Programs for Machine Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A complete guide to the C4.5 system as implemented in C for the UNIX environment, which starts from simple core learning methods and shows how they can be elaborated and extended to deal with typical problems such as missing data and over hitting."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7692065"
                        ],
                        "name": "K. Tieu",
                        "slug": "K.-Tieu",
                        "structuredName": {
                            "firstName": "Kinh",
                            "lastName": "Tieu",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Tieu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731948"
                        ],
                        "name": "Paul A. Viola",
                        "slug": "Paul-A.-Viola",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Viola",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul A. Viola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 197
                            }
                        ],
                        "text": "Boosting has also been applied to text filtering [72] and rout ing [39], \u201cranking\u201d problems [28], learning problems arising in natural langua ge processing [1, 12, 25, 38, 55, 78], image retrieval [74], medical diagnosis [53], a nd customer monitoring and segmentation [56, 57]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 510936,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eda4ebc9529f7b4cd53e47f051999a26134c0191",
            "isKey": false,
            "numCitedBy": 383,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an approach for image retrieval using a very large number of highly selective features and efficient learning of queries. Our approach is predicated on the assumption that each image is generated by a sparse set of visual \u201ccauses\u201d and that images which are visually similar share causes. We propose a mechanism for computing a very large number of highly selective features which capture some aspects of this causal structure (in our implementation there are over 46,000 highly selective features). At query time a user selects a few example images, and the AdaBoost algorithm is used to learn a classification function which depends on a small number of the most appropriate features. This yields a highly efficient classification function. In addition we show that the AdaBoost framework provides a natural mechanism for the incorporation of relevance feedback. Finally we show results on a wide variety of image queries."
            },
            "slug": "Boosting-Image-Retrieval-Tieu-Viola",
            "title": {
                "fragments": [],
                "text": "Boosting Image Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This work proposes a mechanism for computing a very large number of highly selective features which capture some aspects of this causal structure and shows results on a wide variety of image queries."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No.PR00662)"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145115014"
                        ],
                        "name": "Corinna Cortes",
                        "slug": "Corinna-Cortes",
                        "structuredName": {
                            "firstName": "Corinna",
                            "lastName": "Cortes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Corinna Cortes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In addition, the margin theory points to a strong connection between boosting and the support-vector machines of Vapnik and others [7,  14 , 77] which explicitly attempt to maximize the minimum margin."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 52874011,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "52b7bf3ba59b31f362aa07f957f1543a29a4279e",
            "isKey": false,
            "numCitedBy": 33430,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition."
            },
            "slug": "Support-Vector-Networks-Cortes-Vapnik",
            "title": {
                "fragments": [],
                "text": "Support-Vector Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated and the performance of the support- vector network is compared to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2714577"
                        ],
                        "name": "S. D. Pietra",
                        "slug": "S.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Pietra",
                            "middleNames": [
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39944066"
                        ],
                        "name": "V. D. Pietra",
                        "slug": "V.-D.-Pietra",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Pietra",
                            "middleNames": [
                                "J.",
                                "Della"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. D. Pietra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739581"
                        ],
                        "name": "J. Lafferty",
                        "slug": "J.-Lafferty",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Lafferty",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lafferty"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 982,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b951b9f78b98a186ba259027996a48e4189d37e5",
            "isKey": false,
            "numCitedBy": 1305,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a technique for constructing random fields from a set of training samples. The learning paradigm builds increasingly complex fields by allowing potential functions, or features, that are supported by increasingly large subgraphs. Each feature has a weight that is trained by minimizing the Kullback-Leibler divergence between the model and the empirical distribution of the training data. A greedy algorithm determines how features are incrementally added to the field and an iterative scaling algorithm is used to estimate the optimal values of the weights. The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non-Markovian and have a large number of parameters that must be estimated. Relations to other learning approaches, including decision trees, are given. As a demonstration of the method, we describe its application to the problem of automatic word classification in natural language processing."
            },
            "slug": "Inducing-Features-of-Random-Fields-Pietra-Pietra",
            "title": {
                "fragments": [],
                "text": "Inducing Features of Random Fields"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non-Markovian and have a large number of parameters that must be estimated."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47348275"
                        ],
                        "name": "M. Haruno",
                        "slug": "M.-Haruno",
                        "structuredName": {
                            "firstName": "Masahiko",
                            "lastName": "Haruno",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Haruno"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34981957"
                        ],
                        "name": "S. Shirai",
                        "slug": "S.-Shirai",
                        "structuredName": {
                            "firstName": "Satoshi",
                            "lastName": "Shirai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Shirai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2715661"
                        ],
                        "name": "Y. Ooyama",
                        "slug": "Y.-Ooyama",
                        "structuredName": {
                            "firstName": "Yoshifumi",
                            "lastName": "Ooyama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Ooyama"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Boosting has also been applied to text filtering [72] and routing [39], \u201cranking\u201d problems [28], learning problems arising in natural language processing [1, 12, 25, 38, 55, 78], image retrieval [74], medical diagnosis [53], and customer monitoring and segmentation [56, 57]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2823298,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "34215b9e199783016774954baeadb72e2201f3dd",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a novel and practical Japanese parser that uses decision trees. First, we construct a single decision tree to estimate modification probabilities; how one phrase tends to modify another. Next, we introduce a boosting algorithm in which several decision trees are constructed and then combined for probability estimation. The constructed parsers are evaluated using the EDR Japanese annotated corpus. The single-tree method significantly outperforms the conventional Japanese stochastic methods. Moreover, the boosted version of the parser is shown to have great advantages; (1) a better parsing accuracy than its single-tree counterpart for any amount of training data and (2) no over-fitting to data for various iterations. The presented parser, the first non-English stochastic parser with practical performance, should tighten the coupling between natural language processing and machine learning."
            },
            "slug": "Using-Decision-Trees-to-Construct-a-Practical-Haruno-Shirai",
            "title": {
                "fragments": [],
                "text": "Using Decision Trees to Construct a Practical Parser"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The presented parser is the first non-English stochastic parser with practical performance, and should tighten the coupling between natural language processing and machine learning."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 685382,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "015999a72c70a960e59c51078b09c8f672af0d2c",
            "isKey": false,
            "numCitedBy": 1198,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "Sample complexity results from computational learning theory, when applied to neural network learning for pattern classification problems, suggest that for good generalization performance the number of training examples should grow at least linearly with the number of adjustable parameters in the network. Results in this paper show that if a large neural network is used for a pattern classification problem and the learning algorithm finds a network with small weights that has small squared error on the training patterns, then the generalization performance depends on the size of the weights rather than the number of weights. For example, consider a two-layer feedforward network of sigmoid units, in which the sum of the magnitudes of the weights associated with each unit is bounded by A and the input dimension is n. We show that the misclassification probability is no more than a certain error estimate (that is related to squared error on the training set) plus A/sup 3/ /spl radic/((log n)/m) (ignoring log A and log m factors), where m is the number of training patterns. This may explain the generalization performance of neural networks, particularly when the number of training examples is considerably smaller than the number of weights. It also supports heuristics (such as weight decay and early stopping) that attempt to keep the weights small during training. The proof techniques appear to be useful for the analysis of other pattern classifiers: when the input domain is a totally bounded metric space, we use the same approach to give upper bounds on misclassification probability for classifiers with decision boundaries that are far from the training examples."
            },
            "slug": "The-Sample-Complexity-of-Pattern-Classification-The-Bartlett",
            "title": {
                "fragments": [],
                "text": "The Sample Complexity of Pattern Classification with Neural Networks: The Size of the Weights is More Important than the Size of the Network"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "Results in this paper show that if a large neural network is used for a pattern classification problem and the learning algorithm finds a network with small weights that has small squared error on the training patterns, then the generalization performance depends on the size of the weights rather than the number of weights."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1882833"
                        ],
                        "name": "S. Abney",
                        "slug": "S.-Abney",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Abney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Abney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 156
                            }
                        ],
                        "text": "Boosting has also been applied to text filtering [72] and rout ing [39], \u201cranking\u201d problems [28], learning problems arising in natural langua ge processing [1, 12, 25, 38, 55, 78], image retrieval [74], medical diagnosis [53], a nd customer monitoring and segmentation [56, 57]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 4480,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5db4c0f527adc11923c97a05e947e21711572a97",
            "isKey": false,
            "numCitedBy": 151,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Boosting is a machine learning algorithm that is not well known in computational linguistics. We apply it to part-of-speech tagging and prepositional phrase attachment. Performance is very encouraging. We also show how to improve data quality by using boosting to identify annotation errors."
            },
            "slug": "Boosting-Applied-to-Tagging-and-PP-Attachment-Abney-Schapire",
            "title": {
                "fragments": [],
                "text": "Boosting Applied to Tagging and PP Attachment"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "This paper applies boosting, a machine learning algorithm that is not well known in computational linguistics, to part-of-speech tagging and prepositional phrase attachment to improve data quality and identify annotation errors."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 9
                            }
                        ],
                        "text": "Schapire [66] came up with the first provable polynomial-time boosting alg orithm in 1989."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6207294,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "35702d642c5f1b7950d088497837486b4ca682a3",
            "isKey": false,
            "numCitedBy": 2680,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the problem of improving the accuracy of an hypothesis output by a learning algorithm in the distribution-free (PAC) learning model. A concept class is learnable (or strongly learnable) if, given access to a source of examples of the unknown concept, the learner with high probability is able to output an hypothesis that is correct on all but an arbitrarily small fraction of the instances. The concept class is weakly learnable if the learner can produce an hypothesis that performs only slightly better than random guessing. In this paper, it is shown that these two notions of learnability are equivalent.A method is described for converting a weak learning algorithm into one that achieves arbitrarily high accuracy. This construction may have practical applications as a tool for efficiently converting a mediocre learning algorithm into one that performs extremely well. In addition, the construction has some interesting theoretical consequences, including a set of general upper bounds on the complexity of any strong learning algorithm as a function of the allowed error \u2208."
            },
            "slug": "The-Strength-of-Weak-Learnability-Schapire",
            "title": {
                "fragments": [],
                "text": "The Strength of Weak Learnability"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "In this paper, a method is described for converting a weak learning algorithm into one that achieves arbitrarily high accuracy, and it is shown that these two notions of learnability are equivalent."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145786318"
                        ],
                        "name": "G. Escudero",
                        "slug": "G.-Escudero",
                        "structuredName": {
                            "firstName": "Gerard",
                            "lastName": "Escudero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Escudero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3049328"
                        ],
                        "name": "Llu\u00eds M\u00e0rquez i Villodre",
                        "slug": "Llu\u00eds-M\u00e0rquez-i-Villodre",
                        "structuredName": {
                            "firstName": "Llu\u00eds",
                            "lastName": "Villodre",
                            "middleNames": [
                                "M\u00e0rquez",
                                "i"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Llu\u00eds M\u00e0rquez i Villodre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785173"
                        ],
                        "name": "German Rigau",
                        "slug": "German-Rigau",
                        "structuredName": {
                            "firstName": "German",
                            "lastName": "Rigau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "German Rigau"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 156
                            }
                        ],
                        "text": "Boosting has also been applied to text filtering [72] and rout ing [39], \u201cranking\u201d problems [28], learning problems arising in natural langua ge processing [1, 12, 25, 38, 55, 78], image retrieval [74], medical diagnosis [53], a nd customer monitoring and segmentation [56, 57]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 208022631,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8953bdaf26842dc7cd5a76a53a5b9b77d65eef30",
            "isKey": false,
            "numCitedBy": 138,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we apply Schapire and Singer's AdaBoost.MH boosting algorithm to the Word Sense Disambiguation (WSD) \nproblem. Initial experiments on a set of 15 selected polysemous words show that the boosting approach surpasses Naive Bayes \nand Exemplar--based approaches, which represent state--of--the--art accuracy on WSD. In order to make boosting practical for a \nreal learning domain of thousands of words we study several ways of accelerating the algorithm by reducing the feature space. \nThe best variant, which we call LazyBoosting, is tested on a medium--large sense--tagged corpus containing 192,800 examples of \nthe 191 most frequent and ambiguous English words. Again, boosting compares favourably to the other benchmank algorithms."
            },
            "slug": "Boosting-Applied-to-Word-Sense-Disambiguation-Escudero-Villodre",
            "title": {
                "fragments": [],
                "text": "Boosting Applied to Word Sense Disambiguation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Initial experiments show that the boosting approach surpasses Naive Bayes and Exemplar--based approaches, which represent state-of-the-art accuracy on WSD, and compares favourably to the other benchmank algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068005320"
                        ],
                        "name": "Raj D. Iyer",
                        "slug": "Raj-D.-Iyer",
                        "structuredName": {
                            "firstName": "Raj",
                            "lastName": "Iyer",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Raj D. Iyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35153517"
                        ],
                        "name": "D. Lewis",
                        "slug": "D.-Lewis",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lewis",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lewis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145163573"
                        ],
                        "name": "A. Singhal",
                        "slug": "A.-Singhal",
                        "structuredName": {
                            "firstName": "Amit",
                            "lastName": "Singhal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Singhal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 67
                            }
                        ],
                        "text": "Boosting has also been applied to text filtering [72] and rout ing [39], \u201cranking\u201d problems [28], learning problems arising in natural langua ge processing [1, 12, 25, 38, 55, 78], image retrieval [74], medical diagnosis [53], a nd customer monitoring and segmentation [56, 57]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2638818,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a0ba761803c9b11245e9c1cfb0a3279453836d3b",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "RankBoost is a recently proposed algorithm for learning ranking functions. It is simple to implement and has strong justifica tions from computational learning theory. We describe the algorithm and present experimental results on applying it to the document routing problem. The first set of results applies RankBoost t o a text representation produced using modern term weighting methods. Performance of RankBoost is somewhat inferior to that of a state-of-the-art routing algorithm which is, however, more complex and less theoretically justified than RankBoost. RankB oost achieves comparable performance to the state-of-the-art algorithm when combined with feature or example selection heuristics. Our second set of results examines the behavior of RankBoost when it has to learn not only a ranking function but also all aspect s of term weighting from raw data. Performance is usually, though not always, less good here, but the term weighting functions implicit in the resulting ranking functions are intriguing, and the a pproach could easily be adapted to mixtures of textual and nontextual data."
            },
            "slug": "Boosting-for-document-routing-Iyer-Lewis",
            "title": {
                "fragments": [],
                "text": "Boosting for document routing"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The algorithm is described and experimental results on applying it to the document routing problem are presented, finding that RankBoost achieves comparable performance to the state-of-the-art algorithm when combined with feature or example selection heuristics."
            },
            "venue": {
                "fragments": [],
                "text": "CIKM '00"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1818832"
                        ],
                        "name": "M. Rochery",
                        "slug": "M.-Rochery",
                        "structuredName": {
                            "firstName": "Marie",
                            "lastName": "Rochery",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Rochery"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145334214"
                        ],
                        "name": "M. Rahim",
                        "slug": "M.-Rahim",
                        "structuredName": {
                            "firstName": "Mazin",
                            "lastName": "Rahim",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Rahim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2101317381"
                        ],
                        "name": "N. Gupta",
                        "slug": "N.-Gupta",
                        "structuredName": {
                            "firstName": "Narendra",
                            "lastName": "Gupta",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719162"
                        ],
                        "name": "G. Riccardi",
                        "slug": "G.-Riccardi",
                        "structuredName": {
                            "firstName": "Giuseppe",
                            "lastName": "Riccardi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Riccardi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35313721"
                        ],
                        "name": "S. Bangalore",
                        "slug": "S.-Bangalore",
                        "structuredName": {
                            "firstName": "Srinivas",
                            "lastName": "Bangalore",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Bangalore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767307"
                        ],
                        "name": "H. Alshawi",
                        "slug": "H.-Alshawi",
                        "structuredName": {
                            "firstName": "Hiyan",
                            "lastName": "Alshawi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Alshawi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1901883"
                        ],
                        "name": "Shona Douglas",
                        "slug": "Shona-Douglas",
                        "structuredName": {
                            "firstName": "Shona",
                            "lastName": "Douglas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shona Douglas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 0
                            }
                        ],
                        "text": "[64, 65] desc ribe a modification of boosting that combines and balances human expertise with av ailable training data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 3
                            }
                        ],
                        "text": "\u2019s [64, 65] method of incorporating human know ledge into boosting, described in Section 8, was applied to two speech catego rizati n tasks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 373667,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2e9d7c03aadb30c8619bd3be76ae6bb51cef521f",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Data collection and annotation are major bottlenecks in rapid development of accurate syntactic and semantic models for natural-language dialogue systems. In this paper we show how human knowledge can be used when designing a language understanding system in a manner that would alleviate the dependence on large sets of data. In particular, we extend BoosTexter, a member of the boosting family of algorithms, to combine and balance hand-crafted rules with the statistics of available data. Experiments on two voice-enabled applications for customer care and help desk are presented."
            },
            "slug": "Combining-prior-knowledge-and-boosting-for-call-in-Rochery-Schapire",
            "title": {
                "fragments": [],
                "text": "Combining prior knowledge and boosting for call classification in spoken language dialogue"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "BosTexter is extended, a member of the boosting family of algorithms, to combine and balance hand-crafted rules with the statistics of available data to alleviate the dependence on large sets of data."
            },
            "venue": {
                "fragments": [],
                "text": "2002 IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50056360"
                        ],
                        "name": "William W. Cohen",
                        "slug": "William-W.-Cohen",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Cohen",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William W. Cohen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 182
                            }
                        ],
                        "text": "learning algorithm that, when combined with AdaBoost, resu lts in a final classifier consisting of a relatively small set of rules similar to thos e generated by systems like RIPPER [10], IREP [36] and C4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 223
                            }
                        ],
                        "text": "Cohen and Singer [11] showed how to design a base\nlearning algorithm that, when combined with AdaBoost, results in a final classifier consisting of a relatively small set of rules similar to those generated by systems like RIPPER [10], IREP [36] and C4.5rules [60]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6492502,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6665e03447f989c9bdb3432d93e89b516b9d18a7",
            "isKey": false,
            "numCitedBy": 4149,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Fast-Effective-Rule-Induction-Cohen",
            "title": {
                "fragments": [],
                "text": "Fast Effective Rule Induction"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747752"
                        ],
                        "name": "Johannes F\u00fcrnkranz",
                        "slug": "Johannes-F\u00fcrnkranz",
                        "structuredName": {
                            "firstName": "Johannes",
                            "lastName": "F\u00fcrnkranz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Johannes F\u00fcrnkranz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145964711"
                        ],
                        "name": "G. Widmer",
                        "slug": "G.-Widmer",
                        "structuredName": {
                            "firstName": "Gerhard",
                            "lastName": "Widmer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Widmer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 240,
                                "start": 236
                            }
                        ],
                        "text": "Cohen and Singer [11] showed how to design a base\nlearning algorithm that, when combined with AdaBoost, results in a final classifier consisting of a relatively small set of rules similar to those generated by systems like RIPPER [10], IREP [36] and C4.5rules [60]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 191
                            }
                        ],
                        "text": "learning algorithm that, when combined with AdaBoost, results in a final classifier consisting of a relatively small set of rules similar to those generated by systems like RIPPER [10], IREP [36] and C4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5310845,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e37790eae6a0ed842c7260df39aab9161c4d1aa1",
            "isKey": false,
            "numCitedBy": 434,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Incremental-Reduced-Error-Pruning-F\u00fcrnkranz-Widmer",
            "title": {
                "fragments": [],
                "text": "Incremental Reduced Error Pruning"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2586148"
                        ],
                        "name": "L. Mason",
                        "slug": "L.-Mason",
                        "structuredName": {
                            "firstName": "Llew",
                            "lastName": "Mason",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Mason"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47392513"
                        ],
                        "name": "Jonathan Baxter",
                        "slug": "Jonathan-Baxter",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Baxter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Baxter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745169"
                        ],
                        "name": "P. Bartlett",
                        "slug": "P.-Bartlett",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bartlett",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bartlett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40073871"
                        ],
                        "name": "Marcus Frean",
                        "slug": "Marcus-Frean",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Frean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcus Frean"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60744708,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "f5e23d650853dc7f3dbe4370d4ace6be55f931ae",
            "isKey": false,
            "numCitedBy": 324,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter contains sections titled: Introduction, Optimizing Cost Functions of the Margin, A Gradient Descent View of Voting Methods, Theoretically Motivated Cost Functions, Convergence Results, Experiments, Conclusions, Acknowledgments"
            },
            "slug": "Functional-Gradient-Techniques-for-Combining-Mason-Baxter",
            "title": {
                "fragments": [],
                "text": "Functional Gradient Techniques for Combining Hypotheses"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "This chapter contains sections titled: Introduction, Optimizing Cost Functions of the Margin, A Gradient Descent View of Voting Methods, Theoretically Motivated Cost Functions, Convergence Results, Experiments, Conclusions, and Acknowledgments."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423230"
                        ],
                        "name": "L. Breiman",
                        "slug": "L.-Breiman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Breiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Breiman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14488820,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a92684c164b0c46020a371ae5116df74bb37a412",
            "isKey": false,
            "numCitedBy": 551,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The theory behind the success of adaptive reweighting and combining algorithms (arcing) such as Adaboost (Freund & Schapire, 1996a, 1997) and others in reducing generalization error has not been well understood. By formulating prediction as a game where one player makes a selection from instances in the training set and the other a convex linear combination of predictors from a finite set, existing arcing algorithms are shown to be algorithms for finding good game strategies. The minimax theorem is an essential ingredient of the convergence proofs. An arcing algorithm is described that converges to the optimal strategy. A bound on the generalization error for the combined predictors in terms of their maximum error is proven that is sharper than bounds to date. Schapire, Freund, Bartlett, and Lee (1997) offered an explanation of why Adaboost works in terms of its ability to produce generally high margins. The empirical comparison of Adaboost to the optimal arcing algorithm shows that their explanation is not complete."
            },
            "slug": "Prediction-Games-and-Arcing-Algorithms-Breiman",
            "title": {
                "fragments": [],
                "text": "Prediction Games and Arcing Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "The theory behind the success of adaptive reweighting and combining algorithms (arcing) such as Adaboost and others in reducing generalization error has not been well understood, and an explanation of whyAdaboost works in terms of its ability to produce generally high margins is offered."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1638095,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "888c09de60ce427669fe5a264fa3e787803eb9d2",
            "isKey": false,
            "numCitedBy": 397,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the close connections between game theory, on-line prediction and boosting. After a brief review of game theory, we describe an algorithm for learning to play repeated games based on the on-line prediction methods of Littlestone and Warmuth. The analysis of this algorithm yields a simple proof of von Neumann\u2019s famous minmax theorem, as well as a provable method of approximately solving a game. We then show that the on-line prediction model is obtained by applying this gameplaying algorithm to an appropriate choice of game and that boosting is obtained by applying the same algorithm to the \u201cdual\u201d of this game."
            },
            "slug": "Game-theory,-on-line-prediction-and-boosting-Freund-Schapire",
            "title": {
                "fragments": [],
                "text": "Game theory, on-line prediction and boosting"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "An algorithm for learning to play repeated games based on the on-line prediction methods of Littlestone and Warmuth is described, which yields a simple proof of von Neumann\u2019s famous minmax theorem, as well as a provable method of approximately solving a game."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '96"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060101052"
                        ],
                        "name": "Terry Koo",
                        "slug": "Terry-Koo",
                        "structuredName": {
                            "firstName": "Terry",
                            "lastName": "Koo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Terry Koo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Boosting has also been applied to text filtering [72] and routing [39], \u201cranking\u201d problems [28], learning problems arising in natural language processing [1, 12, 25, 38, 55, 78], image retrieval [74], medical diagnosis [53], and customer monitoring and segmentation [56, 57]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 405878,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "844db702be4bc149b06b822b47247e15f5894cc3",
            "isKey": false,
            "numCitedBy": 776,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75 F-measure, a 13 relative decrease in F-measure error over the baseline model's score of 88.2. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative-in terms of both simplicity and efficiency-to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation."
            },
            "slug": "Discriminative-Reranking-for-Natural-Language-Collins-Koo",
            "title": {
                "fragments": [],
                "text": "Discriminative Reranking for Natural Language Parsing"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The boosting approach to ranking problems described in Freund et al. (1998) is applied to parsing the Wall Street Journal treebank, and it is argued that the method is an appealing alternative-in terms of both simplicity and efficiency-to work on feature selection methods within log-linear (maximum-entropy) models."
            },
            "venue": {
                "fragments": [],
                "text": "Computational Linguistics"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 133
                            }
                        ],
                        "text": "In ad dition, the margin theory points to a strong connection between boosting and th e support-vector machines of Vapnik and others [7, 14, 77] which explicitly atte mpt to maximize the minimum margin."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7138354,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8213dbed4db44e113af3ed17d6dad57471a0c048",
            "isKey": false,
            "numCitedBy": 38755,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Setting of the learning problem consistency of learning processes bounds on the rate of convergence of learning processes controlling the generalization ability of learning processes constructing learning algorithms what is important in learning theory?."
            },
            "slug": "The-Nature-of-Statistical-Learning-Theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "The Nature of Statistical Learning Theory"
            },
            "venue": {
                "fragments": [],
                "text": "Statistics for Engineering and Information Science"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2403454"
                        ],
                        "name": "E. Baum",
                        "slug": "E.-Baum",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Baum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Baum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15659829,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "25406e6733a698bfc4ac836f8e74f458e75dad4f",
            "isKey": false,
            "numCitedBy": 1696,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the question of when a network can be expected to generalize from m random training examples chosen from some arbitrary probability distribution, assuming that future test examples are drawn from the same distribution. Among our results are the following bounds on appropriate sample vs. network size. Assume 0 < \u220a 1/8. We show that if m O(W/\u220a log N/\u220a) random examples can be loaded on a feedforward network of linear threshold functions with N nodes and W weights, so that at least a fraction 1 \u220a/2 of the examples are correctly classified, then one has confidence approaching certainty that the network will correctly classify a fraction 1 \u220a of future test examples drawn from the same distribution. Conversely, for fully-connected feedforward nets with one hidden layer, any learning algorithm using fewer than (W/\u220a) random training examples will, for some distributions of examples consistent with an appropriate weight choice, fail at least some fixed fraction of the time to find a weight choice that will correctly classify more than a 1 \u220a fraction of the future test examples."
            },
            "slug": "What-Size-Net-Gives-Valid-Generalization-Baum-Haussler",
            "title": {
                "fragments": [],
                "text": "What Size Net Gives Valid Generalization?"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that if m O(W/ \u220a log N/\u220a) random examples can be loaded on a feedforward network of linear threshold functions with N nodes and W weights, so that at least a fraction 1 \u220a/2 of the examples are correctly classified, then one has confidence approaching certainty that the network will correctly classify a fraction 2 \u220a of future test examples drawn from the same distribution."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741124"
                        ],
                        "name": "L. Valiant",
                        "slug": "L.-Valiant",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Valiant",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Valiant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 73
                            }
                        ],
                        "text": "Working in Valiant\u2019s PAC (probably approximately correct) learning model [75], Kearns and Valiant [41, 42] were the first to pose the question of whether a \u201cweak\u201d learning algorithm that performs just slightly better than r dom guessing can be \u201cboosted\u201d into an arbitrarily accurate \u201cstrong\u201d learning a l orithm."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59712,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "10ddb646feddc12337b5a755c72e153e37088c02",
            "isKey": false,
            "numCitedBy": 4189,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Humans appear to be able to learn new concepts without needing to be programmed explicitly in any conventional sense. In this paper we regard learning as the phenomenon of knowledge acquisition in the absence of explicit programming. We give a precise methodology for studying this phenomenon from a computational viewpoint. It consists of choosing an appropriate information gathering mechanism, the learning protocol, and exploring the class of concepts that can be learnt using it in a reasonable (polynomial) number of steps. We find that inherent algorithmic complexity appears to set serious limits to the range of concepts that can be so learnt. The methodology and results suggest concrete principles for designing realistic learning systems."
            },
            "slug": "A-theory-of-the-learnable-Valiant",
            "title": {
                "fragments": [],
                "text": "A theory of the learnable"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper regards learning as the phenomenon of knowledge acquisition in the absence of explicit programming, and gives a precise methodology for studying this phenomenon from a computational viewpoint."
            },
            "venue": {
                "fragments": [],
                "text": "STOC '84"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145298005"
                        ],
                        "name": "Catherine Blake",
                        "slug": "Catherine-Blake",
                        "structuredName": {
                            "firstName": "Catherine",
                            "lastName": "Blake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Catherine Blake"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For instance, Freund and Schapire [30] tested AdaBoost on a set of UCI benchmark datasets [ 54 ] using C4.5 [60] as a base learning algorithm, as well as an algorithm that finds the best \u201cdecision stump\u201d or single-test decision tree."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62622768,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e068be31ded63600aea068eacd12931efd2a1029",
            "isKey": false,
            "numCitedBy": 13446,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "UCI-Repository-of-machine-learning-databases-Blake",
            "title": {
                "fragments": [],
                "text": "UCI Repository of machine learning databases"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2647026"
                        ],
                        "name": "A. Blumer",
                        "slug": "A.-Blumer",
                        "structuredName": {
                            "firstName": "Anselm",
                            "lastName": "Blumer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blumer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683946"
                        ],
                        "name": "A. Ehrenfeucht",
                        "slug": "A.-Ehrenfeucht",
                        "structuredName": {
                            "firstName": "Andrzej",
                            "lastName": "Ehrenfeucht",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ehrenfeucht"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794034"
                        ],
                        "name": "Manfred K. Warmuth",
                        "slug": "Manfred-K.-Warmuth",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Warmuth",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manfred K. Warmuth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "[6, 76] for its definition and relation to learning theory."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1138467,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0b8fa3496283d4d808fba9ff62d5f024bcf23be",
            "isKey": false,
            "numCitedBy": 1909,
            "numCiting": 73,
            "paperAbstract": {
                "fragments": [],
                "text": "Valiant's learnability model is extended to learning classes of concepts defined by regions in Euclidean space En. The methods in this paper lead to a unified treatment of some of Valiant's results, along with previous results on distribution-free convergence of certain pattern recognition algorithms. It is shown that the essential condition for distribution-free learnability is finiteness of the Vapnik-Chervonenkis dimension, a simple combinatorial parameter of the class of concepts to be learned. Using this parameter, the complexity and closure properties of learnable classes are analyzed, and the necessary and sufficient conditions are provided for feasible learnability."
            },
            "slug": "Learnability-and-the-Vapnik-Chervonenkis-dimension-Blumer-Ehrenfeucht",
            "title": {
                "fragments": [],
                "text": "Learnability and the Vapnik-Chervonenkis dimension"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper shows that the essential condition for distribution-free learnability is finiteness of the Vapnik-Chervonenkis dimension, a simple combinatorial parameter of the class of concepts to be learned."
            },
            "venue": {
                "fragments": [],
                "text": "JACM"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144473519"
                        ],
                        "name": "M. Mozer",
                        "slug": "M.-Mozer",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Mozer",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mozer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2523711"
                        ],
                        "name": "R. Wolniewicz",
                        "slug": "R.-Wolniewicz",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Wolniewicz",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Wolniewicz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784782"
                        ],
                        "name": "D. Grimes",
                        "slug": "D.-Grimes",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Grimes",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Grimes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149502188"
                        ],
                        "name": "Eric Johnson",
                        "slug": "Eric-Johnson",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eric Johnson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2241056"
                        ],
                        "name": "Howard Kaushansky",
                        "slug": "Howard-Kaushansky",
                        "structuredName": {
                            "firstName": "Howard",
                            "lastName": "Kaushansky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Howard Kaushansky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9041817,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a3968296f9d11fddc727869cb6d468556de4addb",
            "isKey": false,
            "numCitedBy": 335,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Competition in the wireless telecommunications industry is fierce. To maintain profitability, wireless carriers must control churn, which is the loss of subscribers who switch from one carrier to another.We explore techniques from statistical machine learning to predict churn and, based on these predictions, to determine what incentives should be offered to subscribers to improve retention and maximize profitability to the carrier. The techniques include logit regression, decision trees, neural networks, and boosting. Our experiments are based on a database of nearly 47,000 U.S. domestic subscribers and includes information about their usage, billing, credit, application, and complaint history. Our experiments show that under a wide variety of assumptions concerning the cost of intervention and the retention rate resulting from intervention, using predictive techniques to identify potential churners and offering incentives can yield significant savings to a carrier. We also show the importance of a data representation crafted by domain experts. Finally, we report on a real-world test of the techniques that validate our simulation experiments."
            },
            "slug": "Predicting-subscriber-dissatisfaction-and-improving-Mozer-Wolniewicz",
            "title": {
                "fragments": [],
                "text": "Predicting subscriber dissatisfaction and improving retention in the wireless telecommunications industry"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "These experiments show that under a wide variety of assumptions concerning the cost of intervention and the retention rate resulting from intervention, using predictive techniques to identify potential churners and offering incentives can yield significant savings to a carrier."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks Learn. Syst."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 31
                            }
                        ],
                        "text": "In another direction, Schapire [68] describes and analyzes th generalization of both AdaBoost and Freund\u2019s earlier \u201cboost-by-majority\u201d algorithm [26] to a broader family of repeated games called \u201cdrifting games."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15419229,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2fb33398ed4223a80659375b1b28e48eec8de8fa",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce and study a general, abstract game played between two players called the shepherd and the adversary. The game is played in a series of rounds using a finite set of \u201cchips\u201d which are moved about in \u211dn. On each round, the shepherd assigns a desired direction of movement and an importance weight to each of the chips. The adversary then moves the chips in any way that need only be weakly correlated with the desired directions assigned by the shepherd. The shepherd's goal is to cause the chips to be moved to low-loss positions, where the loss of each chip at its final position is measured by a given loss function.We present a shepherd algorithm for this game and prove an upper bound on its performance. We also prove a lower bound showing that the algorithm is essentially optimal for a large number of chips. We discuss computational methods for efficiently implementing our algorithm.We show that our general drifting-game algorithm subsumes some well studied boosting and on-line learning algorithms whose analyses follow as easy corollaries of our general result."
            },
            "slug": "Drifting-Games-Schapire",
            "title": {
                "fragments": [],
                "text": "Drifting Games"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It is shown that the general drifting-game algorithm subsumes some well studied boosting and on-line learning algorithms whose analyses follow as easy corollaries of the general result."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '99"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143936663"
                        ],
                        "name": "Thomas Hofmann",
                        "slug": "Thomas-Hofmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hofmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Hofmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682548"
                        ],
                        "name": "J. Buhmann",
                        "slug": "J.-Buhmann",
                        "structuredName": {
                            "firstName": "Joachim",
                            "lastName": "Buhmann",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Buhmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15479156,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3a58c3eafcc642ffa2e571e069e53f20bb1d1150",
            "isKey": false,
            "numCitedBy": 538,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "Partitioning a data set and extracting hidden structure from the data arises in different application areas of pattern recognition, speech and image processing. Pairwise data clustering is a combinatorial optimization method for data grouping which extracts hidden structure from proximity data. We describe a deterministic annealing approach to pairwise clustering which shares the robustness properties of maximum entropy inference. The resulting Gibbs probability distributions are estimated by mean-field approximation. A new structure-preserving algorithm to cluster dissimilarity data and to simultaneously embed these data in a Euclidian vector space is discussed which can be used for dimensionality reduction and data visualization. The suggested embedding algorithm which outperforms conventional approaches has been implemented to analyze dissimilarity data from protein analysis and from linguistics. The algorithm for pairwise data clustering is used to segment textured images."
            },
            "slug": "Pairwise-Data-Clustering-by-Deterministic-Annealing-Hofmann-Buhmann",
            "title": {
                "fragments": [],
                "text": "Pairwise Data Clustering by Deterministic Annealing"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A deterministic annealing approach to pairwise clustering is described which shares the robustness properties of maximum entropy inference and the resulting Gibbs probability distributions are estimated by mean-field approximation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760530"
                        ],
                        "name": "M. Walker",
                        "slug": "M.-Walker",
                        "structuredName": {
                            "firstName": "Marilyn",
                            "lastName": "Walker",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Walker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702447"
                        ],
                        "name": "Owen Rambow",
                        "slug": "Owen-Rambow",
                        "structuredName": {
                            "firstName": "Owen",
                            "lastName": "Rambow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Owen Rambow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1845150"
                        ],
                        "name": "Monica Rogati",
                        "slug": "Monica-Rogati",
                        "structuredName": {
                            "firstName": "Monica",
                            "lastName": "Rogati",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Monica Rogati"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 154
                            }
                        ],
                        "text": "Boosting has also been applied to text filtering [72] and routing [39], \u201cranking\u201d problems [28], learning problems arising in natural language processing [1, 12, 25, 38, 55, 78], image retrieval [74], medical diagnosis [53], and customer monitoring and segmentation [56, 57]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7608649,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ee6892b9c7f1a491e0925b913b66281c48408f74",
            "isKey": false,
            "numCitedBy": 120,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Sentence planning is a set of inter-related but distinct tasks, one of which is sentence scoping, i.e. the choice of syntactic structure for elementary speech acts and the decision of how to combine them into one or more sentences. In this paper, we present SPoT, a sentence planner, and a new methodology for automatically training SPoT on the basis of feedback provided by human judges. We reconceptualize the task into two distinct phases. First, a very simple, randomized sentence-plan-generator (SPG) generates a potentially large list of possible sentence plans for a given text-plan input. Second, the sentence-plan-ranker (SPR) ranks the list of output sentence plans, and then selects the top-ranked plan. The SPR uses ranking rules automatically learned from training data. We show that the trained SPR learns to select a sentence plan whose rating on average is only 5% worse than the top human-ranked sentence plan."
            },
            "slug": "SPoT:-A-Trainable-Sentence-Planner-Walker-Rambow",
            "title": {
                "fragments": [],
                "text": "SPoT: A Trainable Sentence Planner"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper presents SPoT, a sentence planner, and a new methodology for automatically trainingSPoT on the basis of feedback provided by human judges, and shows that the trained SPR learns to select a sentence plan whose rating on average is only 5% worse than the top human-ranked sentence plan."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 14
                            }
                        ],
                        "text": "For instance, Freund and Schapire [30] tested AdaBoost on a set of UCI benchmark datasets [54] using C4.5 [60] as a base learning algorithm, as well as an algorithm that finds the best \u201cdecision stump\u201d or single-test decision tree."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 111
                            }
                        ],
                        "text": "The behavior of AdaBoost can also be understood in a game-theoretic setting as explored by Freund and Schapire [31, 33] (see also Grove and Schuurmans [37] and Breiman [9])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 31
                            }
                        ],
                        "text": "This bound was first proved by Freund and Schapire [32]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 46
                            }
                        ],
                        "text": "The AdaBoost algorithm, introduced in 1995 by Freund and Schapire [32], solved many of the practical difficulties of the earlier boosting algorithms, and is the focus of this paper."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 44
                            }
                        ],
                        "text": "7 taken from an OCR experiment conducted by Freund and Schapire [30]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Freund and Schapire\u2019s [32] algorithm AdaBoost."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 50
                            }
                        ],
                        "text": "Some of these, such as those of Ridgeway [63] and Freund and Schapire [32], attempt to reduce the regression problem to a classification problem."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 69
                            }
                        ],
                        "text": "Specifically, Schapire and Singer [70], in generalizing a theorem of Freund and Schapire [32], show that the training error of the final classifier is bounded as follows:$1 , 9 L O ) $1 X CFEHGI -\" : \\ 6 K 6 (2) where henceforth we define : Z : X 6 ?"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 68
                            }
                        ],
                        "text": "6 T $ \" 6 6 (1) as in the original description of AdaBoost given by Freund and Schapire [32]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Freund and Schapire [32] showed how to bound the generalization error of the final classifier in terms of its training error, the size 1 of the sample, the VC-\ndimension2 of the base classifier space and the number of rounds 3 of boosting."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15295656,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "08894e24c6bf120365c870157c7e3944615a45cc",
            "isKey": false,
            "numCitedBy": 546,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract We present a simple algorithm for playing a repeated game. We show that a player using this algorithm suffers average loss that is guaranteed to come close to the minimum loss achievable by any fixed strategy. Our bounds are nonasymptotic and hold for any opponent. The algorithm, which uses the multiplicative-weight methods of Littlestone and Warmuth, is analyzed using the Kullback\u2013Liebler divergence. This analysis yields a new, simple proof of the min\u2013max theorem, as well as a provable method of approximately solving a game. A variant of our game-playing algorithm is proved to be optimal in a very strong sense. Journal of Economic Literature Classification Numbers: C44, C70, D83."
            },
            "slug": "Adaptive-game-playing-using-multiplicative-weights-Freund-Schapire",
            "title": {
                "fragments": [],
                "text": "Adaptive game playing using multiplicative weights"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A variant of the game-playing algorithm is proved to be optimal in a very strong sense and a new, simple proof of the min\u2013max theorem, as well as a provable method of approximately solving a game."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056642528"
                        ],
                        "name": "M. Kearns",
                        "slug": "M.-Kearns",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Kearns",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kearns"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741124"
                        ],
                        "name": "L. Valiant",
                        "slug": "L.-Valiant",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Valiant",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Valiant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 98
                            }
                        ],
                        "text": "Working in Valiant\u2019s PAC (probably approximately correct) learning model [75], Kearns and Valiant [41, 42] were the first to pose the question of whether a \u201cweak\u201d learning algorithm that performs just slightly better than random guessing can be \u201cboosted\u201d into an arbitrarily accurate \u201cstrong\u201d learning algorithm."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 22304610,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "3ae43d62122bffc2d921a363291ca184645d2376",
            "isKey": false,
            "numCitedBy": 647,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we prove the intractability of learning several classes of Boolean functions in the distribution-free model (also called the Probably Approximately Correct or PAC model) of learning from examples. These results are representation independent, in that they hold regardless of the syntactic form in which the learner chooses to represent its hypotheses.\nOur methods reduce the problems of cracking a number of well-known public-key cryptosystems to the learning problems. We prove that a polynomial-time learning algorithm for Boolean formulae, deterministic finite automata or constant-depth threshold circuits would have dramatic consequences for cryptography and number theory. In particular, such an algorithm could be used to break the RSA cryptosystem, factor Blum integers (composite numbers equivalent to 3 modulo 4), and detect quadratic residues. The results hold even if the learning algorithm is only required to obtain a slight advantage in prediction over random guessing. The techniques used demonstrate an interesting duality between learning and cryptography.\nWe also apply our results to obtain strong intractability results for approximating a generalization of graph coloring."
            },
            "slug": "Cryptographic-limitations-on-learning-Boolean-and-Kearns-Valiant",
            "title": {
                "fragments": [],
                "text": "Cryptographic limitations on learning Boolean formulae and finite automata"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is proved that a polynomial-time learning algorithm for Boolean formulae, deterministic finite automata or constant-depth threshold circuits would have dramatic consequences for cryptography and number theory and is applied to obtain strong intractability results for approximating a generalization of graph coloring."
            },
            "venue": {
                "fragments": [],
                "text": "JACM"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8142232,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a36b028d024bf358c4af1a5e1dc3ca0aed23b553",
            "isKey": false,
            "numCitedBy": 3709,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter reproduces the English translation by B. Seckler of the paper by Vapnik and Chervonenkis in which they gave proofs for the innovative results they had obtained in a draft form in July 1966 and announced in 1968 in their note in Soviet Mathematics Doklady. The paper was first published in Russian as \u0412\u0430\u043f\u043d\u0438\u043a \u0412. \u041d. and \u0427\u0435\u0440\u0432\u043e\u043d\u0435\u043d\u043a\u0438\u0441 \u0410. \u042f. \u041e \u0440\u0430\u0432\u043d\u043e\u043c\u0435\u0440\u043d\u043e\u0419 \u0441\u0445\u043e\u0434\u0438\u043c\u043e\u0441\u0442\u0438 \u0447\u0430\u0441\u0442\u043e\u0442 \u043f\u043e\u044f\u0432\u043b\u0435\u043d\u0438\u044f \u0441\u043e\u0431\u044b\u0442\u0438\u0419 \u043a \u0438\u0445 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044f\u043c. \u0422\u0435\u043e\u0440\u0438\u044f \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0435\u0419 \u0438 \u0435\u0435 \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u044f 16(2), 264\u2013279 (1971)."
            },
            "slug": "Chervonenkis:-On-the-uniform-convergence-of-of-to-Vapnik",
            "title": {
                "fragments": [],
                "text": "Chervonenkis: On the uniform convergence of relative frequencies of events to their probabilities"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This chapter reproduces the English translation by B. Seckler of the paper by Vapnik and Chervonenkis in which they gave proofs for the innovative results they had obtained in a draft form in July 1966 and announced in 1968 in their note in Soviet Mathematics Doklady."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 47
                            }
                        ],
                        "text": "A different technique [67], which incorporates Dietterich and Bakiri\u2019s [19] method of error-correcting output codes, achieves similar provable bounds to those of AdaBoost."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 54
                            }
                        ],
                        "text": "This was demonstrated very convincingly by Dietterich [18]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 53
                            }
                        ],
                        "text": "Boosting seems to be especially susceptible to noise [18] (more on thi s in Sectionsec:exps)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 69
                            }
                        ],
                        "text": "AdaBoost has been tested empirically by many researchers, i nclud ng [4, 18, 21, 40, 49, 59, 73]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An experimental comparison of th ree methods for constructing ensembles of decision trees: Bagging, boosting, and ran omization"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning,"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 17
                            }
                        ],
                        "text": "Cohen and Singer [11] showed how to design a base"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 44
                            }
                        ],
                        "text": "In another set of experiments, Schapire and Singer [71] used boosting for text categorization tasks."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 13
                            }
                        ],
                        "text": "Schapire and Singer\u2019s [70] algorithm AdaBoost."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 57
                            }
                        ],
                        "text": "1 in the slightly generalized form given by Schapire and Singer [70]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 13
                            }
                        ],
                        "text": "Schapire and Singer [70] and Allwein, Schapire and Singer [2] give yet another method of combining boosting with errorcorrecting output codes."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 0
                            }
                        ],
                        "text": "Cohen and Singer\u2019s system, called SLIPPER, is fast, accurate and produces quite compact rule sets."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 19
                            }
                        ],
                        "text": "Here, Schapire and Singer advocate choosing ?"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 44
                            }
                        ],
                        "text": "M2 (which is a special case of Schapire and Singer\u2019s [70] AdaBoost."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 0
                            }
                        ],
                        "text": "Cohen and Singer [11] showed how to design a base\nlearning algorithm that, when combined with AdaBoost, results in a final classifier consisting of a relatively small set of rules similar to those generated by systems like RIPPER [10], IREP [36] and C4.5rules [60]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 13
                            }
                        ],
                        "text": "Schapire and Singer [70] discuss the choice of ?"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 27
                            }
                        ],
                        "text": "Specifically, Schapire and Singer [70], in generalizing a theorem of Freund and Schapire [32], show that the training error of the final classifier is bounded as follows:$1 , 9 L O ) $1 X CFEHGI -\" : \\ 6 K 6 (2) where henceforth we define : Z : X 6 ?"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 63
                            }
                        ],
                        "text": "The modification of AdaBoost proposed by Collins, Schapire and Singer to handle this loss function is particularly simple."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 106
                            }
                        ],
                        "text": "A different, more direct modification of AdaBoost for logistic loss was proposed by Collins, Schapire and Singer [13]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A simple, fast, and ef  fective rule learner"
            },
            "venue": {
                "fragments": [],
                "text": "In Proceedings of the Sixteenth National Conference on Artific  ial Intelligence,"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 168
                            }
                        ],
                        "text": "The behavior of AdaBoost can also be understood in a game-theoretic setting as explored by Freund and Schapire [31, 33] (see also Grove and Schuurmans [37] and Breiman [9])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 141
                            }
                        ],
                        "text": "This is because both algorithms are doing a kind of functional gradient descent, an observation that is spelled out and exploited by Breiman [9], Duffy and Helmbold [23], Mason et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 133
                            }
                        ],
                        "text": "This is because both algorithms are doing a kind of functional gradient descent, an observation that is spelled out and exploited by Breiman [9], Duffy and Helmbold [23], Mason et al. [51, 52] and Friedman [35]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 122
                            }
                        ],
                        "text": "Attempts (not always successful) to use the insights gleaned from the theory of margins have been made by several authors [9, 37, 50]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "Breiman [9], for instance,\nshows empirically that one classifier can have a margin distribution that is uniformly better than that of another classifier, and yet be inferior in test accuracy."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Prediction games and arcing classifiers"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation,"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 123
                            }
                        ],
                        "text": "Attempts (not always successful) to use the insights gleane d from the theory of margins have been made by several authors [9, 37, 50]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 160
                            }
                        ],
                        "text": "The behavior of AdaBoost can also be understood in a game-theoretic setting as explored by Freund and Schapire [31, 33] (see also Grove and Schuurmans [37] and Breiman [9])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 143
                            }
                        ],
                        "text": "This is b ecause both algorithms are doing a kind of functional gradient descent, an observat ion that is spelled out and exploited by Breiman [9], Duffy and Helmbold [23], Mason et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 133
                            }
                        ],
                        "text": "This is because both algorithms are doing a kind of functional gradient descent, an observation that is spelled out and exploited by Breiman [9], Duffy and Helmbold [23], Mason et al. [51, 52] and Friedman [35]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 170
                            }
                        ],
                        "text": "The behavior of AdaBoost can also be understood in a game-the oretic setting as explored by Freund and Schapire [31, 33] (see also Grove and S chuurmans [37] and Breiman [9])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "Breiman [9], for instance,\nshows empirically that one classifier can have a margin distribution that is uniformly better than that of another classifier, and yet be inferior in test accuracy."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Prediction games and arcing classifiers.  Neural Computation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 14
                            }
                        ],
                        "text": "For instance, Freund and Schapire [30] tested AdaBoost on a set of UCI benchmark datasets [54] using C4.5 [60] as a base learning algorithm, as well as an algorithm that finds the best \u201cdecision stump\u201d or single-test decision tree."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 91
                            }
                        ],
                        "text": "The behavior of AdaBoost can also be understood in a game-theoretic setting as explored by Freund and Schapire [31, 33] (see also Grove and Schuurmans [37] and Breiman [9])."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 31
                            }
                        ],
                        "text": "This bound was first proved by Freund and Schapire [32]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 46
                            }
                        ],
                        "text": "The AdaBoost algorithm, introduced in 1995 by Freund and Schapire [32], solved many of the practical difficulties of the earlier boosting algorithms, and is the focus of this paper."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 44
                            }
                        ],
                        "text": "7 taken from an OCR experiment conducted by Freund and Schapire [30]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Freund and Schapire\u2019s [32] algorithm AdaBoost."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 50
                            }
                        ],
                        "text": "Some of these, such as those of Ridgeway [63] and Freund and Schapire [32], attempt to reduce the regression problem to a classification problem."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 69
                            }
                        ],
                        "text": "Specifically, Schapire and Singer [70], in generalizing a theorem of Freund and Schapire [32], show that the training error of the final classifier is bounded as follows:$1 , 9 L O ) $1 X CFEHGI -\" : \\ 6 K 6 (2) where henceforth we define : Z : X 6 ?"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 112
                            }
                        ],
                        "text": "The behavior of AdaBoost can also be understood in a game-the oretic setting as explored by Freund and Schapire [31, 33] (see also Grove and S chuurmans [37] and Breiman [9])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 68
                            }
                        ],
                        "text": "6 T $ \" 6 6 (1) as in the original description of AdaBoost given by Freund and Schapire [32]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 0
                            }
                        ],
                        "text": "Freund and Schapire [32] showed how to bound the generalization error of the final classifier in terms of its training error, the size 1 of the sample, the VC-\ndimension2 of the base classifier space and the number of rounds 3 of boosting."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Adaptive game playi  ng using multiplicative weights.Games and Economic Behavior"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 62
                            }
                        ],
                        "text": "Others, such as those of Friedman [35] and Duffy and Helmbold [24] use the f unctional gradient descent view of boosting to derive algorithms that directly minimize a loss function appropriate for regression."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 146
                            }
                        ],
                        "text": "This is because both algorithms are doing a kind of functional gradient descent, an observation that is spelled out and exploited by Breiman [9], Duffy and Helmbold [23], Mason et al. [51, 52] and Friedman [35]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 43
                            }
                        ],
                        "text": "Others, such as those of Friedman [35] and Duffy and Helmbold [24] use the functional gradient descent view of boosting to derive algorithms that directly minimize a loss function appropriate for regression."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 139
                            }
                        ],
                        "text": "(10), the only necessary modification is to redefine + 6 ,- to be proportional to $$ ' CFE G] 6 $ A very similar algorithm is described by Duffy and Helmbold [23]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 84
                            }
                        ],
                        "text": "This view of boosting and its generalization are examined in considerable detail by Duffy and Helmbold [23], Mason et al. [51, 52] and Friedman [35]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Boosting methods for re  g ssion.Machine Learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 109
                            }
                        ],
                        "text": "This view of boosting and its generalization are examined in considerable detail by Duffy and Helmbold [23], Mason et al. [51, 52] and Friedman [35]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "In other work, Freund and Mason [29] showed how to apply boosting to le arn a generalization of decision trees called \u201calternating trees."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 15
                            }
                        ],
                        "text": "In other work, Freund and Mason [29] showed how to apply boosting to learn a generalization of decision trees called \u201calternating trees.\u201d"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 171
                            }
                        ],
                        "text": "This is because both algorithms are doing a kind of functional gradient descent, an observation that is spelled out and exploited by Breiman [9], Duffy and Helmbold [23], Mason et al. [51, 52] and Friedman [35]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The alternating decision tr  ee learning algorithm"
            },
            "venue": {
                "fragments": [],
                "text": "In Machine Learning: Proceedings of the Sixteenth Internatio  nal Conference"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3335246"
                        ],
                        "name": "C. Merz",
                        "slug": "C.-Merz",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Merz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Merz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 209099422,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b6cf9167aeb2782651156de5e22cad82ee69a225",
            "isKey": false,
            "numCitedBy": 1982,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "UCI-Repository-of-Machine-Learning-Databases-Merz",
            "title": {
                "fragments": [],
                "text": "UCI Repository of Machine Learning Databases"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49223598"
                        ],
                        "name": "J. Darroch",
                        "slug": "J.-Darroch",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Darroch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Darroch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12360582"
                        ],
                        "name": "D. Ratcliff",
                        "slug": "D.-Ratcliff",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Ratcliff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ratcliff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 120862597,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "37c931cbaa9217b829596dd196520a838562a109",
            "isKey": false,
            "numCitedBy": 1329,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Generalized-Iterative-Scaling-for-Log-Linear-Models-Darroch-Ratcliff",
            "title": {
                "fragments": [],
                "text": "Generalized Iterative Scaling for Log-Linear Models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2095718110"
                        ],
                        "name": "\u91d1\u7530 \u91cd\u90ce",
                        "slug": "\u91d1\u7530-\u91cd\u90ce",
                        "structuredName": {
                            "firstName": "\u91d1\u7530",
                            "lastName": "\u91cd\u90ce",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u91d1\u7530 \u91cd\u90ce"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59994655,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c16cf47f2b872e7b2ad06facb5d491857650514",
            "isKey": false,
            "numCitedBy": 1401,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "C4.5:-Programs-for-Machine-Learning-(\u66f8\u8a55)-\u91d1\u7530",
            "title": {
                "fragments": [],
                "text": "C4.5: Programs for Machine Learning (\u66f8\u8a55)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In addition, the margin theory points to a strong connection between boosting and the support-vector machines of Vapnik and others [7, 14, 77] which explicitly attempt to maximize the minimum margin."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59752996,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "5451278e1a11cf3f1be28a05f38d36c8641e68f7",
            "isKey": false,
            "numCitedBy": 4580,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Nature-of-Statistical-Learning-Vapnik",
            "title": {
                "fragments": [],
                "text": "The Nature of Statistical Learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 98
                            }
                        ],
                        "text": "Working in Valiant\u2019s PAC (probably approximately correct) learning model [75], Kearns and Valiant [41, 42] were the first to pose the question of whether a \u201cweak\u201d learning algorithm that performs just slightly better than r dom guessing can be \u201cboosted\u201d into an arbitrarily accurate \u201cstrong\u201d learning a l orithm."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 78
                            }
                        ],
                        "text": "Working in Valiant\u2019s PAC (probably approximately correct) learning model [75], Kearns and Valiant [41, 42] were the first to pose the question of whether a \u201cweak\u201d learning algorithm that performs just slightly better than random guessing can be \u201cboosted\u201d into an arbitrarily accurate \u201cstrong\u201d learning algorithm."
                    },
                    "intents": []
                }
            ],
            "corpusId": 4148010,
            "fieldsOfStudy": [],
            "id": "993de2892811b5dc507bbb618a7233cbf7a7f6a8",
            "isKey": false,
            "numCitedBy": 1456,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Journal-of-the-Association-for-Computing-Machinery",
            "title": {
                "fragments": [],
                "text": "Journal of the Association for Computing Machinery"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1961
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 54
                            }
                        ],
                        "text": "On the other hand, Koltchinskii, Panchenko and Lozano [44, 45, 46, 58] have recently proved new margin-theoretic bounds that are tight enough to give useful quantitative predictions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 236499608,
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "New Zero-Error Bounds for Voting Algorithms."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 274,
                                "start": 266
                            }
                        ],
                        "text": "Boosting has also been applied to text filtering [72] and routing [39], \u201cranking\u201d problems [28], learning problems arising in natural language processing [1, 12, 25, 38, 55, 78], image retrieval [74], medical diagnosis [53], and customer monitoring and segmentation [56, 57]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 236439723,
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Applying Support Vector Machines and Boosting to a Non-Intrusive Monitoring System for Household Electric Appliances with Inverters"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Schapire and Yoram Singer . Improved boosting algorithms using con fi dencerated predictions"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Monica Rogati . SPoT : A trainable sentence planner"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2 nd Annual Meeting of the North American Chapter of the Associataion for Computational Linguistics"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "[6, 76] for its definition and relation to learning theory."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the uniform convergence of relative frequencies of events to their probabilities.  Theory of Probability and its applications"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Schapire and Yoram Singer . Improved boosting algorithms using confidencerated predictions"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 24
                            }
                        ],
                        "text": "Following up on work by Kivinen and Warmuth [43] and Lafferty [47], they derive this algorithm using a unification of logistic regression and boosting based on Bregman distances."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 44
                            }
                        ],
                        "text": "Following up on work by Kivinen and Warmuth [43] and Lafferty [47], they derive this algorithm usin g a unification of logistic regression and boosting based on Bregman distances."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Boosting as entro  py projection"
            },
            "venue": {
                "fragments": [],
                "text": "InProceedings of the Twelfth Annual Conference on Computational Lear ning Theory,"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 98
                            }
                        ],
                        "text": "Working in Valiant\u2019s PAC (probably approximately correct) learning model [75], Kearns and Valiant [41, 42] were the first to pose the question of whether a \u201cweak\u201d learning algorithm that performs just slightly better than r dom guessing can be \u201cboosted\u201d into an arbitrarily accurate \u201cstrong\u201d learning a l orithm."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 78
                            }
                        ],
                        "text": "Working in Valiant\u2019s PAC (probably approximately correct) learning model [75], Kearns and Valiant [41, 42] were the first to pose the question of whether a \u201cweak\u201d learning algorithm that performs just slightly better than random guessing can be \u201cboosted\u201d into an arbitrarily accurate \u201cstrong\u201d learning algorithm."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning Boolean formulae or finite automata is as hard as factoring"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report TR-14-88,"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 85
                            }
                        ],
                        "text": "In response to these empirical findings, Schapire et al. [69], following the work of Bartlett [3], gave an alternative analysis in terms of the margins of the training examples."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 38
                            }
                        ],
                        "text": "[69] , following the work of Bartlett [3], gave an alternative analysis in terms of the marginsof the training examples."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The sample complexity of pattern clas  sification with neural networks: the size of the weights is more important than the size of the network.IEEE Transactions on Information Theory"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 51
                            }
                        ],
                        "text": "In another set of experiments, Schapire and Singer [71] used boosting for text categorization tasks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 194
                            }
                        ],
                        "text": "Figure 4: Comparison of error rates for AdaBoost and four oth er ext categorization methods (naive Bayes, probabilistic TF-IDF, Rocchio a nd sleeping experts) as reported by Schapire and Singer [71]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 204
                            }
                        ],
                        "text": "Figure 5: Comparison of the training (left) and test (right) error using three boosting methods on a six-class text classification problem from t he TREC-AP collection, as reported by Schapire and Singer [70, 71]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "BoosTexter: A boos  ting-based system for text categorization.Machine Learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Valiant . Learning Boolean formulae or finite automata is as hard as factoring"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Schapire and Yoram Singer . BoosTexter : A boostingbased system for text categorization"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 108
                            }
                        ],
                        "text": "The first experiments with these early boosting algorithms were carried out by Drucker, Schapire and Simard [22] on an OCR task."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Boosting performance in neural networks. International Journal of Pattern Recognition and Artificial Intelligence"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 108
                            }
                        ],
                        "text": "The first experiments with these early boosting algorithms were carried out by Drucker, Schapire and Simard [22] on an OCR task."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Boosting performance in neural networks. International Journal of Pattern Recognition and Artificial Intelligence"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 48
                            }
                        ],
                        "text": "However, in early exper iments, several authors [8, 21, 59] observed empirically that boosting often d oesnot overfit, even when run for thousands of rounds."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Arcing classifiers.  The Annals of Statistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Schapire , Yoram Singer , and Amit Singhal . Boosting and Rocchio applied to text fi ltering"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 21 st Annual International Conference on Research and Development in Information Retrieval"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Schapire and Yoram Singer . BoosTexter : A boostingbased system for text categorization"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 193
                            }
                        ],
                        "text": "learning algorithm that, when combined with AdaBoost, resu lts in a final classifier consisting of a relatively small set of rules similar to thos e generated by systems like RIPPER [10], IREP [36] and C4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 240,
                                "start": 236
                            }
                        ],
                        "text": "Cohen and Singer [11] showed how to design a base\nlearning algorithm that, when combined with AdaBoost, results in a final classifier consisting of a relatively small set of rules similar to those generated by systems like RIPPER [10], IREP [36] and C4.5rules [60]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Incremental r educed error pruning"
            },
            "venue": {
                "fragments": [],
                "text": "In Machine Learning: Proceedings of the Eleventh Internation  al Conference"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "Friedman, Hastie and Tibshirani [34] suggested a method for using the output of AdaBoost to make reasonable estimates of such probabilities."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 14
                            }
                        ],
                        "text": "Of course, as Friedman, Hastie and Tibshirani point out, rather than minimizing the exponential loss in Eq."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "Friedman, Hastie and Tibshirani [34] suggested a variant of AdaBoost, called \u201cGentle AdaBoost\u201d that puts less emphasis on outliers."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Additive logistic regression: A statistical view of boosting.  The Annals of Statistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An empirical evaluation f bagging and boost"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Fourteenth National Conference on Artificial Intelligence , pages"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 34
                            }
                        ],
                        "text": "For instance, Freund and Schapire [30] te sted AdaBoost on a set of UCI benchmark datasets [54] using C4."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 65
                            }
                        ],
                        "text": "7 taken from an OCR experim ent conducted by Freund and Schapire [30]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Experiments with a n  ew boosting algorithm"
            },
            "venue": {
                "fragments": [],
                "text": "In Machine Learning: Proceedings of the Thirteenth Internati  onal Conference"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": ", Beth Logan , and Bhiksha Raj . A boosting approach for con fi dence scoring"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 7 th European Conference on Speech Communication and Technology"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 0
                            }
                        ],
                        "text": "Ra\u0308tsch, Onoda and Mu\u0308ller [61] show how to regularize AdaBoost to handle noisy data."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 25
                            }
                        ],
                        "text": "R\u00e4tsch, Onoda and M\u00fcller [61] show how to regu larize AdaBoost to handle noisy data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Soft margins fo  r AdaBoost.Machine Learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Schapire and Yoram Singer . Improved boosting algorithms using con fi dence - rated predictions"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 48
                            }
                        ],
                        "text": "However, in early exper iments, several authors [8, 21, 59] observed empirically that boosting often d oesnot overfit, even when run for thousands of rounds."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 69
                            }
                        ],
                        "text": "AdaBoost has been tested empirically by many researchers, i nclud ng [4, 18, 21, 40, 49, 59, 73]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Boosting decision t rees"
            },
            "venue": {
                "fragments": [],
                "text": "InAdvances in Neural Information Processing Systems"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Schapire . The strength of weak learnability"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Jackson and Mark W . Craven . Learning sparse perceptrons"
            },
            "venue": {
                "fragments": [],
                "text": "In Advances in Neural Information Processing Systems"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 91
                            }
                        ],
                        "text": "For instance, Freund and Schapire [30] te sted AdaBoost on a set of UCI benchmark datasets [54] using C4."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "www.ics.uci.edu/  mlearn/MLRepository.html"
            },
            "venue": {
                "fragments": [],
                "text": "UCI repository of machine lea  rning databases,"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 54
                            }
                        ],
                        "text": "On the other hand, Koltchinskii, Panchenko and Lozano [44, 45, 46, 58] have recently proved new margin-theoretic bounds that are tight enough to give useful quantitative predictions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Empirical margin dis  tributions and bounding the generalization error of combined classifiers.  The Annals of Statistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 156
                            }
                        ],
                        "text": "Boosting has also been applied to text filtering [72] and rout ing [39], \u201cranking\u201d problems [28], learning problems arising in natural langua ge processing [1, 12, 25, 38, 55, 78], image retrieval [74], medical diagnosis [53], a nd customer monitoring and segmentation [56, 57]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Discriminative reranking for natura  l l nguage parsing"
            },
            "venue": {
                "fragments": [],
                "text": "In Proceedings of the Seventeenth International Conference on Machin  e Learning,"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Schapire and Yoram Singer . Improved boostingalgorithms using confidencerated predictions"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Schapire . Drifting games"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 154
                            }
                        ],
                        "text": "Boosting has also been applied to text filtering [72] and routing [39], \u201cranking\u201d problems [28], learning problems arising in natural language processing [1, 12, 25, 38, 55, 78], image retrieval [74], medical diagnosis [53], and customer monitoring and segmentation [56, 57]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "SPoT: A trainable sentence planner. InProceedings of the 2nd Annual Meeting of the North American Chapter of the Associataion for Computational Linguistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 156
                            }
                        ],
                        "text": "Boosting has also been applied to text filtering [72] and rout ing [39], \u201cranking\u201d problems [28], learning problems arising in natural langua ge processing [1, 12, 25, 38, 55, 78], image retrieval [74], medical diagnosis [53], a nd customer monitoring and segmentation [56, 57]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "SPoT  : A trainable sentence planner. InProceedings of the 2nd Annual Meeting of the North American C  hapter of the Associataion for Computational Linguistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 133
                            }
                        ],
                        "text": "In ad dition, the margin theory points to a strong connection between boosting and th e support-vector machines of Vapnik and others [7, 14, 77] which explicitly atte mpt to maximize the minimum margin."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Support-vector ne  tworks"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning,"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 69
                            }
                        ],
                        "text": "AdaBoost has been tested empirically by many researchers, i nclud ng [4, 18, 21, 40, 49, 59, 73]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning sparse p erceptrons"
            },
            "venue": {
                "fragments": [],
                "text": "InAdvances in Neural Information Processing Systems"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Schapire . Drifting games"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 225,
                                "start": 221
                            }
                        ],
                        "text": "Boosting has also been applied to text filtering [72] and rout ing [39], \u201cranking\u201d problems [28], learning problems arising in natural langua ge processing [1, 12, 25, 38, 55, 78], image retrieval [74], medical diagnosis [53], a nd customer monitoring and segmentation [56, 57]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Tuning costsensitive boosting and its application to melanoma diagnos  is"
            },
            "venue": {
                "fragments": [],
                "text": "In Multiple Classifier Systems: Proceedings of the 2nd International Workshop"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 127
                            }
                        ],
                        "text": "Thi work further connects boosting to the maximum-entropy literature, particu larly the iterative-scaling family of algorithms [15, 16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalized iterative sc  aling for log-linear models"
            },
            "venue": {
                "fragments": [],
                "text": "The Annals of Mathematical Statistics"
            },
            "year": 1972
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 43,
            "methodology": 35
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 126,
        "totalPages": 13
    },
    "page_url": "https://www.semanticscholar.org/paper/The-Boosting-Approach-to-Machine-Learning-An-Schapire/84bb60b83f82ad847e19d96403ad0011abfc888f?sort=total-citations"
}