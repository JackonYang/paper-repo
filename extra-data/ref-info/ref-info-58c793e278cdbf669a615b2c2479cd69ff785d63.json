{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112096491"
                        ],
                        "name": "Xiao Yang",
                        "slug": "Xiao-Yang",
                        "structuredName": {
                            "firstName": "Xiao",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiao Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8020964"
                        ],
                        "name": "Ersin Yumer",
                        "slug": "Ersin-Yumer",
                        "structuredName": {
                            "firstName": "Ersin",
                            "lastName": "Yumer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ersin Yumer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2934421"
                        ],
                        "name": "Paul Asente",
                        "slug": "Paul-Asente",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Asente",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul Asente"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1389971134"
                        ],
                        "name": "Mike Kraley",
                        "slug": "Mike-Kraley",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Kraley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mike Kraley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1852261"
                        ],
                        "name": "Daniel Kifer",
                        "slug": "Daniel-Kifer",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Kifer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Kifer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145157784"
                        ],
                        "name": "C. Lee Giles",
                        "slug": "C.-Lee-Giles",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Giles",
                            "middleNames": [
                                "Lee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lee Giles"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 121
                            }
                        ],
                        "text": "Applications of layout analysis range from text and non-text separation to full text segmentation of complex layouts [3]\u2013[7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2272015,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9baae0bdc2884bcf0aa4063914b87d60952cb678",
            "isKey": false,
            "numCitedBy": 145,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an end-to-end, multimodal, fully convolutional network for extracting semantic structures from document images. We consider document semantic structure extraction as a pixel-wise segmentation task, and propose a unified model that classifies pixels based not only on their visual appearance, as in the traditional page segmentation task, but also on the content of underlying text. Moreover, we propose an efficient synthetic document generation process that we use to generate pretraining data for our network. Once the network is trained on a large set of synthetic documents, we fine-tune the network on unlabeled real documents using a semi-supervised approach. We systematically study the optimum network architecture and show that both our multimodal approach and the synthetic data pretraining significantly boost the performance."
            },
            "slug": "Learning-to-Extract-Semantic-Structure-from-Using-Yang-Yumer",
            "title": {
                "fragments": [],
                "text": "Learning to Extract Semantic Structure from Documents Using Multimodal Fully Convolutional Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "An end-to-end, multimodal, fully convolutional network for extracting semantic structures from document images using a unified model that classifies pixels based not only on their visual appearance, as in the traditional page segmentation task, but also on the content of underlying text."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2973998"
                        ],
                        "name": "Fedor Borisyuk",
                        "slug": "Fedor-Borisyuk",
                        "structuredName": {
                            "firstName": "Fedor",
                            "lastName": "Borisyuk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fedor Borisyuk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1821267"
                        ],
                        "name": "Albert Gordo",
                        "slug": "Albert-Gordo",
                        "structuredName": {
                            "firstName": "Albert",
                            "lastName": "Gordo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Albert Gordo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145422368"
                        ],
                        "name": "V. Sivakumar",
                        "slug": "V.-Sivakumar",
                        "structuredName": {
                            "firstName": "Viswanath",
                            "lastName": "Sivakumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Sivakumar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 160
                            }
                        ],
                        "text": "Modern engines that also support handwritten text recognition usually use a Connectionist Temporal Classification (CTC) loss to cope with the alignment problem [18]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 34
                            }
                        ],
                        "text": ", LSTM, GRU) to extract the words [18]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 50773726,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fde3ee5f9f8e217a4d6716013315614811820f21",
            "isKey": false,
            "numCitedBy": 137,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a deployed, scalable optical character recognition (OCR) system, which we call Rosetta , designed to process images uploaded daily at Facebook scale. Sharing of image content has become one of the primary ways to communicate information among internet users within social networks such as Facebook, and the understanding of such media, including its textual information, is of paramount importance to facilitate search and recommendation applications. We present modeling techniques for efficient detection and recognition of text in images and describe Rosetta 's system architecture. We perform extensive evaluation of presented technologies, explain useful practical approaches to build an OCR system at scale, and provide insightful intuitions as to why and how certain components work based on the lessons learnt during the development and deployment of the system."
            },
            "slug": "Rosetta:-Large-Scale-System-for-Text-Detection-and-Borisyuk-Gordo",
            "title": {
                "fragments": [],
                "text": "Rosetta: Large Scale System for Text Detection and Recognition in Images"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "This paper presents a deployed, scalable optical character recognition (OCR) system, which is called Rosetta, designed to process images uploaded daily at Facebook scale, and describes Rosetta 's system architecture."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3373150"
                        ],
                        "name": "Yuchen Dai",
                        "slug": "Yuchen-Dai",
                        "structuredName": {
                            "firstName": "Yuchen",
                            "lastName": "Dai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuchen Dai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1390799037"
                        ],
                        "name": "Zheng Huang",
                        "slug": "Zheng-Huang",
                        "structuredName": {
                            "firstName": "Zheng",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zheng Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2139833611"
                        ],
                        "name": "Yuting Gao",
                        "slug": "Yuting-Gao",
                        "structuredName": {
                            "firstName": "Yuting",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuting Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72387933"
                        ],
                        "name": "Kai Chen",
                        "slug": "Kai-Chen",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Chen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 19056350,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "25ace72fc3e483d5d52c9209f76b04f0fdd08f9b",
            "isKey": false,
            "numCitedBy": 113,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we introduce a novel end-end framework for multi-oriented scene text detection from an instance-aware semantic segmentation perspective. We present Fused Text Segmentation Networks, which combine multi-level features during the feature extracting as text instance may rely on finer feature expression compared to general objects. It detects and segments the text instance jointly and simultaneously, leveraging merits from both semantic segmentation task and region proposal based object detection task. Not involving any extra pipelines, our approach surpasses the current state of the art on multi-oriented scene text detection benchmarks: ICDAR2015 Incidental Scene Text and MSRA-TD500 reaching Hmean 84.1 % and 82.0 % respectively. Morever, we report a baseline on total-text containing curved text which suggests effectiveness of the proposed approach."
            },
            "slug": "Fused-Text-Segmentation-Networks-for-Multi-oriented-Dai-Huang",
            "title": {
                "fragments": [],
                "text": "Fused Text Segmentation Networks for Multi-oriented Scene Text Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "A novel end-end framework for multi-oriented scene text detection from an instance-aware semantic segmentation perspective, Fused Text Segmentation Networks, which combines multi-level features during the feature extracting as text instance may rely on finer feature expression compared to general objects."
            },
            "venue": {
                "fragments": [],
                "text": "2018 24th International Conference on Pattern Recognition (ICPR)"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2148927556"
                        ],
                        "name": "Xinyu Zhou",
                        "slug": "Xinyu-Zhou",
                        "structuredName": {
                            "firstName": "Xinyu",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xinyu Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146721"
                        ],
                        "name": "C. Yao",
                        "slug": "C.-Yao",
                        "structuredName": {
                            "firstName": "Cong",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Yao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2075348632"
                        ],
                        "name": "He Wen",
                        "slug": "He-Wen",
                        "structuredName": {
                            "firstName": "He",
                            "lastName": "Wen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "He Wen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47905836"
                        ],
                        "name": "Yuzhi Wang",
                        "slug": "Yuzhi-Wang",
                        "structuredName": {
                            "firstName": "Yuzhi",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuzhi Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132667"
                        ],
                        "name": "Shuchang Zhou",
                        "slug": "Shuchang-Zhou",
                        "structuredName": {
                            "firstName": "Shuchang",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuchang Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2416953"
                        ],
                        "name": "Weiran He",
                        "slug": "Weiran-He",
                        "structuredName": {
                            "firstName": "Weiran",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weiran He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1387852255"
                        ],
                        "name": "Jiajun Liang",
                        "slug": "Jiajun-Liang",
                        "structuredName": {
                            "firstName": "Jiajun",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiajun Liang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 11
                            }
                        ],
                        "text": "Tesseract, EAST and Google Vision are tested without re-training on the FUNSD training set."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "As EAST and Google Vision output their predictions as quadrangles (i.e., 4 vertices that define a polygon) and the FUNSD dataset is annotated with rectangles, we transform each quadrangle as a rectangle by constructing the smallest rectangle that contains the 4 quadrangle vertices."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 138
                            }
                        ],
                        "text": "The network then predicts heat-maps that represent the probability of each pixel being part of a text jointly with bounding box proposals [11]\u2013[14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 106
                            }
                        ],
                        "text": "This observation is expected as we are specifically retraining the network for\n9https://github.com/argman/EAST 10https://cloud.google.com/vision/docs/detecting-fulltext 11https://github.com/facebookresearch/maskrcnn-benchmark\nthe task."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 90
                            }
                        ],
                        "text": "The text detection on the FUNSD dataset was tested with 4 baselines: Tesseract [15], EAST [11]9, Google Vision API10 and with a Faster R-CNN architecture [16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 706860,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5f1630b4485027eb99ae59b745372ef1f3699c16",
            "isKey": true,
            "numCitedBy": 904,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous approaches for scene text detection have already achieved promising performances across various benchmarks. However, they usually fall short when dealing with challenging scenarios, even when equipped with deep neural network models, because the overall performance is determined by the interplay of multiple stages and components in the pipelines. In this work, we propose a simple yet powerful pipeline that yields fast and accurate text detection in natural scenes. The pipeline directly predicts words or text lines of arbitrary orientations and quadrilateral shapes in full images, eliminating unnecessary intermediate steps (e.g., candidate aggregation and word partitioning), with a single neural network. The simplicity of our pipeline allows concentrating efforts on designing loss functions and neural network architecture. Experiments on standard datasets including ICDAR 2015, COCO-Text and MSRA-TD500 demonstrate that the proposed algorithm significantly outperforms state-of-the-art methods in terms of both accuracy and efficiency. On the ICDAR 2015 dataset, the proposed algorithm achieves an F-score of 0.7820 at 13.2fps at 720p resolution."
            },
            "slug": "EAST:-An-Efficient-and-Accurate-Scene-Text-Detector-Zhou-Yao",
            "title": {
                "fragments": [],
                "text": "EAST: An Efficient and Accurate Scene Text Detector"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work proposes a simple yet powerful pipeline that yields fast and accurate text detection in natural scenes, and significantly outperforms state-of-the-art methods in terms of both accuracy and efficiency."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2764871"
                        ],
                        "name": "C. Clausner",
                        "slug": "C.-Clausner",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Clausner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Clausner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1803149"
                        ],
                        "name": "A. Antonacopoulos",
                        "slug": "A.-Antonacopoulos",
                        "structuredName": {
                            "firstName": "Apostolos",
                            "lastName": "Antonacopoulos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Antonacopoulos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1980669"
                        ],
                        "name": "S. Pletschacher",
                        "slug": "S.-Pletschacher",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Pletschacher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Pletschacher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 117
                            }
                        ],
                        "text": "Applications of layout analysis range from text and non-text separation to full text segmentation of complex layouts [3]\u2013[7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4763136,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c69b8f83a73c216b89df24d66ad0e58f0eda4b8e",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an objective comparative evaluation of page segmentation and region classification methods for documents with complex layouts. It describes the competition (modus operandi, dataset and evaluation methodology) held in the context of ICDAR2017, presenting the results of the evaluation of seven methods \u2013 five submitted, two state-of-the-art systems (commercial and open-source). Three scenarios are reported in this paper, one evaluating the ability of methods to accurately segment regions and two evaluating both segmentation and region classification (one focusing only on text regions). For the first time, nested region content (table cells, chart labels etc.) are evaluated in addition to the top-level page content. Text recognition was a bonus challenge and was not taken up by all participants. The results indicate that an innovative approach has a clear advantage but there is still a considerable need to develop robust methods that deal with layout challenges, especially with the non-textual content."
            },
            "slug": "ICDAR2017-Competition-on-Recognition-of-Documents-Clausner-Antonacopoulos",
            "title": {
                "fragments": [],
                "text": "ICDAR2017 Competition on Recognition of Documents with Complex Layouts - RDCL2017"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The results indicate that an innovative approach has a clear advantage but there is still a considerable need to develop robust methods that deal with layout challenges, especially with the non-textual content."
            },
            "venue": {
                "fragments": [],
                "text": "2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40219976"
                        ],
                        "name": "Zhi Tian",
                        "slug": "Zhi-Tian",
                        "structuredName": {
                            "firstName": "Zhi",
                            "lastName": "Tian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhi Tian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49015548"
                        ],
                        "name": "Weilin Huang",
                        "slug": "Weilin-Huang",
                        "structuredName": {
                            "firstName": "Weilin",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weilin Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118328320"
                        ],
                        "name": "Tong He",
                        "slug": "Tong-He",
                        "structuredName": {
                            "firstName": "Tong",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tong He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50462511"
                        ],
                        "name": "Pan He",
                        "slug": "Pan-He",
                        "structuredName": {
                            "firstName": "Pan",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pan He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143970608"
                        ],
                        "name": "Y. Qiao",
                        "slug": "Y.-Qiao",
                        "structuredName": {
                            "firstName": "Yu",
                            "lastName": "Qiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Qiao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14728290,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b620548e06b03e3cc2e9e775c39f5b3d5a4eb19a",
            "isKey": false,
            "numCitedBy": 647,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a novel Connectionist Text Proposal Network (CTPN) that accurately localizes text lines in natural image. The CTPN detects a text line in a sequence of fine-scale text proposals directly in convolutional feature maps. We develop a vertical anchor mechanism that jointly predicts location and text/non-text score of each fixed-width proposal, considerably improving localization accuracy. The sequential proposals are naturally connected by a recurrent neural network, which is seamlessly incorporated into the convolutional network, resulting in an end-to-end trainable model. This allows the CTPN to explore rich context information of image, making it powerful to detect extremely ambiguous text. The CTPN works reliably on multi-scale and multi-language text without further post-processing, departing from previous bottom-up methods requiring multi-step post filtering. It achieves 0.88 and 0.61 F-measure on the ICDAR 2013 and 2015 benchmarks, surpassing recent results [8, 35] by a large margin. The CTPN is computationally efficient with 0.14 s/image, by using the very deep VGG16 model [27]. Online demo is available: http://textdet.com/."
            },
            "slug": "Detecting-Text-in-Natural-Image-with-Connectionist-Tian-Huang",
            "title": {
                "fragments": [],
                "text": "Detecting Text in Natural Image with Connectionist Text Proposal Network"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A novel Connectionist Text Proposal Network (CTPN) that accurately localizes text lines in natural image and develops a vertical anchor mechanism that jointly predicts location and text/non-text score of each fixed-width proposal, considerably improving localization accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3285734"
                        ],
                        "name": "S. Marinai",
                        "slug": "S.-Marinai",
                        "structuredName": {
                            "firstName": "Simone",
                            "lastName": "Marinai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Marinai"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59911365,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "61634d381d6263cc56327dc7f83014d5f3f3b509",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-Algorithms-for-Document-Layout-Analysis-Marinai",
            "title": {
                "fragments": [],
                "text": "Learning Algorithms for Document Layout Analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2158556318"
                        ],
                        "name": "Zhida Huang",
                        "slug": "Zhida-Huang",
                        "structuredName": {
                            "firstName": "Zhida",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhida Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2665108"
                        ],
                        "name": "Zhuoyao Zhong",
                        "slug": "Zhuoyao-Zhong",
                        "structuredName": {
                            "firstName": "Zhuoyao",
                            "lastName": "Zhong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhuoyao Zhong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110833051"
                        ],
                        "name": "Lei Sun",
                        "slug": "Lei-Sun",
                        "structuredName": {
                            "firstName": "Lei",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lei Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2316043"
                        ],
                        "name": "Qiang Huo",
                        "slug": "Qiang-Huo",
                        "structuredName": {
                            "firstName": "Qiang",
                            "lastName": "Huo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qiang Huo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 53719742,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3e2738f121d9854f8fc10e59d82eeae2efc1d579",
            "isKey": false,
            "numCitedBy": 57,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a new Mask R-CNN based text detection approach which can robustly detect multi-oriented and curved text from natural scene images in a unified manner. To enhance the feature representation ability of Mask R-CNN for text detection tasks, we propose to use the Pyramid Attention Network (PAN) as a new backbone network of Mask R-CNN. Experiments demonstrate that PAN can suppress false alarms caused by text-like backgrounds more effectively. Our proposed approach has achieved superior performance on both multi-oriented (ICDAR-2015, ICDAR-2017 MLT) and curved (SCUT-CTW1500) text detection benchmark tasks by only using single-scale and single-model testing."
            },
            "slug": "Mask-R-CNN-With-Pyramid-Attention-Network-for-Scene-Huang-Zhong",
            "title": {
                "fragments": [],
                "text": "Mask R-CNN With Pyramid Attention Network for Scene Text Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "A new Mask R-CNN based text detection approach which can robustly detect multi-oriented and curved text from natural scene images in a unified manner is presented and the Pyramid Attention Network (PAN) is proposed as a new backbone network of MaskR-CNN."
            },
            "venue": {
                "fragments": [],
                "text": "2019 IEEE Winter Conference on Applications of Computer Vision (WACV)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34939798"
                        ],
                        "name": "Adam W. Harley",
                        "slug": "Adam-W.-Harley",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Harley",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam W. Harley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2079687415"
                        ],
                        "name": "Alex Ufkes",
                        "slug": "Alex-Ufkes",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Ufkes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Ufkes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3150825"
                        ],
                        "name": "K. Derpanis",
                        "slug": "K.-Derpanis",
                        "structuredName": {
                            "firstName": "Konstantinos",
                            "lastName": "Derpanis",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Derpanis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 152
                            }
                        ],
                        "text": "To ensure that real data are used, with a high variability in the structure of the forms and realistic noise, we used a subset of the RVL-CDIP dataset4 [10]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2760893,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd86b4b551b9d3fb498f62008b037e7599365018",
            "isKey": false,
            "numCitedBy": 174,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new state-of-the-art for document image classification and retrieval, using features learned by deep convolutional neural networks (CNNs). In object and scene analysis, deep neural nets are capable of learning a hierarchical chain of abstraction from pixel inputs to concise and descriptive representations. The current work explores this capacity in the realm of document analysis, and confirms that this representation strategy is superior to a variety of popular handcrafted alternatives. Extensive experiments show that (i) features extracted from CNNs are robust to compression, (ii) CNNs trained on non-document images transfer well to document analysis tasks, and (iii) enforcing region-specific feature-learning is unnecessary given sufficient training data. This work also makes available a new labelled subset of the IIT-CDIP collection, containing 400,000 document images across 16 categories."
            },
            "slug": "Evaluation-of-deep-convolutional-nets-for-document-Harley-Ufkes",
            "title": {
                "fragments": [],
                "text": "Evaluation of deep convolutional nets for document image classification and retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "A new state-of-the-art for document image classification and retrieval, using features learned by deep convolutional neural networks (CNNs), and makes available a new labelled subset of the IIT-CDIP collection, containing 400,000 document images across 16 categories."
            },
            "venue": {
                "fragments": [],
                "text": "2015 13th International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31838600"
                        ],
                        "name": "A. Tafti",
                        "slug": "A.-Tafti",
                        "structuredName": {
                            "firstName": "Ahmad",
                            "lastName": "Tafti",
                            "middleNames": [
                                "Pahlavan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tafti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678985"
                        ],
                        "name": "Ahmadreza Baghaie",
                        "slug": "Ahmadreza-Baghaie",
                        "structuredName": {
                            "firstName": "Ahmadreza",
                            "lastName": "Baghaie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ahmadreza Baghaie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3023549"
                        ],
                        "name": "Mehdi Assefi",
                        "slug": "Mehdi-Assefi",
                        "structuredName": {
                            "firstName": "Mehdi",
                            "lastName": "Assefi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mehdi Assefi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144211100"
                        ],
                        "name": "H. Arabnia",
                        "slug": "H.-Arabnia",
                        "structuredName": {
                            "firstName": "Hamid",
                            "lastName": "Arabnia",
                            "middleNames": [
                                "Reza"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Arabnia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33965173"
                        ],
                        "name": "Zeyun Yu",
                        "slug": "Zeyun-Yu",
                        "structuredName": {
                            "firstName": "Zeyun",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zeyun Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7584224"
                        ],
                        "name": "P. Peissig",
                        "slug": "P.-Peissig",
                        "structuredName": {
                            "firstName": "Peggy",
                            "lastName": "Peissig",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Peissig"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 17
                            }
                        ],
                        "text": "We refer to [1], [2] for a review of current OCR systems."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1288848,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "284001e33818e088c659d4104e796a0fea1cb0d7",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Optical character recognition (OCR) as a classic machine learning challenge has been a longstanding topic in a variety of applications in healthcare, education, insurance, and legal industries to convert different types of electronic documents, such as scanned documents, digital images, and PDF files into fully editable and searchable text data. The rapid generation of digital images on a daily basis prioritizes OCR as an imperative and foundational tool for data analysis. With the help of OCR systems, we have been able to save a reasonable amount of effort in creating, processing, and saving electronic documents, adapting them to different purposes. A set of different OCR platforms are now available which, aside from lending theoretical contributions to other practical fields, have demonstrated successful applications in real-world problems. In this work, several qualitative and quantitative experimental evaluations have been performed using four well-know OCR services, including Google Docs OCR, Tesseract, ABBYY FineReader, and Transym. We analyze the accuracy and reliability of the OCR packages employing a dataset including 1227 images from 15 different categories. Furthermore, we review the state-of-the-art OCR applications in healtcare informatics. The present evaluation is expected to advance OCR research, providing new insights and consideration to the research area, and assist researchers to determine which service is ideal for optical character recognition in an accurate and efficient manner."
            },
            "slug": "OCR-as-a-Service:-An-Experimental-Evaluation-of-and-Tafti-Baghaie",
            "title": {
                "fragments": [],
                "text": "OCR as a Service: An Experimental Evaluation of Google Docs OCR, Tesseract, ABBYY FineReader, and Transym"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The present evaluation is expected to advance OCR research, providing new insights and consideration to the research area, and assist researchers to determine which service is ideal for optical character recognition in an accurate and efficient manner."
            },
            "venue": {
                "fragments": [],
                "text": "ISVC"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2603501"
                        ],
                        "name": "Wanchen Sui",
                        "slug": "Wanchen-Sui",
                        "structuredName": {
                            "firstName": "Wanchen",
                            "lastName": "Sui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wanchen Sui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108056226"
                        ],
                        "name": "Qing Zhang",
                        "slug": "Qing-Zhang",
                        "structuredName": {
                            "firstName": "Qing",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qing Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146158921"
                        ],
                        "name": "Jun Yang",
                        "slug": "Jun-Yang",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jun Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057047808"
                        ],
                        "name": "Wei Chu",
                        "slug": "Wei-Chu",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Chu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Chu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 102
                            }
                        ],
                        "text": "Note that some novel architectures perform the text detection and recognition in an end-to-end manner [19]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 53770149,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "90c483bbfe183604f76d84e6bd57f71792b23f80",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a novel integrated framework for learning both text detection and recognition. For most of the existing methods, detection and recognition are treated as two isolated tasks and trained separately, since parameters of detection and recognition models are different and two models target to optimize their own loss functions during individual training processes. In contrast to those methods, by sharing model parameters, we merge the detection model and recognition model into a single end-to-end trainable model and train the joint model for two tasks simultaneously. The shared parameters not only help effectively reduce the computational load in inference process, but also improve the end-to-end text detection-recognition accuracy. In addition, we design a simpler and faster sequence learning method for the recognition network based on a succession of stacked convolutional layers without any recurrent structure, this is proved feasible and dramatically improves inference speed. Extensive experiments on different datasets demonstrate that the proposed method achieves very promising results."
            },
            "slug": "A-Novel-Integrated-Framework-for-Learning-both-Text-Sui-Zhang",
            "title": {
                "fragments": [],
                "text": "A Novel Integrated Framework for Learning both Text Detection and Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "A novel integrated framework for learning both text detection and recognition is proposed, which merge the detection model and recognition model into a single end-to-end trainable model and train the joint model for two tasks simultaneously."
            },
            "venue": {
                "fragments": [],
                "text": "2018 24th International Conference on Pattern Recognition (ICPR)"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2458171"
                        ],
                        "name": "S\u00e9bastien Eskenazi",
                        "slug": "S\u00e9bastien-Eskenazi",
                        "structuredName": {
                            "firstName": "S\u00e9bastien",
                            "lastName": "Eskenazi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S\u00e9bastien Eskenazi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1399368454"
                        ],
                        "name": "Petra Gomez-Kr\u00e4mer",
                        "slug": "Petra-Gomez-Kr\u00e4mer",
                        "structuredName": {
                            "firstName": "Petra",
                            "lastName": "Gomez-Kr\u00e4mer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Petra Gomez-Kr\u00e4mer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695766"
                        ],
                        "name": "J. Ogier",
                        "slug": "J.-Ogier",
                        "structuredName": {
                            "firstName": "Jean-Marc",
                            "lastName": "Ogier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ogier"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4311339,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "08ab557e132322a5f161d7ddee4ad2a42b806621",
            "isKey": false,
            "numCitedBy": 79,
            "numCiting": 133,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-comprehensive-survey-of-mostly-textual-document-Eskenazi-Gomez-Kr\u00e4mer",
            "title": {
                "fragments": [],
                "text": "A comprehensive survey of mostly textual document segmentation algorithms since 2008"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3445138"
                        ],
                        "name": "Waleed Farrukh",
                        "slug": "Waleed-Farrukh",
                        "structuredName": {
                            "firstName": "Waleed",
                            "lastName": "Farrukh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Waleed Farrukh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403825113"
                        ],
                        "name": "A. Foncubierta-Rodr\u00edguez",
                        "slug": "A.-Foncubierta-Rodr\u00edguez",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Foncubierta-Rodr\u00edguez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Foncubierta-Rodr\u00edguez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1410889293"
                        ],
                        "name": "Anca-Nicoleta Ciubotaru",
                        "slug": "Anca-Nicoleta-Ciubotaru",
                        "structuredName": {
                            "firstName": "Anca-Nicoleta",
                            "lastName": "Ciubotaru",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anca-Nicoleta Ciubotaru"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35685584"
                        ],
                        "name": "Guillaume Jaume",
                        "slug": "Guillaume-Jaume",
                        "structuredName": {
                            "firstName": "Guillaume",
                            "lastName": "Jaume",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guillaume Jaume"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10107014"
                        ],
                        "name": "C. Bekas",
                        "slug": "C.-Bekas",
                        "structuredName": {
                            "firstName": "Constantine",
                            "lastName": "Bekas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bekas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3443635"
                        ],
                        "name": "Or\u00e7un G\u00f6ksel",
                        "slug": "Or\u00e7un-G\u00f6ksel",
                        "structuredName": {
                            "firstName": "Or\u00e7un",
                            "lastName": "G\u00f6ksel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Or\u00e7un G\u00f6ksel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2695254"
                        ],
                        "name": "M. Gabrani",
                        "slug": "M.-Gabrani",
                        "structuredName": {
                            "firstName": "Maria",
                            "lastName": "Gabrani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gabrani"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 67
                            }
                        ],
                        "text": "An application closely related to FoUn is table understanding [8], [9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4773150,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3f27ca429e3d8eb2a2aa066383e923b187664d18",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Densely-packed but structured scientific data are typically presented in the form of tables, which often appear in raster image form. To interpret data from scanned tables, understanding their hierarchical structure is vital. To further address the vast variability of table representations, we propose a fully automatic methodology that uses a bottom-up reasoning that is independent on the existence of representation features, such as lines. We evaluate our approach on the ICDAR 2013 dataset and demonstrate its effectiveness on detecting tables cells and their content and classifying header and data cells. For detecting the cell hierarchy, we demonstrate results on synthetic data due to lack of ground truth."
            },
            "slug": "Interpreting-Data-from-Scanned-Tables-Farrukh-Foncubierta-Rodr\u00edguez",
            "title": {
                "fragments": [],
                "text": "Interpreting Data from Scanned Tables"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A fully automatic methodology that uses a bottom-up reasoning that is independent on the existence of representation features, such as lines, is proposed that is effective on detecting tables cells and their content and classifying header and data cells."
            },
            "venue": {
                "fragments": [],
                "text": "2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38398129"
                        ],
                        "name": "Mehmet Yasin Akpinar",
                        "slug": "Mehmet-Yasin-Akpinar",
                        "structuredName": {
                            "firstName": "Mehmet",
                            "lastName": "Akpinar",
                            "middleNames": [
                                "Yasin"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mehmet Yasin Akpinar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3454709"
                        ],
                        "name": "Erdem Emekligil",
                        "slug": "Erdem-Emekligil",
                        "structuredName": {
                            "firstName": "Erdem",
                            "lastName": "Emekligil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Erdem Emekligil"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51396729"
                        ],
                        "name": "S. Arslan",
                        "slug": "S.-Arslan",
                        "structuredName": {
                            "firstName": "Se\u00e7il",
                            "lastName": "Arslan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Arslan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 62
                            }
                        ],
                        "text": "An application closely related to FoUn is table understanding [8], [9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 49657839,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0c51bca532bffb85ac48cbf001e2a80ba09db211",
            "isKey": false,
            "numCitedBy": 5,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The conversion of image-based documents into digital and processible forms can be accomplished quite successfully with optical character recognition (OCR) tools. However, there are still problems with preserving the format on the original document. An important one of these problems is the reading of the tabular data. In this paper, a method is proposed in which the tabular data contents of hard-copy documents is extracted from the text and character positions which are obtained from an OCR tool and transferred to digital forms. The performance of the method is measured by the number of detected rows and columns and presented with the results of other commercial products."
            },
            "slug": "Extracting-table-data-from-images-using-optical-Akpinar-Emekligil",
            "title": {
                "fragments": [],
                "text": "Extracting table data from images using optical character recognition text"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A method is proposed in which the tabular data contents of hard-copy documents is extracted from the text and character positions which are obtained from an OCR tool and transferred to digital forms."
            },
            "venue": {
                "fragments": [],
                "text": "2018 26th Signal Processing and Communications Applications Conference (SIU)"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3080683"
                        ],
                        "name": "Shaoqing Ren",
                        "slug": "Shaoqing-Ren",
                        "structuredName": {
                            "firstName": "Shaoqing",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaoqing Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2032184078"
                        ],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 154
                            }
                        ],
                        "text": "The text detection on the FUNSD dataset was tested with 4 baselines: Tesseract [15], EAST [11]9, Google Vision API10 and with a Faster R-CNN architecture [16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10328909,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "424561d8585ff8ebce7d5d07de8dbf7aae5e7270",
            "isKey": false,
            "numCitedBy": 32561,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available"
            },
            "slug": "Faster-R-CNN:-Towards-Real-Time-Object-Detection-Ren-He",
            "title": {
                "fragments": [],
                "text": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work introduces a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals and further merge RPN and Fast R-CNN into a single network by sharing their convolutionAL features."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771551"
                        ],
                        "name": "X. Zhang",
                        "slug": "X.-Zhang",
                        "structuredName": {
                            "firstName": "X.",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3080683"
                        ],
                        "name": "Shaoqing Ren",
                        "slug": "Shaoqing-Ren",
                        "structuredName": {
                            "firstName": "Shaoqing",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaoqing Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 81
                            }
                        ],
                        "text": "We used a pre-trained network trained on ImageNet with a ResNet-101 architecture [17]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206594692,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "isKey": false,
            "numCitedBy": 95318,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation."
            },
            "slug": "Deep-Residual-Learning-for-Image-Recognition-He-Zhang",
            "title": {
                "fragments": [],
                "text": "Deep Residual Learning for Image Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This work presents a residual learning framework to ease the training of networks that are substantially deeper than those used previously, and provides comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40200035"
                        ],
                        "name": "Noman Islam",
                        "slug": "Noman-Islam",
                        "structuredName": {
                            "firstName": "Noman",
                            "lastName": "Islam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noman Islam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059544054"
                        ],
                        "name": "Zeeshan Islam",
                        "slug": "Zeeshan-Islam",
                        "structuredName": {
                            "firstName": "Zeeshan",
                            "lastName": "Islam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zeeshan Islam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2058621477"
                        ],
                        "name": "Nazia Noor",
                        "slug": "Nazia-Noor",
                        "structuredName": {
                            "firstName": "Nazia",
                            "lastName": "Noor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nazia Noor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 12
                            }
                        ],
                        "text": "We refer to [1], [2] for a review of current OCR systems."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1040908,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6bdb84c02fd56c209c893efda7df083c9ae5c4df",
            "isKey": false,
            "numCitedBy": 68,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Optical Character Recognition (OCR) has been a topic of interest for many years. It is defined as the process of digitizing a document image into its constituent characters. Despite decades of intense research, developing OCR with capabilities comparable to that of human still remains an open challenge. Due to this challenging nature, researchers from industry and academic circles have directed their attentions towards Optical Character Recognition. Over the last few years, the number of academic laboratories and companies involved in research on Character Recognition has increased dramatically. This research aims at summarizing the research so far done in the field of OCR. It provides an overview of different aspects of OCR and discusses corresponding proposals aimed at resolving issues of OCR."
            },
            "slug": "A-Survey-on-Optical-Character-Recognition-System-Islam-Islam",
            "title": {
                "fragments": [],
                "text": "A Survey on Optical Character Recognition System"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This research aims at summarizing the research so far done in the field of OCR and discusses corresponding proposals aimed at resolving issues of O CR."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39172707"
                        ],
                        "name": "Jacob Devlin",
                        "slug": "Jacob-Devlin",
                        "structuredName": {
                            "firstName": "Jacob",
                            "lastName": "Devlin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jacob Devlin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744179"
                        ],
                        "name": "Ming-Wei Chang",
                        "slug": "Ming-Wei-Chang",
                        "structuredName": {
                            "firstName": "Ming-Wei",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming-Wei Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2544107"
                        ],
                        "name": "Kenton Lee",
                        "slug": "Kenton-Lee",
                        "structuredName": {
                            "firstName": "Kenton",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenton Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3259253"
                        ],
                        "name": "Kristina Toutanova",
                        "slug": "Kristina-Toutanova",
                        "structuredName": {
                            "firstName": "Kristina",
                            "lastName": "Toutanova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kristina Toutanova"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 119
                            }
                        ],
                        "text": "We build input features for each semantic entity with: \u2022 semantic features extracted from the pre-trained lan-\nguage model BERT [21]12, \u2022 spatial features based on the bounding box coordinates\nof the semantic entity, \u2022 meta features that encode the length of the sequence."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 52
                            }
                        ],
                        "text": "12https://github.com/huggingface/pytorch-pretrained-BERT"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 71
                            }
                        ],
                        "text": "\u2022 semantic features extracted from the pre-trained language model BERT [21]12, \u2022 spatial features based on the bounding box coordinates of the semantic entity, \u2022 meta features that encode the length of the sequence."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 52967399,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
            "isKey": true,
            "numCitedBy": 33744,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
            },
            "slug": "BERT:-Pre-training-of-Deep-Bidirectional-for-Devlin-Chang",
            "title": {
                "fragments": [],
                "text": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "A new language representation model, BERT, designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers, which can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145695526"
                        ],
                        "name": "S. Mao",
                        "slug": "S.-Mao",
                        "structuredName": {
                            "firstName": "Song",
                            "lastName": "Mao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143766793"
                        ],
                        "name": "A. Rosenfeld",
                        "slug": "A.-Rosenfeld",
                        "structuredName": {
                            "firstName": "Azriel",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Rosenfeld"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143626870"
                        ],
                        "name": "T. Kanungo",
                        "slug": "T.-Kanungo",
                        "structuredName": {
                            "firstName": "Tapas",
                            "lastName": "Kanungo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kanungo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6128200,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "633fd1e2bd089c2c402244037876e879861d6739",
            "isKey": false,
            "numCitedBy": 254,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Document structure analysis can be regarded as a syntactic analysis problem. The order and containment relations among the physical or logical components of a document page can be described by an ordered tree structure and can be modeled by a tree grammar which describes the page at the component level in terms of regions or blocks. This paper provides a detailed survey of past work on document structure analysis algorithms and summarize the limitations of past approaches. In particular, we survey past work on document physical layout representations and algorithms, document logical structure representations and algorithms, and performance evaluation of document structure analysis algorithms. In the last section, we summarize this work and point out its limitations."
            },
            "slug": "Document-structure-analysis-algorithms:-a-survey-Mao-Rosenfeld",
            "title": {
                "fragments": [],
                "text": "Document structure analysis algorithms: a literature survey"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper provides a detailed survey of past work on document structure analysis algorithms and summarize the limitations of past approaches."
            },
            "venue": {
                "fragments": [],
                "text": "IS&T/SPIE Electronic Imaging"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157708855"
                        ],
                        "name": "R. Smith",
                        "slug": "R.-Smith",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Smith",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 60
                            }
                        ],
                        "text": "We evaluate two OCR engines for text recognition: Tesseract [15] and Google Vision."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 79
                            }
                        ],
                        "text": "The text detection on the FUNSD dataset was tested with 4 baselines: Tesseract [15], EAST [11]9, Google Vision API10 and with a Faster R-CNN architecture [16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7038773,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "89d9aae7e0c8b6edd56d0d79b277c07b7ab66fda",
            "isKey": false,
            "numCitedBy": 1509,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "The Tesseract OCR engine, as was the HP Research Prototype in the UNLV Fourth Annual Test of OCR Accuracy, is described in a comprehensive overview. Emphasis is placed on aspects that are novel or at least unusual in an OCR engine, including in particular the line finding, features/classification methods, and the adaptive classifier."
            },
            "slug": "An-Overview-of-the-Tesseract-OCR-Engine-Smith",
            "title": {
                "fragments": [],
                "text": "An Overview of the Tesseract OCR Engine"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The Tesseract OCR engine, as was the HP Research Prototype in the UNLV Fourth Annual Test of OCR Accuracy, is described in a comprehensive overview."
            },
            "venue": {
                "fragments": [],
                "text": "Ninth International Conference on Document Analysis and Recognition (ICDAR 2007)"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "We propose to use the adjusted rand index [20] (ARI) as metric."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Classification \u03031985"
            },
            "venue": {
                "fragments": [],
                "text": "vol. 218, no. 1980, pp. 193\u2013 218, 1985."
            },
            "year": 1980
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 9,
            "methodology": 7
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 21,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/FUNSD:-A-Dataset-for-Form-Understanding-in-Noisy-Jaume-Ekenel/58c793e278cdbf669a615b2c2479cd69ff785d63?sort=total-citations"
}