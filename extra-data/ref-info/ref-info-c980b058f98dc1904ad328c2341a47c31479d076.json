{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719062"
                        ],
                        "name": "J. Kender",
                        "slug": "J.-Kender",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Kender",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kender"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1692339"
                        ],
                        "name": "B. Yeo",
                        "slug": "B.-Yeo",
                        "structuredName": {
                            "firstName": "Boon-Lock",
                            "lastName": "Yeo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Yeo"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [10], the authors detect scene boundaries as local minima of a backward shot coherence measure."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 79
                            }
                        ],
                        "text": "We used k = 9 for the memory width, a value similar to the buffer size used in [10] for computing shot coherence."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 121
                            }
                        ],
                        "text": "In comparison, we obtained an F-score of 43% for scene boundary detection using a model based on backward shot coherence [10] uninformed by screenplay, but optimized over buffer size and non-maximum suppression window size."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17174505,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "55eac211d1dcc3c6eaadbdaa246fe7df30c2c4a5",
            "isKey": true,
            "numCitedBy": 199,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "In extended video sequences, individual frames are grouped into shots which are defined as a sequence taken by a single camera, and related shots are grouped into scenes which are defined as a single dramatic event taken by a small number of related cameras. This hierarchical structure is deliberately constructed, dictated by the limitations and preferences of the human visual and memory systems. We present three novel high-level segmentation results derived from these considerations, some of which are analogous to those involved in the perception of the structure of music. First and primarily, we derive and demonstrate a method for measuring probable scene boundaries, by calculating a short term memory-based model of shot-to-shot \"coherence\". The detection of local minima in this continuous measure permits robust and flexible segmentation of the video into scenes, without the necessity for first aggregating shots into clusters. Second, and independently of the first, we then derive and demonstrate a one-pass on-the-fly shot clustering algorithm. Third, we demonstrate partially successful results on the application of these two new methods to the next higher, \"theme\", level of video structure."
            },
            "slug": "Video-scene-segmentation-via-continuous-video-Kender-Yeo",
            "title": {
                "fragments": [],
                "text": "Video scene segmentation via continuous video coherence"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work derives and demonstrates a method for measuring probable scene boundaries, by calculating a short term memory-based model of shot-to-shot \"coherence\", and derive and demonstrate a one-pass on-the-fly shot clustering algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1998 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.98CB36231)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800421"
                        ],
                        "name": "M. Yeung",
                        "slug": "M.-Yeung",
                        "structuredName": {
                            "firstName": "Minerva",
                            "lastName": "Yeung",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Yeung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1692339"
                        ],
                        "name": "B. Yeo",
                        "slug": "B.-Yeo",
                        "structuredName": {
                            "firstName": "Boon-Lock",
                            "lastName": "Yeo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Yeo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108661679"
                        ],
                        "name": "Bede Liu",
                        "slug": "Bede-Liu",
                        "structuredName": {
                            "firstName": "Bede",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bede Liu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 68
                            }
                        ],
                        "text": "Hierarchical clustering on a shot connectivity graph is proposed in [9]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 20460958,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8f90bffe764178d51a5abce202c594af0bab7e3f",
            "isKey": false,
            "numCitedBy": 251,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Many video programs have story structures that can be recognized through the clustering of video contents based on low-level visual primitives and the analysis of high-level structures imposed by temporal arrangement of composing elements. In this paper we propose techniques and formulations to match and cluster video shots of similar visual contents, taking into account the visual characteristics and temporal dynamics of video. In addition, we extend theScene Transition Graphrepresentation for the analysis of temporal structures extracted from video. The analyses lead to automatic segmentation of scenes and story units that cannot be achieved with existing shot boundary detection schemes and the building of a compact representation of video contents. Furthermore, the segmentation can be performed on a much reduced data set extracted from compressed video and works well on a wide variety of video programming types. Hence, we are able to decompose video into meaningful hierarchies and compact representations that reflect the flow of the story. This offers a mean for the efficient browsing and organization of video."
            },
            "slug": "Segmentation-of-Video-by-Clustering-and-Graph-Yeung-Yeo",
            "title": {
                "fragments": [],
                "text": "Segmentation of Video by Clustering and Graph Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper proposes techniques and formulations to match and cluster video shots of similar visual contents, taking into account the visual characteristics and temporal dynamics of video, and extends the Scene Transition Graphrepresentation for the analysis of temporal structures extracted from video."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Image Underst."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145420180"
                        ],
                        "name": "Y. Zhai",
                        "slug": "Y.-Zhai",
                        "structuredName": {
                            "firstName": "Yun",
                            "lastName": "Zhai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Zhai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145103012"
                        ],
                        "name": "M. Shah",
                        "slug": "M.-Shah",
                        "structuredName": {
                            "firstName": "Mubarak",
                            "lastName": "Shah",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Shah"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 46
                            }
                        ],
                        "text": "An MCMC based clustering framework is used in [8]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2949684,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "401d382331b3f8b3c6a62989726bdd8d00ec2b0a",
            "isKey": false,
            "numCitedBy": 132,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Videos are composed of many shots that are caused by different camera operations, e.g., on/off operations and switching between cameras. One important goal in video analysis is to group the shots into temporal scenes, such that all the shots in a single scene are related to the same subject, which could be a particular physical setting, an ongoing action or a theme. In this paper, we present a general framework for temporal scene segmentation in various video domains. The proposed method is formulated in a statistical fashion and uses the Markov chain Monte Carlo (MCMC) technique to determine the boundaries between video scenes. In this approach, a set of arbitrary scene boundaries are initialized at random locations and are automatically updated using two types of updates: diffusion and jumps. Diffusion is the process of updating the boundaries between adjacent scenes. Jumps consist of two reversible operations: the merging of two scenes and the splitting of an existing scene. The posterior probability of the target distribution of the number of scenes and their corresponding boundary locations is computed based on the model priors and the data likelihood. The updates of the model parameters are controlled by the hypothesis ratio test in the MCMC process, and the samples are collected to generate the final scene boundaries. The major advantage of the proposed framework is two-fold: 1) it is able to find the weak boundaries as well as the strong boundaries, i.e., it does not rely on the fixed threshold; 2) it can be applied to different video domains. We have tested the proposed method on two video domains: home videos and feature films, and accurate results have been obtained"
            },
            "slug": "Video-scene-segmentation-using-Markov-chain-Monte-Zhai-Shah",
            "title": {
                "fragments": [],
                "text": "Video scene segmentation using Markov chain Monte Carlo"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A general framework for temporal scene segmentation in various video domains that is able to find the weak boundaries as well as the strong boundaries, i.e., it does not rely on the fixed threshold and can be applied to different video domains."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Multimedia"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143991676"
                        ],
                        "name": "I. Laptev",
                        "slug": "I.-Laptev",
                        "structuredName": {
                            "firstName": "Ivan",
                            "lastName": "Laptev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Laptev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3502855"
                        ],
                        "name": "Marcin Marszalek",
                        "slug": "Marcin-Marszalek",
                        "structuredName": {
                            "firstName": "Marcin",
                            "lastName": "Marszalek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcin Marszalek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462253"
                        ],
                        "name": "C. Schmid",
                        "slug": "C.-Schmid",
                        "structuredName": {
                            "firstName": "Cordelia",
                            "lastName": "Schmid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Schmid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3261451"
                        ],
                        "name": "Benjamin Rozenfeld",
                        "slug": "Benjamin-Rozenfeld",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Rozenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benjamin Rozenfeld"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 17
                            }
                        ],
                        "text": "Simultaneous work[3] explores similar goals in a more supervised fashion."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12365014,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0f86767732f76f478d5845f2e59f99ba106e9265",
            "isKey": false,
            "numCitedBy": 3595,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "The aim of this paper is to address recognition of natural human actions in diverse and realistic video settings. This challenging but important subject has mostly been ignored in the past due to several problems one of which is the lack of realistic and annotated video datasets. Our first contribution is to address this limitation and to investigate the use of movie scripts for automatic annotation of human actions in videos. We evaluate alternative methods for action retrieval from scripts and show benefits of a text-based classifier. Using the retrieved action samples for visual learning, we next turn to the problem of action classification in video. We present a new method for video classification that builds upon and extends several recent ideas including local space-time features, space-time pyramids and multi-channel non-linear SVMs. The method is shown to improve state-of-the-art results on the standard KTH action dataset by achieving 91.8% accuracy. Given the inherent problem of noisy labels in automatic annotation, we particularly investigate and show high tolerance of our method to annotation errors in the training set. We finally apply the method to learning and classifying challenging action classes in movies and show promising results."
            },
            "slug": "Learning-realistic-human-actions-from-movies-Laptev-Marszalek",
            "title": {
                "fragments": [],
                "text": "Learning realistic human actions from movies"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A new method for video classification that builds upon and extends several recent ideas including local space-time features,space-time pyramids and multi-channel non-linear SVMs is presented and shown to improve state-of-the-art results on the standard KTH action dataset."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143977389"
                        ],
                        "name": "C. Ngo",
                        "slug": "C.-Ngo",
                        "structuredName": {
                            "firstName": "Chong-Wah",
                            "lastName": "Ngo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Ngo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681882"
                        ],
                        "name": "T. Pong",
                        "slug": "T.-Pong",
                        "structuredName": {
                            "firstName": "Ting-Chuen",
                            "lastName": "Pong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Pong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108698841"
                        ],
                        "name": "HongJiang Zhang",
                        "slug": "HongJiang-Zhang",
                        "structuredName": {
                            "firstName": "HongJiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "HongJiang Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 116
                            }
                        ],
                        "text": "The process of segmenting a video sequence into scenes has received some attention in the video analysis literature [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 144
                            }
                        ],
                        "text": "A popular technique is to compute a set of localized color histograms for each image and use a histogram distance function to detect boundaries [6, 7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16972818,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ca5435081cb0a20dcbbaa507596b9e6c9afe6756",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 101,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present major issues in video parsing, abstraction, retrieval and semantic analysis. We discuss the success, the difficulties and the expectations in these areas. In addition, we identify important opened problems that can lead to more sophisticated ways of video content analysis. For video parsing, we discuss topics in video partitioning, motion characterization and object segmentation. The success in video parsing, in general, will have a great impact on video representation and retrieval. We present three levels of abstracting video content by scene, keyframe and key object representations. These representation schemes in overall serve as a good start for video retrieval. We then describe visual features, in particular motion, and similarity measures adopted for retrieval. Next, we discuss the recent computational approaches in bridging the semantic gap for video content understanding."
            },
            "slug": "Recent-Advances-in-Content-Based-Video-Analysis-Ngo-Pong",
            "title": {
                "fragments": [],
                "text": "Recent Advances in Content-Based Video Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "This paper presents major issues in video parsing, abstraction, retrieval and semantic analysis, and discusses the recent computational approaches in bridging the semantic gap for video content understanding."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Image Graph."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782755"
                        ],
                        "name": "Josef Sivic",
                        "slug": "Josef-Sivic",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Sivic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Josef Sivic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056091"
                        ],
                        "name": "M. Everingham",
                        "slug": "M.-Everingham",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Everingham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Everingham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 50
                            }
                        ],
                        "text": "Such text data has been used for character naming [4, 5] and is widely available, which makes our approach applicable to a large number of movies and TV series."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17185128,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6b4778ce78ad7186db1f08ec548d3984b4e440d7",
            "isKey": false,
            "numCitedBy": 237,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Matching people based on their imaged face is hard because of the well known problems of illumination, pose, size and expression variation. Indeed these variations can exceed those due to identity. Fortunately, videos of people have the happy benefit of containing multiple exemplars of each person in a form that can easily be associated automatically using straightforward visual tracking. We describe progress in harnessing these multiple exemplars in order to retrieve humans automatically in videos, given a query face in a shot. There are three areas of interest: (i) the matching of sets of exemplars provided by \u201ctubes\u201d of the spatial-temporal volume; (ii) the description of the face using a spatial orientation field; and, (iii) the structuring of the problem so that retrieval is immediate at run time. \n \nThe result is a person retrieval system, able to retrieve a ranked list of shots containing a particular person in the manner of Google. The method has been implemented and tested on two feature length movies."
            },
            "slug": "Person-Spotting:-Video-Shot-Retrieval-for-Face-Sets-Sivic-Everingham",
            "title": {
                "fragments": [],
                "text": "Person Spotting: Video Shot Retrieval for Face Sets"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Progress is described in harnessing multiple exemplars of each person in a form that can easily be associated automatically using straightforward visual tracking in order to retrieve humans automatically in videos, given a query face in a shot."
            },
            "venue": {
                "fragments": [],
                "text": "CIVR"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770537"
                        ],
                        "name": "D. Ramanan",
                        "slug": "D.-Ramanan",
                        "structuredName": {
                            "firstName": "Deva",
                            "lastName": "Ramanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ramanan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145347688"
                        ],
                        "name": "S. Baker",
                        "slug": "S.-Baker",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Baker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Baker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144695232"
                        ],
                        "name": "S. Kakade",
                        "slug": "S.-Kakade",
                        "structuredName": {
                            "firstName": "Sham",
                            "lastName": "Kakade",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kakade"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 22
                            }
                        ],
                        "text": "Several recent papers [1, 2] have successfully collected very large-scale, diverse datasets of faces \u201cin the wild\u201d using weakly supervised techniques."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2210504,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "06a1382a0fc63fb173fe570e7b6a84158d4e06a5",
            "isKey": false,
            "numCitedBy": 100,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a semi-supervised method for building large, labeled datasets effaces by leveraging archival video. Specifically, we have implemented a system for labeling 11 years worth of archival footage from a television show. We have compiled a dataset of 611,770 faces, orders of magnitude larger than existing collections. It includes variation in appearance due to age, weight gain, changes in hairstyles, and other factors difficult to observe in smaller-scale collections. Face recognition in an uncontrolled setting can be difficult. We argue (and demonstrate) that there is much structure at varying timescales in the video data that make recognition much easier. At local time scales, one can use motion and tracking to group face images together - we may not know the identity, but we know a single label applies to all faces in a track. At medium time scales (say, within a scene), one can use appearance features such as hair and clothing to group tracks across shot boundaries. However, at longer timescales (say, across episodes), one can no longer use clothing as a cue. This suggests that one needs to carefully encode representations of appearance, depending on the timescale at which one intends to match. We assemble our final dataset by classifying groups of tracks in a nearest-neighbors framework. We use a face library obtained by labeling track clusters in a reference episode. We show that this classification is significantly easier when exploiting the hierarchical structure naturally present in the video sequences. From a data-collection point of view, tracking is vital because it adds non-frontal poses to our face collection. This is important because we know of no other method for collecting images of non-frontal faces \"in the wild\"."
            },
            "slug": "Leveraging-archival-video-for-building-face-Ramanan-Baker",
            "title": {
                "fragments": [],
                "text": "Leveraging archival video for building face datasets"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A semi-supervised method for building large, labeled datasets effaces by leveraging archival video, and shows that this classification is significantly easier when exploiting the hierarchical structure naturally present in the video sequences."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE 11th International Conference on Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056091"
                        ],
                        "name": "M. Everingham",
                        "slug": "M.-Everingham",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Everingham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Everingham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782755"
                        ],
                        "name": "Josef Sivic",
                        "slug": "Josef-Sivic",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Sivic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Josef Sivic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "Following [14], we detect onscreen speakers as follows: 1) locate mouth for each face track using a mouth detector based on Viola-Jones, 2) compute a mouth motion score based on the normalized cross correlation between consecutive windows of the mouth track, averaged over temporal segments corresponding to speech portions of the screenplay."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8300220,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a4aba56927d7841c0aaedf5c73d42ccfadd75124",
            "isKey": false,
            "numCitedBy": 649,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the problem of automatically labelling appearances of characters in TV or film material. This is tremendously challenging due to the huge variation in imaged appearance of each character and the weakness and ambiguity of available annotation. However, we demonstrate that high precision can be achieved by combining multiple sources of information, both visual and textual. The principal novelties that we introduce are: (i) automatic generation of time stamped character annotation by aligning subtitles and transcripts; (ii) strengthening the supervisory information by identifying when characters are speaking; (iii) using complementary cues of face matching and clothing matching to propose common annotations for face tracks. Results are presented on episodes of the TV series \u201cBuffy the Vampire Slayer\u201d."
            },
            "slug": "Hello!-My-name-is...-Buffy''-Automatic-Naming-of-in-Everingham-Sivic",
            "title": {
                "fragments": [],
                "text": "Hello! My name is... Buffy'' -- Automatic Naming of Characters in TV Video"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is demonstrated that high precision can be achieved by combining multiple sources of information, both visual and textual, by automatic generation of time stamped character annotation by aligning subtitles and transcripts."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3219900"
                        ],
                        "name": "Gary B. Huang",
                        "slug": "Gary-B.-Huang",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Huang",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gary B. Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2246870"
                        ],
                        "name": "Vidit Jain",
                        "slug": "Vidit-Jain",
                        "structuredName": {
                            "firstName": "Vidit",
                            "lastName": "Jain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vidit Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1389846455"
                        ],
                        "name": "E. Learned-Miller",
                        "slug": "E.-Learned-Miller",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Learned-Miller",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Learned-Miller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 22
                            }
                        ],
                        "text": "Several recent papers [1, 2] have successfully collected very large-scale, diverse datasets of faces \u201cin the wild\u201d using weakly supervised techniques."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3028943,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "592e329fd243f38c4cd1ff393af1f24f6a10e3b8",
            "isKey": false,
            "numCitedBy": 344,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Many recognition algorithms depend on careful positioning of an object into a canonical pose, so the position of features relative to a fixed coordinate system can be examined. Currently, this positioning is done either manually or by training a class-specialized learning algorithm with samples of the class that have been hand-labeled with parts or poses. In this paper, we describe a novel method to achieve this positioning using poorly aligned examples of a class with no additional labeling. Given a set of unaligned examplars of a class, such as faces, we automatically build an alignment mechanism, without any additional labeling of parts or poses in the data set. Using this alignment mechanism, new members of the class, such as faces resulting from a face detector, can be precisely aligned for the recognition process. Our alignment method improves performance on a face recognition task, both over unaligned images and over images aligned with a face alignment algorithm specifically developed for and trained on hand-labeled face images. We also demonstrate its use on an entirely different class of objects (cars), again without providing any information about parts or pose to the learning algorithm."
            },
            "slug": "Unsupervised-Joint-Alignment-of-Complex-Images-Huang-Jain",
            "title": {
                "fragments": [],
                "text": "Unsupervised Joint Alignment of Complex Images"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The alignment method improves performance on a face recognition task, both over unaligned images and over images aligned with a face alignment algorithm specifically developed for and trained on hand-labeled face images."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE 11th International Conference on Computer Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144739319"
                        ],
                        "name": "R. Lienhart",
                        "slug": "R.-Lienhart",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Lienhart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lienhart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 144
                            }
                        ],
                        "text": "A popular technique is to compute a set of localized color histograms for each image and use a histogram distance function to detect boundaries [6, 7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9835525,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "10c8691e23dffadab19c39fb9245496efbf75b2a",
            "isKey": false,
            "numCitedBy": 332,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "A large number of shot boundary detection, or equivalently, transition detection techniques have been developed in recent years. They all can be classified based on a few core concepts underlying the different detection schemes. This survey emphasizes those different core concepts underlying the different detection schemes for the three most widely used video transition effects: hard cuts, fades and dissolves. Representative of each concept one or a few very sound and thoroughly tested approaches are present in detail, while others are just listed. Whenever reliable performance numbers could be found in the literature, they are mentioned. Guidelines for practitioners in video processing are also given."
            },
            "slug": "Reliable-Transition-Detection-in-Videos:-A-Survey-Lienhart",
            "title": {
                "fragments": [],
                "text": "Reliable Transition Detection in Videos: A Survey and Practitioner's Guide"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This survey emphasizes those different core concepts underlying the different detection schemes for the three most widely used video transition effects: hard cuts, fades and dissolves."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Image Graph."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1787377"
                        ],
                        "name": "W. Leow",
                        "slug": "W.-Leow",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Leow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Leow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731570"
                        ],
                        "name": "M. Lew",
                        "slug": "M.-Lew",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lew",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lew"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144078686"
                        ],
                        "name": "Tat-Seng Chua",
                        "slug": "Tat-Seng-Chua",
                        "structuredName": {
                            "firstName": "Tat-Seng",
                            "lastName": "Chua",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tat-Seng Chua"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712167"
                        ],
                        "name": "Wei-Ying Ma",
                        "slug": "Wei-Ying-Ma",
                        "structuredName": {
                            "firstName": "Wei-Ying",
                            "lastName": "Ma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei-Ying Ma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1986874"
                        ],
                        "name": "L. Chaisorn",
                        "slug": "L.-Chaisorn",
                        "structuredName": {
                            "firstName": "Lekha",
                            "lastName": "Chaisorn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Chaisorn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143866184"
                        ],
                        "name": "E. Bakker",
                        "slug": "E.-Bakker",
                        "structuredName": {
                            "firstName": "Erwin",
                            "lastName": "Bakker",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Bakker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 236700,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9467048bd79722b4367fb91585694dee157d8af6",
            "isKey": false,
            "numCitedBy": 341,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We have witnessed a decade of exploding research interest in multimedia content analysis. The goal of content analysis has been to derive automatic methods for high-level description and annotation. In this paper we will summarize the main research topics in this area and state some assumptions that we have been using all along. We will also postulate the main future trends including usage of long term memory, context, dynamic processing, evolvable generalized detectors and user aspects."
            },
            "slug": "Image-and-Video-Retrieval-Leow-Lew",
            "title": {
                "fragments": [],
                "text": "Image and Video Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The main research topics in multimedia content analysis are summarized, some assumptions are state and the main future trends including usage of long term memory, context, dynamic processing, evolvable generalized detectors and user aspects are postulated."
            },
            "venue": {
                "fragments": [],
                "text": "Lecture Notes in Computer Science"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2801204"
                        ],
                        "name": "N. Heess",
                        "slug": "N.-Heess",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Heess",
                            "middleNames": [
                                "Manfred",
                                "Otto"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Heess"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1135019,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d15cdfe4bb19c7da564d4f9a8c8bae41e4f146f7",
            "isKey": false,
            "numCitedBy": 275,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We evaluate the ability of the popular Field-of-Experts (FoE) to model structure in images. As a test case we focus on modeling synthetic and natural textures. We find that even for modeling single textures, the FoE provides insufficient flexibility to learn good generative models \u2010 it does not perform any better than the much simpler Gaussian FoE. We propose an extended version of the FoE (allowing for bimodal potentials) and demonstrate that this novel formulation, when trained with a better approximation of the likelihood gradient, gives rise to a more powerful generative model of specific visual structure that produces significantly better results for the texture task."
            },
            "slug": "Learning-Generative-Texture-Models-with-extended-Heess-Williams",
            "title": {
                "fragments": [],
                "text": "Learning Generative Texture Models with extended Fields-of-Experts"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This work proposes an extended version of the Field-of-Experts (FoE) (allowing for bimodal potentials) and demonstrates that this novel formulation gives rise to a more powerful generative model of specific visual structure that produces significantly better results for the texture task."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33568342"
                        ],
                        "name": "C. Myers",
                        "slug": "C.-Myers",
                        "structuredName": {
                            "firstName": "Cory",
                            "lastName": "Myers",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Myers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712517"
                        ],
                        "name": "L. Rabiner",
                        "slug": "L.-Rabiner",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Rabiner",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rabiner"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 34
                            }
                        ],
                        "text": "We extend the dynamic time warping[12] approach in a straightforward way to time-stamp each element of the screenplay (as opposed to just the dialogues as in [5])."
                    },
                    "intents": []
                }
            ],
            "corpusId": 12857347,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1432bda965619e532630cc95555c0f8fcb2e3c0c",
            "isKey": false,
            "numCitedBy": 465,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Several different algorithms have been proposed for time registering a test pattern and a concatenated (isolated word) sequence of reference patterns for automatic connected-word recognition. These algorithms include the two-level, dynamic programming algorithm, the sampling approach and the level-building approach. In this paper, we discuss the theoretical differences and similarities among the various algorithms. An experimental comparison of these algorithms for a connected-digit recognition task is also given. The comparison shows that for typical applications, the level-building algorithm performs better than either the two-level DP matching or the sampling algorithm."
            },
            "slug": "A-comparative-study-of-several-dynamic-time-warping-Myers-Rabiner",
            "title": {
                "fragments": [],
                "text": "A comparative study of several dynamic time-warping algorithms for connected-word recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The theoretical differences and similarities among the various algorithms for automatic connected-word recognition are discussed and an experimental comparison shows that for typical applications, the level-building algorithm performs better than either the two-level DP matching or the sampling algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "The Bell System Technical Journal"
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145994314"
                        ],
                        "name": "E. Balas",
                        "slug": "E.-Balas",
                        "structuredName": {
                            "firstName": "Egon",
                            "lastName": "Balas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Balas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32450040"
                        ],
                        "name": "Neil Simonetti",
                        "slug": "Neil-Simonetti",
                        "structuredName": {
                            "firstName": "Neil",
                            "lastName": "Simonetti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Neil Simonetti"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 212
                            }
                        ],
                        "text": "Since there are at most k incoming edges per node (corresponding to valid transitions \u03c0t\u22121 \u2192 \u03c0t), the total complexity of the dynamic program is O(k(k + 1)2k\u22122 \u00b7 n), exponential in k (fixed) but linear in n, see [11] for details."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 107
                            }
                        ],
                        "text": "The solution to the simplified problem without scene breaks (1) under constraint (2) has been addressed in [11] (it dealt with a hamiltonian cycle with \u03c01(1) = 1, but this is easily adaptable to our case)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2480595,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f89734e69c933318ca4a1ffaf316562e56fbe064",
            "isKey": false,
            "numCitedBy": 111,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Consider the following restricted (symmetric or asymmetric) traveling-salesman problem (TSP): given an initial ordering of then cities and an integerk > 0, find a minimum-cost feasible tour, where a feasible tour is one in which cityi precedes cityj wheneverj =i +k in the initial ordering. Balas (1996) has proposed a dynamic-programming algorithm that solves this problem in time linear inn, though exponential ink. Some important real-world problems are amenable to this model or some of its close relatives.The algorithm of Balas (1996) constructs a layered network with a layer of nodes for each position in the tour, such that source-sink paths in this network are in one-to-one correspondence with tours that satisfy the postulated precedence constraints. In this paper we discuss an implementation of the dynamic-programming algorithm for the general case when the integerk is replaced with city-specific integersk( j),j = 1, . . .,n.We discuss applications to, and computational experience with, TSPs with time windows, a model frequently used in vehicle routing as well as in scheduling with setup, release and delivery times. We also introduce a new model, the TSP with target times, applicable to Just-in-Time scheduling problems. Finally for TSPs that have no precedence restrictions, we use the algorithm as a heuristic that finds in linear time a local optimum over an exponential-size neighborhood. For this case, we implement an iterated version of our procedure, based on contracting some arcs of the tour produced by a first application of the algorithm, then reapplying the algorithm to the shrunken graph with the samek."
            },
            "slug": "Linear-Time-Dynamic-Programming-Algorithms-for-New-Balas-Simonetti",
            "title": {
                "fragments": [],
                "text": "Linear Time Dynamic-Programming Algorithms for New Classes of Restricted TSPs: A Computational Study"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "An implementation of the dynamic-programming algorithm for the general case when the integerk is replaced with city-specific integersk( j),j = 1, ."
            },
            "venue": {
                "fragments": [],
                "text": "INFORMS J. Comput."
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A comparative study of several dynamic time-warping algorithms for connected word recognition Robust real-time face detection"
            },
            "venue": {
                "fragments": [],
                "text": "The Bell System Technical Journal. International Journal of Computer Vision"
            },
            "year": 1981
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 7,
            "methodology": 6
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 15,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/Movie/Script:-Alignment-and-Parsing-of-Video-and-Cour-Jordan/c980b058f98dc1904ad328c2341a47c31479d076?sort=total-citations"
}