{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143809344"
                        ],
                        "name": "G. Carpenter",
                        "slug": "G.-Carpenter",
                        "structuredName": {
                            "firstName": "Gail",
                            "lastName": "Carpenter",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Carpenter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 42282775,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "42a884615e99887a9b57d1e6d7e1f1b47196afac",
            "isKey": false,
            "numCitedBy": 226,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neural-network-models-for-pattern-recognition-and-Carpenter",
            "title": {
                "fragments": [],
                "text": "Neural network models for pattern recognition and associative memory"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758714"
                        ],
                        "name": "S. Fahlman",
                        "slug": "S.-Fahlman",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Fahlman",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Fahlman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749342"
                        ],
                        "name": "C. Lebiere",
                        "slug": "C.-Lebiere",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Lebiere",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lebiere"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 30443043,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "995a3b11cc8a4751d8e167abc4aa937abc934df0",
            "isKey": false,
            "numCitedBy": 2937,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Cascade-Correlation is a new architecture and supervised learning algorithm for artificial neural networks. Instead of just adjusting the weights in a network of fixed topology. Cascade-Correlation begins with a minimal network, then automatically trains and adds new hidden units one by one, creating a multi-layer structure. Once a new hidden unit has been added to the network, its input-side weights are frozen. This unit then becomes a permanent feature-detector in the network, available for producing outputs or for creating other, more complex feature detectors. The Cascade-Correlation architecture has several advantages over existing algorithms: it learns very quickly, the network determines its own size and topology, it retains the structures it has built even if the training set changes, and it requires no back-propagation of error signals through the connections of the network."
            },
            "slug": "The-Cascade-Correlation-Learning-Architecture-Fahlman-Lebiere",
            "title": {
                "fragments": [],
                "text": "The Cascade-Correlation Learning Architecture"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The Cascade-Correlation architecture has several advantages over existing algorithms: it learns very quickly, the network determines its own size and topology, it retains the structures it has built even if the training set changes, and it requires no back-propagation of error signals through the connections of the network."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3160228"
                        ],
                        "name": "K. Fukushima",
                        "slug": "K.-Fukushima",
                        "structuredName": {
                            "firstName": "Kunihiko",
                            "lastName": "Fukushima",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fukushima"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6527089,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "33c3e56439b11e2d77d99da667ae86afbf6e1ec3",
            "isKey": false,
            "numCitedBy": 968,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neocognitron:-A-hierarchical-neural-network-capable-Fukushima",
            "title": {
                "fragments": [],
                "text": "Neocognitron: A hierarchical neural network capable of visual pattern recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32612966"
                        ],
                        "name": "S. Gulati",
                        "slug": "S.-Gulati",
                        "structuredName": {
                            "firstName": "Sandeep",
                            "lastName": "Gulati",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gulati"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47822700"
                        ],
                        "name": "J. Barhen",
                        "slug": "J.-Barhen",
                        "structuredName": {
                            "firstName": "Jacob",
                            "lastName": "Barhen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Barhen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052535870"
                        ],
                        "name": "S. Iyengar",
                        "slug": "S.-Iyengar",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Iyengar",
                            "middleNames": [
                                "Sitharama"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Iyengar"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 22169525,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a60469b25c5802276459bd37831c0ee59203da8b",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 145,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neurocomputing-Formalisms-for-Computational-and-Gulati-Barhen",
            "title": {
                "fragments": [],
                "text": "Neurocomputing Formalisms for Computational Learning and Machine Intelligence"
            },
            "venue": {
                "fragments": [],
                "text": "Adv. Comput."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144240829"
                        ],
                        "name": "B. Forrest",
                        "slug": "B.-Forrest",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Forrest",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Forrest"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2453872"
                        ],
                        "name": "D. Roweth",
                        "slug": "D.-Roweth",
                        "structuredName": {
                            "firstName": "Duncan",
                            "lastName": "Roweth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Roweth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145901252"
                        ],
                        "name": "N. Stroud",
                        "slug": "N.-Stroud",
                        "structuredName": {
                            "firstName": "N.",
                            "lastName": "Stroud",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Stroud"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9907875"
                        ],
                        "name": "D. J. Wallace",
                        "slug": "D.-J.-Wallace",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Wallace",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. J. Wallace"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2070348705"
                        ],
                        "name": "G. Wilson",
                        "slug": "G.-Wilson",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Wilson",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wilson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11550072,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "54f486eb62d19b0ff84d4097dd836010cdb3d4dc",
            "isKey": false,
            "numCitedBy": 66,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "The remarkable processing capabilities of the nervous system must derive from the large numbers of neurons participating (roughly 10 10 ), since the time-scales involved are of the order of a millisecond, rather than the nanoseconds of modern computers. The neural network models which attempt to capture this behaviour are inherently parallel. We review the implementation of a range of neural network models on SIMD and MIMD computers. On the ICL Distributed Array Processor (DAP), a 4096-processor SIMD machine, we have studied training algorithms in the context of the Hopfield net, with specific applications including the storage of words and continuous text in contentaddressable memory. The Hopfield and Tank analogue neural net has been used for image restoration with the Geman and Geman algorithm. We compare the performance of this scheme on the DAP and on a Meiko Computing Surface, a reconfigurable MIMD array of transputers. We describe also the strategies which we have used to implement the Durbin and Willshaw elastic net model on the Computing Surface. Received June 1987"
            },
            "slug": "Implementing-Neural-Network-Models-on-Parallel-Forrest-Roweth",
            "title": {
                "fragments": [],
                "text": "Implementing Neural Network Models on Parallel Computers"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work reviews the implementation of a range of neural network models on SIMD and MIMD computers, and describes the strategies which have been used to implement the Durbin and Willshaw elastic net model on the Computing Surface."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. J."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308302"
                        ],
                        "name": "D. Ackley",
                        "slug": "D.-Ackley",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Ackley",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ackley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12174018,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a0d16f0e99f7ce5e6fb70b1a68c685e9ad610657",
            "isKey": false,
            "numCitedBy": 3393,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Learning-Algorithm-for-Boltzmann-Machines-Ackley-Hinton",
            "title": {
                "fragments": [],
                "text": "A Learning Algorithm for Boltzmann Machines"
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152362221"
                        ],
                        "name": "M. Brady",
                        "slug": "M.-Brady",
                        "structuredName": {
                            "firstName": "Martin L.",
                            "lastName": "Brady",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Brady"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70761321"
                        ],
                        "name": "R. Raghavan",
                        "slug": "R.-Raghavan",
                        "structuredName": {
                            "firstName": "Raghu",
                            "lastName": "Raghavan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Raghavan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2949045"
                        ],
                        "name": "J. Slawny",
                        "slug": "J.-Slawny",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Slawny",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Slawny"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2184156,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "761e27617a33e5d624a7a502c95c40b60800593f",
            "isKey": false,
            "numCitedBy": 106,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "It is widely believed that the back-propagation algorithm in neural networks, for tasks such as pattern classification, overcomes the limitations of the perceptron. The authors construct several counterexamples to this belief. They also construct linearly separable examples which have a unique minimum which fails to separate two families of vectors, and a simple example with four two-dimensional vectors in a single-layer network showing local minima with a large basin of attraction. Thus, back-propagation is guaranteed to fail in the first example, and likely to fail in the second example. It is shown that even multilayered (hidden-layer) networks can also fail in this way to classify linearly separable problems. Since the authors' examples are all linearly separable, the perceptron would correctly classify them. The results disprove the presumption, made in recent years, that, barring local minima, back-propagation will find the best set of weights for a given problem. >"
            },
            "slug": "Back-propagation-fails-to-separate-where-succeed-Brady-Raghavan",
            "title": {
                "fragments": [],
                "text": "Back propagation fails to separate where perceptrons succeed"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "The authors construct linearly separable examples which have a unique minimum which fails to separate two families of vectors, and a simple example with four two-dimensional vectors in a single-layer network showing local minima with a large basin of attraction."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40396597"
                        ],
                        "name": "Toshiyuki Hanazawa",
                        "slug": "Toshiyuki-Hanazawa",
                        "structuredName": {
                            "firstName": "Toshiyuki",
                            "lastName": "Hanazawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Toshiyuki Hanazawa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9243990"
                        ],
                        "name": "K. Shikano",
                        "slug": "K.-Shikano",
                        "structuredName": {
                            "firstName": "Kiyohiro",
                            "lastName": "Shikano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Shikano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49464494"
                        ],
                        "name": "Kevin J. Lang",
                        "slug": "Kevin-J.-Lang",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Lang",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin J. Lang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9563026,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cd62c9976534a6a2096a38244f6cbb03635a127e",
            "isKey": false,
            "numCitedBy": 2786,
            "numCiting": 92,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors present a time-delay neural network (TDNN) approach to phoneme recognition which is characterized by two important properties: (1) using a three-layer arrangement of simple computing units, a hierarchy can be constructed that allows for the formation of arbitrary nonlinear decision surfaces, which the TDNN learns automatically using error backpropagation; and (2) the time-delay arrangement enables the network to discover acoustic-phonetic features and the temporal relationships between them independently of position in time and therefore not blurred by temporal shifts in the input. As a recognition task, the speaker-dependent recognition of the phonemes B, D, and G in varying phonetic contexts was chosen. For comparison, several discrete hidden Markov models (HMM) were trained to perform the same task. Performance evaluation over 1946 testing tokens from three speakers showed that the TDNN achieves a recognition rate of 98.5% correct while the rate obtained by the best of the HMMs was only 93.7%. >"
            },
            "slug": "Phoneme-recognition-using-time-delay-neural-Waibel-Hanazawa",
            "title": {
                "fragments": [],
                "text": "Phoneme recognition using time-delay neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The authors present a time-delay neural network (TDNN) approach to phoneme recognition which is characterized by two important properties: using a three-layer arrangement of simple computing units, a hierarchy can be constructed that allows for the formation of arbitrary nonlinear decision surfaces, which the TDNN learns automatically using error backpropagation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Acoust. Speech Signal Process."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516142"
                        ],
                        "name": "D. Tank",
                        "slug": "D.-Tank",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Tank",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Tank"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3219867"
                        ],
                        "name": "J. Hopfield",
                        "slug": "J.-Hopfield",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hopfield",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hopfield"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16810163,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8879e718ab012e530608a72cddab3ac0edc30743",
            "isKey": false,
            "numCitedBy": 2070,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe how several optimization problems can be rapidly solved by highly interconnected networks of simple analog processors. Analog-to-digital (A/D) conversion was considered as a simple optimization problem, and an A/D converter of novel architecture was designed. A/D conversion is a simple example of a more general class of signal-decision problems which we show could also be solved by appropriately constructed networks. Circuits to solve these problems were designed using general principles which result from an understanding of the basic collective computational properties of a specific class of analog-processor networks. We also show that a network which solves linear programming problems can be understood from the same concepts."
            },
            "slug": "Simple-'neural'-optimization-networks:-An-A/D-and-a-Tank-Hopfield",
            "title": {
                "fragments": [],
                "text": "Simple 'neural' optimization networks: An A/D converter, signal decision circuit, and a linear programming circuit"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "It is described how several optimization problems can be rapidly solved by highly interconnected networks of simple analog processors, and it is shown that a network which solves linear programming Problems can be understood from the same concepts."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48369537"
                        ],
                        "name": "A. Rajavelu",
                        "slug": "A.-Rajavelu",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Rajavelu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Rajavelu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1803565"
                        ],
                        "name": "M. Musavi",
                        "slug": "M.-Musavi",
                        "structuredName": {
                            "firstName": "Mohamad",
                            "lastName": "Musavi",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Musavi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2931424"
                        ],
                        "name": "M. Shirvaikar",
                        "slug": "M.-Shirvaikar",
                        "structuredName": {
                            "firstName": "Mukul",
                            "lastName": "Shirvaikar",
                            "middleNames": [
                                "Vassant"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Shirvaikar"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 31805961,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b274c528653491c7091a0dec20fe6202b01145d9",
            "isKey": false,
            "numCitedBy": 105,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-neural-network-approach-to-character-recognition-Rajavelu-Musavi",
            "title": {
                "fragments": [],
                "text": "A neural network approach to character recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40396597"
                        ],
                        "name": "Toshiyuki Hanazawa",
                        "slug": "Toshiyuki-Hanazawa",
                        "structuredName": {
                            "firstName": "Toshiyuki",
                            "lastName": "Hanazawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Toshiyuki Hanazawa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9243990"
                        ],
                        "name": "K. Shikano",
                        "slug": "K.-Shikano",
                        "structuredName": {
                            "firstName": "Kiyohiro",
                            "lastName": "Shikano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Shikano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49464494"
                        ],
                        "name": "Kevin J. Lang",
                        "slug": "Kevin-J.-Lang",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Lang",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin J. Lang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12251177,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "834b3738673dacc767563c2714239852a8a6d4b4",
            "isKey": false,
            "numCitedBy": 172,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "A time-delay neural network (TDNN) for phoneme recognition is discussed. By the use of two hidden layers in addition to an input and output layer it is capable of representing complex nonlinear decision surfaces. Three important properties of the TDNNs have been observed. First, it was able to invent without human interference meaningful linguistic abstractions in time and frequency such as formant tracking and segmentation. Second, it has learned to form alternate representations linking different acoustic events with the same higher level concept. In this fashion it can implement trading relations between lower level acoustic events leading to robust recognition performance despite considerable variability in the input speech. Third, the network is translation-invariant and does not rely on precise alignment or segmentation of the input. The TDNNs performance is compared with the best of hidden Markov models (HMMs) on a speaker-dependent phoneme-recognition task. The TDNN achieved a recognition of 98.5% compared to 93.7% for the HMM, i.e., a fourfold reduction in error.<<ETX>>"
            },
            "slug": "Phoneme-recognition:-neural-networks-vs.-hidden-vs.-Waibel-Hanazawa",
            "title": {
                "fragments": [],
                "text": "Phoneme recognition: neural networks vs. hidden Markov models vs. hidden Markov models"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "A time-delay neural network for phoneme recognition that was able to invent without human interference meaningful linguistic abstractions in time and frequency such as formant tracking and segmentation and does not rely on precise alignment or segmentation of the input."
            },
            "venue": {
                "fragments": [],
                "text": "ICASSP-88., International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48085756"
                        ],
                        "name": "E. Gardner",
                        "slug": "E.-Gardner",
                        "structuredName": {
                            "firstName": "E.",
                            "lastName": "Gardner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Gardner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6362891"
                        ],
                        "name": "B. Derrida",
                        "slug": "B.-Derrida",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "Derrida",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Derrida"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 120825406,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "24053f79a7235907d93f6cdfb4360055b11b8d4c",
            "isKey": false,
            "numCitedBy": 403,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors calculate the number, p= alpha N of random N-bit patterns that an optimal neural network can store allowing a given fraction f of bit errors and with the condition that each right bit is stabilised by a local field at least equal to a parameter K. For each value of alpha and K, there is a minimum fraction fmin of wrong bits. They find a critical line, alpha c(K) with alpha c(0)=2. The minimum fraction of wrong bits vanishes for alpha alpha c(K). The calculations are done using a saddle-point method and the order parameters at the saddle point are assumed to be replica symmetric. This solution is locally stable in a finite region of the K, alpha plane including the line, alpha c(K) but there is a line above which the solution becomes unstable and replica symmetry must be broken."
            },
            "slug": "Optimal-storage-properties-of-neural-network-models-Gardner-Derrida",
            "title": {
                "fragments": [],
                "text": "Optimal storage properties of neural network models"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The authors calculate the number, p= alpha N of random N-bit patterns that an optimal neural network can store allowing a given fraction f of bit errors and with the condition that each right bit is stabilised by a local field at least equal to a parameter K."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682174"
                        ],
                        "name": "S. Grossberg",
                        "slug": "S.-Grossberg",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Grossberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Grossberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1549152,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8507e665e304f35981e73752563453146d7185cb",
            "isKey": false,
            "numCitedBy": 154,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "(1) Introduction,-This note describes some nonlinear net\\vorks ,vhich caD learn a spatial pattern, in \"black and white,\" of arbitrary size and complexity. These networks are a special case of a collection of learning machines ~ which were introduced in reference 1, where a machine capable of learxung a list of \"letters\" or \"events\" was described, We list in heuristic terminology some of the properties which arise in the learning of patterns: (a) \"Practice makes perfect\": Given a \"black and white\" pattern of arbitral'y size and complexity, a nonlinear network fit can be found \\vhich learns this pattern to any prescribed degree of accuracy, (b) An isolated machine never forgets: If the pattern is leal'ned to a fixed degree of accuracy by 3Tl:, then fit will remember the pattern to at least this degree of accuracy until a new pattern is imposed upon fit. (c) Overt practice is unnecessary: fit remembers the pattern without practicing it overtly, (d) Contour enhancement: If fit learns the pattern to a \"moderate\" degree of accuracy, then fit's memory of the pa~tern spontaneously improves after practices ceases, As a result, when fit recalls the pattern, its contours are enhanced in the sense that \"darks get darker\" and \"lights get lighter.\" (e) A new pattern can always be learned: Even if 3Tl: kno\\vs one pattern to an arbitrary degree of accuracy, this pattern can be replaced by any other pattern by a sufficient amount of practice. (2) The Machine,-The nonlinear network which describes fit is defined as follows for any fixed number n ?:. 1 of states and any reaction time l' ?:. O."
            },
            "slug": "Some-nonlinear-networks-capable-of-learning-a-of-Grossberg",
            "title": {
                "fragments": [],
                "text": "Some nonlinear networks capable of learning a spatial pattern of arbitrary complexity."
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "This note describes some nonlinear net networks, which learn a spatial pattern, in \"black and white,\" of arbitrary size and complexity, and lists in heuristic terminology some of the properties which arise in the learning of patterns."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 1968
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "104372862"
                        ],
                        "name": "Kanter",
                        "slug": "Kanter",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Kanter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kanter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30053069"
                        ],
                        "name": "Sompolinsky",
                        "slug": "Sompolinsky",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Sompolinsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sompolinsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15471409,
            "fieldsOfStudy": [
                "Materials Science"
            ],
            "id": "f3e2cfad939b7e4f0ea7783883afa997e48920c1",
            "isKey": false,
            "numCitedBy": 317,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "A neural network which is capable of recalling without errors any set of linearly independent patterns is studied. The network is based on a Hamiltonian version of the model of Personnaz et al. The energy of a state of N (\\ifmmode\\pm\\else\\textpm\\fi{}1) neurons is the square of the Euclidean distance\\char22{}in phase space\\char22{}between the state and the linear subspace spanned by the patterns. This energy corresponds to nonlocal updatings of the synapses in the learning mode. Results of the mean-field theory (MFT) of the system as well as computer simulations are presented. The stable and metastable states of the network are studied as a function of ``temperature'' T and \\ensuremath{\\alpha}=p/N, where p is the number of embedded patterns. The maximum capacity of the network is \\ensuremath{\\alpha}=1. For all \\ensuremath{\\alpha} (0\\ensuremath{\\le}\\ensuremath{\\alpha}l1) the embedded patterns are not only locally stable but are global minima of the energy. The patterns appear, as metastable states, below a temperature T=${T}_{M}$(\\ensuremath{\\alpha}). The temperature ${T}_{M}$(\\ensuremath{\\alpha}) decreases to zero as \\ensuremath{\\alpha}\\ensuremath{\\rightarrow}1. The spurious states of the network are studied in detail in the case of random uncorrelated patterns. At finite p, they are identical to the mixture states of Hopfield's model. At finite \\ensuremath{\\alpha}, a spin-glass phase exists as a metastable state. According to the replica symmetric MFT the spin-glass state becomes degenerate with the patterns at \\ensuremath{\\alpha}=${\\ensuremath{\\alpha}}_{g}$=1-2/\\ensuremath{\\pi} and disappears above it. Possible interpretations of this unusual result are discussed. The average radius of attraction R of the patterns has been determined by computer simulations, for sizes up to N=400. The value of R for 0l\\ensuremath{\\alpha}l1 depends on the details of the dynamics. Results for both parallel and serial dynamics are presented. In both cases R is unity (the largest distance in phase space by definition) at \\ensuremath{\\alpha}\\ensuremath{\\rightarrow}0 and decreases monotonically to zero as \\ensuremath{\\alpha}\\ensuremath{\\rightarrow}1. Contrary to the MFT, simulations have not revealed, so far, any singularity in the properties of the spurious states at an intermediate value of \\ensuremath{\\alpha}."
            },
            "slug": "Associative-recall-of-memory-without-errors.-Kanter-Sompolinsky",
            "title": {
                "fragments": [],
                "text": "Associative recall of memory without errors."
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A neural network which is capable of recalling without errors any set of linearly independent patterns is studied, and the spurious states of the network are studied in detail in the case of random uncorrelated patterns."
            },
            "venue": {
                "fragments": [],
                "text": "Physical review. A, General physics"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3219867"
                        ],
                        "name": "J. Hopfield",
                        "slug": "J.-Hopfield",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hopfield",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hopfield"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2537503,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "24b9eebe49cf7e00cf50cf7b7d9243386a23fe7c",
            "isKey": false,
            "numCitedBy": 6252,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "A model for a large network of \"neurons\" with a graded response (or sigmoid input-output relation) is studied. This deterministic system has collective properties in very close correspondence with the earlier stochastic model based on McCulloch - Pitts neurons. The content- addressable memory and other emergent collective properties of the original model also are present in the graded response model. The idea that such collective properties are used in biological systems is given added credence by the continued presence of such properties for more nearly biological \"neurons.\" Collective analog electrical circuits of the kind described will certainly function. The collective states of the two models have a simple correspondence. The original model will continue to be useful for simulations, because its connection to graded response systems is established. Equations that include the effect of action potentials in the graded response system are also developed."
            },
            "slug": "Neurons-with-graded-response-have-collective-like-Hopfield",
            "title": {
                "fragments": [],
                "text": "Neurons with graded response have collective computational properties like those of two-state neurons."
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "A model for a large network of \"neurons\" with a graded response (or sigmoid input-output relation) is studied and collective properties in very close correspondence with the earlier stochastic model based on McCulloch - Pitts neurons are studied."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3219867"
                        ],
                        "name": "J. Hopfield",
                        "slug": "J.-Hopfield",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hopfield",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hopfield"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 784288,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "98b4d4e24aab57ab4e1124ff8106909050645cfa",
            "isKey": false,
            "numCitedBy": 14027,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices."
            },
            "slug": "Neural-networks-and-physical-systems-with-emergent-Hopfield",
            "title": {
                "fragments": [],
                "text": "Neural networks and physical systems with emergent collective computational abilities."
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A model of a system having a large number of simple equivalent components, based on aspects of neurobiology but readily adapted to integrated circuits, produces a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences of the United States of America"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "98040793"
                        ],
                        "name": "Sweet",
                        "slug": "Sweet",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Sweet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sweet"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 203663459,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b05b0a800113f4d8f17c02bf671f165b86f70ff",
            "isKey": false,
            "numCitedBy": 80,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural-like networks which minimize a global energy function have been proposed for solving computationally intensive optimization problems. These networks have several parameters that need to be selected and often carefully tuned for a network to produce a sensible computation. The authors examine the traveling salesperson problem (TSP) as a representative NP-complete optimization problem and present a cookbook approach to setting these parameters. There appears to be a linear relationship between two of the parameters. This relationship and the problem size lead to a simple understanding of why these networks are less and less useful for the TSP computation as the number of cities increases. >"
            },
            "slug": "Determination-of-parameters-in-a-Hopfield/Tank-Sweet",
            "title": {
                "fragments": [],
                "text": "Determination of parameters in a Hopfield/Tank computational network"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The authors examine the traveling salesperson problem (TSP) as a representative NP-complete optimization problem and presents a cookbook approach to setting parameters, finding a linear relationship between two of the parameters."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2831441,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "60944c5243db70a687a320a2622d3bd1610802a8",
            "isKey": false,
            "numCitedBy": 1448,
            "numCiting": 90,
            "paperAbstract": {
                "fragments": [],
                "text": "Many adaptive neural network theories are based on neuronlike adaptive elements that can behave as single unit analogs of associative conditioning. In this article we develop a similar adaptive element, but one which is more closely in accord with the facts of animal learning theory than elements commonly studied in adaptive network research. We suggest that an essential feature of classical conditioning that has been largely overlooked by adaptive network theorists is its predictive nature. The adaptive element we present learns to increase its response rate in anticipation of increased stimulation, producing a conditioned response before the occurrence of the unconditioned stimulus. The element also is in strong agreement with the behavioral data regarding the effects of stimulus context, since it is a temporally refined extension of the Rescorla-Wagner model. We show by computer simulation that the element becomes sensitive to the most reliable, nonredundant, and earliest predictors of reinforcement . We also point out that the model solves many of the stability and saturation problems encountered in network simulations. Finally, we discuss our model in light of recent advances in the physiology and biochemistry of synaptic mechanisms."
            },
            "slug": "Toward-a-modern-theory-of-adaptive-networks:-and-Sutton-Barto",
            "title": {
                "fragments": [],
                "text": "Toward a modern theory of adaptive networks: expectation and prediction."
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The adaptive element presented learns to increase its response rate in anticipation of increased stimulation, producing a conditioned response before the occurrence of the unconditioned stimulus, and is in strong agreement with the behavioral data regarding the effects of stimulus context."
            },
            "venue": {
                "fragments": [],
                "text": "Psychological review"
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16685841,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d4a7e54446d52f066ee692fa38d9aa972519c2f5",
            "isKey": false,
            "numCitedBy": 183,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "It is shown that neural networks for speech recognition can be constructed in a modular fashion by exploiting the hidden structure of previously trained phonetic subcategory networks. The performance of resulting larger phonetic nets was found to be as good as the performance of the subcomponent nets by themselves. This approach avoids the excessive learning times that would be necessary to train larger networks and allows for incremental learning. Large time-delay neural networks constructed incrementally by applying these modular training techniques achieved a recognition performance of 96.0% for all consonants and 94.7% for all phonemes.<<ETX>>"
            },
            "slug": "Consonant-recognition-by-modular-construction-of-Waibel",
            "title": {
                "fragments": [],
                "text": "Consonant recognition by modular construction of large phonemic time-delay neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "It is shown that neural networks for speech recognition can be constructed in a modular fashion by exploiting the hidden structure of previously trained phonetic subcategory networks to avoid the excessive learning times that would be necessary to train larger networks and allow for incremental learning."
            },
            "venue": {
                "fragments": [],
                "text": "International Conference on Acoustics, Speech, and Signal Processing,"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2129769"
                        ],
                        "name": "G. Tagliarini",
                        "slug": "G.-Tagliarini",
                        "structuredName": {
                            "firstName": "Gene",
                            "lastName": "Tagliarini",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Tagliarini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32468306"
                        ],
                        "name": "E. Page",
                        "slug": "E.-Page",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Page",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Page"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9977018,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9054bd4535fa45b0312399a27984751a3f928463",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Networks of simple analog processors having neuron-like properties have been employed to compute good solutions to a variety of optimization problems. This paper presents a neural-net solution to a resource allocation problem that arises in providing local access to the backbone of a wide-area communication network. The problem is described in terms of an energy function that can be mapped onto an analog computational network. Simulation results characterizing the performance of the neural computation are also presented."
            },
            "slug": "A-Neural-Network-Solution-to-the-Concentrator-Tagliarini-Page",
            "title": {
                "fragments": [],
                "text": "A Neural-Network Solution to the Concentrator Assignment Problem"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A neural-net solution to a resource allocation problem that arises in providing local access to the backbone of a wide-area communication network is presented in terms of an energy function that can be mapped onto an analog computational network."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713858"
                        ],
                        "name": "B. Kosko",
                        "slug": "B.-Kosko",
                        "structuredName": {
                            "firstName": "Bart",
                            "lastName": "Kosko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Kosko"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 59875735,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "62fe57e40232add7ede5a683b31e643fc0712c9f",
            "isKey": false,
            "numCitedBy": 1893,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Stability and encoding properties of two-layer nonlinear feedback neural networks are examined. Bidirectionality is introduced in neural nets to produce two-way associative search for stored associations. The bidirectional associative memory (BAM) is the minimal two-layer nonlinear feedback network. The author proves that every n-by-p matrix M is a bidirectionally stable heteroassociative content-addressable memory for both binary/bipolar and continuous neurons. When the BAM neutrons are activated, the network quickly evolves to a stable state of two-pattern reverberation, or resonance. The stable reverberation corresponds to a system energy local minimum. Heteroassociative information is encoded in a BAM by summing correlation matrices. The BAM storage capacity for reliable recall is roughly m<min (n,p). It is also shown that it is better on average to use bipolar (-1,1) coding than binary"
            },
            "slug": "Bidirectional-associative-memories-Kosko",
            "title": {
                "fragments": [],
                "text": "Bidirectional associative memories"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The author proves that every n-by-p matrix M is a bidirectionally stable heteroassociative content-addressable memory for both binary/bipolar and continuous neurons."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Syst. Man Cybern."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3219867"
                        ],
                        "name": "J. Hopfield",
                        "slug": "J.-Hopfield",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hopfield",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hopfield"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 56990107,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f08533a5a731eb92983980c81538a649c1fcb73",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "A collective decision network is described which can function as a computational element in a digital computer or signal processor. It differs from conventional digital circuit designs in emphasizing the large connectivity and analog response that biological \u201ccomputational\u201d systems employ. When such circuits have symmetric connections, they display a dynamics which is complex enough to use for computation, but simple enough to yield some general theorems about the dynamics of the circuits. Associative memory and error correcting codes are both computations which fit naturally onto the network dynamics. A large class of computational problems describable as optimizations can be mapped onto such networks. While the networks can be used to make binary decisions, during the decision process they transit the interior of a logical space of which only the boundaries have defined logical meaning."
            },
            "slug": "Collective-computation,-content-addressable-memory,-Hopfield",
            "title": {
                "fragments": [],
                "text": "Collective computation, content-addressable memory, and optimization problems"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "A collective decision network is described which can function as a computational element in a digital computer or signal processor and differs from conventional digital circuit designs in emphasizing the large connectivity and analog response that biological \u201ccomputational\u201d systems employ."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 3708480,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "419c4635a6bcae1302f03681498de4ce16074e21",
            "isKey": false,
            "numCitedBy": 379,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Various information-processing capabilities of self-organizing nets of threshold elements are studied. A self-organizing net, learning from patterns or pattern sequences given from outside as stimuli, \"remembers\" some of them as stable equilibrium states or state-transition sequences of the net. A condition where many patterns and pattern sequences are remembered in a net at the same time is shown. The stability degree of their remembrance and recalling under noise disturbances is investigated theoretically. For this purpose, the stability of state transition in an autonomous logical net of threshold elements is studied by the use of characteristics of threshold elements."
            },
            "slug": "Learning-Patterns-and-Pattern-Sequences-by-Nets-of-Amari",
            "title": {
                "fragments": [],
                "text": "Learning Patterns and Pattern Sequences by Self-Organizing Nets of Threshold Elements"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "The stability of state transition in an autonomous logical net of threshold elements is studied by the use of characteristics of thresholds elements."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Computers"
            },
            "year": 1972
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144240829"
                        ],
                        "name": "B. Forrest",
                        "slug": "B.-Forrest",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Forrest",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Forrest"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122431608,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8cdf0289095f61eac1271048513a799e4e49d94c",
            "isKey": false,
            "numCitedBy": 100,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The content-addressability of patterns stored in Ising-spin neural network models with symmetric interactions is studied. Numerical results from simulations on the ICL distributed array processor (DAP) involving systems with up to 2048 neurons are presented. Behaviour consistent with finite-size scaling, characteristic of a first-order phase transition, is shown to be exhibited by the basins of attraction of the stored patterns both in the case of the Hopfield model and for systems using a local iterative learning algorithm designed to optimise the basins of attraction. Estimates are obtained for the critical minimum overlaps which an input pattern must have with a stored pattern in order to successfully retrieve it."
            },
            "slug": "Content-addressability-and-learning-in-neural-Forrest",
            "title": {
                "fragments": [],
                "text": "Content-addressability and learning in neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Behaviour consistent with finite-size scaling, characteristic of a first-order phase transition, is shown to be exhibited by the basins of attraction of the stored patterns both in the case of the Hopfield model and for systems using a local iterative learning algorithm designed to optimise the basin of attraction."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15727980,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "de13c62828f41c863cffb7444daaa37d928272d2",
            "isKey": false,
            "numCitedBy": 508,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "SummaryA time-dependent, nonlinear model of neuronal interaction which was probabilistically analyzed in a previous article is shown here to be a natural generalization of the Hartline-Ratliff model of the Limulus retina. Although the primary physical variables in the model are the membrane potentials of neurons, the equations which govern the means and covariances of the membrane potentials are coupled through the average firing rates; as a consequence, the average firing rates control the selective storage and retrieval of covariance information. Motor learning in the cerebellar cortex is treated as a problem of covariance storage, and a prediction is made for the underlying synaptic plasticity: the change in synaptic strength between a parallel fiber and a Purkinje cell should be proportional to the covariance between discharges in the parallel fiber and the climbing fiber. Unlike previous proposals for synaptic plasticity, this prediction requires both facilitation and depression to occur (under different conditions) at the same synapse."
            },
            "slug": "Storing-covariance-with-nonlinearly-interacting-Sejnowski",
            "title": {
                "fragments": [],
                "text": "Storing covariance with nonlinearly interacting neurons"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "A time-dependent, nonlinear model of neuronal interaction which was probabilistically analyzed in a previous article is shown here to be a natural generalization of the Hartline-Ratliff model of the Limulus retina."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of mathematical biology"
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3078169"
                        ],
                        "name": "L. Personnaz",
                        "slug": "L.-Personnaz",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Personnaz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Personnaz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144097910"
                        ],
                        "name": "G. Dreyfus",
                        "slug": "G.-Dreyfus",
                        "structuredName": {
                            "firstName": "G\u00e9rard",
                            "lastName": "Dreyfus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Dreyfus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 34786858,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7da0fa72e851b1bd42c297176cfd998c9930f3fb",
            "isKey": false,
            "numCitedBy": 265,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The link between the structure of a neural network and its attractor states is investigated, with a view to designing associative memories based on such networks. It is shown that, for any preassigned set of states to be memorized, the parameters of the network can be completely calculated in most cases so as to guarantee the stability of these states. The spin glass formulation of the neural network problem leads to particularly simple results which, in some cases, allow an analytical evaluation of the attractivity of the memorized states Dans la perspective de la realisation de memoires associatives a l'aide de reseaux de neurones, nous etudions la relation entre la structure d'un reseau et ses etats attracteurs; nous montrons que, quel que soit l'ensemble des etats que l'on desire memoriser, il est generalement possible de calculer tous les parametres du reseau de facon a assurer la stabilite de ces etats. Le formalisme des verres de spins conduit a des resultats particulierement simples qui permettent, dans certains cas, d'evaluer analytiquement leur attractivite"
            },
            "slug": "Information-storage-and-retrieval-in-spin-glass-Personnaz-Guyon",
            "title": {
                "fragments": [],
                "text": "Information storage and retrieval in spin-glass like neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that, for any preassigned set of states to be memorized, the parameters of the network can be completely calculated in most cases so as to guarantee the stability of these states."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36581341"
                        ],
                        "name": "Kiyonori Yoshida",
                        "slug": "Kiyonori-Yoshida",
                        "structuredName": {
                            "firstName": "Kiyonori",
                            "lastName": "Yoshida",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kiyonori Yoshida"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682208"
                        ],
                        "name": "K. Kanatani",
                        "slug": "K.-Kanatani",
                        "structuredName": {
                            "firstName": "Kenichi",
                            "lastName": "Kanatani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kanatani"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 119737600,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "da19618f3ed4ab4dce3df0166c9c937f5c7e606a",
            "isKey": false,
            "numCitedBy": 94,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The brain is a large-scale system composed of an enormous number of neurons. In order to understand its functioning, we need to know the macroscopic behavior of a nerve net as a whole. Statistical neurodynamics treats an ensemble of nets of randomly connected neurons and derives macroscopic equations from the microscopic state transition laws of the nets. There arises, however, a theoretical difficulty in deriving the macroscopic state equations, because of possible correlations among the microscopic states. The situation is similar to that encountered in deriving the Boltzmann equation in statistical mechanics of gases.We first elucidate the stochastic structures of random nerve nets. We then derive macroscopic state equations which apply to a wide range of ensembles of random nets. These equations are shown to hold in a weak sense: we prove that the probability that these equations are valid within an arbitrarily small error and for an arbitrarily long time converges to 1 as the number n of the componen..."
            },
            "slug": "A-Mathematical-Foundation-for-Statistical-Amari-Yoshida",
            "title": {
                "fragments": [],
                "text": "A Mathematical Foundation for Statistical Neurodynamics"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work first elucidate the stochastic structures of random nerve nets and derives macroscopic state equations which apply to a wide range of ensembles of random nets which are shown to hold in a weak sense."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712517"
                        ],
                        "name": "L. Rabiner",
                        "slug": "L.-Rabiner",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Rabiner",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rabiner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145778742"
                        ],
                        "name": "B. Juang",
                        "slug": "B.-Juang",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Juang",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Juang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11358505,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "9d076613d7c36dbda4a6ff42fbdd076604b96630",
            "isKey": false,
            "numCitedBy": 2944,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The basic theory of Markov chains has been known to mathematicians and engineers for close to 80 years, but it is only in the past decade that it has been applied explicitly to problems in speech processing. One of the major reasons why speech models, based on Markov chains, have not been developed until recently was the lack of a method for optimizing the parameters of the Markov model to match observed signal patterns. Such a method was proposed in the late 1960's and was immediately applied to speech processing in several research institutions. Continued refinements in the theory and implementation of Markov modelling techniques have greatly enhanced the method, leading to a wide range of applications of these models. It is the purpose of this tutorial paper to give an introduction to the theory of Markov models, and to illustrate how they have been applied to problems in speech recognition."
            },
            "slug": "An-introduction-to-hidden-Markov-models-Rabiner-Juang",
            "title": {
                "fragments": [],
                "text": "An introduction to hidden Markov models"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The purpose of this tutorial paper is to give an introduction to the theory of Markov models, and to illustrate how they have been applied to problems in speech recognition."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE ASSP Magazine"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790264"
                        ],
                        "name": "Eduardo Sontag",
                        "slug": "Eduardo-Sontag",
                        "structuredName": {
                            "firstName": "Eduardo",
                            "lastName": "Sontag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eduardo Sontag"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1901890"
                        ],
                        "name": "H. Sussmann",
                        "slug": "H.-Sussmann",
                        "structuredName": {
                            "firstName": "H\u00e9ctor",
                            "lastName": "Sussmann",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Sussmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 69947,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1943969ee23bf0e7bfe079d76cdd08bcd006d5c4",
            "isKey": false,
            "numCitedBy": 103,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "We give an example of a neural net without hidden layers and with a sigmoid transfer function, together with a training set of binary vectors, for which the sum of the squared errors, regarded as a function of the weights, has a local minimum which is not a global minimum. The example consists of a set of 125 training instances, with four weights and a threshold to be learnt. We do not know if substantially smaller binary examples exist."
            },
            "slug": "Backpropagation-Can-Give-Rise-to-Spurious-Local-for-Sontag-Sussmann",
            "title": {
                "fragments": [],
                "text": "Backpropagation Can Give Rise to Spurious Local Minima Even for Networks without Hidden Layers"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "An example of a neural net without hidden layers and with a sigmoid transfer function, together with a training set of binary vectors, for which the sum of the squared errors, regarded as a function of the weights, has a local minimum which is not a global minimum."
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1399079349"
                        ],
                        "name": "B. Cernuschi-Fr\u00edas",
                        "slug": "B.-Cernuschi-Fr\u00edas",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Cernuschi-Fr\u00edas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Cernuschi-Fr\u00edas"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5255357,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5eba478a6f279bc039b3c00a547c84abf5494949",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "A simple generalization of the Hopfield memory is presented. The model proposed updates simultaneously groups of a fixed number of neurons that are disjoint in the sense that each neuron belongs to one and only one group. An analysis is presented of the case in which one of the groups is chosen at random with equal probability and then is updated according to a rule equivalent to the one given by J.J. Hopfield (1982). It is shown that the rule minimizes an energy function in the same way as the original Hopfield model. Sufficient conditions on the corresponding connection matrix as to ensure stability are given. >"
            },
            "slug": "Partial-simultaneous-updating-in-Hopfield-memories-Cernuschi-Fr\u00edas",
            "title": {
                "fragments": [],
                "text": "Partial simultaneous updating in Hopfield memories"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A simple generalization of the Hopfield memory is presented and it is shown that the rule minimizes an energy function in the same way as the original Hopfield model."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Syst. Man Cybern."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681982"
                        ],
                        "name": "H. Oh",
                        "slug": "H.-Oh",
                        "structuredName": {
                            "firstName": "Heekuck",
                            "lastName": "Oh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Oh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145833095"
                        ],
                        "name": "S. Kothari",
                        "slug": "S.-Kothari",
                        "structuredName": {
                            "firstName": "Suresh",
                            "lastName": "Kothari",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kothari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61247011,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "da96928603e0daac3122c3e5f61a11e5e386c220",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "A new learning technique is introduced to solve the problem of the small and restrictive storage capacity of the Hopfield model. The technique exploits the maximum storage capacity. It fails only if appropriate weights do not exist to store the given set of patterns. The technique is not based on the concept of function minimization. Thus, there is no danger of getting stuck in local minima. The technique is free from the step size and moving target problems. Learning speed is very fast and depends on difficulties presented by the training patterns and not so much on the parameters of the algorithm. The technique is scalable. Its performance does not degrade as the problem size increases. An extensive analysis of the learning technique is provided through simulation results.<<ETX>>"
            },
            "slug": "A-new-learning-approach-to-enhance-the-storage-of-Oh-Kothari",
            "title": {
                "fragments": [],
                "text": "A new learning approach to enhance the storage capacity of the Hopfield model"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "A new learning technique is introduced to solve the problem of the small and restrictive storage capacity of the Hopfield model and exploits the maximum storage capacity."
            },
            "venue": {
                "fragments": [],
                "text": "[Proceedings] 1991 IEEE International Joint Conference on Neural Networks"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2679783"
                        ],
                        "name": "M. Hassoun",
                        "slug": "M.-Hassoun",
                        "structuredName": {
                            "firstName": "Mohamad",
                            "lastName": "Hassoun",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hassoun"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5170883,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2ddd271867e6dfe5095e0e85857cd87544163982",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Dynamic-heteroassociative-neural-memories-Hassoun",
            "title": {
                "fragments": [],
                "text": "Dynamic heteroassociative neural memories"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145576200"
                        ],
                        "name": "W. Jeffrey",
                        "slug": "W.-Jeffrey",
                        "structuredName": {
                            "firstName": "Wr",
                            "lastName": "Jeffrey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Jeffrey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145746139"
                        ],
                        "name": "R. Rosner",
                        "slug": "R.-Rosner",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Rosner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rosner"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62294642,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "291c211a8311754f32ae1f456c91e0293a1a6315",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "We summarize our recent work on the development of \u2018\u2018neural network\u2019\u2019\u2010like processing for function optimization, and demonstrate how this method can be designed so as to avoid trapping in local extrema. We illustrate the application of our algorithm by considering the inversion of severely ill\u2010posed remote sensing data and the solution of variational problems. The algorithm described here has been implemented on a serial processor, but is cast in a form which is ideally suited for parallel processing."
            },
            "slug": "Neural-network-processing-as-a-tool-for-function-Jeffrey-Rosner",
            "title": {
                "fragments": [],
                "text": "Neural network processing as a tool for function optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "The recent work on the development of \u2018\u2018neural network\u2019\u2019\u2010like processing for function optimization is summarized, and how this method can be designed so as to avoid trapping in local extrema is demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681982"
                        ],
                        "name": "H. Oh",
                        "slug": "H.-Oh",
                        "structuredName": {
                            "firstName": "Heekuck",
                            "lastName": "Oh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Oh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145833095"
                        ],
                        "name": "S. Kothari",
                        "slug": "S.-Kothari",
                        "structuredName": {
                            "firstName": "Suresh",
                            "lastName": "Kothari",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kothari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60892224,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "372b98d6be97720895b810fbaf54519aecd1970a",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "A fast iterative learning algorithm for the bidirectional associative memory (BAM) called PRLAB is introduced. PRLAB utilizes the pseudo-relaxation method adapted from the relaxation method for solving systems of linear inequalities. PRLAB is very fast, is well suited for a neural network implementation, guarantees the recall of all training patterns, is highly insensitive to learning parameters, and offers high scalability for large applications. PRLAB exploits the maximum storage capacity of the BAM and guarantees perfect recall of all trained pairs. For guaranteed storage, no special form of encoding or preprocessing is necessary.<<ETX>>"
            },
            "slug": "A-pseudo-relaxation-learning-algorithm-for-memory-Oh-Kothari",
            "title": {
                "fragments": [],
                "text": "A pseudo-relaxation learning algorithm for bidirectional associative memory"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A fast iterative learning algorithm for the bidirectional associative memory (BAM) called PRLAB is introduced, which exploits the maximum storage capacity of the BAM and guarantees perfect recall of all trained pairs."
            },
            "venue": {
                "fragments": [],
                "text": "[Proceedings 1992] IJCNN International Joint Conference on Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2181418"
                        ],
                        "name": "R. Golden",
                        "slug": "R.-Golden",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Golden",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Golden"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123602982,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c134c0d4512f75d295aaa455bb4e72e3569c769e",
            "isKey": false,
            "numCitedBy": 77,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-:20Brain-state-in-a-box-Neural-model-is-a-Golden",
            "title": {
                "fragments": [],
                "text": "The :20Brain-state-in-a-box Neural model is a gradient descent algorithm"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723150"
                        ],
                        "name": "R. McEliece",
                        "slug": "R.-McEliece",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "McEliece",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. McEliece"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34900218"
                        ],
                        "name": "E. Posner",
                        "slug": "E.-Posner",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Posner",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Posner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3250540"
                        ],
                        "name": "E. Rodemich",
                        "slug": "E.-Rodemich",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Rodemich",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Rodemich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144694846"
                        ],
                        "name": "S. Venkatesh",
                        "slug": "S.-Venkatesh",
                        "structuredName": {
                            "firstName": "Santosh",
                            "lastName": "Venkatesh",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Venkatesh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14340808,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d5db1cc4a5ea1fcf017ceac7f07496292f37a9a2",
            "isKey": false,
            "numCitedBy": 898,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Techniques from coding theory are applied to study rigorously the capacity of the Hopfield associative memory. Such a memory stores n -tuple of \\pm 1 's. The components change depending on a hard-limited version of linear functions of all other components. With symmetric connections between components, a stable state is ultimately reached. By building up the connection matrix as a sum-of-outer products of m fundamental memories, one hopes to be able to recover a certain one of the m memories by using an initial n -tuple probe vector less than a Hamming distance n/2 away from the fundamental memory. If m fundamental memories are chosen at random, the maximum asympotic value of m in order that most of the m original memories are exactly recoverable is n/(2 \\log n) . With the added restriction that every one of the m fundamental memories be recoverable exactly, m can be no more than n/(4 \\log n) asymptotically as n approaches infinity. Extensions are also considered, in particular to capacity under quantization of the outer-product connection matrix. This quantized memory capacity problem is closely related to the capacity of the quantized Gaussian channel."
            },
            "slug": "The-capacity-of-the-Hopfield-associative-memory-McEliece-Posner",
            "title": {
                "fragments": [],
                "text": "The capacity of the Hopfield associative memory"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "Techniques from coding theory are applied to study rigorously the capacity of the Hopfield associative memory, in particular to capacity under quantization of the outer-product connection matrix."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31543630"
                        ],
                        "name": "K. Haines",
                        "slug": "K.-Haines",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Haines",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Haines"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398229863"
                        ],
                        "name": "R. Hecht-Nielsen",
                        "slug": "R.-Hecht-Nielsen",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Hecht-Nielsen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hecht-Nielsen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1626987,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5ef19c35f196f0fb344d76bf1a054c866d658906",
            "isKey": false,
            "numCitedBy": 74,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "The commonly encountered version of B. Kosko's bidirectional associative memory (BAM) (1988) is generalized to include nonzero thresholds. This modified neural network is referred to as the nonhomogeneous BAM. It is shown that the nonhomogeneous BAM can have any number of stable states between 1 and 2/sup n/, where n is the lowest dimension of either layer of the BAM. Nonhomogeneous BAMs with 1,2, . . ., 2/sup n/-1, 2/sup n/ stable states are constructed. The storage capacity of the BAM is the number of stable states, selected by the user at random from some specified subset of stable states, that can be implemented by the BAM for some matrix selection and some set of thresholds. It is shown that the capacity of the nonhomogeneous BAM can greatly exceed the number of processing elements in the network. This is accomplished without violating information theory, because these stable states utilize a sparse code in which little information is stored in each state.<<ETX>>"
            },
            "slug": "A-BAM-with-increased-information-storage-capacity-Haines-Hecht-Nielsen",
            "title": {
                "fragments": [],
                "text": "A BAM with increased information storage capacity"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "The commonly encountered version of B. Kosko's bidirectional associative memory (BAM) (1988) is generalized to include nonzero thresholds and this modified neural network is referred to as the nonhomogeneous BAM."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE 1988 International Conference on Neural Networks"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713858"
                        ],
                        "name": "B. Kosko",
                        "slug": "B.-Kosko",
                        "structuredName": {
                            "firstName": "Bart",
                            "lastName": "Kosko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Kosko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1920268"
                        ],
                        "name": "C. Guest",
                        "slug": "C.-Guest",
                        "structuredName": {
                            "firstName": "Clark",
                            "lastName": "Guest",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Guest"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62424112,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0ae4dfcda68c7e5d2b979dc76c0569a0d3beac53",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Four optical implementations of bidirectional associative memories (BAMs) are presented. BAMs are heteroassociative content addressable memories (CAMs). A BAM stores the m binary associations (A1, B1), ..., (Am, Bm) , where A is a point in the Boolean n-cube and B is a point in the Boolean p-cube. A is a neural network of n bivalent or continuous neurons ai; B is a network of p bivalent or continuous neurons bi. The fixed synaptic connections between the A and B networks are represented by some n-by-p real matrix M. Bidirectionality, forward and backward information flow, in neural nets produces two-way associative search for the nearest stored pair (Ai, Bi) to an input key. Every matrix is a bidirectionally stable hetero-associative CAM for boh bivalent and continuous networks. This generalizes the well-known unidirectional stability for autoassociative networks with square symmetric M. When the BAM neurons are activated, the network quickly evolves to a stable state of two-pattern reverberation, or pseudo-adaptive resonance. The stable reverberation corresponds to a system energy local minimum. Heteroassociative pairs (Ai, Bi) are encoded in a BAM M by summing bipolar correlation matrices, M = X1T Y1 + ... + XmT Ym , where Xi (Yi) is the bipolar version of Ai (Bi), with -1s replacing Os. the BAM storage capacity for reliable recall is roughly m < min(n, p)--pattern number is bounded by pattern dimensionality. BAM optical implementations are divided into two approaches: matrix vector multipliers and holographic correlators. The four optical BAMs described respectively emphasize a spatial light modulator, laser diodes and high-speed detectors, a reflection hologram, and a transmission hologram."
            },
            "slug": "Optical-Bidirectional-Associative-Memories-Kosko-Guest",
            "title": {
                "fragments": [],
                "text": "Optical Bidirectional Associative Memories"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "Four optical implementations of bidirectional associative memories (BAMs) are presented, each with a spatial light modulator, laser diodes and high-speed detectors, a reflection hologram, and a transmission hologram."
            },
            "venue": {
                "fragments": [],
                "text": "Photonics West - Lasers and Applications in Science and Engineering"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3219867"
                        ],
                        "name": "J. Hopfield",
                        "slug": "J.-Hopfield",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hopfield",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hopfield"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "93006261"
                        ],
                        "name": "D. I. Feinstein",
                        "slug": "D.-I.-Feinstein",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Feinstein",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. I. Feinstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50760571"
                        ],
                        "name": "R. Palmer",
                        "slug": "R.-Palmer",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Palmer",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Palmer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 4269710,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology",
                "Psychology"
            ],
            "id": "786b660e60069ecf01673ade33e287d6adb022b7",
            "isKey": false,
            "numCitedBy": 398,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Crick and Mitchison1 have presented a hypothesis for the functional role of dream sleep involving an \u2018unlearning\u2019 process. We have independently carried out mathematical and computer modelling of learning and \u2018unlearning\u2019 in a collective neural network of 30\u20131,000 neurones. The model network has a content-addressable memory or \u2018associative memory\u2019 which allows it to learn and store many memories. A particular memory can be evoked in its entirety when the network is stimulated by any adequate-sized subpart of the information of that memory2. But different memories of the same size are not equally easy to recall. Also, when memories are learned, spurious memories are also created and can also be evoked. Applying an \u2018unlearning\u2019 process, similar to the learning processes but with a reversed sign and starting from a noise input, enhances the performance of the network in accessing real memories and in minimizing spurious ones. Although our model was not motivated by higher nervous function, our system displays behaviours which are strikingly parallel to those needed for the hypothesized role of \u2018unlearning\u2019 in rapid eye movement (REM) sleep."
            },
            "slug": "\u2018Unlearning\u2019-has-a-stabilizing-effect-in-collective-Hopfield-Feinstein",
            "title": {
                "fragments": [],
                "text": "\u2018Unlearning\u2019 has a stabilizing effect in collective memories"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Although the model was not motivated by higher nervous function, the system displays behaviours which are strikingly parallel to those needed for the hypothesized role of \u2018unlearning\u2019 in rapid eye movement (REM) sleep."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115737829"
                        ],
                        "name": "Yeou-Fang Wang",
                        "slug": "Yeou-Fang-Wang",
                        "structuredName": {
                            "firstName": "Yeou-Fang",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yeou-Fang Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144927810"
                        ],
                        "name": "J. B. Cruz",
                        "slug": "J.-B.-Cruz",
                        "structuredName": {
                            "firstName": "Jose",
                            "lastName": "Cruz",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. B. Cruz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52299408"
                        ],
                        "name": "J. H. Mulligan",
                        "slug": "J.-H.-Mulligan",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Mulligan",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H. Mulligan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 40606358,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f4a40820811a681a91baa4284f38985bea8e6e3f",
            "isKey": false,
            "numCitedBy": 179,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Enhancements of the encoding strategy of a discrete bidirectional associative memory (BAM) reported by B. Kosko (1987) are presented. There are two major concepts in this work: multiple training, which can be guaranteed to achieve recall of a single trained pair under suitable initial conditions of data, and dummy augmentation, which can be guaranteed to achieve recall of all trained pairs if attaching dummy data to the training pairs is allowable. In representative computer simulations, multiple training has been shown to lead to an improvement over the original Kosko strategy for recall of multiple pairs as well. A sufficient condition for a correlation matrix to make the energies of the training pairs be local minima is discussed. The use of multiple training and dummy augmentation concepts are illustrated, and theorems underlying the results are presented."
            },
            "slug": "Two-coding-strategies-for-bidirectional-associative-Wang-Cruz",
            "title": {
                "fragments": [],
                "text": "Two coding strategies for bidirectional associative memory"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "In representative computer simulations, multiple training has been shown to lead to an improvement over the original Kosko strategy for recall of multiple pairs as well, and theorems underlying the results are presented."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701656"
                        ],
                        "name": "James L. McClelland",
                        "slug": "James-L.-McClelland",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "McClelland",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James L. McClelland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15291527,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff2c2e3e83d1e8828695484728393c76ee07a101",
            "isKey": false,
            "numCitedBy": 15715,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The fundamental principles, basic mechanisms, and formal analyses involved in the development of parallel distributed processing (PDP) systems are presented in individual chapters contributed by leading experts. Topics examined include distributed representations, PDP models and general issues in cognitive science, feature discovery by competitive learning, the foundations of harmony theory, learning and relearning in Boltzmann machines, and learning internal representations by error propagation. Consideration is given to linear algebra in PDP, the logic of additive functions, resource requirements of standard and programmable nets, and the P3 parallel-network simulating system."
            },
            "slug": "Parallel-distributed-processing:-explorations-in-of-Rumelhart-McClelland",
            "title": {
                "fragments": [],
                "text": "Parallel distributed processing: explorations in the microstructure of cognition, vol. 1: foundations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3160228"
                        ],
                        "name": "K. Fukushima",
                        "slug": "K.-Fukushima",
                        "structuredName": {
                            "firstName": "Kunihiko",
                            "lastName": "Fukushima",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fukushima"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35250703"
                        ],
                        "name": "N. Wake",
                        "slug": "N.-Wake",
                        "structuredName": {
                            "firstName": "Nobuaki",
                            "lastName": "Wake",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Wake"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 35409661,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2ea3ebebfa4493713dbaaf681488b30e641fc1c2",
            "isKey": false,
            "numCitedBy": 263,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "A pattern recognition system which works with the mechanism of the neocognitron, a neural network model for deformation-invariant visual pattern recognition, is discussed. The neocognition was developed by Fukushima (1980). The system has been trained to recognize 35 handwritten alphanumeric characters. The ability to recognize deformed characters correctly depends strongly on the choice of the training pattern set. Some techniques for selecting training patterns useful for deformation-invariant recognition of a large number of characters are suggested."
            },
            "slug": "Handwritten-alphanumeric-character-recognition-by-Fukushima-Wake",
            "title": {
                "fragments": [],
                "text": "Handwritten alphanumeric character recognition by the neocognitron"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "A pattern recognition system which works with the mechanism of the neocognitron, a neural network model for deformation-invariant visual pattern recognition, is discussed, which has been trained to recognize 35 handwritten alphanumeric characters."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682174"
                        ],
                        "name": "S. Grossberg",
                        "slug": "S.-Grossberg",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Grossberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Grossberg"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123298350,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "360b5596bd022435ec5c5a8752a95f02f54d56ff",
            "isKey": false,
            "numCitedBy": 186,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning of patterns by neural networks obeying general rules of sensory transduction and of converting membrane potentials to spiking frequencies is considered. Any finite number of cellsA can sample a pattern playing on any finite number of cells \u2207 without causing irrevocable sampling bias ifA = \u212c orA \u2229 \u212c =. Total energy transfer from inputs ofA to outputs of \u212c depends on the entropy of the input distribution. Pattern completion on recall trials can occur without destroying perfect memory even ifA = \u212c by choosing the signal thresholds sufficiently large. The mathematical results are global limit and oscillation theorems for a class of nonlinear functional-differential systems."
            },
            "slug": "On-learning-and-energy-entropy-dependence-in-and-Grossberg",
            "title": {
                "fragments": [],
                "text": "On learning and energy-entropy dependence in recurrent and nonrecurrent signed networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The mathematical results are global limit and oscillation theorems for a class of nonlinear functional-differential systems and pattern completion on recall trials can occur without destroying perfect memory even if the signal thresholds sufficiently large."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1969
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752732"
                        ],
                        "name": "T. Cover",
                        "slug": "T.-Cover",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Cover",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cover"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18251470,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "445ad69010658097fc317f7b83f1198179eebae8",
            "isKey": false,
            "numCitedBy": 1840,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper develops the separating capacities of families of nonlinear decision surfaces by a direct application of a theorem in classical combinatorial geometry. It is shown that a family of surfaces having d degrees of freedom has a natural separating capacity of 2d pattern vectors, thus extending and unifying results of Winder and others on the pattern-separating capacity of hyperplanes. Applying these ideas to the vertices of a binary n-cube yields bounds on the number of spherically, quadratically, and, in general, nonlinearly separable Boolean functions of n variables. It is shown that the set of all surfaces which separate a dichotomy of an infinite, random, separable set of pattern vectors can be characterized, on the average, by a subset of only 2d extreme pattern vectors. In addition, the problem of generalizing the classifications on a labeled set of pattern points to the classification of a new point is defined, and it is found that the probability of ambiguous generalization is large unless the number of training patterns exceeds the capacity of the set of separating surfaces."
            },
            "slug": "Geometrical-and-Statistical-Properties-of-Systems-Cover",
            "title": {
                "fragments": [],
                "text": "Geometrical and Statistical Properties of Systems of Linear Inequalities with Applications in Pattern Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that a family of surfaces having d degrees of freedom has a natural separating capacity of 2d pattern vectors, thus extending and unifying results of Winder and others on the pattern-separating capacity of hyperplanes."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Electron. Comput."
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713858"
                        ],
                        "name": "B. Kosko",
                        "slug": "B.-Kosko",
                        "structuredName": {
                            "firstName": "Bart",
                            "lastName": "Kosko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Kosko"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 7945588,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8b01f0da1502554b70f65e47d4ff159a4f16d8f2",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Global stability is examined for nonlinear feedback dynamical systems subject to unsupervised learning. Only differentiable neural models are discussed. The unconditional stability of Hebbian learning systems is summarized in the adaptive bidirectional associative memory (ABAM) theorem. When no learning occurs, the resulting BAM models include Cohen-Grossberg autoassociators, Hopfield circuits, brain-state-in-a box models, and masking field models. The ABAM theorem is extended to arbitrary higher-order Hebbian learning. Conditions for exponential convergence are discussed. Sufficient conditions for global stability are established for dynamical systems that adapt according to competitive and differential Hebbian learning laws.<<ETX>>"
            },
            "slug": "Feedback-stability-and-unsupervised-learning-Kosko",
            "title": {
                "fragments": [],
                "text": "Feedback stability and unsupervised learning"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "Sufficient conditions for global stability are established for dynamical systems that adapt according to competitive and differential Hebbian learning laws for nonlinear feedback dynamical Systems subject to unsupervised learning."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE 1988 International Conference on Neural Networks"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398965769"
                        ],
                        "name": "Y. Abu-Mostafa",
                        "slug": "Y.-Abu-Mostafa",
                        "structuredName": {
                            "firstName": "Yaser",
                            "lastName": "Abu-Mostafa",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Abu-Mostafa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145773547"
                        ],
                        "name": "J. Jacques",
                        "slug": "J.-Jacques",
                        "structuredName": {
                            "firstName": "Jeannine-Marie",
                            "lastName": "Jacques",
                            "middleNames": [
                                "St."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Jacques"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14068666,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ad1698c1f463a396c25e18520ff73a30f8b5cb2c",
            "isKey": false,
            "numCitedBy": 259,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "The information capacity of general forms of memory is formalized. The number of bits of information that can be stored in the Hopfield model of associative memory is estimated. It is found that the asymptotic information capacity of a Hopfield network of N neurons is of the order N^{3} b. The number of arbitrary state vectors that can be made stable in a Hopfield network of N neurons is proved to be bounded above by N ."
            },
            "slug": "Information-capacity-of-the-Hopfield-model-Abu-Mostafa-Jacques",
            "title": {
                "fragments": [],
                "text": "Information capacity of the Hopfield model"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "The information capacity of general forms of memory is formalized and the number of bits of information that can be stored in the Hopfield model of associative memory is estimated."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Inf. Theory"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6075523"
                        ],
                        "name": "P. Kienker",
                        "slug": "P.-Kienker",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Kienker",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Kienker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5660814,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b300e1bbb6ad0d513db2eeb64a2508b4fafb9da6",
            "isKey": false,
            "numCitedBy": 107,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-symmetry-groups-with-hidden-units:-beyond-Sejnowski-Kienker",
            "title": {
                "fragments": [],
                "text": "Learning symmetry groups with hidden units: beyond the perceptron"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2650569"
                        ],
                        "name": "A. Klopf",
                        "slug": "A.-Klopf",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Klopf",
                            "middleNames": [
                                "Harry"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Klopf"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 56546563,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "4265a9c6184dd136b4d4c55b4919c908591d11af",
            "isKey": false,
            "numCitedBy": 72,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "A neuronal learning mechanism is proposed that accounts for the basic animal learning phenomena that have been observed. Among the classical conditioning phenomena predicted by the neuronal model are delay conditioning, trace conditioning, simultaneous conditioning, conditioned stimulus duration and amplitude effects, unconditioned stimulus amplitude effects, interstimulus interval effects, second and higher order conditioning, conditioned inhibition, habituation and extinction, reacquisition effects, backward conditioning, blocking, overshadowing and serial compound conditioning. The proposed neuronal model and learning mechanism offer a new building block for constructing neural network\u2010like computer arthitectures for artificial intelligence."
            },
            "slug": "A-drive-reinforcement-model-of-single-neuron-Klopf",
            "title": {
                "fragments": [],
                "text": "A drive-reinforcement model of single neuron function"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The proposed neuronal model and learning mechanism offer a new building block for constructing neural network\u2010like computer arthitectures for artificial intelligence."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053227062"
                        ],
                        "name": "Richard Durbin",
                        "slug": "Richard-Durbin",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Durbin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Richard Durbin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40227361"
                        ],
                        "name": "D. Willshaw",
                        "slug": "D.-Willshaw",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Willshaw",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Willshaw"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 4321691,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a94be030ccd68f3a5a3bf9245137fe114c549819",
            "isKey": false,
            "numCitedBy": 855,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "The travelling salesman problem1 is a classical problem in the field of combinatorial optimization, concerned with efficient methods for maximizing or minimizing a function of many independent variables. Given the positions of N cities, which in the simplest case lie in the plane, what is the shortest closed tour in which each city can be visited once? We describe how a parallel analogue algorithm, derived from a formal model2\u20133 for the establishment of topographically ordered projections in the brain4\u201310, can be applied to the travelling salesman problem1,11,12. Using an iterative procedure, a circular closed path is gradually elongated non-uniformly until it eventually passes sufficiently near to all the cities to define a tour. This produces shorter tour lengths than another recent parallel analogue algorithm13, scales well with the size of the problem, and is naturally extendable to a large class of optimization problems involving topographic mappings between geometrical structures14."
            },
            "slug": "An-analogue-approach-to-the-travelling-salesman-an-Durbin-Willshaw",
            "title": {
                "fragments": [],
                "text": "An analogue approach to the travelling salesman problem using an elastic net method"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work describes how a parallel analogue algorithm, derived from a formal model for the establishment of topographically ordered projections in the brain, can be applied to the travelling salesman problem, and produces shorter tour lengths than another recent parallel analogue algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143742406"
                        ],
                        "name": "Jehoshua Bruck",
                        "slug": "Jehoshua-Bruck",
                        "structuredName": {
                            "firstName": "Jehoshua",
                            "lastName": "Bruck",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jehoshua Bruck"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16020831,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "69b9f6eb51e204e131ed94fa716ce790943e4206",
            "isKey": false,
            "numCitedBy": 116,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The main contribution of the present work is showing that the known convergence properties of the Hopfield model can be reduced to a very simple case, for which an elementary proof is provided. The convergence properties of the Hopfield model are dependent on the structure of the interconnections matrix W and the method by which the nodes are updated. Three cases are known: (1) convergence to a stable state when operating in a serial mode with symmetric W; (2) convergence to a cycle of length 2, at most, when operating in a fully parallel mode with symmetric W; and (3) convergence to a cycle of length 4 when operating in a fully parallel mode with antisymmetric W. The three known results are reviewed and it is proven that the fully parallel mode of operation is a special case of the serial model of operation. There are three more cases than can be considered using this characterization: serial mode of operation, antisymmetric W; serial mode of operation, arbitrary W; and fully parallel mode of operation, arbitrary W. By exhibiting exponential lower bounds on the length of the cycles in other cases, it is proven that the three known cases are the only interesting ones. >"
            },
            "slug": "On-the-convergence-properties-of-the-Hopfield-model-Bruck",
            "title": {
                "fragments": [],
                "text": "On the convergence properties of the Hopfield model"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1960915"
                        ],
                        "name": "G. Weisbuch",
                        "slug": "G.-Weisbuch",
                        "structuredName": {
                            "firstName": "G\u00e9rard",
                            "lastName": "Weisbuch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Weisbuch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1399006281"
                        ],
                        "name": "F. Fogelman-Souli\u00e9",
                        "slug": "F.-Fogelman-Souli\u00e9",
                        "structuredName": {
                            "firstName": "Fran\u00e7oise",
                            "lastName": "Fogelman-Souli\u00e9",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Fogelman-Souli\u00e9"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16076015,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "20bc25da33314d4dc157419f1321f962c59cca3f",
            "isKey": false,
            "numCitedBy": 96,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Networks of threshold automata are random dynamical systems with a large number of attractors, which J. Hopfield proposed to use as associative memories. We establish the scaling laws relating the maximum number of \u00abuseful\u00bb attractors and the radius of the attraction basin to the number of automata. A by-product of our analysis is a better choice for thresholds which doubles the performances in terms of the maximum number of \u00abuseful\u00bb attractors Les reseaux d'automates a seuil sont des systemes dynamiques a structure aleatoire semblables aux verres de spins dont J. Hopfield a propose l'application comme memoires associatives. Nous etablissons les lois d'echelles reliant le nombre maximum d'attracteurs utiles et la distance d'attraction, au nombre des automates du reseau. Notre approche permet aussi un meilleur choix des seuils, ce qui double les performances du reseau en nombre d'attracteurs."
            },
            "slug": "Scaling-laws-for-the-attractors-of-Hopfield-Weisbuch-Fogelman-Souli\u00e9",
            "title": {
                "fragments": [],
                "text": "Scaling laws for the attractors of Hopfield networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115737829"
                        ],
                        "name": "Yeou-Fang Wang",
                        "slug": "Yeou-Fang-Wang",
                        "structuredName": {
                            "firstName": "Yeou-Fang",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yeou-Fang Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144927810"
                        ],
                        "name": "J. B. Cruz",
                        "slug": "J.-B.-Cruz",
                        "structuredName": {
                            "firstName": "Jose",
                            "lastName": "Cruz",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. B. Cruz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52299408"
                        ],
                        "name": "J. H. Mulligan",
                        "slug": "J.-H.-Mulligan",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Mulligan",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H. Mulligan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 10519036,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7d4af89505b42a83e437b5524ed77d9533261096",
            "isKey": false,
            "numCitedBy": 108,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Necessary and sufficient conditions are derived for the weights of a generalized correlation matrix of a bidirectional associative memory (BAM) which guarantee the recall of all training pairs. A linear programming/multiple training (LP/MT) method that determines weights which satisfy the conditions when a solution is feasible is presented. The sequential multiple training (SMT) method is shown to yield integers for the weights, which are multiplicities of the training pairs. Computer simulation results, including capacity comparisons of BAM, LP/MT BAM, and SMT BAM, are presented."
            },
            "slug": "Guaranteed-recall-of-all-training-pairs-for-memory-Wang-Cruz",
            "title": {
                "fragments": [],
                "text": "Guaranteed recall of all training pairs for bidirectional associative memory"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A linear programming/multiple training (LP/MT) method that determines weights which satisfy the conditions when a solution is feasible is presented and the sequential multiple training (SMT) method is shown to yield integers for the weights, which are multiplicities of the training pairs."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2142511"
                        ],
                        "name": "D. Kleinfeld",
                        "slug": "D.-Kleinfeld",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Kleinfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kleinfeld"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "116777166"
                        ],
                        "name": "D. Pendergraft",
                        "slug": "D.-Pendergraft",
                        "structuredName": {
                            "firstName": "Daryl",
                            "lastName": "Pendergraft",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pendergraft"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 43808377,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d66f19e2988e673db85a10cda9eed3238b27b8eb",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "\"Unlearning\"-increases-the-storage-capacity-of-Kleinfeld-Pendergraft",
            "title": {
                "fragments": [],
                "text": "\"Unlearning\" increases the storage capacity of content addressable memories."
            },
            "venue": {
                "fragments": [],
                "text": "Biophysical journal"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741124"
                        ],
                        "name": "L. Valiant",
                        "slug": "L.-Valiant",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Valiant",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Valiant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 59712,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "10ddb646feddc12337b5a755c72e153e37088c02",
            "isKey": false,
            "numCitedBy": 4184,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Humans appear to be able to learn new concepts without needing to be programmed explicitly in any conventional sense. In this paper we regard learning as the phenomenon of knowledge acquisition in the absence of explicit programming. We give a precise methodology for studying this phenomenon from a computational viewpoint. It consists of choosing an appropriate information gathering mechanism, the learning protocol, and exploring the class of concepts that can be learnt using it in a reasonable (polynomial) number of steps. We find that inherent algorithmic complexity appears to set serious limits to the range of concepts that can be so learnt. The methodology and results suggest concrete principles for designing realistic learning systems."
            },
            "slug": "A-theory-of-the-learnable-Valiant",
            "title": {
                "fragments": [],
                "text": "A theory of the learnable"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper regards learning as the phenomenon of knowledge acquisition in the absence of explicit programming, and gives a precise methodology for studying this phenomenon from a computational viewpoint."
            },
            "venue": {
                "fragments": [],
                "text": "STOC '84"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58779360,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8592e46a5435d18bba70557846f47290b34c1aa5",
            "isKey": false,
            "numCitedBy": 1338,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter contains sections titled: Relaxation Searches, Easy and Hard Learning, The Boltzmann Machine Learning Algorithm, An Example of Hard Learning, Achieving Reliable Computation with Unreliable Hardware, An Example of the Effects of Damage, Conclusion, Acknowledgments, Appendix: Derivation of the Learning Algorithm, References"
            },
            "slug": "Learning-and-relearning-in-Boltzmann-machines-Hinton-Sejnowski",
            "title": {
                "fragments": [],
                "text": "Learning and relearning in Boltzmann machines"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "This chapter contains sections titled: Relaxation Searches, Easy and Hard learning, The Boltzmann Machine Learning Algorithm, An Example of Hard Learning, Achieving Reliable Computation with Unreliable Hardware, and an Example of the Effects of Damage."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31577094"
                        ],
                        "name": "T. Motzkin",
                        "slug": "T.-Motzkin",
                        "structuredName": {
                            "firstName": "Theodore",
                            "lastName": "Motzkin",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Motzkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50456127"
                        ],
                        "name": "I. J. Schoenberg",
                        "slug": "I.-J.-Schoenberg",
                        "structuredName": {
                            "firstName": "I.",
                            "lastName": "Schoenberg",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. J. Schoenberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 123653448,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "3bd7fe1a5c67bb2f1d445e95816922c11db78ae8",
            "isKey": false,
            "numCitedBy": 460,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Let A be a closed set of points in the n-dimensional euclidean space En. If p and p 1 are points of En such that 1.1 then p 1 is said to be point-wise closer than p to the set A. If p is such that there is no point p1 which is point-wise closer than p to A, then p is called a closest point to the set A."
            },
            "slug": "The-Relaxation-Method-for-Linear-Inequalities-Motzkin-Schoenberg",
            "title": {
                "fragments": [],
                "text": "The Relaxation Method for Linear Inequalities"
            },
            "venue": {
                "fragments": [],
                "text": "Canadian Journal of Mathematics"
            },
            "year": 1954
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388625365"
                        ],
                        "name": "E. Ch.",
                        "slug": "E.-Ch.",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Ch.",
                            "middleNames": [
                                "Goles"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Ch."
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1399006281"
                        ],
                        "name": "F. Fogelman-Souli\u00e9",
                        "slug": "F.-Fogelman-Souli\u00e9",
                        "structuredName": {
                            "firstName": "Fran\u00e7oise",
                            "lastName": "Fogelman-Souli\u00e9",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Fogelman-Souli\u00e9"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3429240"
                        ],
                        "name": "D. Pellegrin",
                        "slug": "D.-Pellegrin",
                        "structuredName": {
                            "firstName": "Didier",
                            "lastName": "Pellegrin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pellegrin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 205053955,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "51f94ce356c802b13e738522cbcaa7f2057ae86b",
            "isKey": false,
            "numCitedBy": 255,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Decreasing-energy-functions-as-a-tool-for-studying-Ch.-Fogelman-Souli\u00e9",
            "title": {
                "fragments": [],
                "text": "Decreasing energy functions as a tool for studying threshold networks"
            },
            "venue": {
                "fragments": [],
                "text": "Discret. Appl. Math."
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12141647"
                        ],
                        "name": "S. Agmon",
                        "slug": "S.-Agmon",
                        "structuredName": {
                            "firstName": "Shmuel",
                            "lastName": "Agmon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Agmon"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 124903046,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d1eef5712b9d7eabec2f40adb2352078ecafcb06",
            "isKey": false,
            "numCitedBy": 408,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "In various numerical problems one is confronted with the task of solving a system of linear inequalities: (1.1) (i = 1, \u2026 ,m) assuming, of course, that the above system is consistent. Sometimes one has, in addition, to minimize a given linear form l(x). Thus, in linear programming one obtains a problem of the latter type."
            },
            "slug": "The-Relaxation-Method-for-Linear-Inequalities-Agmon",
            "title": {
                "fragments": [],
                "text": "The Relaxation Method for Linear Inequalities"
            },
            "venue": {
                "fragments": [],
                "text": "Canadian Journal of Mathematics"
            },
            "year": 1954
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388625365"
                        ],
                        "name": "E. Ch.",
                        "slug": "E.-Ch.",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Ch.",
                            "middleNames": [
                                "Goles"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Ch."
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 205092011,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "12f1f198c452f6447ee017b6d8cad15184ab4dcf",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Dynamics-of-Positive-Automata-Networks-Ch.",
            "title": {
                "fragments": [],
                "text": "Dynamics of Positive Automata Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Theor. Comput. Sci."
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4113648"
                        ],
                        "name": "F. Crick",
                        "slug": "F.-Crick",
                        "structuredName": {
                            "firstName": "Francis",
                            "lastName": "Crick",
                            "middleNames": [
                                "H.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Crick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145666307"
                        ],
                        "name": "G. Mitchison",
                        "slug": "G.-Mitchison",
                        "structuredName": {
                            "firstName": "Graeme",
                            "lastName": "Mitchison",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Mitchison"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 41500914,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "736ad2a6e6fa37e6b0fdf0cadcb788e248adfa06",
            "isKey": false,
            "numCitedBy": 819,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose that the function of dream sleep (more properly rapid-eye movement or REM sleep) is to remove certain undesirable modes of interaction in networks of cells in the cerebral cortex. We postulate that this is done in REM sleep by a reverse learning mechanism (see also p. 158), so that the trace in the brain of the unconscious dream is weakened, rather than strengthened, by the dream."
            },
            "slug": "The-function-of-dream-sleep-Crick-Mitchison",
            "title": {
                "fragments": [],
                "text": "The function of dream sleep"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "It is proposed that the function of dream sleep is to remove certain undesirable modes of interaction in networks of cells in the cerebral cortex by a reverse learning mechanism, so that the trace in the brain of the unconscious dream is weakened, rather than strengthened, by the dream."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "87152190"
                        ],
                        "name": "B. Kosco",
                        "slug": "B.-Kosco",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Kosco",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Kosco"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123555516,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "3d36250bc4501f5cec3dcb8df0b4ac1274b64e61",
            "isKey": false,
            "numCitedBy": 141,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Differential-Hebbian-learning-Kosco",
            "title": {
                "fragments": [],
                "text": "Differential Hebbian learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62186699,
            "fieldsOfStudy": [],
            "id": "3302a19539ccfa8ed3a8361ace8947ddbba1acf5",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An introduction to computing with neural nets"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62753680,
            "fieldsOfStudy": [],
            "id": "717725e95923c5b8b4fd6330b8900147f7f7b6c8",
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theory of the backpropagation neural network"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1913418"
                        ],
                        "name": "B. Widrow",
                        "slug": "B.-Widrow",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "Widrow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Widrow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1976925"
                        ],
                        "name": "M. Hoff",
                        "slug": "M.-Hoff",
                        "structuredName": {
                            "firstName": "Marcian",
                            "lastName": "Hoff",
                            "middleNames": [
                                "E."
                            ],
                            "suffix": "Jr."
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hoff"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60830585,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2e14b2ff9dc2234df94fc24d89fc25e797d0e9e7",
            "isKey": false,
            "numCitedBy": 2623,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Adaptive-switching-circuits-Widrow-Hoff",
            "title": {
                "fragments": [],
                "text": "Adaptive switching circuits"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144990248"
                        ],
                        "name": "R. Lippmann",
                        "slug": "R.-Lippmann",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Lippmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lippmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8275028,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b8778bb692cf105254fe767ef11a3a8afac4a068",
            "isKey": false,
            "numCitedBy": 3801,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Artificial neural net models have been studied for many years in the hope of achieving human-like performance in the fields of speech and image recognition. These models are composed of many nonlinear computational elements operating in parallel and arranged in patterns reminiscent of biological neural nets. Computational elements or nodes are connected via weights that are typically adapted during use to improve performance. There has been a recent resurgence in the field of artificial neural nets caused by new net topologies and algorithms, analog VLSI implementation techniques, and the belief that massive parallelism is essential for high performance speech and image recognition. This paper provides an introduction to the field of artificial neural nets by reviewing six important neural net models that can be used for pattern classification. These nets are highly parallel building blocks that illustrate neural net components and design principles and can be used to construct more complex systems. In addition to describing these nets, a major emphasis is placed on exploring how some existing classification and clustering algorithms can be performed using simple neuron-like components. Single-layer nets can implement algorithms required by Gaussian maximum-likelihood classifiers and optimum minimum-error classifiers for binary patterns corrupted by noise. More generally, the decision regions required by any classification algorithm can be generated in a straightforward manner by three-layer feed-forward nets."
            },
            "slug": "An-introduction-to-computing-with-neural-nets-Lippmann",
            "title": {
                "fragments": [],
                "text": "An introduction to computing with neural nets"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper provides an introduction to the field of artificial neural nets by reviewing six important neural net models that can be used for pattern classification and exploring how some existing classification and clustering algorithms can be performed using simple neuron-like components."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE ASSP Magazine"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398229863"
                        ],
                        "name": "R. Hecht-Nielsen",
                        "slug": "R.-Hecht-Nielsen",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Hecht-Nielsen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hecht-Nielsen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5691634,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f4457792a247c0eb6c6fb11a1d92f6f45b82acc1",
            "isKey": false,
            "numCitedBy": 1796,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Theory-of-the-backpropagation-neural-network-Hecht-Nielsen",
            "title": {
                "fragments": [],
                "text": "Theory of the backpropagation neural network"
            },
            "venue": {
                "fragments": [],
                "text": "International 1989 Joint Conference on Neural Networks"
            },
            "year": 1989
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {},
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 66,
        "totalPages": 7
    },
    "page_url": "https://www.semanticscholar.org/paper/Neural-Networks-for-Pattern-Recognition-Kothari-Oh/dbc0a468ab103ae29717703d4aa9f682f6a2b664?sort=total-citations"
}