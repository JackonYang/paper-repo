authors:
- Zhou Yu
- Jun Yu
- Yuhao Cui
- D. Tao
- Q. Tian
badges:
- id: UNPAYWALL
- id: OPEN_ACCESS
corpusId: 195657908
fieldsOfStudy:
- Computer Science
numCitedBy: 323
numCiting: 39
paperAbstract: Visual Question Answering (VQA) requires a fine-grained and simultaneous
  understanding of both the visual content of images and the textual content of questions.
  Therefore, designing an effective `co-attention' model to associate key words in
  questions with key objects in images is central to VQA performance. So far, most
  successful attempts at co-attention learning have been achieved by using shallow
  models, and deep co-attention models show little improvement over their shallow
  counterparts. In this paper, we propose a deep Modular Co-Attention Network (MCAN)
  that consists of Modular Co-Attention (MCA) layers cascaded in depth. Each MCA layer
  models the self-attention of questions and images, as well as the question-guided-attention
  of images jointly using a modular composition of two basic attention units. We quantitatively
  and qualitatively evaluate MCAN on the benchmark VQA-v2 dataset and conduct extensive
  ablation studies to explore the reasons behind MCAN's effectiveness. Experimental
  results demonstrate that MCAN significantly outperforms the previous state-of-the-art.
  Our best single model delivers 70.63% overall accuracy on the test-dev set.
ref_count: 39
references:
- pid: fb9d253258d6b3beceb9d6cd7bba6e0a29ab875b
  title: Hierarchical Question-Image Co-Attention for Visual Question Answering
- pid: 8e9ad6f8b2bc97f0412fa0cc243ac6975864534a
  title: Multi-modal Factorized Bilinear Pooling with Co-attention Learning for Visual
    Question Answering
- pid: f7cc85bed2a3d0b0ef1c0e0258f5b60ee4bb4622
  title: Improved Fusion of Visual and Language Representations by Dense Symmetric
    Co-attention for Visual Question Answering
- pid: 0c0f41d3162e76500d4639557ff4463bd246e395
  title: 'Beyond Bilinear: Generalized Multimodal Factorized High-Order Pooling for
    Visual Question Answering'
- pid: b196bc11ad516c8e6ff96f83acfc443fd7161730
  title: 'ABC-CNN: An Attention Based Convolutional Neural Network for Visual Question
    Answering'
- pid: a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8
  title: Bottom-Up and Top-Down Attention for Image Captioning and Visual Question
    Answering
- pid: a27ed1310b9c0832bde8f906e0fcd23d1f3aa79c
  title: 'Making the V in VQA Matter: Elevating the Role of Image Understanding in
    Visual Question Answering'
- pid: f651593fa6c83d717fc961482696a53b6fca5ab5
  title: Dual Attention Networks for Multimodal Reasoning and Matching
- pid: fddc15480d086629b960be5bff96232f967f2252
  title: Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual
    Grounding
- pid: 2c1890864c1c2b750f48316dc8b650ba4772adc5
  title: Stacked Attention Networks for Image Question Answering
- pid: 30a3eee5e9302108416f6234d739373dde68d373
  title: Learning to Count Objects in Natural Images for Visual Question Answering
- pid: a5d10341717c0519cf63151b496a6d2ed67aa05f
  title: Bilinear Attention Networks
- pid: 97ad70a9fa3f99adf18030e5e38ebe3d90daa2db
  title: 'VQA: Visual Question Answering'
- pid: 1afb710a5b35a2352a6495e4bf6eef66808daf1b
  title: Multimodal Residual Learning for Visual QA
- pid: b14a60a1c3e6bb45baddd754a1cfe83ffc1bbb81
  title: 'Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge'
- pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  title: Attention is All you Need
- pid: fe466e84fa2e838adc3c37ee327cd68004ae08fe
  title: 'MUTAN: Multimodal Tucker Fusion for Visual Question Answering'
- pid: f01fc808592ea7c473a69a6e7484040a435f36d9
  title: Long-term recurrent convolutional networks for visual recognition and description
- pid: afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d
  title: 'Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image
    Annotations'
- pid: 267fb4ac632449dbe84f7acf17c8c7527cb25b0d
  title: Simple Baseline for Visual Question Answering
- pid: 175e9bb50cc062c6c1742a5d90c8dfe31d2e4e22
  title: 'Where to Look: Focus Regions for Visual Question Answering'
- pid: 4d8f2d14af5991d4f0d050d22216825cac3157bd
  title: 'Show, Attend and Tell: Neural Image Caption Generation with Visual Attention'
- pid: 2c03df8b48bf3fa39054345bafabfeff15bfd11d
  title: Deep Residual Learning for Image Recognition
- pid: 424561d8585ff8ebce7d5d07de8dbf7aae5e7270
  title: 'Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks'
- pid: b624504240fa52ab76167acfe3156150ca01cf3b
  title: Attention-Based Models for Speech Recognition
- pid: 8a756d4d25511d92a45d0f4545fa819de993851d
  title: Recurrent Models of Visual Attention
- pid: 71b7178df5d2b112d07e45038cb5637208659ff7
  title: 'Microsoft COCO: Common Objects in Context'
- pid: ac64fb7e6d2ddf236332ec9f371fe85d308c114d
  title: A Multi-World Approach to Question Answering about Real-World Scenes based
    on Uncertain Input
- pid: f37e1b62a767a307c046404ca96bc140b3e68cb5
  title: 'GloVe: Global Vectors for Word Representation'
- pid: fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5
  title: Neural Machine Translation by Jointly Learning to Align and Translate
- pid: a6cb366736791bcccc5c8639de5a8f9636bf87e8
  title: 'Adam: A Method for Stochastic Optimization'
- pid: 44d2abe2175df8153f465f6c39b68b76a0d40ab9
  title: Long Short-Term Memory
slug: Deep-Modular-Co-Attention-Networks-for-Visual-Yu-Yu
title: Deep Modular Co-Attention Networks for Visual Question Answering
url: https://www.semanticscholar.org/paper/Deep-Modular-Co-Attention-Networks-for-Visual-Yu-Yu/8a1744da011375d711ed75fc2d160c6fdca2cf89?sort=total-citations
venue: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
year: 2019
