authors:
- Jens Lehmann
- Robert Isele
- Max Jakob
- Anja Jentzsch
- D. Kontokostas
- Pablo N. Mendes
- Sebastian Hellmann
- M. Morsey
- Patrick van Kleef
- S. Auer
- C. Bizer
badges:
- id: OPEN_ACCESS
corpusId: 1181640
fieldsOfStudy:
- Computer Science
numCitedBy: 2343
numCiting: 62
paperAbstract: The DBpedia community project extracts structured, multilingual knowledge
  from Wikipedia and makes it freely available on the Web using Semantic Web and Linked
  Data technologies. The project extracts knowledge from 111 different language editions
  of Wikipedia. The largest DBpedia knowledge base which is extracted from the English
  edition of Wikipedia consists of over 400 million facts that describe 3.7 million
  things. The DBpedia knowledge bases that are extracted from the other 110 Wikipedia
  editions together consist of 1.46 billion facts and describe 10 million additional
  things. The DBpedia project maps Wikipedia infoboxes from 27 different language
  editions to a single shared ontology consisting of 320 classes and 1,650 properties.
  The mappings are created via a world-wide crowd-sourcing effort and enable knowledge
  from the different Wikipedia editions to be combined. The project publishes releases
  of all DBpedia knowledge bases for download and provides SPARQL query access to
  14 out of the 111 language editions via a global network of local DBpedia chapters.
  In addition to the regular releases, the project maintains a live knowledge base
  which is updated whenever a page in Wikipedia changes. DBpedia sets 27 million RDF
  links pointing into over 30 external data sources and thus enables data from these
  sources to be used together with DBpedia data. Several hundred data sets on the
  Web publish RDF links pointing to DBpedia themselves and make DBpedia one of the
  central interlinking hubs in the Linked Open Data (LOD) cloud. In this system report,
  we give an overview of the DBpedia community project, including its architecture,
  technical implementation, maintenance, internationalisation, usage statistics and
  applications.
ref_count: 62
references:
- pid: 2b2c30dfd3968c5d9418bb2c14b2382d3ccc64b2
  title: 'DBpedia: A Nucleus for a Web of Open Data'
- pid: 00a3f6924f90fcd77e6e7e6534b957a75d0ced07
  title: 'Yago: a core of semantic knowledge'
- pid: 9ef07373873cc0f0b940512dcdde4e7b54b0cfb0
  title: 'Web-scale information extraction in knowitall: (preliminary results)'
- pid: eb04bd49bdc2c9218624435eb277be2a973b21a5
  title: Template-based question answering over RDF data
- pid: 4dfe43ddfcfbe00dd663a4d70b0df9dcc8c92184
  title: 'YAGO2: A Spatially and Temporally Enhanced Knowledge Base from Wikipedia:
    Extended Abstract'
- pid: 57faa0e5f99442d1723d2c5ccb70b1461987a7ed
  title: Autonomously semantifying wikipedia
- pid: 3ff2862a8121cc823a8eb72f3e0a97bbf25c82ec
  title: 'Building Watson: An Overview of the DeepQA Project'
slug: DBpedia-A-large-scale,-multilingual-knowledge-base-Lehmann-Isele
title: DBpedia - A large-scale, multilingual knowledge base extracted from Wikipedia
url: https://www.semanticscholar.org/paper/DBpedia-A-large-scale,-multilingual-knowledge-base-Lehmann-Isele/d2946a868682e4141beabc288d79253ae254c6e1?sort=total-citations
venue: Semantic Web
year: 2015
