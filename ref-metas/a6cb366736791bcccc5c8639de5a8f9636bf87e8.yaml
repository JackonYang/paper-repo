authors:
- Diederik P. Kingma
- Jimmy Ba
badges:
- id: OPEN_ACCESS
corpusId: 6628106
fieldsOfStudy:
- Computer Science
numCitedBy: 89976
numCiting: 30
paperAbstract: We introduce Adam, an algorithm for first-order gradient-based optimization
  of stochastic objective functions, based on adaptive estimates of lower-order moments.
  The method is straightforward to implement, is computationally efficient, has little
  memory requirements, is invariant to diagonal rescaling of the gradients, and is
  well suited for problems that are large in terms of data and/or parameters. The
  method is also appropriate for non-stationary objectives and problems with very
  noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations
  and typically require little tuning. Some connections to related algorithms, on
  which Adam was inspired, are discussed. We also analyze the theoretical convergence
  properties of the algorithm and provide a regret bound on the convergence rate that
  is comparable to the best known results under the online convex optimization framework.
  Empirical results demonstrate that Adam works well in practice and compares favorably
  to other stochastic optimization methods. Finally, we discuss AdaMax, a variant
  of Adam based on the infinity norm.
ref_count: 30
references:
- pid: 413c1142de9d91804d6d11c67ff3fed59c9fc279
  title: Adaptive Subgradient Methods for Online Learning and Stochastic Optimization
- pid: aa7bfd2304201afbb19971ebde87b17e40242e91
  title: On the importance of initialization and momentum in deep learning
- pid: e8f95ccfd13689f672c39dca3eccf1c484533bcc
  title: Revisiting Natural Gradient for Deep Networks
- pid: 5f5dc5b9a2ba710937e2c413b37b053cd673df02
  title: Auto-Encoding Variational Bayes
- pid: 981ce6b655cc06416ff6bf7fac8c6c2076fd7fac
  title: Identifying and attacking the saddle point problem in high-dimensional non-convex
    optimization
- pid: e5a685f40338f9c2f3e68e142efa217aad16dd56
  title: No more pesky learning rates
- pid: 5a767a341364de1f75bea85e0b12ba7d3586a461
  title: Natural Gradient Works Efficiently in Learning
- pid: ec92efde21707ddf4b81f301cd58e2051c1a2443
  title: Fast dropout training
- pid: 8729441d734782c3ed532a7d2d9611b438c0a09a
  title: 'ADADELTA: An Adaptive Learning Rate Method'
- pid: abd1c342495432171beb7ca8fd9551ef13cbd0ff
  title: ImageNet classification with deep convolutional neural networks
- pid: 4177ec52d1b80ed57f2e72b0f9a42365f1a8598d
  title: Speech recognition with deep recurrent neural networks
- pid: 1366de5bb112746a555e9c0cd00de3ad8628aea8
  title: Improving neural networks by preventing co-adaptation of feature detectors
- pid: 46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e
  title: Reducing the Dimensionality of Data with Neural Networks
- pid: 649d03490ef72c5274e3bccd03d7a299d2f8da91
  title: Learning Word Vectors for Sentiment Analysis
- pid: c50dca78e97e335d362d6b991ae0e1448914e9a3
  title: Reducing the Dimensionality of Data with Neural
- pid: 31868290adf1c000c611dfc966b514d5a34e8d23
  title: 'Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared
    Views of Four Research Groups'
- pid: 89b1f4740ae37fd04f6ac007577bdd34621f0861
  title: Generating Sequences With Recurrent Neural Networks
- pid: 6dc61f37ecc552413606d8c89ffbc46ec98ed887
  title: Acceleration of stochastic approximation by averaging
slug: Adam:-A-Method-for-Stochastic-Optimization-Kingma-Ba
title: 'Adam: A Method for Stochastic Optimization'
url: https://www.semanticscholar.org/paper/Adam:-A-Method-for-Stochastic-Optimization-Kingma-Ba/a6cb366736791bcccc5c8639de5a8f9636bf87e8?sort=total-citations
venue: ICLR
year: 2015
