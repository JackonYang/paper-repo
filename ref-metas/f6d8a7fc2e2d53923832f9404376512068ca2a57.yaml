authors:
- M. I. Jordan
- R. Jacobs
badges:
- id: OPEN_ACCESS
corpusId: 67000854
fieldsOfStudy:
- Computer Science
numCitedBy: 2136
numCiting: 41
paperAbstract: We present a tree-structured architecture for supervised learning.
  The statistical model underlying the architecture is a hierarchical mixture model
  in which both the mixture coefficients and the mixture components are generalized
  linear models (GLIM's). Learning is treated as a maximum likelihood problem; in
  particular, we present an Expectation-Maximization (EM) algorithm for adjusting
  the parameters of the architecture. We also develop an on-line learning algorithm
  in which the parameters are updated incrementally. Comparative simulation results
  are presented in the robot dynamics domain.
ref_count: 43
references:
- pid: c8d90974c3f3b40fa05e322df2905fc16204aa56
  title: Adaptive Mixtures of Local Experts
- pid: 59fa47fc237a0781b4bf1c84fedb728d20db26a1
  title: 'Soft competitive adaptation: neural network learning algorithms based on
    fitting statistical mixtures'
- pid: cf895330739ec25aa4077ca375daa2cf3d265215
  title: Fast Learning in Multi-Resolution Hierarchies
- pid: 57c4baa5528ba805fc27eee86613c99503978fed
  title: Maximum Likelihood Competitive Learning
- pid: 54323bf565cea5d2aaee88a03ec9d1d3444a9bfd
  title: Mixture densities, maximum likelihood, and the EM algorithm
- pid: 61039fd2773a00e111d2121a63982a7b7d0b9f92
  title: Learning classification trees
- pid: 1f462943c8d0af69c12a09058251848324135e5a
  title: Probabilistic Interpretation of Feedforward Classification Network Outputs,
    with Relationships to Statistical Pattern Recognition
- pid: a34e35dbbc6911fa7b94894dffdc0076a261b6f0
  title: Neural Networks and the Bias/Variance Dilemma
- pid: 8592e46a5435d18bba70557846f47290b34c1aa5
  title: Learning and relearning in Boltzmann machines
- pid: b07ce649d6f6eb636872527104b0209d3edc8188
  title: Pattern classification and scene analysis
- pid: 54a1f6ab4cc6cb749c2b8d15c1dd3449e072362f
  title: Statistical analysis of finite mixture distributions
- pid: f22f6972e66bdd2e769fa64b0df0a13063c0c101
  title: Multilayer feedforward networks are universal approximators
- pid: 84dae6a2870c68005732b9db6890f375490f2d4e
  title: Inferring Decision Trees Using the Minimum Description Length Principle
- pid: d36efb9ad91e00faa334b549ce989bfae7e2907a
  title: Maximum likelihood from incomplete data via the EM - algorithm plus discussions
    on the paper
slug: Hierarchical-Mixtures-of-Experts-and-the-EM-Jordan-Jacobs
title: Hierarchical mixtures of experts and the EM algorithm
url: https://www.semanticscholar.org/paper/Hierarchical-Mixtures-of-Experts-and-the-EM-Jordan-Jacobs/f6d8a7fc2e2d53923832f9404376512068ca2a57?sort=total-citations
venue: Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya,
  Japan)
year: 1993
