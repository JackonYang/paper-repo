authors:
- D. Mackay
badges:
- id: OPEN_ACCESS
corpusId: 16543854
fieldsOfStudy:
- Computer Science
numCitedBy: 2590
numCiting: 29
paperAbstract: A quantitative and practical Bayesian framework is described for learning
  of mappings in feedforward networks. The framework makes possible (1) objective
  comparisons between solutions using alternative network architectures, (2) objective
  stopping rules for network pruning or growing procedures, (3) objective choice of
  magnitude and type of weight decay terms or additive regularizers (for penalizing
  large weights, etc.), (4) a measure of the effective number of well-determined parameters
  in a model, (5) quantified estimates of the error bars on network parameters and
  on network output, and (6) objective comparisons with alternative learning and interpolation
  models such as splines and radial basis functions. The Bayesian "evidence" automatically
  embodies "Occam's razor," penalizing overflexible and overcomplex models. The Bayesian
  approach helps detect poor underlying assumptions in learning models. For learning
  models well matched to a problem, a good correlation between generalization ability
  and the Bayesian evidence is obtained.
ref_count: 29
references:
- pid: b10440620da8a43a1b97e3da4b1ff13746306475
  title: 'Consistent inference of probabilities in layered networks: predictions and
    generalizations'
- pid: 7abda1941534d3bb558dd959025d67f1df526303
  title: The Evidence Framework Applied to Classification Networks
- pid: 8e68c54f39e87daf3a8bdc0ee005aece3c652d11
  title: Bayesian Interpolation
- pid: 59fa47fc237a0781b4bf1c84fedb728d20db26a1
  title: 'Soft competitive adaptation: neural network learning algorithms based on
    fitting statistical mixtures'
- pid: 2a1e1da81b535e1bead3fc2ab6af8b07877823b9
  title: Exact Calculation of the Hessian Matrix for the Multilayer Perceptron
- pid: 82fa37d5be8e747131a5857992cc33bb95469ce3
  title: Developments in Maximum Entropy Data Analysis
- pid: 052b1d8ce63b07fec3de9dbb583772d860b7c769
  title: Learning representations by back-propagating errors
- pid: e3cd36c092abd65d6ac8e648f3468eeee90ee1fc
  title: Learning from hints in neural networks
- pid: e7297db245c3feb1897720b173a59fe7e36babb7
  title: Optimal Brain Damage
- pid: f707a81a278d1598cd0a4493ba73f22dcdf90639
  title: Generalization by Weight-Elimination with Application to Forecasting
- pid: 7f027c678076d7f2fd817f081079a334466449b1
  title: 'The Vapnik-Chervonenkis Dimension: Information versus Complexity in Learning'
- pid: 8592e46a5435d18bba70557846f47290b34c1aa5
  title: Learning and relearning in Boltzmann machines
- pid: 3e9229dd827dda0d462dffbdec7fdf50b724d587
  title: The Eigenvalues of Mega-dimensional Matrices
- pid: 0a55b22bc98bc997bc31af0244038643e2bae74a
  title: Received
slug: A-Practical-Bayesian-Framework-for-Backpropagation-Mackay
title: A Practical Bayesian Framework for Backpropagation Networks
url: https://www.semanticscholar.org/paper/A-Practical-Bayesian-Framework-for-Backpropagation-Mackay/b959164d1efca4b73986ba5d21e664aadbbc0457?sort=total-citations
venue: Neural Computation
year: 1992
