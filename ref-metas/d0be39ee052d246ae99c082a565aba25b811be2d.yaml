authors:
- Yoshua Bengio
- P. Simard
- P. Frasconi
badges:
- id: OPEN_ACCESS
corpusId: 206457500
fieldsOfStudy:
- Computer Science
numCitedBy: 6144
numCiting: 50
paperAbstract: Recurrent neural networks can be used to map input sequences to output
  sequences, such as for recognition, production or prediction problems. However,
  practical difficulties have been reported in training recurrent neural networks
  to perform tasks in which the temporal contingencies present in the input/output
  sequences span long intervals. We show why gradient based learning algorithms face
  an increasingly difficult problem as the duration of the dependencies to be captured
  increases. These results expose a trade-off between efficient learning by gradient
  descent and latching on information for long periods. Based on an understanding
  of this problem, alternatives to standard gradient descent are considered.
ref_count: 50
references:
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 3832
  pid: ce9a21b93ba29d4145a8ef6bf401e77f261848de
  title: A Learning Algorithm for Continually Running Fully Recurrent Neural Networks
  year: 1989
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 303
  pid: 7fb4d10f6d2ee3133135958aefd50bf22dcced9d
  title: A Focused Backpropagation Algorithm for Temporal Pattern Recognition
  year: 1989
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 160
  pid: e141d68065ce638f9fc4f006eab2f66711e89768
  title: Induction of Multiscale Temporal Structure
  year: 1991
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 62
  pid: af7802a50a8c294ebfd539ad72158475e5ecd9f2
  title: The "Moving Targets" Training Algorithm
  year: 1989
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 151
  pid: 44f2679f8169e7f6449c52e058ebe6a45838b3c0
  title: Learning Process in an Asymmetric Threshold Network
  year: 1986
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 261
  pid: ffbe67c217967b6bfb0a5ecc0dc4cdd5cda65776
  title: Global optimization of a neural network-hidden Markov model hybrid
  year: 1992
- fieldsOfStudy:
  - Biology
  numCitedBy: 4647
  pid: 69d7086300e7f5322c06f2f242a565b3a182efb5
  title: In Advances in Neural Information Processing Systems
  year: 1990
- fieldsOfStudy:
  - Physics
  numCitedBy: 39631
  pid: dd5061631a4d11fa394f4421700ebf7e78dcbc59
  title: Optimization by Simulated Annealing
  year: 1983
- fieldsOfStudy:
  - Biology
  numCitedBy: 19355
  pid: 111fd833a4ae576cfdbb27d87d2f8fc0640af355
  title: Learning internal representations by error propagation
  year: 1986
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 124
  pid: 56b771c4c3a54910dc3e7ff838940de89ed282db
  title: Learning processes in an asymmetric threshold network
  year: 1986
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 411
  pid: 589d377b23e2bdae7ad161b36a5d6613bcfccdde
  title: Improving the convergence of back-propagation learning with second-order
    methods
  year: 1989
slug: Learning-long-term-dependencies-with-gradient-is-Bengio-Simard
title: Learning long-term dependencies with gradient descent is difficult
url: https://www.semanticscholar.org/paper/Learning-long-term-dependencies-with-gradient-is-Bengio-Simard/d0be39ee052d246ae99c082a565aba25b811be2d?sort=total-citations
venue: IEEE Trans. Neural Networks
year: 1994
