authors:
- Marc'Aurelio Ranzato
- Y-Lan Boureau
- Yann LeCun
badges:
- id: OPEN_ACCESS
corpusId: 5867279
fieldsOfStudy:
- Computer Science
numCitedBy: 818
numCiting: 26
paperAbstract: Unsupervised learning algorithms aim to discover the structure hidden
  in the data, and to learn representations that are more suitable as input to a supervised
  machine than the raw input. Many unsupervised methods are based on reconstructing
  the input from the representation, while constraining the representation to have
  certain desirable properties (e.g. low dimension, sparsity, etc). Others are based
  on approximating density by stochastically reconstructing the input from the representation.
  We describe a novel and efficient algorithm to learn sparse representations, and
  compare it theoretically and experimentally with a similar machine trained probabilistically,
  namely a Restricted Boltzmann Machine. We propose a simple criterion to compare
  and select different unsupervised machines based on the trade-off between the reconstruction
  error and the information content of the representation. We demonstrate this method
  by extracting features from a dataset of handwritten numerals, and from a dataset
  of natural image patches. We show that by stacking multiple levels of such machines
  and by training sequentially, high-order dependencies between the input observed
  variables can be captured.
ref_count: 26
references:
- pid: 8978cf7574ceb35f4c3096be768c7547b28a35d0
  title: A Fast Learning Algorithm for Deep Belief Nets
- pid: 932c2a02d462abd75af018125413b1ceaa1ee3f4
  title: Efficient Learning of Sparse Representations with an Energy-Based Model
- pid: 355d44f53428b1ac4fb2ab468d593c720640e5bd
  title: Greedy Layer-Wise Training of Deep Networks
- pid: b95799a25def71b100bd12e7ebb32cbcee6590bf
  title: Energy-Based Models for Sparse Overcomplete Representations
- pid: 42d906c733f273109c0ed716a5ef6e2a379beb26
  title: Learning Overcomplete Representations
- pid: 6fdb77260fc83dff91c44fea0f31a2cb8ed13d04
  title: Scaling learning algorithms towards AI
- pid: 29bae9472203546847ec1352a604566d0f602728
  title: Learning the parts of objects by non-negative matrix factorization
- pid: 3dc3a0efe58eaf8564ca1965c0ffd23ec495b83f
  title: Autoencoders, Minimum Description Length and Helmholtz Free Energy
- pid: 46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e
  title: Reducing the Dimensionality of Data with Neural Networks
- pid: 2805537bec87a6177037b18f9a3a9d3f1038867b
  title: 'Sparse coding with an overcomplete basis set: A strategy employed by V1?'
- pid: 7fc604e1a3e45cd2d2742f96d62741930a363efa
  title: A Tutorial on Energy-Based Learning
- pid: 9360e5ce9c98166bb179ad479a9d2919ff13d022
  title: Training Products of Experts by Minimizing Contrastive Divergence
- pid: 62f4d89a3c1441b47170c7e1380137fb388d0799
  title: Modeling the manifolds of images of handwritten digits
slug: Sparse-Feature-Learning-for-Deep-Belief-Networks-Ranzato-Boureau
title: Sparse Feature Learning for Deep Belief Networks
url: https://www.semanticscholar.org/paper/Sparse-Feature-Learning-for-Deep-Belief-Networks-Ranzato-Boureau/41fef1a197fab9684a4608b725d3ae72e1ab4b39?sort=total-citations
venue: NIPS
year: 2007
