authors:
- Tim Cooijmans
- Nicolas Ballas
- "C\xE9sar Laurent"
- Aaron C. Courville
badges:
- id: OPEN_ACCESS
corpusId: 1107124
fieldsOfStudy:
- Computer Science
numCitedBy: 341
numCiting: 36
paperAbstract: We propose a reparameterization of LSTM that brings the benefits of
  batch normalization to recurrent neural networks. Whereas previous works only apply
  batch normalization to the input-to-hidden transformation of RNNs, we demonstrate
  that it is both possible and beneficial to batch-normalize the hidden-to-hidden
  transition, thereby reducing internal covariate shift between time steps. We evaluate
  our proposal on various sequential problems such as sequence classification, language
  modeling and question answering. Our empirical results show that our batch-normalized
  LSTM consistently leads to faster convergence and improved generalization.
ref_count: 36
references:
- pid: d46b81707786d18499f911b4ab72bb10c65406ba
  title: A Simple Way to Initialize Recurrent Networks of Rectified Linear Units
- pid: 9f0687bcd0a7d7fc91b8c5d36c003a38b8853105
  title: 'Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations'
- pid: 4d376d6978dad0374edfa6709c9556b42d3594d3
  title: 'Batch Normalization: Accelerating Deep Network Training by Reducing Internal
    Covariate Shift'
- pid: 97fb4e3d45bb098e27e0071448b6152217bd35a5
  title: Layer Normalization
- pid: d0be39ee052d246ae99c082a565aba25b811be2d
  title: Learning long-term dependencies with gradient descent is difficult
- pid: 4ef03716945bd3907458efbe1bbf8928dafc1efc
  title: 'Regularization and nonlinearities for neural language models: when are they
    needed?'
- pid: 84069287da0a6b488b8c933f3cb5be759cb6237e
  title: On the difficulty of training recurrent neural networks
- pid: 0d6203718c15f137fda2f295c96269bc2b254644
  title: Learning Recurrent Neural Networks with Hessian-Free Optimization
- pid: 1fd7fc06653723b05abe5f3d1de393ddcf6bdddb
  title: SUBWORD LANGUAGE MODELING WITH NEURAL NETWORKS
- pid: 65eee67dee969fdf8b44c87c560d66ad4d78e233
  title: Hierarchical Multiscale Recurrent Neural Networks
- pid: d1505c6123c102e53eb19dff312cb25cea840b72
  title: Teaching Machines to Read and Comprehend
- pid: 0b544dfe355a5070b60986319a3f51fb45d1348e
  title: "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical\
    \ Machine Translation"
- pid: 44d2abe2175df8153f465f6c39b68b76a0d40ab9
  title: Long Short-Term Memory
- pid: 5f425b7abf2ed3172ed060df85bb1885860a297e
  title: Describing Videos by Exploiting Temporal Structure
- pid: a6cb366736791bcccc5c8639de5a8f9636bf87e8
  title: 'Adam: A Method for Stochastic Optimization'
- pid: 8ff840a40d3f1557c55c19d4d636da77103168ce
  title: 'Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin'
- pid: 4d8f2d14af5991d4f0d050d22216825cac3157bd
  title: 'Show, Attend and Tell: Neural Image Caption Generation with Visual Attention'
- pid: fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5
  title: Neural Machine Translation by Jointly Learning to Align and Translate
- pid: 855d0f722d75cc56a66a00ede18ace96bafee6bd
  title: 'Theano: new features and speed improvements'
- pid: 89b1f4740ae37fd04f6ac007577bdd34621f0861
  title: Generating Sequences With Recurrent Neural Networks
- pid: 59b81ff81da55efc724c84ddc9d3ffd8e57a8d0e
  title: 'Blocks and Fuel: Frameworks for deep learning'
- pid: bcd857d75841aa3e92cd4284a8818aba9f6c0c3f
  title: Published as a conference paper at ICLR 2018 S IMULATING A CTION D YNAMICS
    WITH N EURAL P ROCESS N ETWORKS
- pid: 6b570069f14c7588e066f7138e1f21af59d62e61
  title: 'Theano: A Python framework for fast computation of mathematical expressions'
- pid: 0b44fcbeea9415d400c5f5789d6b892b6f98daff
  title: 'Building a Large Annotated Corpus of English: The Penn Treebank'
- pid: 3f3d13e95c25a8f6a753e38dfce88885097cbd43
  title: Untersuchungen zu dynamischen neuronalen Netzen
slug: Recurrent-Batch-Normalization-Cooijmans-Ballas
title: Recurrent Batch Normalization
url: https://www.semanticscholar.org/paper/Recurrent-Batch-Normalization-Cooijmans-Ballas/952454718139dba3aafc6b3b67c4f514ac3964af?sort=total-citations
venue: ICLR
year: 2017
