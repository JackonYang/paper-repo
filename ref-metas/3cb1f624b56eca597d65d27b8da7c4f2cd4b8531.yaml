authors:
- A. Smeaton
- P. Over
- Wessel Kraaij
badges:
- id: OPEN_ACCESS
corpusId: 7677566
fieldsOfStudy:
- Computer Science
numCitedBy: 1398
numCiting: 39
paperAbstract: The TREC Video Retrieval Evaluation (TRECVid)is an international benchmarking
  activity to encourage research in video information retrieval by providing a large
  test collection, uniform scoring procedures, and a forum for organizations 1 interested
  in comparing their results. TRECVid completed its fifth annual cycle at the end
  of 2005 and in 2006 TRECVid will involve almost 70 research organizations, universities
  and other consortia. Throughout its existence, TRECVid has benchmarked both interactive
  and automatic/manual searching for shots from within a video corpus,automatic detection
  of a variety of semantic and low-level video features, shot boundary detection and
  the detection of story boundaries in broadcast TV news. This paper will give an
  introduction to information retrieval (IR) evaluation from both a user and a system
  perspective, high-lighting that system evaluation is by far the most prevalent type
  of evaluation carried out. We also include a summary of TRECVid as an example of
  a system evaluation bench-marking campaign and this allows us to discuss whether
  such campaigns are a good thing or a bad thing. There are arguments for and against
  these campaigns and we present some of them in the paper concluding that on balance
  they have had a very positive impact on research progress.
ref_count: 39
references: []
slug: Evaluation-campaigns-and-TRECVid-Smeaton-Over
title: Evaluation campaigns and TRECVid
url: https://www.semanticscholar.org/paper/Evaluation-campaigns-and-TRECVid-Smeaton-Over/3cb1f624b56eca597d65d27b8da7c4f2cd4b8531?sort=total-citations
venue: MIR '06
year: 2006
