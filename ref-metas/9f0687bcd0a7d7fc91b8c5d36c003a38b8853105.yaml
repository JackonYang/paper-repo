authors:
- David Krueger
- Tegan Maharaj
- J'anos Kram'ar
- M. Pezeshki
- Nicolas Ballas
- Nan Rosemary Ke
- Anirudh Goyal
- Yoshua Bengio
- H. Larochelle
- Aaron C. Courville
- Chris Pal
badges:
- id: OPEN_ACCESS
corpusId: 12200521
fieldsOfStudy:
- Computer Science
numCitedBy: 266
numCiting: 47
paperAbstract: We propose zoneout, a novel method for regularizing RNNs. At each timestep,
  zoneout stochastically forces some hidden units to maintain their previous values.
  Like dropout, zoneout uses random noise to train a pseudo-ensemble, improving generalization.
  But by preserving instead of dropping hidden units, gradient information and state
  information are more readily propagated through time, as in feedforward stochastic
  depth networks. We perform an empirical investigation of various RNN regularizers,
  and find that zoneout gives significant performance improvements across tasks. We
  achieve competitive results with relatively simple models in character- and word-level
  language modelling on the Penn Treebank and Text8 datasets, and combining with recurrent
  batch normalization yields state-of-the-art results on permuted sequential MNIST.
ref_count: 47
references:
- pid: 952454718139dba3aafc6b3b67c4f514ac3964af
  title: Recurrent Batch Normalization
- pid: 11540131eae85b2e11d53df7f1360eeb6476e7f4
  title: 'Learning to Forget: Continual Prediction with LSTM'
- pid: 5522764282c85aea422f1c4dc92ff7e0ca6987bc
  title: A Clockwork RNN
- pid: d46b81707786d18499f911b4ab72bb10c65406ba
  title: A Simple Way to Initialize Recurrent Networks of Rectified Linear Units
- pid: c0b624c46b51920dfec5aa02cc86323c0beb0df5
  title: Dropout Improves Recurrent Neural Networks for Handwriting Recognition
- pid: 9665247ea3421929f9b6ad721f139f11edb1dbb8
  title: Learning Longer Memory in Recurrent Neural Networks
- pid: 0c1f9ca23f4f09ecfc44bcc8ca1c2736624f4652
  title: A Theoretically Grounded Application of Dropout in Recurrent Neural Networks
- pid: f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97
  title: Recurrent Neural Network Regularization
- pid: 533ee188324b833e059cb59b654e6160776d5812
  title: How to Construct Deep Recurrent Neural Networks
- pid: c5145b1d15fea9340840cc8bb6f0e46e8934827f
  title: Understanding the exploding gradient problem
- pid: ec92efde21707ddf4b81f301cd58e2051c1a2443
  title: Fast dropout training
- pid: 44d2abe2175df8153f465f6c39b68b76a0d40ab9
  title: Long Short-Term Memory
- pid: 5f425b7abf2ed3172ed060df85bb1885860a297e
  title: Describing Videos by Exploiting Temporal Structure
- pid: b13813b49f160e1a2010c44bd4fb3d09a28446e3
  title: Hierarchical Recurrent Neural Networks for Long-Term Dependencies
- pid: 1366de5bb112746a555e9c0cd00de3ad8628aea8
  title: Improving neural networks by preventing co-adaptation of feature detectors
- pid: d0be39ee052d246ae99c082a565aba25b811be2d
  title: Learning long-term dependencies with gradient descent is difficult
- pid: 65eee67dee969fdf8b44c87c560d66ad4d78e233
  title: Hierarchical Multiscale Recurrent Neural Networks
- pid: 51db1f3c8dfc7d4077da39c96bb90a6358128111
  title: Deep Networks with Stochastic Depth
- pid: 62c76ca0b2790c34e85ba1cce09d47be317c7235
  title: Estimating or Propagating Gradients Through Stochastic Neurons for Conditional
    Computation
- pid: a6cb366736791bcccc5c8639de5a8f9636bf87e8
  title: 'Adam: A Method for Stochastic Optimization'
- pid: 34f25a8704614163c4095b3ee2fc969b60de4698
  title: 'Dropout: a simple way to prevent neural networks from overfitting'
- pid: 891ce1687e2befddd19f54e4eef1d3f39c8dbaf7
  title: Character-Aware Neural Language Models
- pid: 97fb4e3d45bb098e27e0071448b6152217bd35a5
  title: Layer Normalization
- pid: 59b81ff81da55efc724c84ddc9d3ffd8e57a8d0e
  title: 'Blocks and Fuel: Frameworks for deep learning'
- pid: 2c03df8b48bf3fa39054345bafabfeff15bfd11d
  title: Deep Residual Learning for Image Recognition
- pid: fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5
  title: Neural Machine Translation by Jointly Learning to Align and Translate
- pid: 94e3e7bc3d23276f0ee2d1cb8f9d14aa19668d5f
  title: Under Review as a Conference Paper at Iclr 2017 Delving into Transferable
    Adversarial Ex- Amples and Black-box Attacks
- pid: 6b570069f14c7588e066f7138e1f21af59d62e61
  title: 'Theano: A Python framework for fast computation of mathematical expressions'
- pid: 0b44fcbeea9415d400c5f5789d6b892b6f98daff
  title: 'Building a Large Annotated Corpus of English: The Penn Treebank'
- pid: 3f3d13e95c25a8f6a753e38dfce88885097cbd43
  title: Untersuchungen zu dynamischen neuronalen Netzen
slug: Zoneout:-Regularizing-RNNs-by-Randomly-Preserving-Krueger-Maharaj
title: 'Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations'
url: https://www.semanticscholar.org/paper/Zoneout:-Regularizing-RNNs-by-Randomly-Preserving-Krueger-Maharaj/9f0687bcd0a7d7fc91b8c5d36c003a38b8853105?sort=total-citations
venue: ICLR
year: 2017
