authors:
- Xiaodong Liu
- Pengcheng He
- Weizhu Chen
- Jianfeng Gao
badges:
- id: OPEN_ACCESS
corpusId: 59523594
fieldsOfStudy:
- Computer Science
numCitedBy: 732
numCiting: 38
paperAbstract: In this paper, we present a Multi-Task Deep Neural Network (MT-DNN)
  for learning representations across multiple natural language understanding (NLU)
  tasks. MT-DNN not only leverages large amounts of cross-task data, but also benefits
  from a regularization effect that leads to more general representations to help
  adapt to new tasks and domains. MT-DNN extends the model proposed in Liu et al.
  (2015) by incorporating a pre-trained bidirectional transformer language model,
  known as BERT (Devlin et al., 2018). MT-DNN obtains new state-of-the-art results
  on ten NLU tasks, including SNLI, SciTail, and eight out of nine GLUE tasks, pushing
  the GLUE benchmark to 82.7% (2.2% absolute improvement) as of February 25, 2019
  on the latest GLUE test set. We also demonstrate using the SNLI and SciTail datasets
  that the representations learned by MT-DNN allow domain adaptation with substantially
  fewer in-domain labels than the pre-trained BERT representations. Our code and pre-trained
  models will be made publicly available.
ref_count: 39
references:
- pid: 1c71771c701aadfd72c5866170a9f5d71464bb88
  title: Unified Language Model Pre-training for Natural Language Understanding and
    Generation
- pid: df2b0e26d0599ce3e70df8a9da02e51594e0e992
  title: 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'
- pid: b47381e04739ea3f392ba6c8faaf64105493c196
  title: 'Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data
    Tasks'
- pid: 93b8da28d006415866bf48f9a6e06b5242129195
  title: 'GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language
    Understanding'
- pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  title: Attention is All you Need
- pid: cd18800a0fe0b668a1cc19f2ec95b5003d0a5035
  title: Improving Language Understanding by Generative Pre-Training
- pid: d76c07211479e233f7c6a6f32d5346c983c5598f
  title: Multi-task Sequence to Sequence Learning
- pid: c3b8367a80181e28c95630b9b63060d895de08ff
  title: Representation Learning Using Multi-Task Deep Neural Networks for Semantic
    Classification and Information Retrieval
- pid: a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096
  title: 'SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual
    Focused Evaluation'
- pid: 687bac2d3320083eb4530bf18bb8f8f721477600
  title: Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank
- pid: 3febb2bed8865945e7fddc99efd791887bb7e14f
  title: Deep Contextualized Word Representations
- pid: bc1022b031dc6c7019696492e8116598097a8c12
  title: Natural Language Processing (Almost) from Scratch
- pid: f04df4e20a18358ea2f689b4c129781628ef7fc1
  title: A large annotated corpus for learning natural language inference
- pid: 05dd7254b632376973f3a1b4d39485da17814df5
  title: 'SQuAD: 100,000+ Questions for Machine Comprehension of Text'
- pid: fdb813d8b927bdd21ae1858cafa6c34b66a36268
  title: Learning deep structured semantic models for web search using clickthrough
    data
- pid: cb0f3ee1e98faf92429d601cdcd76c69c1e484eb
  title: Neural Network Acceptability Judgments
- pid: a6cb366736791bcccc5c8639de5a8f9636bf87e8
  title: 'Adam: A Method for Stochastic Optimization'
- pid: 475354f10798f110d34792b6d88f31d6d5cb099e
  title: Automatically Constructing a Corpus of Sentential Paraphrases
- pid: 128cb6b891aee1b5df099acb48e2efecfcff689f
  title: The Winograd Schema Challenge
- pid: 4d031e39474f2b622e87316314cb6c33eeda0786
  title: Multitask Learning
slug: Multi-Task-Deep-Neural-Networks-for-Natural-Liu-He
title: Multi-Task Deep Neural Networks for Natural Language Understanding
url: https://www.semanticscholar.org/paper/Multi-Task-Deep-Neural-Networks-for-Natural-Liu-He/658721bc13b0fa97366d38c05a96bf0a9f4bb0ac?sort=total-citations
venue: ACL
year: 2019
