authors:
- Peter Shaw
- Jakob Uszkoreit
- Ashish Vaswani
badges:
- id: OPEN_ACCESS
corpusId: 3725815
fieldsOfStudy:
- Computer Science
numCitedBy: 923
numCiting: 17
paperAbstract: Relying entirely on an attention mechanism, the Transformer introduced
  by Vaswani et al. (2017) achieves state-of-the-art results for machine translation.
  In contrast to recurrent and convolutional neural networks, it does not explicitly
  model relative or absolute position information in its structure. Instead, it requires
  adding representations of absolute positions to its inputs. In this work we present
  an alternative approach, extending the self-attention mechanism to efficiently consider
  representations of the relative positions, or distances between sequence elements.
  On the WMT 2014 English-to-German and English-to-French translation tasks, this
  approach yields improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations,
  respectively. Notably, we observe that combining relative and absolute position
  representations yields no further improvement in translation quality. We describe
  an efficient implementation of our method and cast it as an instance of relation-aware
  self-attention mechanisms that can generalize to arbitrary graph-labeled inputs.
ref_count: 17
references:
- pid: 93499a7c7f699b6630a86fad964536f9423bb6d0
  title: Effective Approaches to Attention-based Neural Machine Translation
- pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  title: Attention is All you Need
- pid: 4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e
  title: End-To-End Memory Networks
- pid: 33998aff64ce51df8dee45989cdca4b6b1329ec4
  title: Graph Attention Networks
- pid: 23ffaa0fe06eae05817f527a47ac3291077f9e58
  title: Rethinking the Inception Architecture for Computer Vision
- pid: dbde7dfa6cae81df8ac19ef500c42db96c3d1edd
  title: 'Google''s Neural Machine Translation System: Bridging the Gap between Human
    and Machine Translation'
- pid: cea967b59209c6be22829699f05b8b1ac4dc092d
  title: Sequence to Sequence Learning with Neural Networks
- pid: 98445f4172659ec5e891e031d8202c102135c644
  title: Neural Machine Translation in Linear Time
- pid: fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5
  title: Neural Machine Translation by Jointly Learning to Align and Translate
- pid: 43428880d75b3a14257c3ee9bda054e61eb869c0
  title: Convolutional Sequence to Sequence Learning
- pid: 2cd8e8f510c89c7c18268e8ad51c061e459ad321
  title: A Decomposable Attention Model for Natural Language Inference
- pid: a6cb366736791bcccc5c8639de5a8f9636bf87e8
  title: 'Adam: A Method for Stochastic Optimization'
- pid: 0b544dfe355a5070b60986319a3f51fb45d1348e
  title: "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical\
    \ Machine Translation"
- pid: 97fb4e3d45bb098e27e0071448b6152217bd35a5
  title: Layer Normalization
slug: Self-Attention-with-Relative-Position-Shaw-Uszkoreit
title: Self-Attention with Relative Position Representations
url: https://www.semanticscholar.org/paper/Self-Attention-with-Relative-Position-Shaw-Uszkoreit/c8efcc854d97dfc2a42b83316a2109f9d166e43f?sort=total-citations
venue: NAACL
year: 2018
