authors:
- Peter Shaw
- Jakob Uszkoreit
- Ashish Vaswani
badges:
- id: OPEN_ACCESS
corpusId: 3725815
fieldsOfStudy:
- Computer Science
meta_key: self-attention-with-relative-position-representations
numCitedBy: 923
numCiting: 17
paperAbstract: Relying entirely on an attention mechanism, the Transformer introduced
  by Vaswani et al. (2017) achieves state-of-the-art results for machine translation.
  In contrast to recurrent and convolutional neural networks, it does not explicitly
  model relative or absolute position information in its structure. Instead, it requires
  adding representations of absolute positions to its inputs. In this work we present
  an alternative approach, extending the self-attention mechanism to efficiently consider
  representations of the relative positions, or distances between sequence elements.
  On the WMT 2014 English-to-German and English-to-French translation tasks, this
  approach yields improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations,
  respectively. Notably, we observe that combining relative and absolute position
  representations yields no further improvement in translation quality. We describe
  an efficient implementation of our method and cast it as an instance of relation-aware
  self-attention mechanisms that can generalize to arbitrary graph-labeled inputs.
ref_count: 17
references:
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 5892
  pid: 93499a7c7f699b6630a86fad964536f9423bb6d0
  show_ref_link: true
  title: Effective Approaches to Attention-based Neural Machine Translation
  year: 2015
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 35148
  pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  show_ref_link: true
  title: Attention is All you Need
  year: 2017
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1990
  pid: 4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e
  show_ref_link: true
  title: End-To-End Memory Networks
  year: 2015
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 5524
  pid: 33998aff64ce51df8dee45989cdca4b6b1329ec4
  show_ref_link: true
  title: Graph Attention Networks
  year: 2018
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 15542
  pid: 23ffaa0fe06eae05817f527a47ac3291077f9e58
  show_ref_link: true
  title: Rethinking the Inception Architecture for Computer Vision
  year: 2016
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 4645
  pid: dbde7dfa6cae81df8ac19ef500c42db96c3d1edd
  show_ref_link: true
  title: 'Google''s Neural Machine Translation System: Bridging the Gap between Human
    and Machine Translation'
  year: 2016
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 14880
  pid: cea967b59209c6be22829699f05b8b1ac4dc092d
  show_ref_link: true
  title: Sequence to Sequence Learning with Neural Networks
  year: 2014
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 454
  pid: 98445f4172659ec5e891e031d8202c102135c644
  show_ref_link: true
  title: Neural Machine Translation in Linear Time
  year: 2016
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 19339
  pid: fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5
  show_ref_link: true
  title: Neural Machine Translation by Jointly Learning to Align and Translate
  year: 2015
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 2422
  pid: 43428880d75b3a14257c3ee9bda054e61eb869c0
  show_ref_link: true
  title: Convolutional Sequence to Sequence Learning
  year: 2017
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1059
  pid: 2cd8e8f510c89c7c18268e8ad51c061e459ad321
  show_ref_link: true
  title: A Decomposable Attention Model for Natural Language Inference
  year: 2016
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 90054
  pid: a6cb366736791bcccc5c8639de5a8f9636bf87e8
  show_ref_link: true
  title: 'Adam: A Method for Stochastic Optimization'
  year: 2015
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 15050
  pid: 0b544dfe355a5070b60986319a3f51fb45d1348e
  show_ref_link: true
  title: Learning Phrase Representations using RNN Encoder-Decoder for Statistical
    Machine Translation
  year: 2014
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 3049
  pid: 97fb4e3d45bb098e27e0071448b6152217bd35a5
  show_ref_link: true
  title: Layer Normalization
  year: 2016
slug: Self-Attention-with-Relative-Position-Shaw-Uszkoreit
title: Self-Attention with Relative Position Representations
url: https://www.semanticscholar.org/paper/Self-Attention-with-Relative-Position-Shaw-Uszkoreit/c8efcc854d97dfc2a42b83316a2109f9d166e43f?sort=total-citations
venue: NAACL
year: 2018
