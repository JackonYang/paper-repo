authors:
- Dan Hendrycks
- Kevin Gimpel
badges:
- id: OPEN_ACCESS
corpusId: 2359786
fieldsOfStudy:
- Computer Science
numCitedBy: 289
numCiting: 24
paperAbstract: We propose the Gaussian Error Linear Unit (GELU), a high-performing
  neural network activation function. The GELU nonlinearity is the expected transformation
  of a stochastic regularizer which randomly applies the identity or zero map, combining
  the intuitions of dropout and zoneout while respecting neuron values. This connection
  suggests a new probabilistic understanding of nonlinearities. We perform an empirical
  evaluation of the GELU nonlinearity against the ReLU and ELU activations and find
  performance improvements across all tasks.
ref_count: 24
references:
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 266
  pid: 9f0687bcd0a7d7fc91b8c5d36c003a38b8853105
  title: 'Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations'
  year: 2017
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1264
  pid: 99c970348b8f70ce23d6641e201904ea49266b6e
  title: Exact solutions to the nonlinear dynamics of learning in deep linear neural
    networks
  year: 2014
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 3667
  pid: f63e917638553414526a0cc8550de4ad2d83fe7a
  title: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)
  year: 2016
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 4401
  pid: 367f2c63a6f6a10b3b64b8729d601e69337ee3cc
  title: Rectifier Nonlinearities Improve Neural Network Acoustic Models
  year: 2013
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 377
  pid: ec92efde21707ddf4b81f301cd58e2051c1a2443
  title: Fast dropout training
  year: 2013
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 12808
  pid: a538b05ebb01a40323997629e171c91aa28b8e2f
  title: Rectified Linear Units Improve Restricted Boltzmann Machines
  year: 2010
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1290
  pid: 3d2c6941a9b4608ba52b328369a3352db2092ae0
  title: 'Weight Normalization: A Simple Reparameterization to Accelerate Training
    of Deep Neural Networks'
  year: 2016
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 90063
  pid: a6cb366736791bcccc5c8639de5a8f9636bf87e8
  title: 'Adam: A Method for Stochastic Optimization'
  year: 2015
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 28149
  pid: 34f25a8704614163c4095b3ee2fc969b60de4698
  title: 'Dropout: a simple way to prevent neural networks from overfitting'
  year: 2014
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 2724
  pid: b022f2a277a4bf5f42382e86e4380b96340b9e86
  title: 'SGDR: Stochastic Gradient Descent with Warm Restarts'
  year: 2017
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 236
  pid: 5d5d4f49d6443c8529a6f5ebef5c499d47a869da
  title: Improving Neural Networks with Dropout
  year: 2013
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 16693
  pid: 98b4d4e24aab57ab4e1124ff8106909050645cfa
  title: Neural networks and physical systems with emergent collective computational
    abilities.
  year: 1982
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1424
  pid: 51db1f3c8dfc7d4077da39c96bb90a6358128111
  title: Deep Networks with Stochastic Depth
  year: 2016
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1641
  pid: d2b62f77cb2864e465aa60bca6c26bb1d2f84963
  title: Acoustic Modeling Using Deep Belief Networks
  year: 2012
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 489
  pid: 97dc8df45972e4ed7423fc992a5092ba25b33411
  title: All you need is a good init
  year: 2016
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 62223
  pid: eb42cf88027de515750f230b23b1a057dc782108
  title: Very Deep Convolutional Networks for Large-Scale Image Recognition
  year: 2015
slug: Bridging-Nonlinearities-and-Stochastic-Regularizers-Hendrycks-Gimpel
title: Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear
  Units
url: https://www.semanticscholar.org/paper/Bridging-Nonlinearities-and-Stochastic-Regularizers-Hendrycks-Gimpel/4361e64f2d12d63476fdc88faf72a0f70d9a2ffb?sort=total-citations
venue: ArXiv
year: 2016
