authors:
- Jyrki Kivinen
- Manfred K. Warmuth
badges:
- id: OPEN_ACCESS
corpusId: 15718458
fieldsOfStudy:
- Computer Science
numCitedBy: 281
numCiting: 24
paperAbstract: We consider two algorithms for on-line prediction based on a linear
  model. The algorithms are the well-known Gradient Descent (GD) algorithm and a new
  algorithm, which we call EG *. They both maintain a weight vector using simple updates.
  For the GD algorithm, the weight vector is updated by subtracting from it the gradient
  of the squared error made on a prediction multiplied by a parameter called the learning
  rate. The EG* uses the components of the gradient in the exponents of factors that
  are used in updating the weight vector multiplicatively. We present worst-case on-line
  loss bounds for EG* and compare them to previously known bounds for the GD algorithm.
  The bounds suggest that although the on-line losses of the algorithms are in general
  incomparable, EG * has a much smaller loss if only few of the input variables are
  relevant for the predictions. Experiments show that the worst-case upper bounds
  are quite tight already on simple artificial data. Our main methodological idea
  is using a distance function between weight vectors both in motivating the algorithms
  and as a potential function in an amortized analysis that leads to worst-case loss
  bounds. Using squared Euclidean distance leads to the GD algorithm, and using the
  relative entropy leads to the EG* algorithm.
ref_count: 24
references:
- pid: 98eed3f082351c4821d1edb315846207a8fefbe9
  title: Exponentiated Gradient Versus Gradient Descent for Linear Predictors
- pid: 239ae23dc2f934cfa005261ade01023fe7950b82
  title: How to use expert advice
- pid: 35582a30685083c62dca992553eec44123be9d07
  title: The Weighted Majority Algorithm
- pid: 2599131a4bc2fa957338732a37c744cfe3e17b24
  title: A training algorithm for optimal margin classifiers
- pid: e0b8fa3496283d4d808fba9ff62d5f024bcf23be
  title: Learnability and the Vapnik-Chervonenkis dimension
- pid: 5d11aad09f65431b5d3cb1d85328743c9e53ba96
  title: 'The perceptron: a probabilistic model for information storage and organization
    in the brain.'
- pid: 6c192b91f4b4ac35ae8385fa190fdfc146f419b8
  title: Adaptive Signal Processing
- pid: a36b028d024bf358c4af1a5e1dc3ca0aed23b553
  title: 'Chervonenkis: On the uniform convergence of relative frequencies of events
    to their probabilities'
- pid: 045310b06e8a3983a363a118cc9dcc3f292970b4
  title: 'Neural Networks: A Comprehensive Foundation'
- pid: b07ce649d6f6eb636872527104b0209d3edc8188
  title: Pattern classification and scene analysis
- pid: 8f80ae7531ae8c8e06f53ca78d5ad8a2dfbc8697
  title: Aggregating strategies
slug: Additive-versus-exponentiated-gradient-updates-for-Kivinen-Warmuth
title: Additive versus exponentiated gradient updates for linear prediction
url: https://www.semanticscholar.org/paper/Additive-versus-exponentiated-gradient-updates-for-Kivinen-Warmuth/4dc175e8f6e7ca5c40ffd6fb9c6b92323bf7daf2?sort=total-citations
venue: STOC '95
year: 1995
