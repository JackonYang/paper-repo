authors:
- Yann Dauphin
- Angela Fan
- Michael Auli
- David Grangier
badges:
- id: OPEN_ACCESS
corpusId: 16119010
fieldsOfStudy:
- Computer Science
numCitedBy: 1284
numCiting: 42
paperAbstract: The pre-dominant approach to language modeling to date is based on
  recurrent neural networks. Their success on this task is often linked to their ability
  to capture unbounded context. In this paper we develop a finite context approach
  through stacked convolutions, which can be more efficient since they allow parallelization
  over sequential tokens. We propose a novel simplified gating mechanism that outperforms
  Oord et al (2016) and investigate the impact of key architectural decisions. The
  proposed approach achieves state-of-the-art on the WikiText-103 benchmark, even
  though it features long-term dependencies, as well as competitive results on the
  Google Billion Words benchmark. Our model reduces the latency to score a sentence
  by an order of magnitude compared to a recurrent baseline. To our knowledge, this
  is the first time a non-recurrent approach is competitive with strong recurrent
  models on these large scale language tasks.
ref_count: 42
references:
- pid: 2f2d8f8072e5cc9b296fad551f65f183bdbff7aa
  title: Exploring the Limits of Language Modeling
- pid: efbd381493bb9636f489b965a2034d529cd56bcd
  title: Pointer Sentinel Mixture Models
- pid: 5d833331b0e22ff359db05c62a8bca18c4f04b68
  title: One billion word benchmark for measuring progress in statistical language
    modeling
- pid: 510e26733aaff585d65701b9f1be7ca9d5afc586
  title: 'Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts
    Layer'
- pid: c19fbefdeead6a4154a22a9c8551a18b1530033a
  title: Hierarchical Probabilistic Neural Network Language Model
- pid: 9819b600a828a57e1cde047bbe710d3446b30da5
  title: Recurrent neural network based language model
- pid: 3d2c6941a9b4608ba52b328369a3352db2092ae0
  title: 'Weight Normalization: A Simple Reparameterization to Accelerate Training
    of Deep Neural Networks'
- pid: aa7bfd2304201afbb19971ebde87b17e40242e91
  title: On the importance of initialization and momentum in deep learning
- pid: 6c2b28f9354f667cd5bd07afc0471d8334430da7
  title: A Neural Probabilistic Language Model
- pid: 44d2abe2175df8153f465f6c39b68b76a0d40ab9
  title: Long Short-Term Memory
- pid: 41f1d50c85d3180476c4c7b3eea121278b0d8474
  title: Pixel Recurrent Neural Networks
- pid: 9548ac30c113562a51e603dbbc8e9fa651cfd3ab
  title: Improved backing-off for M-gram language modeling
- pid: 84069287da0a6b488b8c933f3cb5be759cb6237e
  title: On the difficulty of training recurrent neural networks
- pid: bd7d93193aad6c4b71cc8942e808753019e87706
  title: Three new graphical models for statistical language modelling
- pid: 79baf48bd560060549998d7b61751286de062e2a
  title: Factorization tricks for LSTM networks
- pid: d6f2f611da110b5b5061731be3fc4c7f45d8ee23
  title: 'Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet
    Classification'
- pid: 2c03df8b48bf3fa39054345bafabfeff15bfd11d
  title: Deep Residual Learning for Image Recognition
- pid: b71ac1e9fb49420d13e084ac67254a0bbd40f83f
  title: Understanding the difficulty of training deep feedforward neural networks
- pid: 0936352b78a52bc5d2b5e3f04233efc56664af51
  title: Conditional Image Generation with PixelCNN Decoders
- pid: b3e89f05876d47b9bd6ece225aaeee457a6824e8
  title: Statistical Machine Translation
- pid: 3449b65008b27f6e60a73d80c1fd990f0481126b
  title: 'Torch7: A Matlab-like Environment for Machine Learning'
- pid: 084c55d6432265785e3ff86a2e900a49d501c00a
  title: Foundations of statistical natural language processing
- pid: e3ce36b9deb47aa6bb2aa19c4bfa71283b505025
  title: 'Noise-contrastive estimation: A new estimation principle for unnormalized
    statistical models'
- pid: ce8cca19455e8d3055c57a9bafe882984c95a201
  title: Syntactic Process
- pid: d4e8bed3b50a035e1eabad614fe4218a34b3b178
  title: An empirical study of smoothing techniques for language modeling
- pid: 563e821bb5ea825efb56b77484f5287f08cf3753
  title: Convolutional networks for images, speech, and time series
slug: Language-Modeling-with-Gated-Convolutional-Networks-Dauphin-Fan
title: Language Modeling with Gated Convolutional Networks
url: https://www.semanticscholar.org/paper/Language-Modeling-with-Gated-Convolutional-Networks-Dauphin-Fan/88caa4a0253a8b0076176745ebc072864eab66e1?sort=total-citations
venue: ICML
year: 2017
