authors:
- Yann Dauphin
- Angela Fan
- Michael Auli
- David Grangier
badges:
- id: OPEN_ACCESS
corpusId: 16119010
fieldsOfStudy:
- Computer Science
numCitedBy: 1284
numCiting: 42
paperAbstract: The pre-dominant approach to language modeling to date is based on
  recurrent neural networks. Their success on this task is often linked to their ability
  to capture unbounded context. In this paper we develop a finite context approach
  through stacked convolutions, which can be more efficient since they allow parallelization
  over sequential tokens. We propose a novel simplified gating mechanism that outperforms
  Oord et al (2016) and investigate the impact of key architectural decisions. The
  proposed approach achieves state-of-the-art on the WikiText-103 benchmark, even
  though it features long-term dependencies, as well as competitive results on the
  Google Billion Words benchmark. Our model reduces the latency to score a sentence
  by an order of magnitude compared to a recurrent baseline. To our knowledge, this
  is the first time a non-recurrent approach is competitive with strong recurrent
  models on these large scale language tasks.
ref_count: 42
references:
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 951
  pid: 2f2d8f8072e5cc9b296fad551f65f183bdbff7aa
  title: Exploring the Limits of Language Modeling
  year: 2016
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1042
  pid: efbd381493bb9636f489b965a2034d529cd56bcd
  title: Pointer Sentinel Mixture Models
  year: 2017
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 903
  pid: 5d833331b0e22ff359db05c62a8bca18c4f04b68
  title: One billion word benchmark for measuring progress in statistical language
    modeling
  year: 2014
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 862
  pid: 510e26733aaff585d65701b9f1be7ca9d5afc586
  title: 'Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts
    Layer'
  year: 2017
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 942
  pid: c19fbefdeead6a4154a22a9c8551a18b1530033a
  title: Hierarchical Probabilistic Neural Network Language Model
  year: 2005
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 4900
  pid: 9819b600a828a57e1cde047bbe710d3446b30da5
  title: Recurrent neural network based language model
  year: 2010
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1290
  pid: 3d2c6941a9b4608ba52b328369a3352db2092ae0
  title: 'Weight Normalization: A Simple Reparameterization to Accelerate Training
    of Deep Neural Networks'
  year: 2016
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 3557
  pid: aa7bfd2304201afbb19971ebde87b17e40242e91
  title: On the importance of initialization and momentum in deep learning
  year: 2013
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 6009
  pid: 6c2b28f9354f667cd5bd07afc0471d8334430da7
  title: A Neural Probabilistic Language Model
  year: 2000
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 51694
  pid: 44d2abe2175df8153f465f6c39b68b76a0d40ab9
  title: Long Short-Term Memory
  year: 1997
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1748
  pid: 41f1d50c85d3180476c4c7b3eea121278b0d8474
  title: Pixel Recurrent Neural Networks
  year: 2016
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1792
  pid: 9548ac30c113562a51e603dbbc8e9fa651cfd3ab
  title: Improved backing-off for M-gram language modeling
  year: 1995
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 3802
  pid: 84069287da0a6b488b8c933f3cb5be759cb6237e
  title: On the difficulty of training recurrent neural networks
  year: 2013
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 606
  pid: bd7d93193aad6c4b71cc8942e808753019e87706
  title: Three new graphical models for statistical language modelling
  year: 2007
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 91
  pid: 79baf48bd560060549998d7b61751286de062e2a
  title: Factorization tricks for LSTM networks
  year: 2017
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 12381
  pid: d6f2f611da110b5b5061731be3fc4c7f45d8ee23
  title: 'Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet
    Classification'
  year: 2015
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 95326
  pid: 2c03df8b48bf3fa39054345bafabfeff15bfd11d
  title: Deep Residual Learning for Image Recognition
  year: 2016
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 12433
  pid: b71ac1e9fb49420d13e084ac67254a0bbd40f83f
  title: Understanding the difficulty of training deep feedforward neural networks
  year: 2010
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1607
  pid: 0936352b78a52bc5d2b5e3f04233efc56664af51
  title: Conditional Image Generation with PixelCNN Decoders
  year: 2016
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 769
  pid: b3e89f05876d47b9bd6ece225aaeee457a6824e8
  title: Statistical Machine Translation
  year: 2010
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1490
  pid: 3449b65008b27f6e60a73d80c1fd990f0481126b
  title: 'Torch7: A Matlab-like Environment for Machine Learning'
  year: 2011
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 7801
  pid: 084c55d6432265785e3ff86a2e900a49d501c00a
  title: Foundations of statistical natural language processing
  year: 2002
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1227
  pid: e3ce36b9deb47aa6bb2aa19c4bfa71283b505025
  title: 'Noise-contrastive estimation: A new estimation principle for unnormalized
    statistical models'
  year: 2010
- fieldsOfStudy:
  - Linguistics
  numCitedBy: 911
  pid: ce8cca19455e8d3055c57a9bafe882984c95a201
  title: Syntactic Process
  year: 1979
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 2861
  pid: d4e8bed3b50a035e1eabad614fe4218a34b3b178
  title: An empirical study of smoothing techniques for language modeling
  year: 1999
- fieldsOfStudy:
  - Mathematics
  numCitedBy: 4091
  pid: 563e821bb5ea825efb56b77484f5287f08cf3753
  title: Convolutional networks for images, speech, and time series
  year: 1998
slug: Language-Modeling-with-Gated-Convolutional-Networks-Dauphin-Fan
title: Language Modeling with Gated Convolutional Networks
url: https://www.semanticscholar.org/paper/Language-Modeling-with-Gated-Convolutional-Networks-Dauphin-Fan/88caa4a0253a8b0076176745ebc072864eab66e1?sort=total-citations
venue: ICML
year: 2017
