authors:
- Wei Wang
- Bin Bi
- Ming Yan
- Chen Wu
- Zuyi Bao
- Liwei Peng
- Luo Si
badges:
- id: OPEN_ACCESS
corpusId: 199552081
fieldsOfStudy:
- Computer Science
meta_key: structbert-incorporating-language-structures-into-pre-training-for-deep-language-understanding
numCitedBy: 140
numCiting: 62
paperAbstract: Recently, the pre-trained language model, BERT (and its robustly optimized
  version RoBERTa), has attracted a lot of attention in natural language understanding
  (NLU), and achieved state-of-the-art accuracy in various NLU tasks, such as sentiment
  classification, natural language inference, semantic textual similarity and question
  answering. Inspired by the linearization exploration work of Elman [8], we extend
  BERT to a new model, StructBERT, by incorporating language structures into pre-training.
  Specifically, we pre-train StructBERT with two auxiliary tasks to make the most
  of the sequential order of words and sentences, which leverage language structures
  at the word and sentence levels, respectively. As a result, the new model is adapted
  to different levels of language understanding required by downstream tasks. The
  StructBERT with structural pre-training gives surprisingly good empirical results
  on a variety of downstream tasks, including pushing the state-of-the-art on the
  GLUE benchmark to 89.0 (outperforming all published models), the F1 score on SQuAD
  v1.1 question answering to 93.0, the accuracy on SNLI to 91.7.
ref_count: 62
references:
- fieldsOfStudy:
  - Computer Science
  meta_key: bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding
  numCitedBy: 33768
  pid: df2b0e26d0599ce3e70df8a9da02e51594e0e992
  show_ref_link: true
  title: 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: improving-language-understanding-by-generative-pre-training
  numCitedBy: 3535
  pid: cd18800a0fe0b668a1cc19f2ec95b5003d0a5035
  show_ref_link: true
  title: Improving Language Understanding by Generative Pre-Training
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: glue-a-multi-task-benchmark-and-analysis-platform-for-natural-language-understanding
  numCitedBy: 2636
  pid: 93b8da28d006415866bf48f9a6e06b5242129195
  show_ref_link: true
  title: 'GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language
    Understanding'
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: multi-task-deep-neural-networks-for-natural-language-understanding
  numCitedBy: 732
  pid: 658721bc13b0fa97366d38c05a96bf0a9f4bb0ac
  show_ref_link: true
  title: Multi-Task Deep Neural Networks for Natural Language Understanding
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: sentence-encoders-on-stilts-supplementary-training-on-intermediate-labeled-data-tasks
  numCitedBy: 264
  pid: b47381e04739ea3f392ba6c8faaf64105493c196
  show_ref_link: true
  title: 'Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data
    Tasks'
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: xlnet-generalized-autoregressive-pretraining-for-language-understanding
  numCitedBy: 4228
  pid: e0c6abdbdecf04ffac65c440da77fb9d66bb474c
  show_ref_link: true
  title: 'XLNet: Generalized Autoregressive Pretraining for Language Understanding'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: recursive-deep-models-for-semantic-compositionality-over-a-sentiment-treebank
  numCitedBy: 5367
  pid: 687bac2d3320083eb4530bf18bb8f8f721477600
  show_ref_link: true
  title: Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank
  year: 2013
- fieldsOfStudy:
  - Computer Science
  meta_key: enhancing-pre-trained-language-representations-with-rich-knowledge-for-machine-reading-comprehension
  numCitedBy: 82
  pid: e7046bf945ad6326537a1ac78a96fd2f45acc900
  show_ref_link: false
  title: Enhancing Pre-Trained Language Representations with Rich Knowledge for Machine
    Reading Comprehension
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: deep-contextualized-word-representations
  numCitedBy: 7987
  pid: 3febb2bed8865945e7fddc99efd791887bb7e14f
  show_ref_link: true
  title: Deep Contextualized Word Representations
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: qanet-combining-local-convolution-with-global-self-attention-for-reading-comprehension
  numCitedBy: 800
  pid: 8c1b00128e74f1cd92aede3959690615695d5101
  show_ref_link: true
  title: 'QANet: Combining Local Convolution with Global Self-Attention for Reading
    Comprehension'
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: learned-in-translation-contextualized-word-vectors
  numCitedBy: 710
  pid: bc8fa64625d9189f5801837e7b133e7fe3c581f7
  show_ref_link: true
  title: 'Learned in Translation: Contextualized Word Vectors'
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: attention-is-all-you-need
  numCitedBy: 35164
  pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  show_ref_link: true
  title: Attention is All you Need
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: a-broad-coverage-challenge-corpus-for-sentence-understanding-through-inference
  numCitedBy: 2037
  pid: 5ded2b8c64491b4a67f6d39ce473d4b9347a672e
  show_ref_link: true
  title: A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: a-large-annotated-corpus-for-learning-natural-language-inference
  numCitedBy: 2518
  pid: f04df4e20a18358ea2f689b4c129781628ef7fc1
  show_ref_link: true
  title: A large annotated corpus for learning natural language inference
  year: 2015
- fieldsOfStudy:
  - Computer Science
  meta_key: google-s-neural-machine-translation-system-bridging-the-gap-between-human-and-machine-translation
  numCitedBy: 4645
  pid: dbde7dfa6cae81df8ac19ef500c42db96c3d1edd
  show_ref_link: true
  title: 'Google''s Neural Machine Translation System: Bridging the Gap between Human
    and Machine Translation'
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: discriminative-syntax-based-word-ordering-for-text-generation
  numCitedBy: 33
  pid: f67ec8d10f04442c55ada6821031cf39e06aaa8e
  show_ref_link: false
  title: Discriminative Syntax-Based Word Ordering for Text Generation
  year: 2015
- fieldsOfStudy:
  - Computer Science
  meta_key: i-know-what-you-want-semantic-learning-for-text-comprehension
  numCitedBy: 18
  pid: e2a9e0cb1d5376d5f03d9e3f12cf962c49adb133
  show_ref_link: false
  title: 'I Know What You Want: Semantic Learning for Text Comprehension'
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: word-ordering-without-syntax
  numCitedBy: 35
  pid: 7b5af1758963babf3740a39615b469b70513a413
  show_ref_link: false
  title: Word Ordering Without Syntax
  year: 2016
- fieldsOfStudy:
  - Computer Science
  - Psychology
  meta_key: semeval-2017-task-1-semantic-textual-similarity-multilingual-and-crosslingual-focused-evaluation
  numCitedBy: 935
  pid: a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096
  show_ref_link: true
  title: 'SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual
    Focused Evaluation'
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: snorkel-rapid-training-data-creation-with-weak-supervision
  numCitedBy: 618
  pid: 18bc1d4271abe8dd6e16179cdb06524a4f396e16
  show_ref_link: false
  title: 'Snorkel: Rapid Training Data Creation with Weak Supervision'
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: neural-sentence-ordering
  numCitedBy: 46
  pid: bdc1d773390a804487056c24f56c3c275b9cd0a0
  show_ref_link: false
  title: Neural Sentence Ordering
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: spanbert-improving-pre-training-by-representing-and-predicting-spans
  numCitedBy: 879
  pid: 81f5810fbbab9b7203b9556f4ce3c741875407bc
  show_ref_link: true
  title: 'SpanBERT: Improving Pre-training by Representing and Predicting Spans'
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: squad-100-000-questions-for-machine-comprehension-of-text
  numCitedBy: 4265
  pid: 05dd7254b632376973f3a1b4d39485da17814df5
  show_ref_link: true
  title: 'SQuAD: 100,000+ Questions for Machine Comprehension of Text'
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: a-bottom-up-approach-to-sentence-ordering-for-multi-document-summarization
  numCitedBy: 79
  pid: 1290e0f6e9bcc37f84b82fdec120f587388db25e
  show_ref_link: false
  title: A Bottom-Up Approach to Sentence Ordering for Multi-Document Summarization
  year: 2006
- fieldsOfStudy:
  - Computer Science
  meta_key: a-comparison-of-neural-models-for-word-ordering
  numCitedBy: 17
  pid: c0512119ecb75d8ed5097b7b43ab88c53cf65969
  show_ref_link: false
  title: A Comparison of Neural Models for Word Ordering
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: roberta-a-robustly-optimized-bert-pretraining-approach
  numCitedBy: 7268
  pid: 077f8329a7b6fa3b7c877a57b81eb6c18b5f87de
  show_ref_link: true
  title: 'RoBERTa: A Robustly Optimized BERT Pretraining Approach'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: the-sixth-pascal-recognizing-textual-entailment-challenge
  numCitedBy: 439
  pid: db8885a0037fe47d973ade79d696586453710233
  show_ref_link: false
  title: The Sixth PASCAL Recognizing Textual Entailment Challenge
  year: 2009
- fieldsOfStudy:
  - Computer Science
  meta_key: building-applied-natural-language-generation-systems
  numCitedBy: 587
  pid: 32622973ea102abb5f0dea32e2944b7656cd4798
  show_ref_link: false
  title: Building applied natural language generation systems
  year: 1997
- fieldsOfStudy:
  - Computer Science
  - Linguistics
  meta_key: neural-network-acceptability-judgments
  numCitedBy: 546
  pid: cb0f3ee1e98faf92429d601cdcd76c69c1e484eb
  show_ref_link: true
  title: Neural Network Acceptability Judgments
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: aligning-books-and-movies-towards-story-like-visual-explanations-by-watching-movies-and-reading-books
  numCitedBy: 1419
  pid: 0e6824e137847be0599bb0032e37042ed2ef5045
  show_ref_link: true
  title: 'Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching
    Movies and Reading Books'
  year: 2015
- fieldsOfStudy:
  - Computer Science
  meta_key: the-seventh-pascal-recognizing-textual-entailment-challenge
  numCitedBy: 390
  pid: 0f8468de03ee9f12d693237bec87916311bf1c24
  show_ref_link: false
  title: The Seventh PASCAL Recognizing Textual Entailment Challenge
  year: 2011
- fieldsOfStudy:
  - Computer Science
  meta_key: recurrent-neural-network-based-language-model
  numCitedBy: 4901
  pid: 9819b600a828a57e1cde047bbe710d3446b30da5
  show_ref_link: true
  title: Recurrent neural network based language model
  year: 2010
- fieldsOfStudy:
  - Computer Science
  meta_key: long-short-term-memory
  numCitedBy: 51693
  pid: 44d2abe2175df8153f465f6c39b68b76a0d40ab9
  show_ref_link: true
  title: Long Short-Term Memory
  year: 1997
- fieldsOfStudy:
  - Psychology
  meta_key: finding-structure-in-time
  numCitedBy: 9863
  pid: 668087f0ae7ce1de6e0bd0965dbb480c08103260
  show_ref_link: false
  title: Finding Structure in Time
  year: 1990
- fieldsOfStudy:
  - Computer Science
  meta_key: automatically-constructing-a-corpus-of-sentential-paraphrases
  numCitedBy: 834
  pid: 475354f10798f110d34792b6d88f31d6d5cb099e
  show_ref_link: false
  title: Automatically Constructing a Corpus of Sentential Paraphrases
  year: 2005
- fieldsOfStudy:
  - Linguistics
  meta_key: the-winograd-schema-challenge
  numCitedBy: 691
  pid: 128cb6b891aee1b5df099acb48e2efecfcff689f
  show_ref_link: false
  title: The Winograd Schema Challenge
  year: 2011
- fieldsOfStudy:
  - Computer Science
  meta_key: gaussian-error-linear-units-gelus
  numCitedBy: 971
  pid: 15f4c35889ccc1ae258b680c2ca2fcbfe1e260f7
  show_ref_link: true
  title: Gaussian Error Linear Units (GELUs)
  year: 2016
slug: StructBERT:-Incorporating-Language-Structures-into-Wang-Bi
title: 'StructBERT: Incorporating Language Structures into Pre-training for Deep Language
  Understanding'
url: https://www.semanticscholar.org/paper/StructBERT:-Incorporating-Language-Structures-into-Wang-Bi/d56c1fc337fb07ec004dc846f80582c327af717c?sort=total-citations
venue: ICLR
year: 2020
