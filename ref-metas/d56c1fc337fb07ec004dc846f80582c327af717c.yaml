authors:
- Wei Wang
- Bin Bi
- Ming Yan
- Chen Wu
- Zuyi Bao
- Liwei Peng
- Luo Si
badges:
- id: OPEN_ACCESS
corpusId: 199552081
fieldsOfStudy:
- Computer Science
numCitedBy: 140
numCiting: 62
paperAbstract: Recently, the pre-trained language model, BERT (and its robustly optimized
  version RoBERTa), has attracted a lot of attention in natural language understanding
  (NLU), and achieved state-of-the-art accuracy in various NLU tasks, such as sentiment
  classification, natural language inference, semantic textual similarity and question
  answering. Inspired by the linearization exploration work of Elman [8], we extend
  BERT to a new model, StructBERT, by incorporating language structures into pre-training.
  Specifically, we pre-train StructBERT with two auxiliary tasks to make the most
  of the sequential order of words and sentences, which leverage language structures
  at the word and sentence levels, respectively. As a result, the new model is adapted
  to different levels of language understanding required by downstream tasks. The
  StructBERT with structural pre-training gives surprisingly good empirical results
  on a variety of downstream tasks, including pushing the state-of-the-art on the
  GLUE benchmark to 89.0 (outperforming all published models), the F1 score on SQuAD
  v1.1 question answering to 93.0, the accuracy on SNLI to 91.7.
ref_count: 62
references:
- pid: df2b0e26d0599ce3e70df8a9da02e51594e0e992
  title: 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'
- pid: cd18800a0fe0b668a1cc19f2ec95b5003d0a5035
  title: Improving Language Understanding by Generative Pre-Training
- pid: 93b8da28d006415866bf48f9a6e06b5242129195
  title: 'GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language
    Understanding'
- pid: 658721bc13b0fa97366d38c05a96bf0a9f4bb0ac
  title: Multi-Task Deep Neural Networks for Natural Language Understanding
- pid: b47381e04739ea3f392ba6c8faaf64105493c196
  title: 'Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data
    Tasks'
- pid: e0c6abdbdecf04ffac65c440da77fb9d66bb474c
  title: 'XLNet: Generalized Autoregressive Pretraining for Language Understanding'
- pid: 687bac2d3320083eb4530bf18bb8f8f721477600
  title: Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank
- pid: 3febb2bed8865945e7fddc99efd791887bb7e14f
  title: Deep Contextualized Word Representations
- pid: 8c1b00128e74f1cd92aede3959690615695d5101
  title: 'QANet: Combining Local Convolution with Global Self-Attention for Reading
    Comprehension'
- pid: bc8fa64625d9189f5801837e7b133e7fe3c581f7
  title: 'Learned in Translation: Contextualized Word Vectors'
- pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  title: Attention is All you Need
- pid: 5ded2b8c64491b4a67f6d39ce473d4b9347a672e
  title: A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference
- pid: f04df4e20a18358ea2f689b4c129781628ef7fc1
  title: A large annotated corpus for learning natural language inference
- pid: dbde7dfa6cae81df8ac19ef500c42db96c3d1edd
  title: 'Google''s Neural Machine Translation System: Bridging the Gap between Human
    and Machine Translation'
- pid: a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096
  title: 'SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual
    Focused Evaluation'
- pid: 81f5810fbbab9b7203b9556f4ce3c741875407bc
  title: 'SpanBERT: Improving Pre-training by Representing and Predicting Spans'
- pid: 05dd7254b632376973f3a1b4d39485da17814df5
  title: 'SQuAD: 100,000+ Questions for Machine Comprehension of Text'
- pid: 077f8329a7b6fa3b7c877a57b81eb6c18b5f87de
  title: 'RoBERTa: A Robustly Optimized BERT Pretraining Approach'
- pid: db8885a0037fe47d973ade79d696586453710233
  title: The Sixth PASCAL Recognizing Textual Entailment Challenge
- pid: cb0f3ee1e98faf92429d601cdcd76c69c1e484eb
  title: Neural Network Acceptability Judgments
- pid: 0e6824e137847be0599bb0032e37042ed2ef5045
  title: 'Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching
    Movies and Reading Books'
- pid: 0f8468de03ee9f12d693237bec87916311bf1c24
  title: The Seventh PASCAL Recognizing Textual Entailment Challenge
- pid: 9819b600a828a57e1cde047bbe710d3446b30da5
  title: Recurrent neural network based language model
- pid: 44d2abe2175df8153f465f6c39b68b76a0d40ab9
  title: Long Short-Term Memory
- pid: 668087f0ae7ce1de6e0bd0965dbb480c08103260
  title: Finding Structure in Time
- pid: 475354f10798f110d34792b6d88f31d6d5cb099e
  title: Automatically Constructing a Corpus of Sentential Paraphrases
- pid: 128cb6b891aee1b5df099acb48e2efecfcff689f
  title: The Winograd Schema Challenge
- pid: 15f4c35889ccc1ae258b680c2ca2fcbfe1e260f7
  title: Gaussian Error Linear Units (GELUs)
slug: StructBERT:-Incorporating-Language-Structures-into-Wang-Bi
title: 'StructBERT: Incorporating Language Structures into Pre-training for Deep Language
  Understanding'
url: https://www.semanticscholar.org/paper/StructBERT:-Incorporating-Language-Structures-into-Wang-Bi/d56c1fc337fb07ec004dc846f80582c327af717c?sort=total-citations
venue: ICLR
year: 2020
