authors:
- Taku Kudo
badges:
- id: OPEN_ACCESS
corpusId: 13753208
fieldsOfStudy:
- Computer Science
numCitedBy: 541
numCiting: 35
paperAbstract: Subword units are an effective way to alleviate the open vocabulary
  problems in neural machine translation (NMT). While sentences are usually converted
  into unique subword sequences, subword segmentation is potentially ambiguous and
  multiple segmentations are possible even with the same vocabulary. The question
  addressed in this paper is whether it is possible to harness the segmentation ambiguity
  as a noise to improve the robustness of NMT. We present a simple regularization
  method, subword regularization, which trains the model with multiple subword segmentations
  probabilistically sampled during training. In addition, for better subword sampling,
  we propose a new subword segmentation algorithm based on a unigram language model.
  We experiment with multiple corpora and report consistent improvements especially
  on low resource and out-of-domain settings.
ref_count: 35
references:
- pid: 1af68821518f03568f913ab03fc02080247a27ff
  title: Neural Machine Translation of Rare Words with Subword Units
- pid: 93499a7c7f699b6630a86fad964536f9423bb6d0
  title: Effective Approaches to Attention-based Neural Machine Translation
- pid: d86227948b6000e5d7ed63cf2054ad600b7994a0
  title: Deep Unordered Composition Rivals Syntactic Methods for Text Classification
- pid: 5082a1a13daea5c7026706738f8528391a1e6d59
  title: A Neural Attention Model for Abstractive Sentence Summarization
- pid: fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5
  title: Neural Machine Translation by Jointly Learning to Align and Translate
- pid: 85315b64a4c73cb86f156ef5b0a085d6ebc8a65d
  title: A Neural Conversational Model
- pid: 32de44f01a96d4473d21099d15e25bc2b9f08e2f
  title: Improved Semantic Representations From Tree-Structured Long Short-Term Memory
    Networks
- pid: dbde7dfa6cae81df8ac19ef500c42db96c3d1edd
  title: 'Google''s Neural Machine Translation System: Bridging the Gap between Human
    and Machine Translation'
- pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  title: Attention is All you Need
- pid: d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0
  title: 'Show and tell: A neural image caption generator'
- pid: 43428880d75b3a14257c3ee9bda054e61eb869c0
  title: Convolutional Sequence to Sequence Learning
- pid: 34f25a8704614163c4095b3ee2fc969b60de4698
  title: 'Dropout: a simple way to prevent neural networks from overfitting'
- pid: d7da009f457917aa381619facfa5ffae9329a6e9
  title: 'Bleu: a Method for Automatic Evaluation of Machine Translation'
- pid: a6cb366736791bcccc5c8639de5a8f9636bf87e8
  title: 'Adam: A Method for Stochastic Optimization'
- pid: 843959ffdccf31c6694d135fad07425924f785b1
  title: Extracting and composing robust features with denoising autoencoders
- pid: bee044c8e8903fb67523c1f8c105ab4718600cdb
  title: Explaining and Harnessing Adversarial Examples
- pid: 9b5c1d0844714588cf59629cbbc8e5f2e01f4a15
  title: Error Bounds for Convolutional Codes and an Asymptotically Optimum Decoding
    Algorithm
slug: Subword-Regularization:-Improving-Neural-Network-Kudo
title: 'Subword Regularization: Improving Neural Network Translation Models with Multiple
  Subword Candidates'
url: https://www.semanticscholar.org/paper/Subword-Regularization:-Improving-Neural-Network-Kudo/e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e?sort=total-citations
venue: ACL
year: 2018
