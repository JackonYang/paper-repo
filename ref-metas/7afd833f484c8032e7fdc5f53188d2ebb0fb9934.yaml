authors:
- Dahua Lin
- S. Fidler
- Chen Kong
- R. Urtasun
badges:
- id: OPEN_ACCESS
corpusId: 1753262
fieldsOfStudy:
- Computer Science
numCitedBy: 137
numCiting: 33
paperAbstract: In this paper, we tackle the problem of retrieving videos using complex
  natural language queries. Towards this goal, we first parse the sentential descriptions
  into a semantic graph, which is then matched to visual concepts using a generalized
  bipartite matching algorithm. Our approach exploits object appearance, motion and
  spatial relations, and learns the importance of each term using structure prediction.
  We demonstrate the effectiveness of our approach on a new dataset designed for semantic
  search in the context of autonomous driving, which exhibits complex and highly dynamic
  scenes with many objects. We show that our approach is able to locate a major portion
  of the objects described in the query with high accuracy, and improve the relevance
  in video retrieval.
ref_count: 33
references:
- pid: 642e328cae81c5adb30069b680cf60ba6b475153
  title: 'Video Google: a text retrieval approach to object matching in videos'
- pid: e8cd37fbd8bd5e690eef5861cf92af8e002d4533
  title: Translating Video Content to Natural Language Descriptions
- pid: eed4e6967c7a96e4cc2c590db40269cd97c8c98e
  title: 'Towards total scene understanding: Classification, annotation and segmentation
    in an automatic framework'
- pid: 13549b4e6fffbb7932b7a83a8eb6be27e6a60eca
  title: What Are You Talking About? Text-to-Image Coreference
- pid: 05e074abddd3fe987b9bebd46f6cf4bf8465c37e
  title: 'I2T: Image Parsing to Text Description'
- pid: eaaed23a2d94feb2f1c3ff22a25777c7a78f3141
  title: 'Every Picture Tells a Story: Generating Sentences from Images'
- pid: 6a26268d2ba9d34e5b59ae6e5c11a83cdca1a85e
  title: Matching Words and Pictures
- pid: 793c1c908672ea71aef9e1b41a46272aa27598f7
  title: Video In Sentences Out
- pid: acc4e56c44771ebf69302a06af51498aeb0a6ac8
  title: Parsing with Compositional Vector Grammars
- pid: 58bd0afc8a1b98e16a67ebda436e60c6f6410f56
  title: A Joint Model of Language and Perception for Grounded Attribute Learning
- pid: c1994ba5946456fc70948c549daf62363f13fa2d
  title: Indoor Segmentation and Support Inference from RGBD Images
- pid: 8d56b2a75aa5624660b60787e1f38ee2c70d493a
  title: 'Learning structured prediction models: a large margin approach'
- pid: e79272fe3d65197100eae8be9fec6469107969ae
  title: Object Detection with Discriminatively Trained Part Based Models
- pid: de5b0fd02ea4f4d67fe3ae0d74603b9822df4e42
  title: Are we ready for autonomous driving? The KITTI vision benchmark suite
- pid: dc97e7dbb821a4edfb5151bff4352655eedca9ee
  title: Large Margin Methods for Structured and Interdependent Output Variables
- pid: faf8444bad76e8aa727c8b2df42fefe7b8242957
  title: Shape matching and object recognition using shape contexts
slug: Visual-Semantic-Search:-Retrieving-Videos-via-Lin-Fidler
title: 'Visual Semantic Search: Retrieving Videos via Complex Textual Queries'
url: https://www.semanticscholar.org/paper/Visual-Semantic-Search:-Retrieving-Videos-via-Lin-Fidler/7afd833f484c8032e7fdc5f53188d2ebb0fb9934?sort=total-citations
venue: 2014 IEEE Conference on Computer Vision and Pattern Recognition
year: 2014
