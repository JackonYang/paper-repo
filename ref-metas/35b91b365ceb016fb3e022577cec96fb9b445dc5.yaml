authors:
- Felix Hill
- Antoine Bordes
- S. Chopra
- J. Weston
badges:
- id: OPEN_ACCESS
corpusId: 14915449
fieldsOfStudy:
- Computer Science
numCitedBy: 530
numCiting: 40
paperAbstract: 'We introduce a new test of how well language models capture meaning
  in children''s books. Unlike standard language modelling benchmarks, it distinguishes
  the task of predicting syntactic function words from that of predicting lower-frequency
  words, which carry greater semantic content. We compare a range of state-of-the-art
  models, each with a different way of encoding what has been previously read. We
  show that models which store explicit representations of long-term contexts outperform
  state-of-the-art neural language models at predicting semantic content words, although
  this advantage is not observed for syntactic function words. Interestingly, we find
  that the amount of text encoded in a single memory representation is highly influential
  to the performance: there is a sweet-spot, not too big and not too small, between
  single words and full sentences that allows the most meaningful information in a
  text to be effectively retained and recalled. Further, the attention over such window-based
  memories can be trained effectively through self-supervision. We then assess the
  generality of this principle by applying it to the CNN QA benchmark, which involves
  identifying named entities in paraphrased summaries of news articles, and achieve
  state-of-the-art performance.'
ref_count: 40
references:
- pid: 71ae756c75ac89e2d731c9c79649562b5768ff39
  title: Memory Networks
- pid: 2b669398c4cf2ebe04375c8b1beae20f4ac802fa
  title: Improving Word Representations via Global Context and Multiple Word Prototypes
- pid: fac2ca048fdd7e848f0b9ba2f7be25bb49186770
  title: The Microsoft Research Sentence Completion Challenge
- pid: 452059171226626718eb677358836328f884298e
  title: 'Ask Me Anything: Dynamic Memory Networks for Natural Language Processing'
- pid: be1fed9544830df1137e72b1d2396c40d3e18365
  title: A Cache-Based Natural Language Model for Speech Recognition
- pid: 564257469fa44cdb57e4272f85253efb9acfd69d
  title: 'MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of
    Text'
- pid: abb33d75dc297993fcc3fb75e0f4498f413eb4f6
  title: 'Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks'
- pid: 6e565308c8081e807709cb4a917443b737e6cdb4
  title: Large-scale Simple Question Answering with Memory Networks
- pid: d1505c6123c102e53eb19dff312cb25cea840b72
  title: Teaching Machines to Read and Comprehend
- pid: d1275b2a2ab53013310e759e5c6878b96df643d4
  title: Context dependent recurrent neural network language model
- pid: 4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e
  title: End-To-End Memory Networks
- pid: 4d8f2d14af5991d4f0d050d22216825cac3157bd
  title: 'Show, Attend and Tell: Neural Image Caption Generation with Visual Attention'
- pid: aea0f946e8dcddb65cc2e907456c42453f246a50
  title: "Large scale image annotation: learning\_to\_rank with\_joint word-image\
    \ embeddings"
- pid: 5082a1a13daea5c7026706738f8528391a1e6d59
  title: A Neural Attention Model for Abstractive Sentence Summarization
- pid: e837b79de602c69395498c1fbbe39bbb4e6f75ad
  title: Learning to Transduce with Unbounded Memory
- pid: 774e560a2cadcb84f4b1def7b152e5398b062efb
  title: Scalable Modified Kneser-Ney Language Model Estimation
- pid: d38e8631bba0720becdaf7b89f79d9f9dca45d82
  title: Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets
- pid: 1366de5bb112746a555e9c0cd00de3ad8628aea8
  title: Improving neural networks by preventing co-adaptation of feature detectors
- pid: d0be39ee052d246ae99c082a565aba25b811be2d
  title: Learning long-term dependencies with gradient descent is difficult
- pid: 68db33b01ef82cbafb440e5f4bee30458cbb9871
  title: Unconstrained On-line Handwriting Recognition with Recurrent Neural Networks
- pid: bcd857d75841aa3e92cd4284a8818aba9f6c0c3f
  title: Published as a conference paper at ICLR 2018 S IMULATING A CTION D YNAMICS
    WITH N EURAL P ROCESS N ETWORKS
- pid: b36b7f7c68923d14ba2859b5d28a1124616a8c89
  title: Transition-Based Dependency Parsing with Stack Long Short-Term Memory
- pid: 4c915c1eecb217c123a36dc6d3ce52d12c742614
  title: Simple statistical gradient-following algorithms for connectionist reinforcement
    learning
- pid: 2f5102ec3f70d0dea98c957cc2cab4d15d83a2da
  title: The Stanford CoreNLP Natural Language Processing Toolkit
- pid: 38f35dd624cd1cf827416e31ac5e0e0454028eca
  title: Regularization of Neural Networks using DropConnect
- pid: 94e3e7bc3d23276f0ee2d1cb8f9d14aa19668d5f
  title: Under Review as a Conference Paper at Iclr 2017 Delving into Transferable
    Adversarial Ex- Amples and Black-box Attacks
slug: The-Goldilocks-Principle:-Reading-Children's-Books-Hill-Bordes
title: 'The Goldilocks Principle: Reading Children''s Books with Explicit Memory Representations'
url: https://www.semanticscholar.org/paper/The-Goldilocks-Principle:-Reading-Children's-Books-Hill-Bordes/35b91b365ceb016fb3e022577cec96fb9b445dc5?sort=total-citations
venue: ICLR
year: 2016
