authors:
- Adriana Romero
- Nicolas Ballas
- S. Kahou
- Antoine Chassang
- C. Gatta
- Yoshua Bengio
badges:
- id: OPEN_ACCESS
corpusId: 2723173
fieldsOfStudy:
- Computer Science
numCitedBy: 2001
numCiting: 47
paperAbstract: While depth tends to improve network performances, it also makes gradient-based
  training more difficult since deeper networks tend to be more non-linear. The recently
  proposed knowledge distillation approach is aimed at obtaining small and fast-to-execute
  models, and it has shown that a student network could imitate the soft output of
  a larger teacher network or ensemble of networks. In this paper, we extend this
  idea to allow the training of a student that is deeper and thinner than the teacher,
  using not only the outputs but also the intermediate representations learned by
  the teacher as hints to improve the training process and final performance of the
  student. Because the student intermediate hidden layer will generally be smaller
  than the teacher's intermediate hidden layer, additional parameters are introduced
  to map the student hidden layer to the prediction of the teacher hidden layer. This
  allows one to train deeper students that can generalize better or run faster, a
  trade-off that is controlled by the chosen student capacity. For example, on CIFAR-10,
  a deep student network with almost 10.4 times less parameters outperforms a larger,
  state-of-the-art teacher network.
ref_count: 47
references:
- pid: 355d44f53428b1ac4fb2ab468d593c720640e5bd
  title: Greedy Layer-Wise Training of Deep Networks
- pid: fb91db6aa4f710814f8aec28a7f3ecbc4e5ad4fd
  title: Deeply-Supervised Nets
- pid: ccf415df5a83b343dae261286d29a40e8b80e6c6
  title: The Difficulty of Training Deep Architectures and the Effect of Unsupervised
    Pre-Training
- pid: e60ff004dde5c13ec53087872cfcdd12e85beb57
  title: Learning Deep Architectures for AI
- pid: 0c908739fbff75f03469d13d4a1a07de3414ee19
  title: Distilling the Knowledge in a Neural Network
- pid: 8978cf7574ceb35f4c3096be768c7547b28a35d0
  title: A Fast Learning Algorithm for Deep Belief Nets
- pid: e15cf50aa89fee8535703b9f9512fca5bfc43327
  title: Going deeper with convolutions
- pid: 5d90f06bb70a0a3dced62413346235c02b1aa086
  title: Learning Multiple Layers of Features from Tiny Images
- pid: 021fc345d40d3e6332cd2ef276e2eaa5e71102e4
  title: Speeding up Convolutional Neural Networks with Low Rank Expansions
- pid: 184ac0766262312ba76bbdece4e7ffad0aa8180b
  title: 'Representation Learning: A Review and New Perspectives'
- pid: eb42cf88027de515750f230b23b1a057dc782108
  title: Very Deep Convolutional Networks for Large-Scale Image Recognition
- pid: b7b915d508987b73b61eccd2b237e7ed099a2d29
  title: Maxout Networks
- pid: b8012351bc5ebce4a4b3039bbbba3ce393bc3315
  title: An empirical evaluation of deep architectures on problems with many factors
    of variation
- pid: 7ee368e60d0b826e78f965aad8d6c7d406127104
  title: Deep learning via semi-supervised embedding
- pid: e5ae8ab688051931b4814f6d32b18391f8d1fa8d
  title: Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation
- pid: 30c9bb327b7f2b9f1d1e5b69b9d0c97b410948d9
  title: Model compression
- pid: 162d958ff885f1462aeda91cd72582323fd6a1f4
  title: Gradient-based learning applied to document recognition
- pid: 02227c94dd41fe0b439e050d377b0beb5d427cda
  title: Reading Digits in Natural Images with Unsupervised Feature Learning
- pid: 855d0f722d75cc56a66a00ede18ace96bafee6bd
  title: 'Theano: new features and speed improvements'
- pid: 981ce6b655cc06416ff6bf7fac8c6c2076fd7fac
  title: Identifying and attacking the saddle point problem in high-dimensional non-convex
    optimization
- pid: e74f9b7f8eec6ba4704c206b93bc8079af3da4bd
  title: ImageNet Large Scale Visual Recognition Challenge
- pid: 4748d22348e72e6e06c2476486afddbc76e5eca7
  title: Product Quantization for Nearest Neighbor Search
- pid: 836acf6fc99ebf81d219e2b67f7ab25efc29a6a4
  title: 'Pylearn2: a machine learning research library'
slug: FitNets:-Hints-for-Thin-Deep-Nets-Romero-Ballas
title: 'FitNets: Hints for Thin Deep Nets'
url: https://www.semanticscholar.org/paper/FitNets:-Hints-for-Thin-Deep-Nets-Romero-Ballas/cd85a549add0c7c7def36aca29837efd24b24080?sort=total-citations
venue: ICLR
year: 2015
