authors:
- B. Efron
- T. Hastie
- I. Johnstone
- R. Tibshirani
badges:
- id: OPEN_ACCESS
corpusId: 121570279
fieldsOfStudy:
- Computer Science
numCitedBy: 7890
numCiting: 38
paperAbstract: 'The purpose of model selection algorithms such as All Subsets, Forward
  Selection and Backward Elimination is to choose a linear model on the basis of the
  same set of data to which the model will be applied. Typically we have available
  a large collection of possible covariates from which we hope to select a parsimonious
  set for the efficient prediction of a response variable. Least Angle Regression
  (LARS), a new model selection algorithm, is a useful and less greedy version of
  traditional forward selection methods. Three main properties are derived: (1) A
  simple modification of the LARS algorithm implements the Lasso, an attractive version
  of ordinary least squares that constrains the sum of the absolute regression coefficients;
  the LARS modification calculates all possible Lasso estimates for a given problem,
  using an order of magnitude less computer time than previous methods. (2) A different
  LARS modification efficiently implements Forward Stagewise linear regression, another
  promising new model selection method; this connection explains the similar numerical
  results previously observed for the Lasso and Stagewise, and helps us understand
  the properties of both methods, which are seen as constrained versions of the simpler
  LARS algorithm. (3) A simple approximation for the degrees of freedom of a LARS
  estimate is available, from which we derive a Cp estimate of prediction error; this
  allows a principled choice among the range of possible LARS estimates. LARS and
  its variants are computationally efficient: the paper describes a publicly available
  algorithm that requires only the same order of magnitude of computational effort
  as ordinary least squares applied to the full set of covariates.'
ref_count: 38
references:
- pid: b365b8e45b7d81f081de44ac8f9eadf9144f3ca5
  title: Regression Shrinkage and Selection via the Lasso
- pid: 1679beddda3a183714d380e944fe6bf586c083cd
  title: 'Greedy function approximation: A gradient boosting machine.'
- pid: 225ca57add3b3fb12ef01cc97c4683350dc93fe4
  title: Matrix computations
- pid: a92684c164b0c46020a371ae5116df74bb37a412
  title: Prediction Games and Arcing Algorithms
- pid: 225ca57add3b3fb12ef01cc97c4683350dc93fe4
  title: Matrix computations
- pid: 225ca57add3b3fb12ef01cc97c4683350dc93fe4
  title: Matrix computations
- pid: 4ba566223e426677d12a9a18418c023a4deec77e
  title: A decision-theoretic generalization of on-line learning and an application
    to boosting
- pid: 8017699564136f93af21575810d557dba1ee6fc6
  title: Classification and Regression Trees
- pid: e068be31ded63600aea068eacd12931efd2a1029
  title: UCI Repository of machine learning databases
slug: Least-angle-regression-Efron-Hastie
title: Least angle regression
url: https://www.semanticscholar.org/paper/Least-angle-regression-Efron-Hastie/1c7c5595dc7a1f5d360acf5c360ca1ca49536ba5?sort=total-citations
venue: ''
year: 2004
