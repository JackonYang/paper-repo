authors:
- Wojciech Zaremba
- Ilya Sutskever
- Oriol Vinyals
badges:
- id: OPEN_ACCESS
corpusId: 17719760
fieldsOfStudy:
- Computer Science
numCitedBy: 1970
numCiting: 39
paperAbstract: We present a simple regularization technique for Recurrent Neural Networks
  (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique
  for regularizing neural networks, does not work well with RNNs and LSTMs. In this
  paper, we show how to correctly apply dropout to LSTMs, and show that it substantially
  reduces overfitting on a variety of tasks. These tasks include language modeling,
  speech recognition, image caption generation, and machine translation.
ref_count: 39
references:
- pid: f9a1b3850dfd837793743565a8af95973d395a4e
  title: LSTM Neural Networks for Language Modeling
- pid: d1275b2a2ab53013310e759e5c6878b96df643d4
  title: Context dependent recurrent neural network language model
- pid: 9819b600a828a57e1cde047bbe710d3446b30da5
  title: Recurrent neural network based language model
- pid: 0894b06cff1cd0903574acaa7fcf071b144ae775
  title: Fast and Robust Neural Network Joint Models for Statistical Machine Translation
- pid: 4ef03716945bd3907458efbe1bbf8928dafc1efc
  title: 'Regularization and nonlinearities for neural language models: when are they
    needed?'
- pid: c0b624c46b51920dfec5aa02cc86323c0beb0df5
  title: Dropout Improves Recurrent Neural Networks for Handwriting Recognition
- pid: 4177ec52d1b80ed57f2e72b0f9a42365f1a8598d
  title: Speech recognition with deep recurrent neural networks
- pid: cb45e9217fe323fbc199d820e7735488fca2a9b3
  title: Strategies for training large scale neural network language models
- pid: 533ee188324b833e059cb59b654e6160776d5812
  title: How to Construct Deep Recurrent Neural Networks
- pid: 0b544dfe355a5070b60986319a3f51fb45d1348e
  title: "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical\
    \ Machine Translation"
- pid: cea967b59209c6be22829699f05b8b1ac4dc092d
  title: Sequence to Sequence Learning with Neural Networks
- pid: 89b1f4740ae37fd04f6ac007577bdd34621f0861
  title: Generating Sequences With Recurrent Neural Networks
- pid: 5522764282c85aea422f1c4dc92ff7e0ca6987bc
  title: A Clockwork RNN
- pid: 38f35dd624cd1cf827416e31ac5e0e0454028eca
  title: Regularization of Neural Networks using DropConnect
- pid: 5d5d4f49d6443c8529a6f5ebef5c499d47a869da
  title: Improving Neural Networks with Dropout
- pid: 96364af2d208ea75ca3aeb71892d2f7ce7326b55
  title: Statistical Language Models Based on Neural Networks
- pid: 44d2abe2175df8153f465f6c39b68b76a0d40ab9
  title: Long Short-Term Memory
- pid: d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0
  title: 'Show and tell: A neural image caption generator'
- pid: 944a1cfd79dbfb6fef460360a0765ba790f4027a
  title: Recurrent Continuous Translation Models
- pid: ec92efde21707ddf4b81f301cd58e2051c1a2443
  title: Fast dropout training
- pid: 3d82e058a5c40954b8f5db170a298a889a254c37
  title: 'Connectionist Speech Recognition: A Hybrid Approach'
- pid: e15cf50aa89fee8535703b9f9512fca5bfc43327
  title: Going deeper with convolutions
- pid: b8fe93d3e5205a450fdd8a9fb94cea0ab73b067f
  title: 'BYBLOS: The BBN continuous speech recognition system'
- pid: 0b44fcbeea9415d400c5f5789d6b892b6f98daff
  title: 'Building a Large Annotated Corpus of English: The Penn Treebank'
- pid: 71b7178df5d2b112d07e45038cb5637208659ff7
  title: 'Microsoft COCO: Common Objects in Context'
- pid: 375214ac340226e23ec428e92ec499fb89f508b8
  title: A Novel Connectionist System for Unconstrained Handwriting Recognition
slug: Recurrent-Neural-Network-Regularization-Zaremba-Sutskever
title: Recurrent Neural Network Regularization
url: https://www.semanticscholar.org/paper/Recurrent-Neural-Network-Regularization-Zaremba-Sutskever/f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97?sort=total-citations
venue: ArXiv
year: 2014
