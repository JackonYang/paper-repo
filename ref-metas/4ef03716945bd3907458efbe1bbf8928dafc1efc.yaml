authors:
- Marius Pachitariu
- M. Sahani
badges:
- id: OPEN_ACCESS
corpusId: 14444765
fieldsOfStudy:
- Computer Science
numCitedBy: 40
numCiting: 18
paperAbstract: Neural language models (LMs) based on recurrent neural networks (RNN)
  are some of the most successful word and character-level LMs. Why do they work so
  well, in particular better than linear neural LMs? Possible explanations are that
  RNNs have an implicitly better regularization or that RNNs have a higher capacity
  for storing patterns due to their nonlinearities or both. Here we argue for the
  first explanation in the limit of little training data and the second explanation
  for large amounts of text data. We show state-of-the-art performance on the popular
  and small Penn dataset when RNN LMs are regularized with random dropout. Nonetheless,
  we show even better performance from a simplified, much less expressive linear RNN
  model without off-diagonal entries in the recurrent matrix. We call this model an
  impulse-response LM (IRLM). Using random dropout, column normalization and annealed
  learning rates, IRLMs develop neurons that keep a memory of up to 50 words in the
  past and achieve a perplexity of 102.5 on the Penn dataset. On two large datasets
  however, the same regularization methods are unsuccessful for both models and the
  RNN's expressivity allows it to overtake the IRLM by 10 and 20 percent perplexity,
  respectively. Despite the perplexity gap, IRLMs still outperform RNNs on the Microsoft
  Research Sentence Completion (MRSC) task. We develop a slightly modified IRLM that
  separates long-context units (LCUs) from short-context units and show that the LCUs
  alone achieve a state-of-the-art performance on the MRSC task of 60.8%. Our analysis
  indicates that a fruitful direction of research for neural LMs lies in developing
  more accessible internal representations, and suggests an optimization regime of
  very high momentum terms for effectively training such models.
ref_count: 18
references:
- pid: e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de
  title: Generating Text with Recurrent Neural Networks
- pid: 5b0d644f5c4b9880cbaf79932c0a4fa98996f068
  title: A fast and simple algorithm for training neural probabilistic language models
- pid: 1fd7fc06653723b05abe5f3d1de393ddcf6bdddb
  title: SUBWORD LANGUAGE MODELING WITH NEURAL NETWORKS
- pid: 96364af2d208ea75ca3aeb71892d2f7ce7326b55
  title: Statistical Language Models Based on Neural Networks
- pid: aa7bfd2304201afbb19971ebde87b17e40242e91
  title: On the importance of initialization and momentum in deep learning
- pid: 6c2b28f9354f667cd5bd07afc0471d8334430da7
  title: A Neural Probabilistic Language Model
- pid: 44d2abe2175df8153f465f6c39b68b76a0d40ab9
  title: Long Short-Term Memory
- pid: d0be39ee052d246ae99c082a565aba25b811be2d
  title: Learning long-term dependencies with gradient descent is difficult
- pid: bd7d93193aad6c4b71cc8942e808753019e87706
  title: Three new graphical models for statistical language modelling
- pid: 1366de5bb112746a555e9c0cd00de3ad8628aea8
  title: Improving neural networks by preventing co-adaptation of feature detectors
- pid: 84069287da0a6b488b8c933f3cb5be759cb6237e
  title: On the difficulty of training recurrent neural networks
- pid: fac2ca048fdd7e848f0b9ba2f7be25bb49186770
  title: The Microsoft Research Sentence Completion Challenge
- pid: 77dfe038a9bdab27c4505444931eaa976e9ec667
  title: Empirical Evaluation and Combination of Advanced Language Modeling Techniques
- pid: e3ce36b9deb47aa6bb2aa19c4bfa71283b505025
  title: 'Noise-contrastive estimation: A new estimation principle for unnormalized
    statistical models'
- pid: c5145b1d15fea9340840cc8bb6f0e46e8934827f
  title: Understanding the exploding gradient problem
- pid: 0d073966e48ffb6dccde1e4eb3f0380c10c6a766
  title: 'Harnessing Nonlinearity: Predicting Chaotic Systems and Saving Energy in
    Wireless Communication'
slug: Regularization-and-nonlinearities-for-neural-when-Pachitariu-Sahani
title: 'Regularization and nonlinearities for neural language models: when are they
  needed?'
url: https://www.semanticscholar.org/paper/Regularization-and-nonlinearities-for-neural-when-Pachitariu-Sahani/4ef03716945bd3907458efbe1bbf8928dafc1efc?sort=total-citations
venue: ArXiv
year: 2013
