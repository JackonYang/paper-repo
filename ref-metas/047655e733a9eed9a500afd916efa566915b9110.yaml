authors:
- F. Gers
- N. Schraudolph
- J. Schmidhuber
badges:
- id: OPEN_ACCESS
corpusId: 474078
fieldsOfStudy:
- Computer Science
numCitedBy: 1270
numCiting: 27
paperAbstract: The temporal distance between events conveys information essential
  for numerous sequential tasks such as motor control and rhythm detection. While
  Hidden Markov Models tend to ignore this information, recurrent neural networks
  (RNNs) can in principle learn to make use of it. We focus on Long Short-Term Memory
  (LSTM) because it has been shown to outperform other RNNs on tasks involving long
  time lags. We find that LSTM augmented by "peephole connections" from its internal
  cells to its multiplicative gates can learn the fine distinction between sequences
  of spikes spaced either 50 or 49 time steps apart without the help of any short
  training exemplars. Without external resets or teacher forcing, our LSTM variant
  also learns to generate stable streams of precisely timed spikes and other highly
  nonlinear periodic patterns. This makes LSTM a promising approach for tasks that
  require the accurate measurement or generation of time intervals.
ref_count: 27
references:
- pid: 11540131eae85b2e11d53df7f1360eeb6476e7f4
  title: 'Learning to Forget: Continual Prediction with LSTM'
- pid: 44d2abe2175df8153f465f6c39b68b76a0d40ab9
  title: Long Short-Term Memory
- pid: d0be39ee052d246ae99c082a565aba25b811be2d
  title: Learning long-term dependencies with gradient descent is difficult
- pid: f828b401c86e0f8fddd8e77774e332dfd226cb05
  title: LSTM recurrent networks learn simple context-free and context-sensitive languages
- pid: 32e97eef94beacace020e79322cef0e1e5a76ee0
  title: 'Gradient calculations for dynamic recurrent neural networks: a survey'
- pid: ce9a21b93ba29d4145a8ef6bf401e77f261848de
  title: A Learning Algorithm for Continually Running Fully Recurrent Neural Networks
- pid: fd0da2f1d2b95e5b62221a00ff132219d0c853b7
  title: Adaptive neural oscillator using continuous-time back-propagation learning
- pid: 4a42b2104ca8ff891ae77c40a915d4c94c8f8428
  title: Experiments on Learning by Back Propagation.
- pid: 26bc0449360d7016f684eafae5b5d2feded32041
  title: An Efficient Gradient-Based Algorithm for On-Line Training of Recurrent Network
    Trajectories
- pid: 89b9a181801f32bf62c4237c4265ba036a79f9dc
  title: A Fixed Size Storage O(n3) Time Complexity Learning Algorithm for Fully Recurrent
    Continually Running Networks
- pid: aed054834e2c696807cc8b227ac7a4197196e211
  title: 'Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies'
- pid: 29085cdffb3277c1c8fd10ac09e0d89452c8db83
  title: An Input Output HMM Architecture
- pid: 3f3d13e95c25a8f6a753e38dfce88885097cbd43
  title: Untersuchungen zu dynamischen neuronalen Netzen
- pid: 10dae7fca6b65b61d155a622f0c6ca2bc3922251
  title: Gradient-based learning algorithms for recurrent networks and their computational
    complexity
slug: Learning-Precise-Timing-with-LSTM-Recurrent-Gers-Schraudolph
title: Learning Precise Timing with LSTM Recurrent Networks
url: https://www.semanticscholar.org/paper/Learning-Precise-Timing-with-LSTM-Recurrent-Gers-Schraudolph/047655e733a9eed9a500afd916efa566915b9110?sort=total-citations
venue: J. Mach. Learn. Res.
year: 2002
