authors:
- E. Baum
- D. Haussler
badges:
- id: OPEN_ACCESS
corpusId: 15659829
fieldsOfStudy:
- Mathematics
- Computer Science
numCitedBy: 1696
numCiting: 50
paperAbstract: "We address the question of when a network can be expected to generalize\
  \ from m random training examples chosen from some arbitrary probability distribution,\
  \ assuming that future test examples are drawn from the same distribution. Among\
  \ our results are the following bounds on appropriate sample vs. network size. Assume\
  \ 0 < \u220A 1/8. We show that if m O(W/\u220A log N/\u220A) random examples can\
  \ be loaded on a feedforward network of linear threshold functions with N nodes\
  \ and W weights, so that at least a fraction 1 \u220A/2 of the examples are correctly\
  \ classified, then one has confidence approaching certainty that the network will\
  \ correctly classify a fraction 1 \u220A of future test examples drawn from the\
  \ same distribution. Conversely, for fully-connected feedforward nets with one hidden\
  \ layer, any learning algorithm using fewer than (W/\u220A) random training examples\
  \ will, for some distributions of examples consistent with an appropriate weight\
  \ choice, fail at least some fixed fraction of the time to find a weight choice\
  \ that will correctly classify more than a 1 \u220A fraction of the future test\
  \ examples."
ref_count: 50
references:
- pid: 1d20eff70cb168111fb5cc320cb692a11f1adf62
  title: On the capabilities of multilayer perceptrons
- pid: a1dace286582d91916fe470d08f30381cf453f20
  title: 'Learning Quickly When Irrelevant Attributes Abound: A New Linear-Threshold
    Algorithm'
- pid: 33fdc91c520b54e097f5e09fae1cfc94793fbfcf
  title: Large Automatic Learning, Rule Extraction, and Generalization
- pid: b83396caf4762c906530c9219a9e4dd0658232b0
  title: A general lower bound on the number of examples needed for learning
- pid: 7f027c678076d7f2fd817f081079a334466449b1
  title: 'The Vapnik-Chervonenkis Dimension: Information versus Complexity in Learning'
- pid: a57c6d627ffc667ae3547073876c35d6420accff
  title: Connectionist Learning Procedures
- pid: 3b814ad3055d6bfd7828effdbfbf1372646b7c22
  title: 'Quantifying Inductive Bias: AI Learning Algorithms and Valiant''s Learning
    Framework'
- pid: 5e6dfb46ed298ff037e166291c128a465f90bfc0
  title: Some special vapnik-chervonenkis classes
- pid: de996c32045df6f7b404dda2a753b6a9becf3c08
  title: Parallel Networks that Learn to Pronounce English Text
- pid: 656a33c1db546da8490d6eba259e2a849d73a001
  title: 'Learning in Artificial Neural Networks: A Statistical Perspective'
- pid: 445ad69010658097fc317f7b83f1198179eebae8
  title: Geometrical and Statistical Properties of Systems of Linear Inequalities
    with Applications in Pattern Recognition
- pid: 10ddb646feddc12337b5a755c72e153e37088c02
  title: A theory of the learnable
- pid: 788c6d1b1419a0f7b7695c0e7e9e41cf54fbfe1b
  title: On the Density of Families of Sets
- pid: e85a68602abf92fcc1efb8b7aa90d27d141a80c2
  title: 'Automatic Pattern Recognition: A Study of the Probability of Error'
- pid: 01a1d065a5292be740e75029622a3ab5e71e3150
  title: Convergence of stochastic processes
- pid: a8e8f3c8d4418c8d62e306538c9c1292635e9d27
  title: Backpropagation Applied to Handwritten Zip Code Recognition
- pid: e0b8fa3496283d4d808fba9ff62d5f024bcf23be
  title: Learnability and the Vapnik-Chervonenkis dimension
- pid: 406033f22b6a671b94bcbdfaf63070b7ce6f3e48
  title: 'NETtalk: a parallel network that learns to read aloud'
- pid: a36b028d024bf358c4af1a5e1dc3ca0aed23b553
  title: 'Chervonenkis: On the uniform convergence of relative frequencies of events
    to their probabilities'
- pid: b07ce649d6f6eb636872527104b0209d3edc8188
  title: Pattern classification and scene analysis
- pid: 0b4d43ef0051a225e07af8194e81007ebba8d787
  title: Occam's razor
slug: What-Size-Net-Gives-Valid-Generalization-Baum-Haussler
title: What Size Net Gives Valid Generalization?
url: https://www.semanticscholar.org/paper/What-Size-Net-Gives-Valid-Generalization-Baum-Haussler/25406e6733a698bfc4ac836f8e74f458e75dad4f?sort=total-citations
venue: Neural Computation
year: 1989
