authors:
- V. Kazemi
- A. Elqursh
badges:
- id: OPEN_ACCESS
corpusId: 12446195
fieldsOfStudy:
- Computer Science
numCitedBy: 135
numCiting: 34
paperAbstract: This paper presents a new baseline for visual question answering task.
  Given an image and a question in natural language, our model produces accurate answers
  according to the content of the image. Our model, while being architecturally simple
  and relatively small in terms of trainable parameters, sets a new state of the art
  on both unbalanced and balanced VQA benchmark. On VQA 1.0 open ended challenge,
  our model achieves 64.6% accuracy on the test-standard set without using additional
  data, an improvement of 0.4% over state of the art, and on newly released VQA 2.0,
  our model scores 59.7% on validation set outperforming best previously reported
  results by 0.5%. The results presented in this paper are especially interesting
  because very similar models have been tried before but significantly lower performance
  were reported. In light of the new results we hope to see more meaningful research
  on visual question answering in the future.
ref_count: 34
references:
- pid: a27ed1310b9c0832bde8f906e0fcd23d1f3aa79c
  title: 'Making the V in VQA Matter: Elevating the Role of Image Understanding in
    Visual Question Answering'
- pid: 97ad70a9fa3f99adf18030e5e38ebe3d90daa2db
  title: 'VQA: Visual Question Answering'
- pid: fb9d253258d6b3beceb9d6cd7bba6e0a29ab875b
  title: Hierarchical Question-Image Co-Attention for Visual Question Answering
- pid: 20dbdf02497aa84510970d0f5e8b599073bca1bc
  title: 'Ask Me Anything: Free-Form Visual Question Answering Based on Knowledge
    from External Sources'
- pid: d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0
  title: 'Show and tell: A neural image caption generator'
- pid: fddc15480d086629b960be5bff96232f967f2252
  title: Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual
    Grounding
- pid: f96898d15a1bf1fa8925b1280d0e07a7a8e72194
  title: Dynamic Memory Networks for Visual and Textual Question Answering
- pid: 2c1890864c1c2b750f48316dc8b650ba4772adc5
  title: Stacked Attention Networks for Image Question Answering
- pid: b58e08741fb9803fa2a870eee139137d3bade332
  title: Training Recurrent Answering Units with Joint Loss Minimization for VQA
- pid: 21c99706bb26e9012bfb4d8d48009a3d45af59b2
  title: Neural Module Networks
- pid: 4d8f2d14af5991d4f0d050d22216825cac3157bd
  title: 'Show, Attend and Tell: Neural Image Caption Generation with Visual Attention'
- pid: d7ce5665a72c0b607f484c1b448875f02ddfac3b
  title: 'DenseCap: Fully Convolutional Localization Networks for Dense Captioning'
- pid: 1afb710a5b35a2352a6495e4bf6eef66808daf1b
  title: Multimodal Residual Learning for Visual QA
- pid: 14d9be7962a4ec5a6e55755f4c7588ea00793652
  title: 'Return of the Devil in the Details: Delving Deep into Convolutional Nets'
- pid: 23ffaa0fe06eae05817f527a47ac3291077f9e58
  title: Rethinking the Inception Architecture for Computer Vision
- pid: e65142010431ffc089b272a1174214e00693e503
  title: Generation and Comprehension of Unambiguous Object Descriptions
- pid: f651593fa6c83d717fc961482696a53b6fca5ab5
  title: Dual Attention Networks for Multimodal Reasoning and Matching
- pid: 2c03df8b48bf3fa39054345bafabfeff15bfd11d
  title: Deep Residual Learning for Image Recognition
- pid: 71b7178df5d2b112d07e45038cb5637208659ff7
  title: 'Microsoft COCO: Common Objects in Context'
- pid: eb42cf88027de515750f230b23b1a057dc782108
  title: Very Deep Convolutional Networks for Large-Scale Image Recognition
- pid: a6cb366736791bcccc5c8639de5a8f9636bf87e8
  title: 'Adam: A Method for Stochastic Optimization'
- pid: fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5
  title: Neural Machine Translation by Jointly Learning to Align and Translate
- pid: abd1c342495432171beb7ca8fd9551ef13cbd0ff
  title: ImageNet classification with deep convolutional neural networks
- pid: e15cf50aa89fee8535703b9f9512fca5bfc43327
  title: Going deeper with convolutions
- pid: b71ac1e9fb49420d13e084ac67254a0bbd40f83f
  title: Understanding the difficulty of training deep feedforward neural networks
- pid: 8a756d4d25511d92a45d0f4545fa819de993851d
  title: Recurrent Models of Visual Attention
- pid: 1366de5bb112746a555e9c0cd00de3ad8628aea8
  title: Improving neural networks by preventing co-adaptation of feature detectors
- pid: 44d2abe2175df8153f465f6c39b68b76a0d40ab9
  title: Long Short-Term Memory
- pid: 067e07b725ab012c80aa2f87857f6791c1407f6d
  title: Long short-term memory recurrent neural network architectures for large scale
    acoustic modeling
- pid: a538b05ebb01a40323997629e171c91aa28b8e2f
  title: Rectified Linear Units Improve Restricted Boltzmann Machines
slug: Show,-Ask,-Attend,-and-Answer:-A-Strong-Baseline-Kazemi-Elqursh
title: 'Show, Ask, Attend, and Answer: A Strong Baseline For Visual Question Answering'
url: https://www.semanticscholar.org/paper/Show,-Ask,-Attend,-and-Answer:-A-Strong-Baseline-Kazemi-Elqursh/d674b540dcd968bc302ea4360df3f4e85e994b55?sort=total-citations
venue: ArXiv
year: 2017
