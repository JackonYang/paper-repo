authors:
- Yoshua Bengio
- G. Mesnil
- Yann Dauphin
- S. Rifai
badges:
- id: OPEN_ACCESS
corpusId: 1334653
fieldsOfStudy:
- Computer Science
numCitedBy: 267
numCiting: 42
paperAbstract: 'It has been hypothesized, and supported with experimental evidence,
  that deeper representations, when well trained, tend to do a better job at disentangling
  the underlying factors of variation. We study the following related conjecture:
  better representations, in the sense of better disentangling, can be exploited to
  produce Markov chains that mix faster between modes. Consequently, mixing between
  modes would be more efficient at higher levels of representation. To better understand
  this, we propose a secondary conjecture: the higher-level samples fill more uniformly
  the space they occupy and the high-density manifolds tend to unfold when represented
  at higher levels. The paper discusses these hypotheses and tests them experimentally
  through visualization and measurements of mixing between modes and interpolating
  between samples.'
ref_count: 42
references:
- pid: aaaea06da21f22221d5fbfd61bb3a02439f0fe02
  title: A Generative Process for sampling Contractive Auto-Encoders
- pid: e60ff004dde5c13ec53087872cfcdd12e85beb57
  title: Learning Deep Architectures for AI
- pid: 6fdb77260fc83dff91c44fea0f31a2cb8ed13d04
  title: Scaling learning algorithms towards AI
- pid: 8978cf7574ceb35f4c3096be768c7547b28a35d0
  title: A Fast Learning Algorithm for Deep Belief Nets
- pid: 8a9a10170ee907acb3e582742bec5fa09116f302
  title: The Manifold Tangent Classifier
- pid: 85021c84383d18a7a4434d76dc8135fc6bdc0aa6
  title: Deep Boltzmann Machines
- pid: 3137bc367c61c0e507a5e3c1f8caeb26f292d79f
  title: Measuring Invariances in Deep Networks
- pid: 195d0a8233a7a46329c742eaff56c276f847fadc
  title: 'Contractive Auto-Encoders: Explicit Invariance During Feature Extraction'
- pid: b7d471970467a99bec4bce34c7dba5ef6745ad06
  title: The Curse of Highly Variable Functions for Local Kernel Machines
- pid: bf38dfb13352449b965c08282b66d3ffc5a0539f
  title: Unsupervised feature learning for audio classification using convolutional
    deep belief networks
- pid: 1bd6e929ed8384ea2212d50ab3c103ec018cc9fd
  title: A Bayesian/Information Theoretic Model of Learning to Learn via Multiple
    Task Sampling
- pid: 63f27307cb028f9341544fff32eceb2c3c652bf2
  title: Large-scale kernel machines
- pid: 57458bc1cffe5caa45a885af986d70f723f406b4
  title: 'A unified architecture for natural language processing: deep neural networks
    with multitask learning'
- pid: 6f4065f0cc99a0839b0248ffb4457e5f0277b30d
  title: 'Domain Adaptation for Large-Scale Sentiment Classification: A Deep Learning
    Approach'
- pid: 3dc3a0efe58eaf8564ca1965c0ffd23ec495b83f
  title: Autoencoders, Minimum Description Length and Helmholtz Free Energy
- pid: 162d958ff885f1462aeda91cd72582323fd6a1f4
  title: Gradient-based learning applied to document recognition
- pid: 56e6db25c6e72c0cff4ee099fa6e79bfbd20c7fb
  title: Almost optimal lower bounds for small depth circuits
- pid: 3653692a9d4d51909c9dd231d567bad928430654
  title: On the power of small-depth threshold circuits
- pid: 01b6affe3ea4eae1978aec54e87087feb76d9215
  title: Generalization and network design strategies
slug: Better-Mixing-via-Deep-Representations-Bengio-Mesnil
title: Better Mixing via Deep Representations
url: https://www.semanticscholar.org/paper/Better-Mixing-via-Deep-Representations-Bengio-Mesnil/d0965d8f9842f2db960b36b528107ca362c00d1a?sort=total-citations
venue: ICML
year: 2013
