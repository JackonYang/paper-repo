authors:
- Noam M. Shazeer
- Azalia Mirhoseini
- Krzysztof Maziarz
- Andy Davis
- Quoc V. Le
- Geoffrey E. Hinton
- J. Dean
badges:
- id: OPEN_ACCESS
corpusId: 12462234
fieldsOfStudy:
- Computer Science
numCitedBy: 862
numCiting: 45
paperAbstract: The capacity of a neural network to absorb information is limited by
  its number of parameters. Conditional computation, where parts of the network are
  active on a per-example basis, has been proposed in theory as a way of dramatically
  increasing model capacity without a proportional increase in computation. In practice,
  however, there are significant algorithmic and performance challenges. In this work,
  we address these challenges and finally realize the promise of conditional computation,
  achieving greater than 1000x improvements in model capacity with only minor losses
  in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated
  Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks.
  A trainable gating network determines a sparse combination of these experts to use
  for each example. We apply the MoE to the tasks of language modeling and machine
  translation, where model capacity is critical for absorbing the vast quantities
  of knowledge available in the training corpora. We present model architectures in
  which a MoE with up to 137 billion parameters is applied convolutionally between
  stacked LSTM layers. On large language modeling and machine translation benchmarks,
  these models achieve significantly better results than state-of-the-art at lower
  computational cost.
ref_count: 45
references:
- pid: 4d376d6978dad0374edfa6709c9556b42d3594d3
  title: 'Batch Normalization: Accelerating Deep Network Training by Reducing Internal
    Covariate Shift'
- pid: 72e93aa6767ee683de7f001fa72f1314e40a8f35
  title: Building high-level features using large scale unsupervised learning
- pid: b60abe57bc195616063be10638c6437358c81d1e
  title: Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation
- pid: cea967b59209c6be22829699f05b8b1ac4dc092d
  title: Sequence to Sequence Learning with Neural Networks
- pid: 2f2d8f8072e5cc9b296fad551f65f183bdbff7aa
  title: Exploring the Limits of Language Modeling
- pid: abd1c342495432171beb7ca8fd9551ef13cbd0ff
  title: ImageNet classification with deep convolutional neural networks
- pid: a6cb366736791bcccc5c8639de5a8f9636bf87e8
  title: 'Adam: A Method for Stochastic Optimization'
- pid: 2c03df8b48bf3fa39054345bafabfeff15bfd11d
  title: Deep Residual Learning for Image Recognition
- pid: 067e07b725ab012c80aa2f87857f6791c1407f6d
  title: Long short-term memory recurrent neural network architectures for large scale
    acoustic modeling
- pid: 62c76ca0b2790c34e85ba1cce09d47be317c7235
  title: Estimating or Propagating Gradients Through Stochastic Neurons for Conditional
    Computation
- pid: 5d833331b0e22ff359db05c62a8bca18c4f04b68
  title: One billion word benchmark for measuring progress in statistical language
    modeling
- pid: 93499a7c7f699b6630a86fad964536f9423bb6d0
  title: Effective Approaches to Attention-based Neural Machine Translation
- pid: dbde7dfa6cae81df8ac19ef500c42db96c3d1edd
  title: 'Google''s Neural Machine Translation System: Bridging the Gap between Human
    and Machine Translation'
- pid: 9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d
  title: 'TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems'
- pid: f267934e9de60c5badfa9d3f28918e67ae7a2bf4
  title: Generative Image Modeling Using Spatial LSTMs
- pid: 8ff840a40d3f1557c55c19d4d636da77103168ce
  title: 'Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin'
- pid: 44d2abe2175df8153f465f6c39b68b76a0d40ab9
  title: Long Short-Term Memory
- pid: 413c1142de9d91804d6d11c67ff3fed59c9fc279
  title: Adaptive Subgradient Methods for Online Learning and Stochastic Optimization
- pid: f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97
  title: Recurrent Neural Network Regularization
- pid: fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5
  title: Neural Machine Translation by Jointly Learning to Align and Translate
- pid: c8d90974c3f3b40fa05e322df2905fc16204aa56
  title: Adaptive Mixtures of Local Experts
- pid: 9548ac30c113562a51e603dbbc8e9fa651cfd3ab
  title: Improved backing-off for M-gram language modeling
- pid: 1956c239b3552e030db1b78951f64781101125ed
  title: Addressing the Rare Word Problem in Neural Machine Translation
- pid: 94e3e7bc3d23276f0ee2d1cb8f9d14aa19668d5f
  title: Under Review as a Conference Paper at Iclr 2017 Delving into Transferable
    Adversarial Ex- Amples and Black-box Attacks
- pid: 31868290adf1c000c611dfc966b514d5a34e8d23
  title: 'Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared
    Views of Four Research Groups'
- pid: f6d8a7fc2e2d53923832f9404376512068ca2a57
  title: Hierarchical mixtures of experts and the EM algorithm
- pid: 97cedf99252026f58e8154bc61d49cf885d42030
  title: "Edinburgh\u2019s Phrase-based Machine Translation Systems for WMT-14"
slug: Outrageously-Large-Neural-Networks:-The-Layer-Shazeer-Mirhoseini
title: 'Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts
  Layer'
url: https://www.semanticscholar.org/paper/Outrageously-Large-Neural-Networks:-The-Layer-Shazeer-Mirhoseini/510e26733aaff585d65701b9f1be7ca9d5afc586?sort=total-citations
venue: ICLR
year: 2017
