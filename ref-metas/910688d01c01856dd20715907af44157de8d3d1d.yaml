authors:
- A. Krogh
- Jesper Vedelsby
badges:
- id: OPEN_ACCESS
corpusId: 5846986
fieldsOfStudy:
- Computer Science
numCitedBy: 1971
numCiting: 8
paperAbstract: Learning of continuous valued functions using neural network ensembles
  (committees) can give improved accuracy, reliable estimation of the generalization
  error, and active learning. The ambiguity is defined as the variation of the output
  of ensemble members averaged over unlabeled data, so it quantifies the disagreement
  among the networks. It is discussed how to use the ambiguity in combination with
  cross-validation to give a reliable estimate of the ensemble generalization error,
  and how this type of ensemble cross-validation can sometimes improve performance.
  It is shown how to estimate the optimal weights of the ensemble members using unlabeled
  data. By a generalization of query by committee, it is finally shown how the ambiguity
  can be used to select new training data to be labeled in an active learning scheme.
ref_count: 8
references:
- pid: 7e1291583873fb890e7922ec0dfefd4846df46c9
  title: Stacked generalization
- pid: a34e35dbbc6911fa7b94894dffdc0076a261b6f0
  title: Neural Networks and the Bias/Variance Dilemma
- pid: 941ef255d31b5becbf0a3281bcf7ac0122e4c833
  title: Query by committee
slug: Neural-Network-Ensembles,-Cross-Validation,-and-Krogh-Vedelsby
title: Neural Network Ensembles, Cross Validation, and Active Learning
url: https://www.semanticscholar.org/paper/Neural-Network-Ensembles,-Cross-Validation,-and-Krogh-Vedelsby/910688d01c01856dd20715907af44157de8d3d1d?sort=total-citations
venue: NIPS
year: 1994
