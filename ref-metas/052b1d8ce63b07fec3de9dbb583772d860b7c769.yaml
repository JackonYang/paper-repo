authors:
- D. Rumelhart
- Geoffrey E. Hinton
- Ronald J. Williams
badges:
- id: OPEN_ACCESS
corpusId: 205001834
fieldsOfStudy:
- Computer Science
numCitedBy: 20335
numCiting: 3
paperAbstract: "We describe a new learning procedure, back-propagation, for networks\
  \ of neurone-like units. The procedure repeatedly adjusts the weights of the connections\
  \ in the network so as to minimize a measure of the difference between the actual\
  \ output vector of the net and the desired output vector. As a result of the weight\
  \ adjustments, internal \u2018hidden\u2019 units which are not part of the input\
  \ or output come to represent important features of the task domain, and the regularities\
  \ in the task are captured by the interactions of these units. The ability to create\
  \ useful new features distinguishes back-propagation from earlier, simpler methods\
  \ such as the perceptron-convergence procedure1."
ref_count: 3
references:
- pid: ff2c2e3e83d1e8828695484728393c76ee07a101
  title: 'Parallel distributed processing: explorations in the microstructure of cognition,
    vol. 1: foundations'
- pid: cccc0a4817fd5f6d8758c66b4065a23897d49f1d
  title: Principles of neurodynamics
slug: Learning-representations-by-back-propagating-errors-Rumelhart-Hinton
title: Learning representations by back-propagating errors
url: https://www.semanticscholar.org/paper/Learning-representations-by-back-propagating-errors-Rumelhart-Hinton/052b1d8ce63b07fec3de9dbb583772d860b7c769?sort=total-citations
venue: Nature
year: 1986
