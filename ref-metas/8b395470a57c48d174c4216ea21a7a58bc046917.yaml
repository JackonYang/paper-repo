authors:
- Holger Schwenk
- J. Gauvain
badges:
- id: UNPAYWALL
- id: OPEN_ACCESS
corpusId: 12469208
fieldsOfStudy:
- Computer Science
numCitedBy: 128
numCiting: 28
paperAbstract: During the last years there has been growing interest in using neural
  networks for language modeling. In contrast to the well known back-off n-gram language
  models, the neural network approach attempts to overcome the data sparseness problem
  by performing the estimation in a continuous space. This type of language model
  was mostly used for tasks for which only a very limited amount of in-domain training
  data is available.In this paper we present new algorithms to train a neural network
  language model on very large text corpora. This makes possible the use of the approach
  in domains where several hundreds of millions words of texts are available. The
  neural network language model is evaluated in a state-of-the-art real-time continuous
  speech recognizer for French Broadcast News. Word error reductions of 0.5% absolute
  are reported using only a very limited amount of additional processing time.
ref_count: 28
references:
- pid: d6fb7546a29320eadad868af66835059db93d99f
  title: Efficient training of large neural networks for language modeling
- pid: e41498c05d4c68e4750fb84a380317a112d97b01
  title: Connectionist language modeling for large vocabulary continuous speech recognition
- pid: 9319ca5a532462f9f3515ac3d317668aa9650d5b
  title: Exact training of a neural syntactic language model
- pid: 6c2b28f9354f667cd5bd07afc0471d8334430da7
  title: A Neural Probabilistic Language Model
- pid: a1c3748820d6b5ab4e7334524815df9bb6d20aed
  title: Structured language modeling
- pid: b0130277677e5b915d5cd86b3afafd77fd08eb2e
  title: Estimation of probabilities from sparse data for the language model component
    of a speech recognizer
- pid: 076fa8d095c37c657f2aff39cf90bc2ea883b7cb
  title: A maximum entropy approach to adaptive statistical language modelling
- pid: 3de5d40b60742e3dfa86b19e7f660962298492af
  title: Class-Based n-gram Models of Natural Language
- pid: 399da68d3b97218b6c80262df7963baa89dcc71b
  title: SRILM - an extensible language modeling toolkit
- pid: 121afb1502c90d510f64a0b3276a5454616a64e7
  title: Boosting a weak learning algorithm by majority
- pid: d4e8bed3b50a035e1eabad614fe4218a34b3b178
  title: An empirical study of smoothing techniques for language modeling
- pid: 385197d4c02593e2823c71e4f90a0993b703620e
  title: Statistical learning theory
- pid: 8213dbed4db44e113af3ed17d6dad57471a0c048
  title: The Nature of Statistical Learning Theory
slug: Training-Neural-Network-Language-Models-on-Very-Schwenk-Gauvain
title: Training Neural Network Language Models on Very Large Corpora
url: https://www.semanticscholar.org/paper/Training-Neural-Network-Language-Models-on-Very-Schwenk-Gauvain/8b395470a57c48d174c4216ea21a7a58bc046917?sort=total-citations
venue: HLT
year: 2005
