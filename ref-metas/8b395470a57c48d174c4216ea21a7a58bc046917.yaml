authors:
- Holger Schwenk
- J. Gauvain
badges:
- id: UNPAYWALL
- id: OPEN_ACCESS
corpusId: 12469208
fieldsOfStudy:
- Computer Science
numCitedBy: 128
numCiting: 28
paperAbstract: During the last years there has been growing interest in using neural
  networks for language modeling. In contrast to the well known back-off n-gram language
  models, the neural network approach attempts to overcome the data sparseness problem
  by performing the estimation in a continuous space. This type of language model
  was mostly used for tasks for which only a very limited amount of in-domain training
  data is available.In this paper we present new algorithms to train a neural network
  language model on very large text corpora. This makes possible the use of the approach
  in domains where several hundreds of millions words of texts are available. The
  neural network language model is evaluated in a state-of-the-art real-time continuous
  speech recognizer for French Broadcast News. Word error reductions of 0.5% absolute
  are reported using only a very limited amount of additional processing time.
ref_count: 28
references:
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 38
  pid: d6fb7546a29320eadad868af66835059db93d99f
  title: Efficient training of large neural networks for language modeling
  year: 2004
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 156
  pid: e41498c05d4c68e4750fb84a380317a112d97b01
  title: Connectionist language modeling for large vocabulary continuous speech recognition
  year: 2002
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 26
  pid: 9319ca5a532462f9f3515ac3d317668aa9650d5b
  title: Exact training of a neural syntactic language model
  year: 2004
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 6011
  pid: 6c2b28f9354f667cd5bd07afc0471d8334430da7
  title: A Neural Probabilistic Language Model
  year: 2000
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 316
  pid: a1c3748820d6b5ab4e7334524815df9bb6d20aed
  title: Structured language modeling
  year: 2000
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1908
  pid: b0130277677e5b915d5cd86b3afafd77fd08eb2e
  title: Estimation of probabilities from sparse data for the language model component
    of a speech recognizer
  year: 1987
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 608
  pid: 076fa8d095c37c657f2aff39cf90bc2ea883b7cb
  title: A maximum entropy approach to adaptive statistical language modelling
  year: 1996
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 3318
  pid: 3de5d40b60742e3dfa86b19e7f660962298492af
  title: Class-Based n-gram Models of Natural Language
  year: 1992
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 4997
  pid: 399da68d3b97218b6c80262df7963baa89dcc71b
  title: SRILM - an extensible language modeling toolkit
  year: 2002
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1285
  pid: 121afb1502c90d510f64a0b3276a5454616a64e7
  title: Boosting a weak learning algorithm by majority
  year: 1990
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 2861
  pid: d4e8bed3b50a035e1eabad614fe4218a34b3b178
  title: An empirical study of smoothing techniques for language modeling
  year: 1999
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 26321
  pid: 385197d4c02593e2823c71e4f90a0993b703620e
  title: Statistical learning theory
  year: 1998
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 38756
  pid: 8213dbed4db44e113af3ed17d6dad57471a0c048
  title: The Nature of Statistical Learning Theory
  year: 2000
slug: Training-Neural-Network-Language-Models-on-Very-Schwenk-Gauvain
title: Training Neural Network Language Models on Very Large Corpora
url: https://www.semanticscholar.org/paper/Training-Neural-Network-Language-Models-on-Very-Schwenk-Gauvain/8b395470a57c48d174c4216ea21a7a58bc046917?sort=total-citations
venue: HLT
year: 2005
