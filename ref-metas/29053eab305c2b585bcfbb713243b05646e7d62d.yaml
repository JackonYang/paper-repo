authors:
- A. Stolcke
badges:
- id: OPEN_ACCESS
corpusId: 8150809
fieldsOfStudy:
- Computer Science
numCitedBy: 348
numCiting: 14
paperAbstract: A criterion for pruning parameters from N-gram backoff language models
  is developed, based on the relative entropy between the original and the pruned
  model. It is shown that the relative entropy resulting from pruning a single N-gram
  can be computed exactly and efficiently for backoff models. The relative entropy
  measure can be expressed as a relative change in training set perplexity. This leads
  to a simple pruning criterion whereby all N-grams that change perplexity by less
  than a threshold are removed from the model. Experiments show that a production-quality
  Hub4 LM can be reduced to 26% its original size without increasing recognition error.
  We also compare the approach to a heuristic pruning criterion by Seymore and Rosenfeld
  (1996), and show that their approach can be interpreted as an approximation to the
  relative entropy criterion. Experimentally, both approaches select similar sets
  of N-grams (about 85% overlap), with the exact relative entropy criterion giving
  marginally better performance.
ref_count: 14
references:
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 109
  pid: d2c182f105d8ba97a7f26364055cdc4fb65b5a7f
  title: Scalable backoff language models
  year: 1996
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 130
  pid: 11376222763d16f39a5be1a5151d55556997236e
  title: The Power of Amnesia
  year: 1993
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1908
  pid: b0130277677e5b915d5cd86b3afafd77fd08eb2e
  title: Estimation of probabilities from sparse data for the language model component
    of a speech recognizer
  year: 1987
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 317
  pid: 837fcdfe8fdcc9c7f2f8a8c58b2afd7e64b43ee0
  title: Hidden Markov Model} Induction by Bayesian Model Merging
  year: 1992
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 42795
  pid: 7dbdb4209626fd92d2436a058663206216036e68
  title: Elements of Information Theory
  year: 1991
- fieldsOfStudy:
  - Philosophy
  numCitedBy: 672670
  pid: 90006064cafcb0a9ad8a30cffeb56efe7e14129b
  title: '"J."'
  year: 2021
- fieldsOfStudy:
  - Mathematics
  numCitedBy: 3274
  pid: b2986b25f50babd536dd0ecf2237d9eabf5843c2
  title: THE POPULATION FREQUENCIES OF SPECIES AND THE ESTIMATION OF POPULATION PARAMETERS
  year: 1953
slug: Entropy-based-Pruning-of-Backoff-Language-Models-Stolcke
title: Entropy-based Pruning of Backoff Language Models
url: https://www.semanticscholar.org/paper/Entropy-based-Pruning-of-Backoff-Language-Models-Stolcke/29053eab305c2b585bcfbb713243b05646e7d62d?sort=total-citations
venue: ArXiv
year: 2000
