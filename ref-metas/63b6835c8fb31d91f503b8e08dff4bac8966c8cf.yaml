authors:
- J. Schmidhuber
- M. Mozer
- D. Prelinger
badges:
- id: OPEN_ACCESS
corpusId: 1881904
fieldsOfStudy:
- Computer Science
numCitedBy: 18
numCiting: 29
paperAbstract: Neural networks have proven poor at learning the structure in complex
  and extended temporal sequences in which contingencies among elements can span long
  time lags. The principle of history compression 18] provides a means of transforming
  long sequences with redundant information into equivalent shorter sequences; the
  shorter sequences are more easily manipulated and learned by neural networks. The
  principle states that expected sequence elements can be removed from the sequence
  to form an equivalent, more compact sequence without loss of information. The principle
  was embodied in a neural net predictive architecture that attempted to anticipate
  the next element of a sequence given the previous elements. If the prediction was
  accurate, the next element was discarded; otherwise, it was passed on to a second
  network that processed the sequence in some fashion (e.g., recognition, classiication,
  autoencoding, etc.). As originally proposed, a binary judgement was made as to the
  predictability of each element. Here, we describe a contininuous version of history
  compression in which elements are discarded in a graded fashion dependent on their
  predictability, embodied by their (Shannon) information. We implement continuous
  history compression using a RAAM architecture, yielding a class of sequence learning
  algorithms that are both entirely local and still able to bridge long time lags
  between correlated events.
ref_count: 29
references:
- pid: 50c770b425a5bb25c77387f687a9910a9d130722
  title: Learning Complex, Extended Sequences Using the Principle of History Compression
- pid: 2f7c4048a03281e976f28d35c2f9fef3a58346e6
  title: Learning Unambiguous Reduced Sequence Descriptions
- pid: e141d68065ce638f9fc4f006eab2f66711e89768
  title: Induction of Multiscale Temporal Structure
- pid: 6d72a0e83e772468c6084ae7c79e43a4f5989feb
  title: A Local Learning Algorithm for Dynamic Feedforward and Recurrent Networks
- pid: 424710825d726e10b016204ed2bc979e2a342d10
  title: Experimental Analysis of the Real-time Recurrent Learning Algorithm
- pid: 668087f0ae7ce1de6e0bd0965dbb480c08103260
  title: Finding Structure in Time
- pid: 34468c0aa95a7aea212d8738ab899a69b2fc14c6
  title: Learning State Space Trajectories in Recurrent Neural Networks
- pid: 266e07d0dd9a75b61e3632e9469993dbaf063f1c
  title: Generalization of backpropagation with application to a recurrent gas market
    model
- pid: 89b9a181801f32bf62c4237c4265ba036a79f9dc
  title: A Fixed Size Storage O(n3) Time Complexity Learning Algorithm for Fully Recurrent
    Continually Running Networks
- pid: b247fe6efc9e1011555428647f390dd98bdd3446
  title: Static and Dynamic Error Propagation Networks with Application to Speech
    Coding
- pid: 111fd833a4ae576cfdbb27d87d2f8fc0640af355
  title: Learning internal representations by error propagation
- pid: 56623a496727d5c71491850e04512ddf4152b487
  title: 'Beyond Regression : "New Tools for Prediction and Analysis in the Behavioral
    Sciences'
- pid: 10dae7fca6b65b61d155a622f0c6ca2bc3922251
  title: Gradient-based learning algorithms for recurrent networks and their computational
    complexity
slug: Continuous-history-compression-Schmidhuber-Mozer
title: Continuous history compression
url: https://www.semanticscholar.org/paper/Continuous-history-compression-Schmidhuber-Mozer/63b6835c8fb31d91f503b8e08dff4bac8966c8cf?sort=total-citations
venue: ''
year: 1993
