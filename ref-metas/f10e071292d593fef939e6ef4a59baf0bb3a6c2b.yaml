authors:
- Wojciech Zaremba
- Ilya Sutskever
badges:
- id: OPEN_ACCESS
corpusId: 16228924
fieldsOfStudy:
- Computer Science
numCitedBy: 164
numCiting: 27
paperAbstract: The expressive power of a machine learning model is closely related
  to the number of sequential computational steps it can learn. For example, Deep
  Neural Networks have been more successful than shallow networks because they can
  perform a greater number of sequential computational steps (each highly parallel).
  The Neural Turing Machine (NTM) [8] is a model that can compactly express an even
  greater number of sequential computational steps, so it is even more powerful than
  a DNN. Its memory addressing operations are designed to be differentiable; thus
  the NTM can be trained with backpropagation. While differentiable memory is relatively
  easy to implement and train, it necessitates accessing the entire memory content
  at each computational step. This makes it difficult to implement a fast NTM. In
  this work, we use the Re inforce algorithm to learn where to access the memory,
  while using backpropagation to learn what to write to the memory. We call this model
  the RL-NTM. Reinforce allows our model to access a constant number of memory cells
  at each computational step, so its implementation can be faster. The RL-NTM is the
  first mo del that can, in principle, learn programs of unbounded running time. We
  successfully trained the RL-NTM to solve a number of algorithmic tasks that are
  simpler than the ones solvable by the fully differentiable NTM. As the RL-NTM is
  a fairly intricate model, we needed a method for verifying the correctness of our
  implementation. To do so, we developed a simple technique for numerically checking
  arbitrary implementations of models that use Reinforce, which may be of independent
  interest.
ref_count: 27
references:
- pid: 0d0b3ff23fbf1ec7a0dfebd923a3ef224306491a
  title: Learning to Execute
- pid: c3823aacea60bc1f2cabb9283144690a3d015db5
  title: Neural Turing Machines
- pid: d38e8631bba0720becdaf7b89f79d9f9dca45d82
  title: Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets
- pid: 44d2abe2175df8153f465f6c39b68b76a0d40ab9
  title: Long Short-Term Memory
- pid: 8a756d4d25511d92a45d0f4545fa819de993851d
  title: Recurrent Models of Visual Attention
- pid: fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5
  title: Neural Machine Translation by Jointly Learning to Align and Translate
- pid: 8de174ab5419b9d3127695405efd079808e956e8
  title: Curriculum learning
- pid: 4c915c1eecb217c123a36dc6d3ce52d12c742614
  title: Simple statistical gradient-following algorithms for connectionist reinforcement
    learning
- pid: 89b1f4740ae37fd04f6ac007577bdd34621f0861
  title: Generating Sequences With Recurrent Neural Networks
- pid: bef2ae523cd4447af687fae13bfbb606e4a4a5ca
  title: A Formal Theory of Inductive Inference. Part II
- pid: a583af2696030bcf5f556edc74573fbee902be0b
  title: Weakly Supervised Memory Networks
- pid: 4d8f2d14af5991d4f0d050d22216825cac3157bd
  title: 'Show, Attend and Tell: Neural Image Caption Generation with Visual Attention'
- pid: 47d2dc34e1d02a8109f5c04bb6939725de23716d
  title: 'End-to-end Continuous Speech Recognition using Attention-based Recurrent
    NN: First Results'
- pid: ab4aec5e0714b352e6c90d063fe830cbc70912bc
  title: Connectionist models and their properties
- pid: 7845f1d3e796b5704d4bd37a945e0cf3fb8bbf1f
  title: Multiple Object Recognition with Visual Attention
- pid: 40b5e2fa3eaae17886c066c9f107c8c865b4808b
  title: A Formal Theory of Inductive Inference. Part I
slug: Reinforcement-Learning-Neural-Turing-Machines-Zaremba-Sutskever
title: Reinforcement Learning Neural Turing Machines
url: https://www.semanticscholar.org/paper/Reinforcement-Learning-Neural-Turing-Machines-Zaremba-Sutskever/f10e071292d593fef939e6ef4a59baf0bb3a6c2b?sort=total-citations
venue: ArXiv
year: 2015
