authors:
- Wojciech Zaremba
- Ilya Sutskever
badges:
- id: OPEN_ACCESS
corpusId: 16228924
fieldsOfStudy:
- Computer Science
numCitedBy: 164
numCiting: 27
paperAbstract: The expressive power of a machine learning model is closely related
  to the number of sequential computational steps it can learn. For example, Deep
  Neural Networks have been more successful than shallow networks because they can
  perform a greater number of sequential computational steps (each highly parallel).
  The Neural Turing Machine (NTM) [8] is a model that can compactly express an even
  greater number of sequential computational steps, so it is even more powerful than
  a DNN. Its memory addressing operations are designed to be differentiable; thus
  the NTM can be trained with backpropagation. While differentiable memory is relatively
  easy to implement and train, it necessitates accessing the entire memory content
  at each computational step. This makes it difficult to implement a fast NTM. In
  this work, we use the Re inforce algorithm to learn where to access the memory,
  while using backpropagation to learn what to write to the memory. We call this model
  the RL-NTM. Reinforce allows our model to access a constant number of memory cells
  at each computational step, so its implementation can be faster. The RL-NTM is the
  first mo del that can, in principle, learn programs of unbounded running time. We
  successfully trained the RL-NTM to solve a number of algorithmic tasks that are
  simpler than the ones solvable by the fully differentiable NTM. As the RL-NTM is
  a fairly intricate model, we needed a method for verifying the correctness of our
  implementation. To do so, we developed a simple technique for numerically checking
  arbitrary implementations of models that use Reinforce, which may be of independent
  interest.
ref_count: 27
references:
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 460
  pid: 0d0b3ff23fbf1ec7a0dfebd923a3ef224306491a
  title: Learning to Execute
  year: 2014
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1634
  pid: c3823aacea60bc1f2cabb9283144690a3d015db5
  title: Neural Turing Machines
  year: 2014
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 346
  pid: d38e8631bba0720becdaf7b89f79d9f9dca45d82
  title: Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets
  year: 2015
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 51707
  pid: 44d2abe2175df8153f465f6c39b68b76a0d40ab9
  title: Long Short-Term Memory
  year: 1997
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 2410
  pid: 8a756d4d25511d92a45d0f4545fa819de993851d
  title: Recurrent Models of Visual Attention
  year: 2014
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 19346
  pid: fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5
  title: Neural Machine Translation by Jointly Learning to Align and Translate
  year: 2015
- fieldsOfStudy:
  - Education
  numCitedBy: 3193
  pid: 8de174ab5419b9d3127695405efd079808e956e8
  title: Curriculum learning
  year: 2009
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 5181
  pid: 4c915c1eecb217c123a36dc6d3ce52d12c742614
  title: Simple statistical gradient-following algorithms for connectionist reinforcement
    learning
  year: 2004
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 3153
  pid: 89b1f4740ae37fd04f6ac007577bdd34621f0861
  title: Generating Sequences With Recurrent Neural Networks
  year: 2013
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1585
  pid: bef2ae523cd4447af687fae13bfbb606e4a4a5ca
  title: A Formal Theory of Inductive Inference. Part II
  year: 1964
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 99
  pid: a583af2696030bcf5f556edc74573fbee902be0b
  title: Weakly Supervised Memory Networks
  year: 2015
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 7252
  pid: 4d8f2d14af5991d4f0d050d22216825cac3157bd
  title: 'Show, Attend and Tell: Neural Image Caption Generation with Visual Attention'
  year: 2015
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 375
  pid: 47d2dc34e1d02a8109f5c04bb6939725de23716d
  title: 'End-to-end Continuous Speech Recognition using Attention-based Recurrent
    NN: First Results'
  year: 2014
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 736
  pid: ab4aec5e0714b352e6c90d063fe830cbc70912bc
  title: Connectionist models and their properties
  year: 1988
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 848
  pid: 7845f1d3e796b5704d4bd37a945e0cf3fb8bbf1f
  title: Multiple Object Recognition with Visual Attention
  year: 2015
- fieldsOfStudy:
  - Philosophy
  numCitedBy: 1286
  pid: 40b5e2fa3eaae17886c066c9f107c8c865b4808b
  title: A Formal Theory of Inductive Inference. Part I
  year: 1964
slug: Reinforcement-Learning-Neural-Turing-Machines-Zaremba-Sutskever
title: Reinforcement Learning Neural Turing Machines
url: https://www.semanticscholar.org/paper/Reinforcement-Learning-Neural-Turing-Machines-Zaremba-Sutskever/f10e071292d593fef939e6ef4a59baf0bb3a6c2b?sort=total-citations
venue: ArXiv
year: 2015
