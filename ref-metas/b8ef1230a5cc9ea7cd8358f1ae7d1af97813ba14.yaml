authors:
- T. Raiko
- H. Valpola
- Yann LeCun
badges:
- id: OPEN_ACCESS
corpusId: 2204994
fieldsOfStudy:
- Computer Science
numCitedBy: 178
numCiting: 17
paperAbstract: We transform the outputs of each hidden neuron in a multi-layer perceptron
  network to have zero output and zero slope on average, and use separate shortcut
  connections to model the linear dependencies instead. This transformation aims at
  separating the problems of learning the linear and nonlinear parts of the whole
  input-output mapping, which has many benefits. We study the theoretical properties
  of the transformation by noting that they make the Fisher information matrix closer
  to a diagonal matrix, and thus standard gradient closer to the natural gradient.
  We experimentally confirm the usefulness of the transformations by noting that they
  make basic stochastic gradient learning competitive with state-of-the-art learning
  algorithms in speed, and that they seem also to help find solutions that generalize
  better. The experiments include both classification of small images and learning
  a lowdimensional representation for images by using a deep unsupervised auto-encoder
  network. The transformations were beneficial in all cases, with and without regularization
  and with networks from two to five hidden layers.
ref_count: 17
references:
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 17080
  pid: 5d90f06bb70a0a3dced62413346235c02b1aa086
  title: Learning Multiple Layers of Features from Tiny Images
  year: 2009
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 12426
  pid: b71ac1e9fb49420d13e084ac67254a0bbd40f83f
  title: Understanding the difficulty of training deep feedforward neural networks
  year: 2010
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 5467
  pid: 843959ffdccf31c6694d135fad07425924f785b1
  title: Extracting and composing robust features with denoising autoencoders
  year: 2008
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 2728
  pid: 5a767a341364de1f75bea85e0b12ba7d3586a461
  title: Natural Gradient Works Efficiently in Learning
  year: 1998
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 14638
  pid: 46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e
  title: Reducing the Dimensionality of Data with Neural Networks
  year: 2006
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 844
  pid: 4c46347fbc272b21468efe3d9af34b4b2bad6684
  title: Deep learning via Hessian-free optimization
  year: 2010
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 35241
  pid: 162d958ff885f1462aeda91cd72582323fd6a1f4
  title: Gradient-based learning applied to document recognition
  year: 1998
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1216
  pid: bbf6f07e699587c8d52faf829a289f8cbc7f11a5
  title: 'First- and Second-Order Methods for Learning: Between Steepest Descent and
    Newton''s Method'
  year: 1992
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 179
  pid: 6ed460701019072ee2e364a1a491f73dd931f27f
  title: Topmoumoute Online Natural Gradient Algorithm
  year: 2007
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 875
  pid: b98cd08b75ebf2bd1d1ec47c51ef75777a7e64bd
  title: Deep, Big, Simple Neural Nets for Handwritten Digit Recognition
  year: 2010
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 414
  pid: 9e2964d36f154cc13b0af39c5b36cb3d76b27da1
  title: Deep Big Simple Neural Nets Excel on Handwritten Digit Recognition
  year: 2010
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 2630
  pid: b87274e6d9aa4e6ba5148898aa92941617d2b6ed
  title: Efficient BackProp
  year: 2012
slug: Deep-Learning-Made-Easier-by-Linear-Transformations-Raiko-Valpola
title: Deep Learning Made Easier by Linear Transformations in Perceptrons
url: https://www.semanticscholar.org/paper/Deep-Learning-Made-Easier-by-Linear-Transformations-Raiko-Valpola/b8ef1230a5cc9ea7cd8358f1ae7d1af97813ba14?sort=total-citations
venue: AISTATS
year: 2012
