authors:
- Jasper Snoek
- H. Larochelle
- Ryan P. Adams
badges:
- id: OPEN_ACCESS
corpusId: 632197
fieldsOfStudy:
- Computer Science
numCitedBy: 5017
numCiting: 50
paperAbstract: The use of machine learning algorithms frequently involves careful
  tuning of learning parameters and model hyperparameters. Unfortunately, this tuning
  is often a "black art" requiring expert experience, rules of thumb, or sometimes
  brute-force search. There is therefore great appeal for automatic approaches that
  can optimize the performance of any given learning algorithm to the problem at hand.
  In this work, we consider this problem through the framework of Bayesian optimization,
  in which a learning algorithm's generalization performance is modeled as a sample
  from a Gaussian process (GP). We show that certain choices for the nature of the
  GP, such as the type of kernel and the treatment of its hyperparameters, can play
  a crucial role in obtaining a good optimizer that can achieve expertlevel performance.
  We describe new algorithms that take into account the variable cost (duration) of
  learning algorithm experiments and that can leverage the presence of multiple cores
  for parallel experimentation. We show that these proposed algorithms improve on
  previous automatic procedures and can reach or surpass human expert-level optimization
  for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional
  neural networks.
ref_count: 50
references:
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 17868
  pid: 82266f6103bade9005ec555ed06ba20b5210ff22
  title: Gaussian Processes for Machine Learning
  year: 2009
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 221
  pid: 182015c5edff1956cbafbcb3e7bbe294aa54f9fc
  title: Selecting Receptive Fields in Deep Networks
  year: 2011
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 17080
  pid: 5d90f06bb70a0a3dced62413346235c02b1aa086
  title: Learning Multiple Layers of Features from Tiny Images
  year: 2009
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 5666
  pid: 188e247506ad992b8bc62d6c74789e89891a984f
  title: Random Search for Hyper-Parameter Optimization
  year: 2012
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 390
  pid: 265069b3670930fd884b02062d7e7b79ff2a49d5
  title: On Random Weights and Unsupervised Feature Learning
  year: 2011
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 3369
  pid: 398c296d0cc7f9d180f84969f8937e6d3a413796
  title: Multi-column deep neural networks for image classification
  year: 2012
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 714
  pid: 5ae20e0bdfddc1888148e0fcde88d937e96318d2
  title: Learning structural SVMs with latent variables
  year: 2009
slug: Practical-Bayesian-Optimization-of-Machine-Learning-Snoek-Larochelle
title: Practical Bayesian Optimization of Machine Learning Algorithms
url: https://www.semanticscholar.org/paper/Practical-Bayesian-Optimization-of-Machine-Learning-Snoek-Larochelle/2e2089ae76fe914706e6fa90081a79c8fe01611e?sort=total-citations
venue: NIPS
year: 2012
