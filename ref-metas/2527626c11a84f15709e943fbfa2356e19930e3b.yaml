authors:
- Weijie Su
- Xizhou Zhu
- Yue Cao
- B. Li
- Lewei Lu
- Furu Wei
- Jifeng Dai
badges:
- id: OPEN_ACCESS
corpusId: 201317624
fieldsOfStudy:
- Computer Science
numCitedBy: 707
numCiting: 56
paperAbstract: We introduce a new pre-trainable generic representation for visual-linguistic
  tasks, called Visual-Linguistic BERT (VL-BERT for short). VL-BERT adopts the simple
  yet powerful Transformer model as the backbone, and extends it to take both visual
  and linguistic embedded features as input. In it, each element of the input is either
  of a word from the input sentence, or a region-of-interest (RoI) from the input
  image. It is designed to fit for most of the visual-linguistic downstream tasks.
  To better exploit the generic representation, we pre-train VL-BERT on the massive-scale
  Conceptual Captions dataset, together with text-only corpus. Extensive empirical
  analysis demonstrates that the pre-training procedure can better align the visual-linguistic
  clues and benefit the downstream tasks, such as visual commonsense reasoning, visual
  question answering and referring expression comprehension. It is worth noting that
  VL-BERT achieved the first place of single model on the leaderboard of the VCR benchmark.
  Code is released at \url{this https URL}.
ref_count: 56
references:
- pid: 65a9c7b0800c86a196bc14e7621ff895cc6ab287
  title: 'ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language
    Tasks'
- pid: 5aec474c31a2f4b74703c6f786c0a8ff85c450da
  title: 'VisualBERT: A Simple and Performant Baseline for Vision and Language'
- pid: 2bc1c8bd00bbf7401afcb5460277840fd8bab029
  title: 'Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal
    Pre-training'
- pid: df2b0e26d0599ce3e70df8a9da02e51594e0e992
  title: 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'
- pid: def584565d05d6a8ba94de6621adab9e301d375d
  title: 'Visual7W: Grounded Question Answering in Images'
- pid: cd18800a0fe0b668a1cc19f2ec95b5003d0a5035
  title: Improving Language Understanding by Generative Pre-Training
- pid: 9405cc0d6169988371b2755e573cc28650d14dfe
  title: Language Models are Unsupervised Multitask Learners
- pid: b82153bf85d5d1edd3f170aace830e5328ca9ed0
  title: Fusion of Detected Objects in Text for Visual Question Answering
- pid: fdce9cbe5c726201575b3c8a8c1af0752f1af53f
  title: 'MAttNet: Modular Attention Network for Referring Expression Comprehension'
- pid: 6e795c6e9916174ae12349f5dc3f516570c17ce8
  title: Skip-Thought Vectors
- pid: 79c93274429d6355959f1e4374c2147bb81ea649
  title: 'LXMERT: Learning Cross-Modality Encoder Representations from Transformers'
- pid: b8de958fead0d8a9619b55c7299df3257c624a96
  title: 'DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition'
- pid: 97ad70a9fa3f99adf18030e5e38ebe3d90daa2db
  title: 'VQA: Visual Question Answering'
- pid: afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d
  title: 'Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image
    Annotations'
- pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  title: Attention is All you Need
- pid: 6dfc2ff03534a4325d06c6f88c3144831996629b
  title: 'From Recognition to Cognition: Visual Commonsense Reasoning'
- pid: a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8
  title: Bottom-Up and Top-Down Attention for Image Captioning and Visual Question
    Answering
- pid: a27ed1310b9c0832bde8f906e0fcd23d1f3aa79c
  title: 'Making the V in VQA Matter: Elevating the Role of Image Understanding in
    Visual Question Answering'
- pid: 1ab7f7c1d328589f25c79515b9a5d824d7ffbbd1
  title: 'GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question
    Answering'
- pid: f259bc7ef31c4ec7dd041c94bfd6b2f93b99b47c
  title: Contrastive Bidirectional Transformer for Temporal Representation Learning
- pid: ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc
  title: Cross-lingual Language Model Pretraining
- pid: e0c6abdbdecf04ffac65c440da77fb9d66bb474c
  title: 'XLNet: Generalized Autoregressive Pretraining for Language Understanding'
- pid: 317aee7fc081f2b137a85c4f20129007fd8e717e
  title: Fully Convolutional Networks for Semantic Segmentation
- pid: 03eb382e04cca8cca743f7799070869954f1402a
  title: 'CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual
    Reasoning'
- pid: f37e1b62a767a307c046404ca96bc140b3e68cb5
  title: 'GloVe: Global Vectors for Word Representation'
- pid: 0e6824e137847be0599bb0032e37042ed2ef5045
  title: 'Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching
    Movies and Reading Books'
- pid: 2f4df08d9072fc2ac181b7fced6a245315ce05c8
  title: Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation
- pid: dbde7dfa6cae81df8ac19ef500c42db96c3d1edd
  title: 'Google''s Neural Machine Translation System: Bridging the Gap between Human
    and Machine Translation'
- pid: 71b7178df5d2b112d07e45038cb5637208659ff7
  title: 'Microsoft COCO: Common Objects in Context'
- pid: 342786659379879f58bf5c4ff43c84c83a6a7389
  title: Simultaneous Detection and Segmentation
- pid: 44040913380206991b1991daf1192942e038fe31
  title: 'From image descriptions to visual denotations: New similarity metrics for
    semantic inference over event descriptions'
- pid: 077f8329a7b6fa3b7c877a57b81eb6c18b5f87de
  title: 'RoBERTa: A Robustly Optimized BERT Pretraining Approach'
- pid: b4df354db88a70183a64dbc9e56cf14e7669a6c0
  title: 'Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic
    Image Captioning'
- pid: 1b47265245e8db53a553049dcb27ed3e495fd625
  title: 'ImageNet: A large-scale hierarchical image database'
- pid: d79ac7a7bafdc9a782fb8c53285ca11c7f2e3f18
  title: 'BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language
    Model'
- pid: 025a0dc4a2a98742f1b410b6318a46de2c854b22
  title: Learning Video Representations using Contrastive Bidirectional Transformer
- pid: 6a0aaefce8a27a8727d896fa444ba27558b2d381
  title: Relation Networks for Object Detection
- pid: abd1c342495432171beb7ca8fd9551ef13cbd0ff
  title: ImageNet classification with deep convolutional neural networks
- pid: 92c141447f51b6732242376164ff961e464731c8
  title: 'ReferItGame: Referring to Objects in Photographs of Natural Scenes'
- pid: a6cb366736791bcccc5c8639de5a8f9636bf87e8
  title: 'Adam: A Method for Stochastic Optimization'
- pid: 7ffdbc358b63378f07311e883dddacc9faeeaf4b
  title: Fast R-CNN
- pid: 696ca58d93f6404fea0fc75c62d1d7b378f47628
  title: 'Microsoft COCO Captions: Data Collection and Evaluation Server'
- pid: 424561d8585ff8ebce7d5d07de8dbf7aae5e7270
  title: 'Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks'
- pid: c41a11c0e9b8b92b4faaf97749841170b760760a
  title: 'VideoBERT: A Joint Model for Video and Language Representation Learning'
- pid: 4152d2c8585f7e3f85d3b3d84036171de104cbd7
  title: Rethinking ImageNet Pre-Training
- pid: 330da625c15427c6e42ccfa3b747fb29e5835bf0
  title: Efficient Estimation of Word Representations in Vector Space
slug: VL-BERT:-Pre-training-of-Generic-Visual-Linguistic-Su-Zhu
title: 'VL-BERT: Pre-training of Generic Visual-Linguistic Representations'
url: https://www.semanticscholar.org/paper/VL-BERT:-Pre-training-of-Generic-Visual-Linguistic-Su-Zhu/2527626c11a84f15709e943fbfa2356e19930e3b?sort=total-citations
venue: ICLR
year: 2020
