authors:
- Abhishek Das
- Harsh Agrawal
- C. L. Zitnick
- Devi Parikh
- Dhruv Batra
badges:
- id: UNPAYWALL
- id: OPEN_ACCESS
corpusId: 220553
fieldsOfStudy:
- Computer Science
numCitedBy: 338
numCiting: 64
paperAbstract: We conduct large-scale studies on `human attention' in Visual Question
  Answering (VQA) to understand where humans choose to look to answer questions about
  images. We design and test multiple game-inspired novel attention-annotation interfaces
  that require the subject to sharpen regions of a blurred image to answer a question.
  Thus, we introduce the VQA-HAT (Human ATtention) dataset. We evaluate attention
  maps generated by state-of-the-art VQA models against human attention both qualitatively
  (via visualizations) and quantitatively (via rank-order correlation). Overall, our
  experiments show that current attention models in VQA do not seem to be looking
  at the same regions as humans.
ref_count: 0
references:
- pid: fb9d253258d6b3beceb9d6cd7bba6e0a29ab875b
  title: Hierarchical Question-Image Co-Attention for Visual Question Answering
- pid: 2c1890864c1c2b750f48316dc8b650ba4772adc5
  title: Stacked Attention Networks for Image Question Answering
- pid: 1cf6bc0866226c1f8e282463adc8b75d92fba9bb
  title: 'Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for
    Visual Question Answering'
- pid: 97ad70a9fa3f99adf18030e5e38ebe3d90daa2db
  title: 'VQA: Visual Question Answering'
- pid: f96898d15a1bf1fa8925b1280d0e07a7a8e72194
  title: Dynamic Memory Networks for Visual and Textual Question Answering
- pid: 2231f44be9a8472a46d8e8a628b4e52b9a8f44e0
  title: Visual Dialog
- pid: 4d8f2d14af5991d4f0d050d22216825cac3157bd
  title: 'Show, Attend and Tell: Neural Image Caption Generation with Visual Attention'
- pid: 75ddc7ee15be14013a3462c01b38b0548486fbcb
  title: Learning to Compose Neural Networks for Question Answering
slug: Human-Attention-in-Visual-Question-Answering:-Do-at-Das-Agrawal
title: 'Human Attention in Visual Question Answering: Do Humans and Deep Networks
  look at the same regions?'
url: https://www.semanticscholar.org/paper/Human-Attention-in-Visual-Question-Answering:-Do-at-Das-Agrawal/58cb0c24c936b8a14ca7b2d56ba80de733c545b3?sort=total-citations
venue: EMNLP
year: 2016
