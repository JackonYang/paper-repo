authors:
- Abhishek Das
- Harsh Agrawal
- C. L. Zitnick
- Devi Parikh
- Dhruv Batra
badges:
- id: UNPAYWALL
- id: OPEN_ACCESS
corpusId: 220553
fieldsOfStudy:
- Computer Science
meta_key: human-attention-in-visual-question-answering-do-humans-and-deep-networks-look-at-the-same-regions
numCitedBy: 338
numCiting: 64
paperAbstract: We conduct large-scale studies on `human attention' in Visual Question
  Answering (VQA) to understand where humans choose to look to answer questions about
  images. We design and test multiple game-inspired novel attention-annotation interfaces
  that require the subject to sharpen regions of a blurred image to answer a question.
  Thus, we introduce the VQA-HAT (Human ATtention) dataset. We evaluate attention
  maps generated by state-of-the-art VQA models against human attention both qualitatively
  (via visualizations) and quantitatively (via rank-order correlation). Overall, our
  experiments show that current attention models in VQA do not seem to be looking
  at the same regions as humans.
ref_count: 0
references:
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 19
  pid: 287c5be2610e1c61798851feb32b88c424acfbf9
  show_ref_link: false
  title: Hierarchical Co-Attention for Visual Question Answering
  year: 2016
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1121
  pid: fb9d253258d6b3beceb9d6cd7bba6e0a29ab875b
  show_ref_link: true
  title: Hierarchical Question-Image Co-Attention for Visual Question Answering
  year: 2016
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1475
  pid: 2c1890864c1c2b750f48316dc8b650ba4772adc5
  show_ref_link: true
  title: Stacked Attention Networks for Image Question Answering
  year: 2016
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 652
  pid: 1cf6bc0866226c1f8e282463adc8b75d92fba9bb
  show_ref_link: true
  title: 'Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for
    Visual Question Answering'
  year: 2016
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 2887
  pid: 97ad70a9fa3f99adf18030e5e38ebe3d90daa2db
  show_ref_link: true
  title: 'VQA: Visual Question Answering'
  year: 2015
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 659
  pid: f96898d15a1bf1fa8925b1280d0e07a7a8e72194
  show_ref_link: true
  title: Dynamic Memory Networks for Visual and Textual Question Answering
  year: 2016
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 529
  pid: 2231f44be9a8472a46d8e8a628b4e52b9a8f44e0
  show_ref_link: true
  title: Visual Dialog
  year: 2017
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 7252
  pid: 4d8f2d14af5991d4f0d050d22216825cac3157bd
  show_ref_link: true
  title: 'Show, Attend and Tell: Neural Image Caption Generation with Visual Attention'
  year: 2015
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 457
  pid: c71db5d3546e22227662ee0f0ce586495ef18899
  show_ref_link: false
  title: 'SALICON: Saliency in Context'
  year: 2015
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 476
  pid: 75ddc7ee15be14013a3462c01b38b0548486fbcb
  show_ref_link: true
  title: Learning to Compose Neural Networks for Question Answering
  year: 2016
slug: Human-Attention-in-Visual-Question-Answering:-Do-at-Das-Agrawal
title: 'Human Attention in Visual Question Answering: Do Humans and Deep Networks
  look at the same regions?'
url: https://www.semanticscholar.org/paper/Human-Attention-in-Visual-Question-Answering:-Do-at-Das-Agrawal/58cb0c24c936b8a14ca7b2d56ba80de733c545b3?sort=total-citations
venue: EMNLP
year: 2016
