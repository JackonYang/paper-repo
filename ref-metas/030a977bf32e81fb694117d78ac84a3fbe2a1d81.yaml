authors:
- Kam-Chuen Jim
- C. Lee Giles
- B. Horne
badges: []
corpusId: 15729978
fieldsOfStudy:
- Computer Science
numCitedBy: 128
numCiting: 40
paperAbstract: Concerns the effect of noise on the performance of feedforward neural
  nets. We introduce and analyze various methods of injecting synaptic noise into
  dynamically driven recurrent nets during training. Theoretical results show that
  applying a controlled amount of noise during training may improve convergence and
  generalization performance. We analyze the effects of various noise parameters and
  predict that best overall performance can be achieved by injecting additive noise
  at each time step. Noise contributes a second-order gradient term to the error function
  which can be viewed as an anticipatory agent to aid convergence. This term appears
  to find promising regions of weight space in the beginning stages of training when
  the training error is large and should improve convergence on error surfaces with
  local minima. The first-order term is a regularization term that can improve generalization.
  Specifically, it can encourage internal representations where the state nodes operate
  in the saturated regions of the sigmoid discriminant function. While this effect
  can improve performance on automata inference problems with binary inputs and target
  outputs, it is unclear what effect it will have on other types of problems. To substantiate
  these predictions, we present simulations on learning the dual parity grammar from
  temporal strings for all noise models, and present simulations on learning a randomly
  generated six-state grammar using the predicted best noise model.
ref_count: 40
references:
- fieldsOfStudy:
  - Mathematics
  numCitedBy: 993
  pid: c3ecd8e19e016d15670c8953b4b9afaa5186b0f3
  title: Training with Noise is Equivalent to Tikhonov Regularization
  year: 1995
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 64
  pid: 34f8c5769899dfd9450bb13c3f52c18c88444515
  title: Experimental Comparison of the Effect of Order in Recurrent Neural Networks
  year: 1993
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 489
  pid: 872cdc269f3cb59f8a227818f35041415091545f
  title: Learning and Extracting Finite State Automata with Second-Order Recurrent
    Neural Networks
  year: 1992
- fieldsOfStudy:
  - Mathematics
  - Computer Science
  numCitedBy: 1696
  pid: 25406e6733a698bfc4ac836f8e74f458e75dad4f
  title: What Size Net Gives Valid Generalization?
  year: 1989
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 3490
  pid: e7297db245c3feb1897720b173a59fe7e36babb7
  title: Optimal Brain Damage
  year: 1989
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 3393
  pid: a0d16f0e99f7ce5e6fb70b1a68c685e9ad610657
  title: A Learning Algorithm for Boltzmann Machines
  year: 1985
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 215
  pid: a64ca771a733d58dcbf8f7a3fe65a09310424bf8
  title: Induction of Finite-State Languages Using Second-Order Recurrent Networks
  year: 1992
- fieldsOfStudy:
  - Physics
  numCitedBy: 39631
  pid: dd5061631a4d11fa394f4421700ebf7e78dcbc59
  title: Optimization by Simulated Annealing
  year: 1983
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1338
  pid: 8592e46a5435d18bba70557846f47290b34c1aa5
  title: Learning and relearning in Boltzmann machines
  year: 1986
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 567
  pid: 10dae7fca6b65b61d155a622f0c6ca2bc3922251
  title: Gradient-based learning algorithms for recurrent networks and their computational
    complexity
  year: 1995
slug: An-analysis-of-noise-in-recurrent-neural-networks:-Jim-Giles
title: 'An analysis of noise in recurrent neural networks: convergence and generalization'
url: https://www.semanticscholar.org/paper/An-analysis-of-noise-in-recurrent-neural-networks:-Jim-Giles/030a977bf32e81fb694117d78ac84a3fbe2a1d81?sort=total-citations
venue: IEEE Trans. Neural Networks
year: 1996
