authors:
- Tomas Mikolov
- Stefan Kombrink
- L. Burget
- "J. Cernock\xFD"
- S. Khudanpur
badges:
- id: OPEN_ACCESS
corpusId: 14850173
fieldsOfStudy:
- Computer Science
numCitedBy: 1423
numCiting: 23
paperAbstract: We present several modifications of the original recurrent neural network
  language model (RNN LM).While this model has been shown to significantly outperform
  many competitive language modeling techniques in terms of accuracy, the remaining
  problem is the computational complexity. In this work, we show approaches that lead
  to more than 15 times speedup for both training and testing phases. Next, we show
  importance of using a backpropagation through time algorithm. An empirical comparison
  with feedforward networks is also provided. In the end, we discuss possibilities
  how to reduce the amount of parameters in the model. The resulting RNN model can
  thus be smaller, faster both during training and testing, and more accurate than
  the basic one.
ref_count: 23
references:
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 4902
  pid: 9819b600a828a57e1cde047bbe710d3446b30da5
  title: Recurrent neural network based language model
  year: 2010
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 942
  pid: c19fbefdeead6a4154a22a9c8551a18b1530033a
  title: Hierarchical Probabilistic Neural Network Language Model
  year: 2005
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 192
  pid: 699d5ab38deee78b1fd17cc8ad233c74196d16e9
  title: Adaptive Importance Sampling to Accelerate Training of a Neural Probabilistic
    Language Model
  year: 2008
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 128
  pid: 8b395470a57c48d174c4216ea21a7a58bc046917
  title: Training Neural Network Language Models on Very Large Corpora
  year: 2005
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 20332
  pid: 052b1d8ce63b07fec3de9dbb583772d860b7c769
  title: Learning representations by back-propagating errors
  year: 1986
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 6144
  pid: d0be39ee052d246ae99c082a565aba25b811be2d
  title: Learning long-term dependencies with gradient descent is difficult
  year: 1994
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 233
  pid: 4af41f4d838daa7ca6995aeb4918b61989d1ed80
  title: Classes for fast maximum entropy training
  year: 2001
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 26
  pid: 9319ca5a532462f9f3515ac3d317668aa9650d5b
  title: Exact training of a neural syntactic language model
  year: 2004
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 98
  pid: a73ae2ce1cfafae61238b3fdb1bbb61093962b4d
  title: Factored Neural Language Models
  year: 2006
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 6011
  pid: 6c2b28f9354f667cd5bd07afc0471d8334430da7
  title: A Neural Probabilistic Language Model
  year: 2000
- fieldsOfStudy:
  - Psychology
  numCitedBy: 9863
  pid: 668087f0ae7ce1de6e0bd0965dbb480c08103260
  title: Finding Structure in Time
  year: 1990
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 548
  pid: 09c76da2361d46689825c4efc37ad862347ca577
  title: A bit of progress in language modeling
  year: 2001
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1116
  pid: 6fdb77260fc83dff91c44fea0f31a2cb8ed13d04
  title: Scaling learning algorithms towards AI
  year: 2007
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 524
  pid: 1d453386011ef21285fa81fb4f87fdf811c6ad7a
  title: Learning internal representations by back-propagating errors
  year: 1986
slug: Extensions-of-recurrent-neural-network-language-Mikolov-Kombrink
title: Extensions of recurrent neural network language model
url: https://www.semanticscholar.org/paper/Extensions-of-recurrent-neural-network-language-Mikolov-Kombrink/07ca885cb5cc4328895bfaec9ab752d5801b14cd?sort=total-citations
venue: 2011 IEEE International Conference on Acoustics, Speech and Signal Processing
  (ICASSP)
year: 2011
