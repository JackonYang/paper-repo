authors:
- Yoshua Bengio
- "Jean-S\xE9bastien Senecal"
badges:
- id: OPEN_ACCESS
corpusId: 9147661
fieldsOfStudy:
- Computer Science
numCitedBy: 192
numCiting: 38
paperAbstract: Previous work on statistical language modeling has shown that it is
  possible to train a feedforward neural network to approximate probabilities over
  sequences of words, resulting in significant error reduction when compared to standard
  baseline models based on n-grams. However, training the neural network model with
  the maximum-likelihood criterion requires computations proportional to the number
  of words in the vocabulary. In this paper, we introduce adaptive importance sampling
  as a way to accelerate training of the model. The idea is to use an adaptive n-gram
  model to track the conditional distributions produced by the neural network. We
  show that a very significant speedup can be obtained on standard problems.
ref_count: 38
references:
- pid: d6fb7546a29320eadad868af66835059db93d99f
  title: Efficient training of large neural networks for language modeling
- pid: 6c2b28f9354f667cd5bd07afc0471d8334430da7
  title: A Neural Probabilistic Language Model
- pid: 9548ac30c113562a51e603dbbc8e9fa651cfd3ab
  title: Improved backing-off for M-gram language modeling
- pid: bfab4ffa229c8af0174a683ff1eda524c4f59d00
  title: Can artificial neural networks learn language models?
- pid: e41498c05d4c68e4750fb84a380317a112d97b01
  title: Connectionist language modeling for large vocabulary continuous speech recognition
- pid: 09c76da2361d46689825c4efc37ad862347ca577
  title: A bit of progress in language modeling
- pid: b0130277677e5b915d5cd86b3afafd77fd08eb2e
  title: Estimation of probabilities from sparse data for the language model component
    of a speech recognizer
- pid: 9360e5ce9c98166bb179ad479a9d2919ff13d022
  title: Training Products of Experts by Minimizing Contrastive Divergence
- pid: fb486e03369a64de2d5b0df86ec0a7b55d3907db
  title: A Maximum Entropy Approach to Natural Language Processing
- pid: b95799a25def71b100bd12e7ebb32cbcee6590bf
  title: Energy-Based Models for Sparse Overcomplete Representations
- pid: c6586e7c73cc1c9e9a251947425c54c5051be626
  title: 'Two decades of statistical language modeling: where do we go from here?'
- pid: 084c55d6432265785e3ff86a2e900a49d501c00a
  title: Foundations of statistical natural language processing
- pid: 8592e46a5435d18bba70557846f47290b34c1aa5
  title: Learning and relearning in Boltzmann machines
- pid: 17a09383cf450da8fe9830b9420914fa47707916
  title: Monte Carlo Statistical Methods
- pid: 6a923c9f89ed53b6e835b3807c0c1bd8d532687b
  title: Interpolated estimation of Markov source parameters from sparse data
- pid: 4ade4934db522fe6d634ff6f48887da46eedb4d1
  title: Learning distributed representations of concepts.
- pid: 2928de5400a920a6a29af41821c680cef5d35f91
  title: A latent semantic analysis framework for large-Span language modeling
slug: Adaptive-Importance-Sampling-to-Accelerate-Training-Bengio-Senecal
title: Adaptive Importance Sampling to Accelerate Training of a Neural Probabilistic
  Language Model
url: https://www.semanticscholar.org/paper/Adaptive-Importance-Sampling-to-Accelerate-Training-Bengio-Senecal/699d5ab38deee78b1fd17cc8ad233c74196d16e9?sort=total-citations
venue: IEEE Transactions on Neural Networks
year: 2008
