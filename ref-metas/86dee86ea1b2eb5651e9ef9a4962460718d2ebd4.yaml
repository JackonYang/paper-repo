authors:
- Anthony V. W. Smith
- D. Zipser
badges: []
corpusId: 207107675
fieldsOfStudy:
- Computer Science
numCitedBy: 40
numCiting: 0
paperAbstract: Recurrent connections in neural networks potentially allow information
  about events occurring in the past to be preserved and used in current computations.
  How effectively this potential is realized depends on the power of the learning
  algorithm used. As an example of a task requiring recurrency, Servan-Schreiber,
  Cleeremans, and McClelland1 have applied a simple recurrent learning algorithm to
  the task of recognizing finite-state grammars of increasing difficulty. These nets
  showed considerable power and were able to learn fairly complex grammars by emulating
  the state machines that produced them. However, there was a limit to the difficulty
  of the grammars that could be learned. We have applied a more powerful recurrent
  learning procedure, called real-time recurrent learning2,6 (RTRL), to some of the
  same problems studied by Servan-Schreiber, Cleeremans, and McClelland. The RTRL
  algorithm solved more difficult forms of the task than the simple recurrent networks.
  The internal representations developed by RTRL networks revealed that they learn
  a rich set of internal states that represent more about the past than is required
  by the underlying grammar. The dynamics of the networks are determined by the state
  structure and are not chaotic.
ref_count: 0
references: []
slug: Learning-Sequential-Structure-with-the-Real-Time-Smith-Zipser
title: Learning Sequential Structure with the Real-Time Recurrent Learning Algorithm
url: https://www.semanticscholar.org/paper/Learning-Sequential-Structure-with-the-Real-Time-Smith-Zipser/86dee86ea1b2eb5651e9ef9a4962460718d2ebd4?sort=total-citations
venue: Int. J. Neural Syst.
year: 1989
