authors:
- E. Oja
badges: []
corpusId: 207107700
fieldsOfStudy:
- Computer Science
numCitedBy: 907
numCiting: 0
paperAbstract: A single neuron with Hebbian-type learning for the connection weights,
  and with nonlinear internal feedback, has been shown to extract the statistical
  principal components of its stationary input pattern sequence. A generalization
  of this model to a layer of neuron units is given, called the Subspace Network,
  which yields a multi-dimensional, principal component subspace. This can be used
  as an associative memory for the input vectors or as a module in nonsupervised learning
  of data clusters in the input space. It is also able to realize a powerful pattern
  classifier based on projections on class subspaces. Some classification results
  for natural textures are given.
ref_count: 0
references: []
slug: Neural-Networks,-Principal-Components,-and-Oja
title: Neural Networks, Principal Components, and Subspaces
url: https://www.semanticscholar.org/paper/Neural-Networks,-Principal-Components,-and-Oja/0eddb03e19bcf7555042508145426451da1d5c7f?sort=total-citations
venue: Int. J. Neural Syst.
year: 1989
