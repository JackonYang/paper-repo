authors:
- H. Larochelle
- Yoshua Bengio
- J. Louradour
- Pascal Lamblin
badges:
- id: OPEN_ACCESS
corpusId: 996073
fieldsOfStudy:
- Computer Science
numCitedBy: 1027
numCiting: 65
paperAbstract: Deep multi-layer neural networks have many levels of non-linearities
  allowing them to compactly represent highly non-linear and highly-varying functions.
  However, until recently it was not clear how to train such deep networks, since
  gradient-based optimization starting from random initialization often appears to
  get stuck in poor solutions. Hinton et al. recently proposed a greedy layer-wise
  unsupervised learning procedure relying on the training algorithm of restricted
  Boltzmann machines (RBM) to initialize the parameters of a deep belief network (DBN),
  a generative model with many layers of hidden causal variables. This was followed
  by the proposal of another greedy layer-wise procedure, relying on the usage of
  autoassociator networks. In the context of the above optimization problem, we study
  these algorithms empirically to better understand their success. Our experiments
  confirm the hypothesis that the greedy layer-wise unsupervised training strategy
  helps the optimization by initializing weights in a region near a good local minimum,
  but also implicitly acts as a sort of regularization that brings better generalization
  and encourages internal distributed representations that are high-level abstractions
  of the input. We also present a series of experiments aimed at evaluating the link
  between the performance of deep neural networks and practical aspects of their topology,
  for example, demonstrating cases where the addition of more depth helps. Finally,
  we empirically explore simple variants of these training algorithms, such as the
  use of different RBM input unit distributions, a simple way of combining gradient
  estimators to improve performance, as well as on-line versions of those algorithms.
ref_count: 65
references:
- pid: 355d44f53428b1ac4fb2ab468d593c720640e5bd
  title: Greedy Layer-Wise Training of Deep Networks
- pid: 8978cf7574ceb35f4c3096be768c7547b28a35d0
  title: A Fast Learning Algorithm for Deep Belief Nets
- pid: e60ff004dde5c13ec53087872cfcdd12e85beb57
  title: Learning Deep Architectures for AI
- pid: 41fef1a197fab9684a4608b725d3ae72e1ab4b39
  title: Sparse Feature Learning for Deep Belief Networks
- pid: 08d0ea90b53aba0008d25811268fe46562cfb38c
  title: On the quantitative analysis of deep belief networks
- pid: a57c6d627ffc667ae3547073876c35d6420accff
  title: Connectionist Learning Procedures
- pid: 6fdb77260fc83dff91c44fea0f31a2cb8ed13d04
  title: Scaling learning algorithms towards AI
- pid: a120c05ad7cd4ce2eb8fb9697e16c7c4877208a5
  title: Connectionist Learning of Belief Networks
- pid: 9552ac39a57daacf3d75865a268935b5a0df9bbb
  title: 'Neural networks and principal component analysis: Learning from examples
    without local minima'
- pid: 995a3b11cc8a4751d8e167abc4aa937abc934df0
  title: The Cascade-Correlation Learning Architecture
- pid: 80c330eee12decb84aaebcc85dc7ce414134ad61
  title: Modeling image patches with a directed hierarchy of Markov random fields
- pid: b8012351bc5ebce4a4b3039bbbba3ce393bc3315
  title: An empirical evaluation of deep architectures on problems with many factors
    of variation
- pid: f2e95236f0fccc0b70e757ac2ebbc79b7f51de0a
  title: Using Deep Belief Nets to Learn Covariance Kernels for Gaussian Processes
- pid: 932c2a02d462abd75af018125413b1ceaa1ee3f4
  title: Efficient Learning of Sparse Representations with an Energy-Based Model
- pid: e270bfa5b662c531a61a5b274da636603c23a734
  title: On Contrastive Divergence Learning
- pid: 843959ffdccf31c6694d135fad07425924f785b1
  title: Extracting and composing robust features with denoising autoencoders
- pid: a53da9916b87fa295837617c16ef2ca6462cafb8
  title: Classification using discriminative restricted Boltzmann machines
- pid: 46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e
  title: Reducing the Dimensionality of Data with Neural Networks
- pid: b543de6755fc1612b2fb449e0282727d0835d9cf
  title: Justifying and Generalizing Contrastive Divergence
- pid: b7d471970467a99bec4bce34c7dba5ef6745ad06
  title: The Curse of Highly Variable Functions for Local Kernel Machines
- pid: 605402e235bd62437baf3c9ebefe77fb4d92ee95
  title: The Helmholtz Machine
- pid: 9360e5ce9c98166bb179ad479a9d2919ff13d022
  title: Training Products of Experts by Minimizing Contrastive Divergence
- pid: 1d7d0e8c4791700defd4b0df82a26b50055346e0
  title: An Information-Maximization Approach to Blind Separation and Blind Deconvolution
- pid: f5d565d307a746d8bc0feb52c873995af698deca
  title: Principled Hybrids of Generative and Discriminative Models
- pid: 57458bc1cffe5caa45a885af986d70f723f406b4
  title: 'A unified architecture for natural language processing: deep neural networks
    with multitask learning'
- pid: 162d958ff885f1462aeda91cd72582323fd6a1f4
  title: Gradient-based learning applied to document recognition
- pid: ad33d1fa8628cb55c32fb52feb537f65184c3b29
  title: Learning a Nonlinear Embedding by Preserving Class Neighbourhood Structure
- pid: 1626c940a64ad96a7ed53d7d6c0df63c6696956b
  title: Restricted Boltzmann machines for collaborative filtering
- pid: 6dd01cd9c17d1491ead8c9f97597fbc61dead8ea
  title: The "wake-sleep" algorithm for unsupervised neural networks.
- pid: 63f27307cb028f9341544fff32eceb2c3c652bf2
  title: Large-scale kernel machines
- pid: ccd52aff02b0f902f4ce7247c4fee7273014c41c
  title: Unsupervised Learning of Invariant Feature Hierarchies with Applications
    to Object Recognition
- pid: 944e1a7b2c5c62e952418d7684e3cade89c76f87
  title: A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled
    Data
- pid: 7ee368e60d0b826e78f965aad8d6c7d406127104
  title: Deep learning via semi-supervised embedding
- pid: e45c2420e6dc59ba6d357fb0c996ebf43c861560
  title: Exploiting Generative Models in Discriminative Classifiers
- pid: 51ff037291582df4c205d4a9cbe6e7dcec8f5973
  title: To recognize shapes, first learn to generate images.
- pid: 2184fb6d32bc46f252b940035029273563c4fc82
  title: Exponential Family Harmoniums with an Application to Information Retrieval
- pid: cd5af41a81e7fc9588dc74f3831fb14daf2f8e2a
  title: Semantic hashing
- pid: b7b5bea7b4d40003a6887794652ea07196a97134
  title: A New Learning Algorithm for Mean Field Boltzmann Machines
- pid: f22f6972e66bdd2e769fa64b0df0a13063c0c101
  title: Multilayer feedforward networks are universal approximators
- pid: 90929a6aa901ba958eb4960aeeb594c752e08369
  title: 'On Discriminative vs. Generative Classifiers: A comparison of logistic regression
    and naive Bayes'
- pid: 2c3cac0f568ae9261ff9c80eeda55a13e83ae7fb
  title: A discriminative framework for modelling object classes
- pid: 4f7476037408ac3d993f5088544aab427bc319c1
  title: 'Information processing in dynamical systems: foundations of harmony theory'
- pid: 0a79433b5feacd9e8feeafa629dae5a85f362fef
  title: Mean Field Theory for Sigmoid Belief Networks
- pid: 2e73081ed096c62c073b3faa1b3b80aab89998c5
  title: 'Blind separation of sources, part I: An adaptive algorithm based on neuromimetic
    architecture'
- pid: 96a1effa4be3f8caa88270d6d258de418993d2e7
  title: Independent component analysis, A new concept?
- pid: 56e6db25c6e72c0cff4ee099fa6e79bfbd20c7fb
  title: Almost optimal lower bounds for small depth circuits
- pid: 3653692a9d4d51909c9dd231d567bad928430654
  title: On the power of small-depth threshold circuits
- pid: 629cc74dcaf655feea40f64cd74617ac884ed0f8
  title: Graphical Models for Machine Learning and Digital Communication
slug: Exploring-Strategies-for-Training-Deep-Neural-Larochelle-Bengio
title: Exploring Strategies for Training Deep Neural Networks
url: https://www.semanticscholar.org/paper/Exploring-Strategies-for-Training-Deep-Neural-Larochelle-Bengio/05fd1da7b2e34f86ec7f010bef068717ae964332?sort=total-citations
venue: J. Mach. Learn. Res.
year: 2009
