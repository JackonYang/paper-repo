authors:
- Geoffrey E. Hinton
badges:
- id: OPEN_ACCESS
corpusId: 207596505
fieldsOfStudy:
- Computer Science
numCitedBy: 4571
numCiting: 34
paperAbstract: It is possible to combine multiple latent-variable models of the same
  data by multiplying their probability distributions together and then renormalizing.
  This way of combining individual expert models makes it hard to generate samples
  from the combined model but easy to infer the values of the latent variables of
  each expert, because the combination rule ensures that the latent variables of different
  experts are conditionally independent when given the data. A product of experts
  (PoE) is therefore an interesting candidate for a perceptual system in which rapid
  inference is vital and generation is unnecessary. Training a PoE by maximizing the
  likelihood of the data is difficult because it is hard even to approximate the derivatives
  of the renormalization term in the combination rule. Fortunately, a PoE can be trained
  using a different objective function called contrastive divergence whose derivatives
  with regard to the parameters can be approximated accurately and efficiently. Examples
  are presented of contrastive divergence learning using several types of expert on
  several types of data.
ref_count: 34
references:
- pid: a120c05ad7cd4ce2eb8fb9697e16c7c4877208a5
  title: Connectionist Learning of Belief Networks
- pid: 73e93d0346e8eee6c2ab45e46c26eaafb66e12a8
  title: Rate-coded Restricted Boltzmann Machines for Face Recognition
- pid: 939d584316be99e2db3fec3fbf7d71f22a477f67
  title: Unsupervised Learning of Distributions of Binary Vectors Using 2-Layer Networks
- pid: 56efc84e0858f1e0a7cf052e5c4275d4c46c21c2
  title: Using Generative Models for Handwritten Digit Recognition
- pid: 459b30a9a960080f3b313e41886b1aa0e51e882c
  title: Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration
    of Images
- pid: 9b20ad513361a26e98289e5a517291c6ff49960d
  title: Learning Continuous Attractors in Recurrent Networks
- pid: 4f7476037408ac3d993f5088544aab427bc319c1
  title: 'Information processing in dynamical systems: foundations of harmony theory'
- pid: fb486e03369a64de2d5b0df86ec0a7b55d3907db
  title: A Maximum Entropy Approach to Natural Language Processing
- pid: 0a79433b5feacd9e8feeafa629dae5a85f362fef
  title: Mean Field Theory for Sigmoid Belief Networks
- pid: 8592e46a5435d18bba70557846f47290b34c1aa5
  title: Learning and relearning in Boltzmann machines
- pid: 6dd01cd9c17d1491ead8c9f97597fbc61dead8ea
  title: The "wake-sleep" algorithm for unsupervised neural networks.
- pid: a7eb50210a468d0878666e8f82fb55f2b179f802
  title: Learning Structural Descriptions From Examples
- pid: 9aad8a4c8a94209beb49d97a40c0e6deeffa50e7
  title: The psychology of computer vision
slug: Training-Products-of-Experts-by-Minimizing-Hinton
title: Training Products of Experts by Minimizing Contrastive Divergence
url: https://www.semanticscholar.org/paper/Training-Products-of-Experts-by-Minimizing-Hinton/9360e5ce9c98166bb179ad479a9d2919ff13d022?sort=total-citations
venue: Neural Computation
year: 2002
