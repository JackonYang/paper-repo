authors:
- Myle Ott
- Sergey Edunov
- David Grangier
- Michael Auli
badges:
- id: OPEN_ACCESS
corpusId: 44131019
fieldsOfStudy:
- Computer Science
numCitedBy: 474
numCiting: 41
paperAbstract: "Sequence to sequence learning models still require several days to\
  \ reach state of the art performance on large benchmark datasets using a single\
  \ machine. This paper shows that reduced precision and large batch training can\
  \ speedup training by nearly 5x on a single 8-GPU machine with careful tuning and\
  \ implementation. On WMT\u201914 English-German translation, we match the accuracy\
  \ of Vaswani et al. (2017) in under 5 hours when training on 8 GPUs and we obtain\
  \ a new state of the art of 29.3 BLEU after training for 85 minutes on 128 GPUs.\
  \ We further improve these results to 29.8 BLEU by training on the much larger Paracrawl\
  \ dataset. On the WMT\u201914 English-French task, we obtain a state-of-the-art\
  \ BLEU of 43.2 in 8.5 hours on 128 GPUs."
ref_count: 41
references:
- pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  title: Attention is All you Need
- pid: 43428880d75b3a14257c3ee9bda054e61eb869c0
  title: Convolutional Sequence to Sequence Learning
- pid: 0d57ba12a6d958e178d83be4c84513f7e42b24e5
  title: 'Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour'
- pid: fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5
  title: Neural Machine Translation by Jointly Learning to Align and Translate
- pid: 1af68821518f03568f913ab03fc02080247a27ff
  title: Neural Machine Translation of Rare Words with Subword Units
- pid: 23ffaa0fe06eae05817f527a47ac3291077f9e58
  title: Rethinking the Inception Architecture for Computer Vision
- pid: 032274e57f7d8b456bd255fe76b909b2c1d7458e
  title: A Deep Reinforced Model for Abstractive Summarization
- pid: c8efcc854d97dfc2a42b83316a2109f9d166e43f
  title: Self-Attention with Relative Position Representations
- pid: 668db48c6a79826456341680ee1175dfc4cced71
  title: 'Get To The Point: Summarization with Pointer-Generator Networks'
- pid: d1208ac421cf8ff67b27d93cd19ae42b8d596f95
  title: Deep learning with COTS HPC systems
- pid: 97fb4e3d45bb098e27e0071448b6152217bd35a5
  title: Layer Normalization
- pid: 3127190433230b3dc1abd0680bb58dced4bcd90e
  title: Large Scale Distributed Deep Networks
- pid: a6cb366736791bcccc5c8639de5a8f9636bf87e8
  title: 'Adam: A Method for Stochastic Optimization'
- pid: 2c03df8b48bf3fa39054345bafabfeff15bfd11d
  title: Deep Residual Learning for Image Recognition
- pid: 34f25a8704614163c4095b3ee2fc969b60de4698
  title: 'Dropout: a simple way to prevent neural networks from overfitting'
- pid: 4ee2eab4c298c1824a9fb8799ad8eed21be38d21
  title: 'Moses: Open Source Toolkit for Statistical Machine Translation'
slug: Scaling-Neural-Machine-Translation-Ott-Edunov
title: Scaling Neural Machine Translation
url: https://www.semanticscholar.org/paper/Scaling-Neural-Machine-Translation-Ott-Edunov/bf8fe437f779f2098f9af82b534aa51dc9edb06f?sort=total-citations
venue: WMT
year: 2018
