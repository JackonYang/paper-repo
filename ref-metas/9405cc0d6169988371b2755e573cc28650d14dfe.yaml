authors:
- Alec Radford
- Jeff Wu
- Rewon Child
- D. Luan
- Dario Amodei
- Ilya Sutskever
badges:
- id: OPEN_ACCESS
corpusId: 160025533
fieldsOfStudy:
- Computer Science
numCitedBy: 6284
numCiting: 75
paperAbstract: Natural language processing tasks, such as question answering, machine
  translation, reading comprehension, and summarization, are typically approached
  with supervised learning on taskspecific datasets. We demonstrate that language
  models begin to learn these tasks without any explicit supervision when trained
  on a new dataset of millions of webpages called WebText. When conditioned on a document
  plus questions, the answers generated by the language model reach 55 F1 on the CoQA
  dataset matching or exceeding the performance of 3 out of 4 baseline systems without
  using the 127,000+ training examples. The capacity of the language model is essential
  to the success of zero-shot task transfer and increasing it improves performance
  in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter
  Transformer that achieves state of the art results on 7 out of 8 tested language
  modeling datasets in a zero-shot setting but still underfits WebText. Samples from
  the model reflect these improvements and contain coherent paragraphs of text. These
  findings suggest a promising path towards building language processing systems which
  learn to perform tasks from their naturally occurring demonstrations.
ref_count: 75
references:
- pid: cd18800a0fe0b668a1cc19f2ec95b5003d0a5035
  title: Improving Language Understanding by Generative Pre-Training
- pid: 93b8da28d006415866bf48f9a6e06b5242129195
  title: 'GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language
    Understanding'
- pid: ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c
  title: Supervised Learning of Universal Sentence Representations from Natural Language
    Inference Data
- pid: cea967b59209c6be22829699f05b8b1ac4dc092d
  title: Sequence to Sequence Learning with Neural Networks
- pid: 85315b64a4c73cb86f156ef5b0a085d6ebc8a65d
  title: A Neural Conversational Model
- pid: df2b0e26d0599ce3e70df8a9da02e51594e0e992
  title: 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'
- pid: 85f94d8098322f8130512b4c6c4627548ce4a6cc
  title: Unsupervised Pretraining for Sequence to Sequence Learning
- pid: 5d833331b0e22ff359db05c62a8bca18c4f04b68
  title: One billion word benchmark for measuring progress in statistical language
    modeling
- pid: 1e077413b25c4d34945cc2707e17e46ed4fe784a
  title: Universal Language Model Fine-tuning for Text Classification
- pid: 2f2d8f8072e5cc9b296fad551f65f183bdbff7aa
  title: Exploring the Limits of Language Modeling
- pid: d7b6753a2d4a2b286c396854063bde3a91b75535
  title: A Simple Method for Commonsense Reasoning
- pid: bc8fa64625d9189f5801837e7b133e7fe3c581f7
  title: 'Learned in Translation: Contextualized Word Vectors'
- pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  title: Attention is All you Need
- pid: 4dabd6182ce2681c758f654561d351739e8df7bf
  title: Multilingual Language Processing From Bytes
- pid: 1af68821518f03568f913ab03fc02080247a27ff
  title: Neural Machine Translation of Rare Words with Subword Units
- pid: 17dbd7b72029181327732e4d11b52a08ed4630d0
  title: 'Natural Questions: A Benchmark for Question Answering Research'
- pid: c4744a7c2bb298e4a52289a1e085c71cc3d37bc6
  title: 'Transformer-XL: Attentive Language Models beyond a Fixed-Length Context'
- pid: bc1022b031dc6c7019696492e8116598097a8c12
  title: Natural Language Processing (Almost) from Scratch
- pid: 3febb2bed8865945e7fddc99efd791887bb7e14f
  title: Deep Contextualized Word Representations
- pid: 4aa9f5150b46320f534de4747a2dd0cd7f3fe292
  title: Semi-supervised Sequence Learning
- pid: 6c2b28f9354f667cd5bd07afc0471d8334430da7
  title: A Neural Probabilistic Language Model
- pid: 40be3888daa5c2e5af4d36ae22f690bcc8caf600
  title: Visualizing and Understanding Recurrent Networks
- pid: efbd381493bb9636f489b965a2034d529cd56bcd
  title: Pointer Sentinel Mixture Models
- pid: 26e743d5bd465f49b9538deaf116c15e61b7951f
  title: Learning Distributed Representations of Sentences from Unlabelled Data
- pid: f37076f426023241f19cdc2fb0a0fd733a6fa7fa
  title: Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond
- pid: 664ec878de4b7170712baae4a7821fc2602bba25
  title: Learning to Generate Reviews and Discovering Sentiment
- pid: 35b91b365ceb016fb3e022577cec96fb9b445dc5
  title: 'The Goldilocks Principle: Reading Children''s Books with Explicit Memory
    Representations'
- pid: ffb949d3493c3b2f3c9acf9c75cb03938933ddf0
  title: Adversarial Examples for Evaluating Reading Comprehension Systems
- pid: 668db48c6a79826456341680ee1175dfc4cced71
  title: 'Get To The Point: Summarization with Pointer-Generator Networks'
- pid: b9de9599d7241459db9213b5cdd7059696f5ef8d
  title: Character-Level Language Modeling with Deeper Self-Attention
- pid: f37e1b62a767a307c046404ca96bc140b3e68cb5
  title: 'GloVe: Global Vectors for Word Representation'
- pid: 8ff840a40d3f1557c55c19d4d636da77103168ce
  title: 'Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin'
- pid: 7260c0692f8d265e11c4e9c4c8ef4c185bd587ad
  title: Building machines that learn and think like people
- pid: c889d6f98e6d79b89c3a6adf8a921f88fa6ba518
  title: Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks
- pid: 47aaeb6dc682162dfe5659c2cad64e5d825ad910
  title: Multitask Learning
- pid: 87f40e6f3022adbc1f1905e3e506abad05a9964f
  title: Distributed Representations of Words and Phrases and their Compositionality
- pid: abd1c342495432171beb7ca8fd9551ef13cbd0ff
  title: ImageNet classification with deep convolutional neural networks
- pid: 77f0a39b8e02686fd85b01971f8feb7f60971f80
  title: Identity Mappings in Deep Residual Networks
- pid: 128cb6b891aee1b5df099acb48e2efecfcff689f
  title: The Winograd Schema Challenge
- pid: 9784fbf77295860b2e412137b86356d70b25e3c0
  title: 'The Natural Language Decathlon: Multitask Learning as Question Answering'
- pid: 6a923c9f89ed53b6e835b3807c0c1bd8d532687b
  title: Interpolated estimation of Markov source parameters from sparse data
slug: Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu
title: Language Models are Unsupervised Multitask Learners
url: https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe?sort=total-citations
venue: ''
year: 2019
