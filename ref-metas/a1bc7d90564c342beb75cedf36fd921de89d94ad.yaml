authors:
- Aude Genevay
- "G. Peyr\xE9"
- Marco Cuturi
badges:
- id: OPEN_ACCESS
corpusId: 4879286
fieldsOfStudy:
- Computer Science
numCitedBy: 393
numCiting: 37
paperAbstract: 'The ability to compare two degenerate probability distributions (i.e.
  two probability distributions supported on two distinct low-dimensional manifolds
  living in a much higher-dimensional space) is a crucial problem arising in the estimation
  of generative models for high-dimensional observations such as those arising in
  computer vision or natural language. It is known that optimal transport metrics
  can represent a cure for this problem, since they were specifically designed as
  an alternative to information divergences to handle such problematic scenarios.
  Unfortunately, training generative machines using OT raises formidable computational
  and statistical challenges, because of (i) the computational burden of evaluating
  OT losses, (ii) the instability and lack of smoothness of these losses, (iii) the
  difficulty to estimate robustly these losses and their gradients in high dimension.
  This paper presents the first tractable computational method to train large scale
  generative models using an optimal transport loss, and tackles these three issues
  by relying on two key ideas: (a) entropic smoothing, which turns the original OT
  loss into one that can be computed using Sinkhorn fixed point iterations; (b) algorithmic
  (automatic) differentiation of these iterations. These two approximations result
  in a robust and differentiable approximation of the OT loss with streamlined GPU
  execution. Entropic smoothing generates a family of losses interpolating between
  Wasserstein (OT) and Maximum Mean Discrepancy (MMD), thus allowing to find a sweet
  spot leveraging the geometry of OT and the favorable high-dimensional sample complexity
  of MMD which comes with unbiased gradient estimates. The resulting computational
  architecture complements nicely standard deep network generative models by a stack
  of extra layers implementing the loss function.'
ref_count: 37
references:
- pid: 54e325aee6b2d476bbbb88615ac15e251c6e8214
  title: Generative Adversarial Nets
- pid: a6cb366736791bcccc5c8639de5a8f9636bf87e8
  title: 'Adam: A Method for Stochastic Optimization'
- pid: 571b0750085ae3d939525e62af510ee2cee9d5ea
  title: Improved Techniques for Training GANs
- pid: 0080118b0eb02af581ff32b85a1bb6aed7081f45
  title: 'Sinkhorn Distances: Lightspeed Computation of Optimal Transport'
- pid: 69902406e7d08f8865f02185699978db499d25e7
  title: Improving GANs Using Optimal Transport
- pid: b71ac1e9fb49420d13e084ac67254a0bbd40f83f
  title: Understanding the difficulty of training deep feedforward neural networks
slug: "Learning-Generative-Models-with-Sinkhorn-Genevay-Peyr\xE9"
title: Learning Generative Models with Sinkhorn Divergences
url: "https://www.semanticscholar.org/paper/Learning-Generative-Models-with-Sinkhorn-Genevay-Peyr\xE9\
  /a1bc7d90564c342beb75cedf36fd921de89d94ad?sort=total-citations"
venue: AISTATS
year: 2018
