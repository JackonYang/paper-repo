authors:
- Noam M. Shazeer
- Youlong Cheng
- Niki Parmar
- Dustin Tran
- Ashish Vaswani
- Penporn Koanantakool
- Peter Hawkins
- HyoukJoong Lee
- Mingsheng Hong
- C. Young
- Ryan Sepassi
- Blake A. Hechtman
badges:
- id: OPEN_ACCESS
corpusId: 53236433
fieldsOfStudy:
- Computer Science
numCitedBy: 209
numCiting: 53
paperAbstract: Batch-splitting (data-parallelism) is the dominant distributed Deep
  Neural Network (DNN) training strategy, due to its universal applicability and its
  amenability to Single-Program-Multiple-Data (SPMD) programming. However, batch-splitting
  suffers from problems including the inability to train very large models (due to
  memory constraints), high latency, and inefficiency at small batch sizes. All of
  these can be solved by more general distribution strategies (model-parallelism).
  Unfortunately, efficient model-parallel algorithms tend to be complicated to discover,
  describe, and to implement, particularly on large clusters. We introduce Mesh-TensorFlow,
  a language for specifying a general class of distributed tensor computations. Where
  data-parallelism can be viewed as splitting tensors and operations along the "batch"
  dimension, in Mesh-TensorFlow, the user can specify any tensor-dimensions to be
  split across any dimensions of a multi-dimensional mesh of processors. A Mesh-TensorFlow
  graph compiles into a SPMD program consisting of parallel operations coupled with
  collective communication primitives such as Allreduce. We use Mesh-TensorFlow to
  implement an efficient data-parallel, model-parallel version of the Transformer
  sequence-to-sequence model. Using TPU meshes of up to 512 cores, we train Transformer
  models with up to 5 billion parameters, surpassing state of the art results on WMT'14
  English-to-French translation task and the one-billion-word language modeling benchmark.
  Mesh-Tensorflow is available at this https URL .
ref_count: 53
references:
- pid: 3127190433230b3dc1abd0680bb58dced4bcd90e
  title: Large Scale Distributed Deep Networks
- pid: 510e26733aaff585d65701b9f1be7ca9d5afc586
  title: 'Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts
    Layer'
- pid: abd1c342495432171beb7ca8fd9551ef13cbd0ff
  title: ImageNet classification with deep convolutional neural networks
- pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  title: Attention is All you Need
- pid: 2f2d8f8072e5cc9b296fad551f65f183bdbff7aa
  title: Exploring the Limits of Language Modeling
slug: Mesh-TensorFlow:-Deep-Learning-for-Supercomputers-Shazeer-Cheng
title: 'Mesh-TensorFlow: Deep Learning for Supercomputers'
url: https://www.semanticscholar.org/paper/Mesh-TensorFlow:-Deep-Learning-for-Supercomputers-Shazeer-Cheng/2270b8628fd8ca67ae39d277f45bc3c38ac63d5f?sort=total-citations
venue: NeurIPS
year: 2018
