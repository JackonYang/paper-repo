authors:
- Andrew M. Dai
- Quoc V. Le
badges:
- id: OPEN_ACCESS
corpusId: 7138078
fieldsOfStudy:
- Computer Science
numCitedBy: 881
numCiting: 54
paperAbstract: We present two approaches to use unlabeled data to improve Sequence
  Learning with recurrent networks. The first approach is to predict what comes next
  in a sequence, which is a language model in NLP. The second approach is to use a
  sequence autoencoder, which reads the input sequence into a vector and predicts
  the input sequence again. These two algorithms can be used as a "pretraining" algorithm
  for a later supervised sequence learning algorithm. In other words, the parameters
  obtained from the pretraining step can then be used as a starting point for other
  supervised training models. In our experiments, we find that long short term memory
  recurrent networks after pretrained with the two approaches become more stable to
  train and generalize better. With pretraining, we were able to achieve strong performance
  in many classification tasks, such as text classification with IMDB, DBpedia or
  image recognition in CIFAR-10.
ref_count: 55
references:
- pid: cea967b59209c6be22829699f05b8b1ac4dc092d
  title: Sequence to Sequence Learning with Neural Networks
- pid: 829510ad6f975c939d589eeb01a3cf6fc6c8ce4d
  title: Unsupervised Learning of Video Representations using LSTMs
- pid: 6e795c6e9916174ae12349f5dc3f516570c17ce8
  title: Skip-Thought Vectors
- pid: 85315b64a4c73cb86f156ef5b0a085d6ebc8a65d
  title: A Neural Conversational Model
- pid: d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0
  title: 'Show and tell: A neural image caption generator'
- pid: a7976c2bacfbb194ddbe7fd10c2e50a545cf4081
  title: 'LSTM: A Search Space Odyssey'
- pid: 6c2b28f9354f667cd5bd07afc0471d8334430da7
  title: A Neural Probabilistic Language Model
- pid: f527bcfb09f32e6a4a8afc0b37504941c1ba2cee
  title: Distributed Representations of Sentences and Documents
- pid: 11540131eae85b2e11d53df7f1360eeb6476e7f4
  title: 'Learning to Forget: Continual Prediction with LSTM'
- pid: 9819b600a828a57e1cde047bbe710d3446b30da5
  title: Recurrent neural network based language model
- pid: 1938624bb9b0f999536dcc8d8f519810bb4e1b3b
  title: On Using Very Large Target Vocabulary for Neural Machine Translation
- pid: f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97
  title: Recurrent Neural Network Regularization
- pid: 44d2abe2175df8153f465f6c39b68b76a0d40ab9
  title: Long Short-Term Memory
- pid: 1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba
  title: Convolutional Neural Networks for Sentence Classification
- pid: abd1c342495432171beb7ca8fd9551ef13cbd0ff
  title: ImageNet classification with deep convolutional neural networks
- pid: 052b1d8ce63b07fec3de9dbb583772d860b7c769
  title: Learning representations by back-propagating errors
- pid: 1956c239b3552e030db1b78951f64781101125ed
  title: Addressing the Rare Word Problem in Neural Machine Translation
- pid: dc555e8156c956f823587ebbff018863e6d2a95e
  title: Listen, Attend and Spell
- pid: 687bac2d3320083eb4530bf18bb8f8f721477600
  title: Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank
- pid: 47d2dc34e1d02a8109f5c04bb6939725de23716d
  title: 'End-to-end Continuous Speech Recognition using Attention-based Recurrent
    NN: First Results'
- pid: 89b1f4740ae37fd04f6ac007577bdd34621f0861
  title: Generating Sequences With Recurrent Neural Networks
- pid: 649d03490ef72c5274e3bccd03d7a299d2f8da91
  title: Learning Word Vectors for Sentiment Analysis
- pid: bea5780d621e669e8069f05d0f2fc0db9df4b50f
  title: Convolutional Deep Belief Networks on CIFAR-10
- pid: dc0975ae518a5b30e60fde23a41c74bafd7c6f8c
  title: 'Baselines and Bigrams: Simple, Good Sentiment and Topic Classification'
- pid: 47570e7f63e296f224a0e7f9a0d08b0de3cbaf40
  title: Grammar as a Foreign Language
- pid: 38b3a4447a47a6a6ed1869f3da03352c487f8fe3
  title: 'NewsWeeder: Learning to Filter Netnews'
- pid: 27e38351e48fe4b7da2775bf94341738bc4da07e
  title: Semantic Compositionality through Recursive Matrix-Vector Spaces
- pid: 51a55df1f023571a7e07e338ee45a3e3d66ef73e
  title: Character-level Convolutional Networks for Text Classification
- pid: 6af58c061f2e4f130c3b795c21ff0c7e3903278f
  title: 'Seeing Stars: Exploiting Class Relationships for Sentiment Categorization
    with Respect to Rating Scales'
- pid: aed054834e2c696807cc8b227ac7a4197196e211
  title: 'Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies'
- pid: d2946a868682e4141beabc288d79253ae254c6e1
  title: DBpedia - A large-scale, multilingual knowledge base extracted from Wikipedia
- pid: 749ce8ccd9453d1b34901143cddf5f9bee2977cf
  title: Learning representations by back-propagation errors, nature
- pid: 56623a496727d5c71491850e04512ddf4152b487
  title: 'Beyond Regression : "New Tools for Prediction and Analysis in the Behavioral
    Sciences'
- pid: 944e1a7b2c5c62e952418d7684e3cade89c76f87
  title: A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled
    Data
slug: Semi-supervised-Sequence-Learning-Dai-Le
title: Semi-supervised Sequence Learning
url: https://www.semanticscholar.org/paper/Semi-supervised-Sequence-Learning-Dai-Le/4aa9f5150b46320f534de4747a2dd0cd7f3fe292?sort=total-citations
venue: NIPS
year: 2015
