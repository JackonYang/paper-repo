authors:
- R. Battiti
badges: []
corpusId: 27960650
fieldsOfStudy:
- Computer Science
numCitedBy: 1216
numCiting: 65
paperAbstract: 'On-line first-order backpropagation is sufficiently fast and effective
  for many large-scale classification problems but for very high precision mappings,
  batch processing may be the method of choice. This paper reviews first- and second-order
  optimization methods for learning in feedforward neural networks. The viewpoint
  is that of optimization: many methods can be cast in the language of optimization
  techniques, allowing the transfer to neural nets of detailed results about computational
  complexity and safety procedures to ensure convergence and to avoid numerical problems.
  The review is not intended to deliver detailed prescriptions for the most appropriate
  methods in specific applications, but to illustrate the main characteristics of
  the different methods and their mutual relations.'
ref_count: 65
references:
- pid: 2f4a097b2131784d7ac3fc3c47d1e9283e9ac207
  title: A scaled conjugate gradient algorithm for fast supervised learning
- pid: 4abd4e51705e74f1739bd3a1e47ac10e45f6468b
  title: Regularization Algorithms for Learning That Are Equivalent to Multilayer
    Networks
- pid: 934e49dac717a924bfda841bf6e54c32e900f0d1
  title: 'Learning Algorithms for Connectionist Networks: Applied Gradient Methods
    of Nonlinear Optimization'
- pid: a9ef2995e8e1bd57a74343073219364811c2ace0
  title: Increased rates of convergence through learning rate adaptation
- pid: 656a33c1db546da8490d6eba259e2a849d73a001
  title: 'Learning in Artificial Neural Networks: A Statistical Perspective'
- pid: d5558a34dfd1dbb572895664d38fca04029a99cb
  title: Radial Basis Functions, Multi-Variable Functional Interpolation and Adaptive
    Networks
- pid: d2aaf56fb183ad66d099ac6c9110c5c365ab27f3
  title: Updating Quasi-Newton Matrices With Limited Storage
- pid: e1053197256c6c3c0631377ec23a3f7dc1cb4781
  title: Numerical methods for unconstrained optimization and nonlinear equations
- pid: 406033f22b6a671b94bcbdfaf63070b7ce6f3e48
  title: 'NETtalk: a parallel network that learns to read aloud'
- pid: b8d9abd1c078573188b13d36c1b1efb7cb2fa865
  title: Practical optimization
- pid: a7d78b005150b873a1b72423cdc045267e03daa7
  title: Adaptive Signal Processing
- pid: 95c23bb25a03d296a7eedb7c8dffe1748bb614c6
  title: Introduction to linear and nonlinear programming
- pid: 01b6affe3ea4eae1978aec54e87087feb76d9215
  title: Generalization and network design strategies
- pid: 589d377b23e2bdae7ad161b36a5d6613bcfccdde
  title: Improving the convergence of back-propagation learning with second-order
    methods
- pid: b08ba914037af6d88d16e2657a65cd9dc5cf5da1
  title: Multivariable Functional Interpolation and Adaptive Networks
slug: First-and-Second-Order-Methods-for-Learning:-and-Battiti
title: 'First- and Second-Order Methods for Learning: Between Steepest Descent and
  Newton''s Method'
url: https://www.semanticscholar.org/paper/First-and-Second-Order-Methods-for-Learning:-and-Battiti/bbf6f07e699587c8d52faf829a289f8cbc7f11a5?sort=total-citations
venue: Neural Computation
year: 1992
