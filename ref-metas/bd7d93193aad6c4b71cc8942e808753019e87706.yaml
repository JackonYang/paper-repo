authors:
- A. Mnih
- Geoffrey E. Hinton
badges:
- id: OPEN_ACCESS
corpusId: 577005
fieldsOfStudy:
- Computer Science
numCitedBy: 606
numCiting: 14
paperAbstract: The supremacy of n-gram models in statistical language modelling has
  recently been challenged by parametric models that use distributed representations
  to counteract the difficulties caused by data sparsity. We propose three new probabilistic
  language models that define the distribution of the next word in a sequence given
  several preceding words by using distributed representations of those words. We
  show how real-valued distributed representations for words can be learned at the
  same time as learning a large set of stochastic binary hidden features that are
  used to predict the distributed representation of the next word from previous distributed
  representations. Adding connections from the previous states of the binary hidden
  features improves performance as does adding direct connections between the real-valued
  distributed representations. One of our models significantly outperforms the very
  best n-gram models.
ref_count: 15
references:
- pid: 6c2b28f9354f667cd5bd07afc0471d8334430da7
  title: A Neural Probabilistic Language Model
- pid: c19fbefdeead6a4154a22a9c8551a18b1530033a
  title: Hierarchical Probabilistic Neural Network Language Model
- pid: 3d6036af971c1f11ab712cc41487376a94e63673
  title: Using a connectionist model in a syntactical based language model
- pid: 8b395470a57c48d174c4216ea21a7a58bc046917
  title: Training Neural Network Language Models on Very Large Corpora
- pid: c74e230a5a6fd5e2db6ace765ce38afe65f96214
  title: Learning Multilevel Distributed Representations for High-Dimensional Sequences
- pid: 399da68d3b97218b6c80262df7963baa89dcc71b
  title: SRILM - an extensible language modeling toolkit
- pid: 8978cf7574ceb35f4c3096be768c7547b28a35d0
  title: A Fast Learning Algorithm for Deep Belief Nets
- pid: 9360e5ce9c98166bb179ad479a9d2919ff13d022
  title: Training Products of Experts by Minimizing Contrastive Divergence
- pid: 769dbbe88801b57a9b44f89c5516264f16cbed60
  title: An empirical study of smoothing techniques for language modeling
- pid: 4ade4934db522fe6d634ff6f48887da46eedb4d1
  title: Learning distributed representations of concepts.
- pid: 6b388f0151ab37adb3d57738b8f52a3f943f86c8
  title: Quick Training of Probabilistic Neural Nets by Importance Sampling
slug: Three-new-graphical-models-for-statistical-language-Mnih-Hinton
title: Three new graphical models for statistical language modelling
url: https://www.semanticscholar.org/paper/Three-new-graphical-models-for-statistical-language-Mnih-Hinton/bd7d93193aad6c4b71cc8942e808753019e87706?sort=total-citations
venue: ICML '07
year: 2007
