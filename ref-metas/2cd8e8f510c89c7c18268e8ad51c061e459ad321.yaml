authors:
- Ankur P. Parikh
- "Oscar T\xE4ckstr\xF6m"
- Dipanjan Das
- Jakob Uszkoreit
badges:
- id: OPEN_ACCESS
corpusId: 8495258
fieldsOfStudy:
- Computer Science
numCitedBy: 1059
numCiting: 35
paperAbstract: We propose a simple neural architecture for natural language inference.
  Our approach uses attention to decompose the problem into subproblems that can be
  solved separately, thus making it trivially parallelizable. On the Stanford Natural
  Language Inference (SNLI) dataset, we obtain state-of-the-art results with almost
  an order of magnitude fewer parameters than previous work and without relying on
  any word-order information. Adding intra-sentence attention that takes a minimum
  amount of order into account yields further improvements.
ref_count: 35
references:
- pid: f04df4e20a18358ea2f689b4c129781628ef7fc1
  title: A large annotated corpus for learning natural language inference
- pid: 596c882de006e4bb4a93f1fa08a5dd467bee060a
  title: Learning Natural Language Inference with LSTM
- pid: 46b8cbcdff87b842c2c1d4a003c831f845096ba7
  title: Order-Embeddings of Images and Language
- pid: 2846e83d405cbe3bf2f0f3b5f635dd8b3c680c45
  title: Reasoning about Entailment with Neural Attention
- pid: 36c097a225a95735271960e2b63a2cb9e98bff83
  title: A Fast Unified Model for Parsing and Sentence Understanding
- pid: c0be2ac2f45681f1852fc1d298af5dceb85834f4
  title: Paraphrase-Driven Learning for Open Question Answering
- pid: 13fe71da009484f240c46f14d9330e932f8de210
  title: Long Short-Term Memory-Networks for Machine Reading
- pid: 9f08b01251cb99f4ffae8c7b3e4468d3af9c98d3
  title: Convolutional Neural Network Architectures for Matching Natural Language
    Sentences
- pid: f37e1b62a767a307c046404ca96bc140b3e68cb5
  title: 'GloVe: Global Vectors for Word Representation'
- pid: fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5
  title: Neural Machine Translation by Jointly Learning to Align and Translate
- pid: b3e89f05876d47b9bd6ece225aaeee457a6824e8
  title: Statistical Machine Translation
- pid: 34f25a8704614163c4095b3ee2fc969b60de4698
  title: 'Dropout: a simple way to prevent neural networks from overfitting'
- pid: 44d2abe2175df8153f465f6c39b68b76a0d40ab9
  title: Long Short-Term Memory
- pid: 413c1142de9d91804d6d11c67ff3fed59c9fc279
  title: Adaptive Subgradient Methods for Online Learning and Stochastic Optimization
- pid: 86ab4cae682fbd49c5a5bedb630e5a40fa7529f6
  title: Handwritten Digit Recognition with a Back-Propagation Network
- pid: 67107f78a84bdb2411053cb54e94fa226eea6d8e
  title: Deep Sparse Rectifier Neural Networks
slug: "A-Decomposable-Attention-Model-for-Natural-Language-Parikh-T\xE4ckstr\xF6m"
title: A Decomposable Attention Model for Natural Language Inference
url: "https://www.semanticscholar.org/paper/A-Decomposable-Attention-Model-for-Natural-Language-Parikh-T\xE4\
  ckstr\xF6m/2cd8e8f510c89c7c18268e8ad51c061e459ad321?sort=total-citations"
venue: EMNLP
year: 2016
