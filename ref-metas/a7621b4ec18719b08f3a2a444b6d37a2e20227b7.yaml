authors:
- "Micha\xEBl Mathieu"
- Mikael Henaff
- Yann LeCun
badges:
- id: OPEN_ACCESS
corpusId: 18233038
fieldsOfStudy:
- Computer Science
numCitedBy: 454
numCiting: 11
paperAbstract: Convolutional networks are one of the most widely employed architectures
  in computer vision and machine learning. In order to leverage their ability to learn
  complex functions, large amounts of data are required for training. Training a large
  convolutional network to produce state-of-the-art results can take weeks, even when
  using modern GPUs. Producing labels using a trained network can also be costly when
  dealing with web-scale datasets. In this work, we present a simple algorithm which
  accelerates training and inference by a significant factor, and can yield improvements
  of over an order of magnitude compared to existing state-of-the-art implementations.
  This is done by computing convolutions as pointwise products in the Fourier domain
  while reusing the same transformed feature map many times. The algorithm is implemented
  on a GPU architecture and addresses a number of related challenges.
ref_count: 11
references:
- pid: abd1c342495432171beb7ca8fd9551ef13cbd0ff
  title: ImageNet classification with deep convolutional neural networks
- pid: 3449b65008b27f6e60a73d80c1fd990f0481126b
  title: 'Torch7: A Matlab-like Environment for Machine Learning'
- pid: aedb8df8f953429ec5a6df99fda5c5d71dbee4ff
  title: 'Learning Generative Visual Models from Few Training Examples: An Incremental
    Bayesian Approach Tested on 101 Object Categories'
- pid: b98c68f01d84ac07dc7fc51af782018070da748f
  title: Representing shape with a spatial pyramid kernel
- pid: b87274e6d9aa4e6ba5148898aa92941617d2b6ed
  title: Efficient BackProp
slug: Fast-Training-of-Convolutional-Networks-through-Mathieu-Henaff
title: Fast Training of Convolutional Networks through FFTs
url: https://www.semanticscholar.org/paper/Fast-Training-of-Convolutional-Networks-through-Mathieu-Henaff/a7621b4ec18719b08f3a2a444b6d37a2e20227b7?sort=total-citations
venue: ICLR
year: 2014
