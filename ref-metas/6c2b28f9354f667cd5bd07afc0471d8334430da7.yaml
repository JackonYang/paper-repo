authors:
- Yoshua Bengio
- "R\xE9jean Ducharme"
- Pascal Vincent
- Christian Janvin
badges:
- id: OPEN_ACCESS
corpusId: 221275765
fieldsOfStudy:
- Computer Science
numCitedBy: 6011
numCiting: 54
paperAbstract: 'A goal of statistical language modeling is to learn the joint probability
  function of sequences of words in a language. This is intrinsically difficult because
  of the curse of dimensionality: a word sequence on which the model will be tested
  is likely to be different from all the word sequences seen during training. Traditional
  but very successful approaches based on n-grams obtain generalization by concatenating
  very short overlapping sequences seen in the training set. We propose to fight the
  curse of dimensionality by learning a distributed representation for words which
  allows each training sentence to inform the model about an exponential number of
  semantically neighboring sentences. The model learns simultaneously (1) a distributed
  representation for each word along with (2) the probability function for word sequences,
  expressed in terms of these representations. Generalization is obtained because
  a sequence of words that has never been seen before gets high probability if it
  is made of words that are similar (in the sense of having a nearby representation)
  to words forming an already seen sentence. Training such large models (with millions
  of parameters) within a reasonable time is itself a significant challenge. We report
  on experiments using neural networks for the probability function, showing on two
  text corpora that the proposed approach significantly improves on state-of-the-art
  n-gram models, and that the proposed approach allows to take advantage of longer
  contexts.'
ref_count: 55
references:
- pid: e41498c05d4c68e4750fb84a380317a112d97b01
  title: Connectionist language modeling for large vocabulary continuous speech recognition
- pid: 3de5d40b60742e3dfa86b19e7f660962298492af
  title: Class-Based n-gram Models of Natural Language
- pid: 09c76da2361d46689825c4efc37ad862347ca577
  title: A bit of progress in language modeling
- pid: b0130277677e5b915d5cd86b3afafd77fd08eb2e
  title: Estimation of probabilities from sparse data for the language model component
    of a speech recognizer
- pid: 5eb328cf7e94995199e4c82a1f4d0696430a80b5
  title: Distributional Clustering of English Words
- pid: 668087f0ae7ce1de6e0bd0965dbb480c08103260
  title: Finding Structure in Time
- pid: bfab4ffa229c8af0174a683ff1eda524c4f59d00
  title: Can artificial neural networks learn language models?
- pid: 9548ac30c113562a51e603dbbc8e9fa651cfd3ab
  title: Improved backing-off for M-gram language modeling
- pid: fb486e03369a64de2d5b0df86ec0a7b55d3907db
  title: A Maximum Entropy Approach to Natural Language Processing
- pid: 9360e5ce9c98166bb179ad479a9d2919ff13d022
  title: Training Products of Experts by Minimizing Contrastive Divergence
- pid: 5d24afe3a62331ebfad400c3fec77c836d2b99db
  title: Word Space
- pid: d87ceda3042f781c341ac17109d1e94a717f5f60
  title: 'WordNet : an electronic lexical database'
- pid: 399da68d3b97218b6c80262df7963baa89dcc71b
  title: SRILM - an extensible language modeling toolkit
- pid: 190e4800c67ef445e4bd0944a55debaccebcf43f
  title: Modeling High-Dimensional Discrete Data with Multi-Layer Neural Networks
- pid: e733226b881f11f25c87e8bac8d602ba3d9c220e
  title: Distributional clustering of words for text classification
- pid: 20a80a7356859daa4170fb4da6b87b84adbb547f
  title: Indexing by Latent Semantic Analysis
- pid: 769dbbe88801b57a9b44f89c5516264f16cbed60
  title: An empirical study of smoothing techniques for language modeling
- pid: 6a923c9f89ed53b6e835b3807c0c1bd8d532687b
  title: Interpolated estimation of Markov source parameters from sparse data
- pid: 4ade4934db522fe6d634ff6f48887da46eedb4d1
  title: Learning distributed representations of concepts.
- pid: b9ed0b35c9eaba0328492de65c4cdc5545094df4
  title: Improved clustering techniques for class-based statistical language modelling
- pid: 2928de5400a920a6a29af41821c680cef5d35f91
  title: A latent semantic analysis framework for large-Span language modeling
slug: A-Neural-Probabilistic-Language-Model-Bengio-Ducharme
title: A Neural Probabilistic Language Model
url: https://www.semanticscholar.org/paper/A-Neural-Probabilistic-Language-Model-Bengio-Ducharme/6c2b28f9354f667cd5bd07afc0471d8334430da7?sort=total-citations
venue: J. Mach. Learn. Res.
year: 2000
