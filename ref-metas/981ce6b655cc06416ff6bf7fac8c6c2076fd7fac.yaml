authors:
- Yann Dauphin
- Razvan Pascanu
- "\xC7aglar G\xFCl\xE7ehre"
- Kyunghyun Cho
- S. Ganguli
- Yoshua Bengio
badges:
- id: OPEN_ACCESS
corpusId: 11657534
fieldsOfStudy:
- Computer Science
numCitedBy: 1074
numCiting: 34
paperAbstract: A central challenge to many fields of science and engineering involves
  minimizing non-convex error functions over continuous, high dimensional spaces.
  Gradient descent or quasi-Newton methods are almost ubiquitously used to perform
  such minimizations, and it is often thought that a main source of difficulty for
  these local methods to find the global minimum is the proliferation of local minima
  with much higher error than the global minimum. Here we argue, based on results
  from statistical physics, random matrix theory, neural network theory, and empirical
  evidence, that a deeper and more profound difficulty originates from the proliferation
  of saddle points, not local minima, especially in high dimensional problems of practical
  interest. Such saddle points are surrounded by high error plateaus that can dramatically
  slow down learning, and give the illusory impression of the existence of a local
  minimum. Motivated by these arguments, we propose a new approach to second-order
  optimization, the saddle-free Newton method, that can rapidly escape high dimensional
  saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm
  to deep or recurrent neural network training, and provide numerical evidence for
  its superior optimization performance.
ref_count: 34
references:
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1265
  pid: 99c970348b8f70ce23d6641e201904ea49266b6e
  title: Exact solutions to the nonlinear dynamics of learning in deep linear neural
    networks
  year: 2014
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 119
  pid: a98483785378bde7e2384a3035b2b501ee03654b
  title: Krylov Subspace Descent for Deep Learning
  year: 2012
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1336
  pid: 9552ac39a57daacf3d75865a268935b5a0df9bbb
  title: 'Neural networks and principal component analysis: Learning from examples
    without local minima'
  year: 1989
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 586
  pid: c6867b6b564462d6b902f68e0bfa58f4717ca1cc
  title: Fast Exact Multiplication by the Hessian
  year: 1994
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 5672
  pid: 188e247506ad992b8bc62d6c74789e89891a984f
  title: Random Search for Hyper-Parameter Optimization
  year: 2012
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 179
  pid: 6ed460701019072ee2e364a1a491f73dd931f27f
  title: Topmoumoute Online Natural Gradient Algorithm
  year: 2007
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 3557
  pid: aa7bfd2304201afbb19971ebde87b17e40242e91
  title: On the importance of initialization and momentum in deep learning
  year: 2013
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 845
  pid: 4c46347fbc272b21468efe3d9af34b4b2bad6684
  title: Deep learning via Hessian-free optimization
  year: 2010
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 280
  pid: e8f95ccfd13689f672c39dca3eccf1c484533bcc
  title: Revisiting Natural Gradient for Deep Networks
  year: 2014
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 3803
  pid: 84069287da0a6b488b8c933f3cb5be759cb6237e
  title: On the difficulty of training recurrent neural networks
  year: 2013
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 6144
  pid: d0be39ee052d246ae99c082a565aba25b811be2d
  title: Learning long-term dependencies with gradient descent is difficult
  year: 1994
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1374
  pid: 855d0f722d75cc56a66a00ede18ace96bafee6bd
  title: 'Theano: new features and speed improvements'
  year: 2012
- fieldsOfStudy:
  - Business
  numCitedBy: 10989
  pid: bf86896c23300a46b7fc76298e365984c0b05105
  title: Numerical Optimization
  year: 2001
slug: Identifying-and-attacking-the-saddle-point-problem-Dauphin-Pascanu
title: Identifying and attacking the saddle point problem in high-dimensional non-convex
  optimization
url: https://www.semanticscholar.org/paper/Identifying-and-attacking-the-saddle-point-problem-Dauphin-Pascanu/981ce6b655cc06416ff6bf7fac8c6c2076fd7fac?sort=total-citations
venue: NIPS
year: 2014
