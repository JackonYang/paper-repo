authors:
- Tomas Mikolov
- Anoop Deoras
- Daniel Povey
- L. Burget
- "J. Cernock\xFD"
badges:
- id: OPEN_ACCESS
corpusId: 15076873
fieldsOfStudy:
- Computer Science
numCitedBy: 474
numCiting: 21
paperAbstract: We describe how to effectively train neural network based language
  models on large data sets. Fast convergence during training and better overall performance
  is observed when the training data are sorted by their relevance. We introduce hash-based
  implementation of a maximum entropy model, that can be trained as a part of the
  neural network model. This leads to significant reduction of computational complexity.
  We achieved around 10% relative reduction of word error rate on English Broadcast
  News speech recognition task, against large 4-gram model trained on 400M tokens.
ref_count: 21
references:
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 128
  pid: 8b395470a57c48d174c4216ea21a7a58bc046917
  title: Training Neural Network Language Models on Very Large Corpora
  year: 2005
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 554
  pid: 0fcc184b3b90405ec3ceafd6a4007c749df7c363
  title: Continuous space language models
  year: 2007
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 4902
  pid: 9819b600a828a57e1cde047bbe710d3446b30da5
  title: Recurrent neural network based language model
  year: 2010
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 942
  pid: c19fbefdeead6a4154a22a9c8551a18b1530033a
  title: Hierarchical Probabilistic Neural Network Language Model
  year: 2005
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 157
  pid: 3aaa1e4974800767fcbd2c24c2f2af42bf412f97
  title: Structured Output Layer neural network language model
  year: 2011
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 86
  pid: bfab4ffa229c8af0174a683ff1eda524c4f59d00
  title: Can artificial neural networks learn language models?
  year: 2000
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 314
  pid: 77dfe038a9bdab27c4505444931eaa976e9ec667
  title: Empirical Evaluation and Combination of Advanced Language Modeling Techniques
  year: 2011
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 6011
  pid: 6c2b28f9354f667cd5bd07afc0471d8334430da7
  title: A Neural Probabilistic Language Model
  year: 2000
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1423
  pid: 07ca885cb5cc4328895bfaec9ab752d5801b14cd
  title: Extensions of recurrent neural network language model
  year: 2011
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 233
  pid: 4af41f4d838daa7ca6995aeb4918b61989d1ed80
  title: Classes for fast maximum entropy training
  year: 2001
- fieldsOfStudy:
  - Psychology
  numCitedBy: 1723
  pid: d5ddb30bf421bdfdf728b636993dc48b1e879176
  title: 'Learning and development in neural networks: the importance of starting
    small'
  year: 1993
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 608
  pid: 076fa8d095c37c657f2aff39cf90bc2ea883b7cb
  title: A maximum entropy approach to adaptive statistical language modelling
  year: 1996
- fieldsOfStudy:
  - Education
  numCitedBy: 3193
  pid: 8de174ab5419b9d3127695405efd079808e956e8
  title: Curriculum learning
  year: 2009
slug: Strategies-for-training-large-scale-neural-network-Mikolov-Deoras
title: Strategies for training large scale neural network language models
url: https://www.semanticscholar.org/paper/Strategies-for-training-large-scale-neural-network-Mikolov-Deoras/cb45e9217fe323fbc199d820e7735488fca2a9b3?sort=total-citations
venue: 2011 IEEE Workshop on Automatic Speech Recognition & Understanding
year: 2011
