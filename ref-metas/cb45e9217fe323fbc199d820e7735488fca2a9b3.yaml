authors:
- Tomas Mikolov
- Anoop Deoras
- Daniel Povey
- L. Burget
- "J. Cernock\xFD"
badges:
- id: OPEN_ACCESS
corpusId: 15076873
fieldsOfStudy:
- Computer Science
numCitedBy: 474
numCiting: 21
paperAbstract: We describe how to effectively train neural network based language
  models on large data sets. Fast convergence during training and better overall performance
  is observed when the training data are sorted by their relevance. We introduce hash-based
  implementation of a maximum entropy model, that can be trained as a part of the
  neural network model. This leads to significant reduction of computational complexity.
  We achieved around 10% relative reduction of word error rate on English Broadcast
  News speech recognition task, against large 4-gram model trained on 400M tokens.
ref_count: 21
references:
- pid: 8b395470a57c48d174c4216ea21a7a58bc046917
  title: Training Neural Network Language Models on Very Large Corpora
- pid: 0fcc184b3b90405ec3ceafd6a4007c749df7c363
  title: Continuous space language models
- pid: 9819b600a828a57e1cde047bbe710d3446b30da5
  title: Recurrent neural network based language model
- pid: c19fbefdeead6a4154a22a9c8551a18b1530033a
  title: Hierarchical Probabilistic Neural Network Language Model
- pid: 3aaa1e4974800767fcbd2c24c2f2af42bf412f97
  title: Structured Output Layer neural network language model
- pid: bfab4ffa229c8af0174a683ff1eda524c4f59d00
  title: Can artificial neural networks learn language models?
- pid: 77dfe038a9bdab27c4505444931eaa976e9ec667
  title: Empirical Evaluation and Combination of Advanced Language Modeling Techniques
- pid: 6c2b28f9354f667cd5bd07afc0471d8334430da7
  title: A Neural Probabilistic Language Model
- pid: 07ca885cb5cc4328895bfaec9ab752d5801b14cd
  title: Extensions of recurrent neural network language model
- pid: 4af41f4d838daa7ca6995aeb4918b61989d1ed80
  title: Classes for fast maximum entropy training
- pid: d5ddb30bf421bdfdf728b636993dc48b1e879176
  title: 'Learning and development in neural networks: the importance of starting
    small'
- pid: 076fa8d095c37c657f2aff39cf90bc2ea883b7cb
  title: A maximum entropy approach to adaptive statistical language modelling
- pid: 8de174ab5419b9d3127695405efd079808e956e8
  title: Curriculum learning
slug: Strategies-for-training-large-scale-neural-network-Mikolov-Deoras
title: Strategies for training large scale neural network language models
url: https://www.semanticscholar.org/paper/Strategies-for-training-large-scale-neural-network-Mikolov-Deoras/cb45e9217fe323fbc199d820e7735488fca2a9b3?sort=total-citations
venue: 2011 IEEE Workshop on Automatic Speech Recognition & Understanding
year: 2011
