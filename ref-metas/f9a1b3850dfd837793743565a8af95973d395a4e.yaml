authors:
- M. Sundermeyer
- "R. Schl\xFCter"
- H. Ney
badges:
- id: OPEN_ACCESS
corpusId: 18939716
fieldsOfStudy:
- Computer Science
numCitedBy: 1491
numCiting: 18
paperAbstract: Neural networks have become increasingly popular for the task of language
  modeling. Whereas feed-forward networks only exploit a fixed context length to predict
  the next word of a sequence, conceptually, standard recurrent neural networks can
  take into account all of the predecessor words. On the other hand, it is well known
  that recurrent networks are difficult to train and therefore are unlikely to show
  the full potential of recurrent models. These problems are addressed by a the Long
  Short-Term Memory neural network architecture. In this work, we analyze this type
  of network on an English and a large French language modeling task. Experiments
  show improvements of about 8 % relative in perplexity over standard recurrent neural
  network LMs. In addition, we gain considerable improvements in WER on top of a state-of-the-art
  speech recognition system.
ref_count: 18
references:
- pid: 9819b600a828a57e1cde047bbe710d3446b30da5
  title: Recurrent neural network based language model
- pid: c19fbefdeead6a4154a22a9c8551a18b1530033a
  title: Hierarchical Probabilistic Neural Network Language Model
- pid: 07ca885cb5cc4328895bfaec9ab752d5801b14cd
  title: Extensions of recurrent neural network language model
- pid: 0fcc184b3b90405ec3ceafd6a4007c749df7c363
  title: Continuous space language models
- pid: 2f83f6e1afadf0963153974968af6b8342775d82
  title: Framewise phoneme classification with bidirectional LSTM and other neural
    network architectures
- pid: d0be39ee052d246ae99c082a565aba25b811be2d
  title: Learning long-term dependencies with gradient descent is difficult
- pid: 6c2b28f9354f667cd5bd07afc0471d8334430da7
  title: A Neural Probabilistic Language Model
- pid: 047655e733a9eed9a500afd916efa566915b9110
  title: Learning Precise Timing with LSTM Recurrent Networks
- pid: 0d6203718c15f137fda2f295c96269bc2b254644
  title: Learning Recurrent Neural Networks with Hessian-Free Optimization
- pid: 4af41f4d838daa7ca6995aeb4918b61989d1ed80
  title: Classes for fast maximum entropy training
- pid: 44d2abe2175df8153f465f6c39b68b76a0d40ab9
  title: Long Short-Term Memory
- pid: 052b1d8ce63b07fec3de9dbb583772d860b7c769
  title: Learning representations by back-propagating errors
- pid: 9548ac30c113562a51e603dbbc8e9fa651cfd3ab
  title: Improved backing-off for M-gram language modeling
- pid: 668087f0ae7ce1de6e0bd0965dbb480c08103260
  title: Finding Structure in Time
- pid: b9b1b1654ce0eea729c4160bfedcbb3246460b1d
  title: Neural networks for pattern recognition
slug: "LSTM-Neural-Networks-for-Language-Modeling-Sundermeyer-Schl\xFCter"
title: LSTM Neural Networks for Language Modeling
url: "https://www.semanticscholar.org/paper/LSTM-Neural-Networks-for-Language-Modeling-Sundermeyer-Schl\xFC\
  ter/f9a1b3850dfd837793743565a8af95973d395a4e?sort=total-citations"
venue: INTERSPEECH
year: 2012
