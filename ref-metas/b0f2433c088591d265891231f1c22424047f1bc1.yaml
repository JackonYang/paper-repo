authors:
- D. Mackay
badges: []
corpusId: 15883988
fieldsOfStudy:
- Computer Science
numCitedBy: 254
numCiting: 39
paperAbstract: 'A quantitative and practical Bayesian framework is described for learning
  of mappings in feedforward networks. The framework makes possible: (1) objective
  comparisons between solutions using alternative network architectures; (2) objective
  stopping rules for deletion of weights; (3) objective choice of magnitude and type
  of weight decay terms or additive regularisers (for penalising large weights, etc.);
  (4) a measure of the e ective number of well{determined parameters in a model; (5)
  quanti ed estimates of the error bars on network parameters and on network output;
  (6) objective comparisons with alternative learning and interpolation models such
  as splines and radial basis functions. The Bayesian `evidence'' automatically embodies
  `Occam''s razor,'' penalising over{ exible and over{complex architectures. The Bayesian
  approach helps detect poor underlying assumptions in learning models. For learning
  models well{ matched to a problem, a good correlation between generalisation ability
  and the Bayesian evidence is obtained.'
ref_count: 39
references:
- pid: b10440620da8a43a1b97e3da4b1ff13746306475
  title: 'Consistent inference of probabilities in layered networks: predictions and
    generalizations'
- pid: 7abda1941534d3bb558dd959025d67f1df526303
  title: The Evidence Framework Applied to Classification Networks
- pid: 59fa47fc237a0781b4bf1c84fedb728d20db26a1
  title: 'Soft competitive adaptation: neural network learning algorithms based on
    fitting statistical mixtures'
- pid: 2a1e1da81b535e1bead3fc2ab6af8b07877823b9
  title: Exact Calculation of the Hessian Matrix for the Multilayer Perceptron
- pid: 82fa37d5be8e747131a5857992cc33bb95469ce3
  title: Developments in Maximum Entropy Data Analysis
- pid: 1718965f492d4e9fe1d98a3fb83efe671a4aed2c
  title: OPTIMAL PERCEPTUAL INFERENCE
- pid: 052b1d8ce63b07fec3de9dbb583772d860b7c769
  title: Learning representations by back-propagating errors
- pid: e3cd36c092abd65d6ac8e648f3468eeee90ee1fc
  title: Learning from hints in neural networks
- pid: 7f027c678076d7f2fd817f081079a334466449b1
  title: 'The Vapnik-Chervonenkis Dimension: Information versus Complexity in Learning'
- pid: 8592e46a5435d18bba70557846f47290b34c1aa5
  title: Learning and relearning in Boltzmann machines
- pid: 3e9229dd827dda0d462dffbdec7fdf50b724d587
  title: The Eigenvalues of Mega-dimensional Matrices
- pid: 749ce8ccd9453d1b34901143cddf5f9bee2977cf
  title: Learning representations by back-propagation errors, nature
slug: A-Practical-Bayesian-Framework-for-Backprop-Mackay
title: A Practical Bayesian Framework for Backprop Networks
url: https://www.semanticscholar.org/paper/A-Practical-Bayesian-Framework-for-Backprop-Mackay/b0f2433c088591d265891231f1c22424047f1bc1?sort=total-citations
venue: ''
year: 1991
