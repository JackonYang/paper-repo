authors:
- D. Erhan
- Aaron C. Courville
- Yoshua Bengio
- Pascal Vincent
badges:
- id: OPEN_ACCESS
corpusId: 15796526
fieldsOfStudy:
- Computer Science
numCitedBy: 1726
numCiting: 72
paperAbstract: 'Much recent research has been devoted to learning algorithms for deep
  architectures such as Deep Belief Networks and stacks of auto-encoder variants,
  with impressive results obtained in several areas, mostly on vision and language
  data sets. The best results obtained on supervised learning tasks involve an unsupervised
  learning component, usually in an unsupervised pre-training phase. Even though these
  new algorithms have enabled training deep models, many questions remain as to the
  nature of this difficult learning problem. The main question investigated here is
  the following: how does unsupervised pre-training work? Answering this questions
  is important if learning in deep architectures is to be further improved. We propose
  several explanatory hypotheses and test them through extensive simulations. We empirically
  show the influence of pre-training with respect to architecture depth, model capacity,
  and number of training examples. The experiments confirm and clarify the advantage
  of unsupervised pre-training. The results suggest that unsupervised pre-training
  guides the learning towards basins of attraction of minima that support better generalization
  from the training data set; the evidence from these results supports a regularization
  explanation for the effect of pre-training.'
ref_count: 72
references:
- pid: ccf415df5a83b343dae261286d29a40e8b80e6c6
  title: The Difficulty of Training Deep Architectures and the Effect of Unsupervised
    Pre-Training
- pid: 355d44f53428b1ac4fb2ab468d593c720640e5bd
  title: Greedy Layer-Wise Training of Deep Networks
- pid: 05fd1da7b2e34f86ec7f010bef068717ae964332
  title: Exploring Strategies for Training Deep Neural Networks
- pid: e60ff004dde5c13ec53087872cfcdd12e85beb57
  title: Learning Deep Architectures for AI
- pid: 41fef1a197fab9684a4608b725d3ae72e1ab4b39
  title: Sparse Feature Learning for Deep Belief Networks
- pid: 3137bc367c61c0e507a5e3c1f8caeb26f292d79f
  title: Measuring Invariances in Deep Networks
- pid: 8978cf7574ceb35f4c3096be768c7547b28a35d0
  title: A Fast Learning Algorithm for Deep Belief Nets
- pid: 843959ffdccf31c6694d135fad07425924f785b1
  title: Extracting and composing robust features with denoising autoencoders
- pid: 6fdb77260fc83dff91c44fea0f31a2cb8ed13d04
  title: Scaling learning algorithms towards AI
- pid: 1e80f755bcbf10479afd2338cec05211fdbd325c
  title: Convolutional deep belief networks for scalable unsupervised learning of
    hierarchical representations
- pid: 8de174ab5419b9d3127695405efd079808e956e8
  title: Curriculum learning
- pid: 65d994fb778a8d9e0f632659fb33a082949a50d3
  title: Visualizing Higher-Layer Features of a Deep Network
- pid: 202cbbf671743aefd380d2f23987bd46b9caaf97
  title: Sparse deep belief net model for visual area V2
- pid: b8012351bc5ebce4a4b3039bbbba3ce393bc3315
  title: An empirical evaluation of deep architectures on problems with many factors
    of variation
- pid: f5d565d307a746d8bc0feb52c873995af698deca
  title: Principled Hybrids of Generative and Discriminative Models
- pid: 5ee8a371fc5adc5469435020a52fb815f3b57a71
  title: Semi-Supervised Learning
- pid: a53da9916b87fa295837617c16ef2ca6462cafb8
  title: Classification using discriminative restricted Boltzmann machines
- pid: f2e95236f0fccc0b70e757ac2ebbc79b7f51de0a
  title: Using Deep Belief Nets to Learn Covariance Kernels for Gaussian Processes
- pid: b7d471970467a99bec4bce34c7dba5ef6745ad06
  title: The Curse of Highly Variable Functions for Local Kernel Machines
- pid: 7ee368e60d0b826e78f965aad8d6c7d406127104
  title: Deep learning via semi-supervised embedding
- pid: 932c2a02d462abd75af018125413b1ceaa1ee3f4
  title: Efficient Learning of Sparse Representations with an Energy-Based Model
- pid: 57458bc1cffe5caa45a885af986d70f723f406b4
  title: 'A unified architecture for natural language processing: deep neural networks
    with multitask learning'
- pid: 9360e5ce9c98166bb179ad479a9d2919ff13d022
  title: Training Products of Experts by Minimizing Contrastive Divergence
- pid: 80c330eee12decb84aaebcc85dc7ce414134ad61
  title: Modeling image patches with a directed hierarchy of Markov random fields
- pid: 46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e
  title: Reducing the Dimensionality of Data with Neural Networks
- pid: e2c477de72bb7718f5304c6f38457fda9c8334b1
  title: Deep learning from temporal coherence in video
- pid: 63f27307cb028f9341544fff32eceb2c3c652bf2
  title: Large-scale kernel machines
- pid: b543de6755fc1612b2fb449e0282727d0835d9cf
  title: Justifying and Generalizing Contrastive Divergence
- pid: e45c2420e6dc59ba6d357fb0c996ebf43c861560
  title: Exploiting Generative Models in Discriminative Classifiers
- pid: 162d958ff885f1462aeda91cd72582323fd6a1f4
  title: Gradient-based learning applied to document recognition
- pid: 51ff037291582df4c205d4a9cbe6e7dcec8f5973
  title: To recognize shapes, first learn to generate images.
- pid: f42b865e20e61a954239f421b42007236e671f19
  title: GradientBased Learning Applied to Document Recognition
- pid: 9b20ad513361a26e98289e5a517291c6ff49960d
  title: Learning Continuous Attractors in Recurrent Networks
- pid: 1626c940a64ad96a7ed53d7d6c0df63c6696956b
  title: Restricted Boltzmann machines for collaborative filtering
- pid: 90929a6aa901ba958eb4960aeeb594c752e08369
  title: 'On Discriminative vs. Generative Classifiers: A comparison of logistic regression
    and naive Bayes'
- pid: 1c46943103bd7b7a2c7be86859995a4144d1938b
  title: Visualizing Data using t-SNE
- pid: 3537fcd0ff99a3b3cb3d279012df826358420556
  title: A global geometric framework for nonlinear dimensionality reduction.
- pid: cd5af41a81e7fc9588dc74f3831fb14daf2f8e2a
  title: Semantic hashing
- pid: c50dca78e97e335d362d6b991ae0e1448914e9a3
  title: Reducing the Dimensionality of Data with Neural
- pid: 2184fb6d32bc46f252b940035029273563c4fc82
  title: Exponential Family Harmoniums with an Application to Information Retrieval
- pid: 6bbc0c752570c46a772f2982728f9ad4191f25dd
  title: Cluster Kernels for Semi-Supervised Learning
- pid: 9d16c547d15a08091e68c86a99731b14366e3f0d
  title: Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering
- pid: 56e6db25c6e72c0cff4ee099fa6e79bfbd20c7fb
  title: Almost optimal lower bounds for small depth circuits
- pid: b5f09ce0dd760857e0d0e4879f6e2543f04c5d33
  title: Maximum mutual information estimation of hidden Markov model parameters for
    speech recognition
- pid: 3653692a9d4d51909c9dd231d567bad928430654
  title: On the power of small-depth threshold circuits
- pid: 6f8fbd0873eb98519d7047c13251aef32e769dfe
  title: 'PhD thesis: Modeles connexionnistes de l''apprentissage (connectionist learning
    models)'
slug: Why-Does-Unsupervised-Pre-training-Help-Deep-Erhan-Courville
title: Why Does Unsupervised Pre-training Help Deep Learning?
url: https://www.semanticscholar.org/paper/Why-Does-Unsupervised-Pre-training-Help-Deep-Erhan-Courville/0d2336389dff3031910bd21dd1c44d1b4cd51725?sort=total-citations
venue: AISTATS
year: 2010
