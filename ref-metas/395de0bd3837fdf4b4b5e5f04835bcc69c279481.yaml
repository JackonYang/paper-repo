authors:
- M. Lewis
- Yinhan Liu
- Naman Goyal
- Marjan Ghazvininejad
- Abdelrahman Mohamed
- Omer Levy
- Veselin Stoyanov
- Luke Zettlemoyer
badges:
- id: OPEN_ACCESS
corpusId: 204960716
fieldsOfStudy:
- Computer Science
numCitedBy: 2422
numCiting: 34
paperAbstract: We present BART, a denoising autoencoder for pretraining sequence-to-sequence
  models. BART is trained by (1) corrupting text with an arbitrary noising function,
  and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based
  neural machine translation architecture which, despite its simplicity, can be seen
  as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right
  decoder), and other recent pretraining schemes. We evaluate a number of noising
  approaches, finding the best performance by both randomly shuffling the order of
  sentences and using a novel in-filling scheme, where spans of text are replaced
  with a single mask token. BART is particularly effective when fine tuned for text
  generation but also works well for comprehension tasks. It matches the performance
  of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range
  of abstractive dialogue, question answering, and summarization tasks, with gains
  of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation
  system for machine translation, with only target language pretraining. We also replicate
  other pretraining schemes within the BART framework, to understand their effect
  on end-task performance.
ref_count: 34
references:
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 599
  pid: 145b8b5d99a2beba6029418ca043585b90138d12
  title: 'MASS: Masked Sequence to Sequence Pre-training for Language Generation'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 33768
  pid: df2b0e26d0599ce3e70df8a9da02e51594e0e992
  title: 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 732
  pid: 1c71771c701aadfd72c5866170a9f5d71464bb88
  title: Unified Language Model Pre-training for Natural Language Understanding and
    Generation
  year: 2019
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 35164
  pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  title: Attention is All you Need
  year: 2017
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 3766
  pid: 3cfb319689f06bf04c2e28399361f414ca32c4b3
  title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer
  year: 2020
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 2464
  pid: 668db48c6a79826456341680ee1175dfc4cced71
  title: 'Get To The Point: Summarization with Pointer-Generator Networks'
  year: 2017
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 4228
  pid: e0c6abdbdecf04ffac65c440da77fb9d66bb474c
  title: 'XLNet: Generalized Autoregressive Pretraining for Language Understanding'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 6288
  pid: 9405cc0d6169988371b2755e573cc28650d14dfe
  title: Language Models are Unsupervised Multitask Learners
  year: 2019
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 3535
  pid: cd18800a0fe0b668a1cc19f2ec95b5003d0a5035
  title: Improving Language Understanding by Generative Pre-Training
  year: 2018
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 2708
  pid: 7a064df1aeada7e69e5173f7d4c8606f4470365b
  title: 'ALBERT: A Lite BERT for Self-supervised Learning of Language Representations'
  year: 2020
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1510
  pid: ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc
  title: Cross-lingual Language Model Pretraining
  year: 2019
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 879
  pid: 81f5810fbbab9b7203b9556f4ce3c741875407bc
  title: 'SpanBERT: Improving Pre-training by Representing and Predicting Spans'
  year: 2020
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 2636
  pid: 93b8da28d006415866bf48f9a6e06b5242129195
  title: 'GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language
    Understanding'
  year: 2018
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 7987
  pid: 3febb2bed8865945e7fddc99efd791887bb7e14f
  title: Deep Contextualized Word Representations
  year: 2018
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 2432
  pid: d1505c6123c102e53eb19dff312cb25cea840b72
  title: Teaching Machines to Read and Comprehend
  year: 2015
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 5367
  pid: 687bac2d3320083eb4530bf18bb8f8f721477600
  title: Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank
  year: 2013
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 7268
  pid: 077f8329a7b6fa3b7c877a57b81eb6c18b5f87de
  title: 'RoBERTa: A Robustly Optimized BERT Pretraining Approach'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 2037
  pid: 5ded2b8c64491b4a67f6d39ce473d4b9347a672e
  title: A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference
  year: 2018
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 4265
  pid: 05dd7254b632376973f3a1b4d39485da17814df5
  title: 'SQuAD: 100,000+ Questions for Machine Comprehension of Text'
  year: 2016
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 21886
  pid: 330da625c15427c6e42ccfa3b747fb29e5835bf0
  title: Efficient Estimation of Word Representations in Vector Space
  year: 2013
- fieldsOfStudy:
  - Computer Science
  - Linguistics
  numCitedBy: 546
  pid: cb0f3ee1e98faf92429d601cdcd76c69c1e484eb
  title: Neural Network Acceptability Judgments
  year: 2019
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 834
  pid: 475354f10798f110d34792b6d88f31d6d5cb099e
  title: Automatically Constructing a Corpus of Sentential Paraphrases
  year: 2005
- fieldsOfStudy:
  - Linguistics
  numCitedBy: 691
  pid: 128cb6b891aee1b5df099acb48e2efecfcff689f
  title: The Winograd Schema Challenge
  year: 2011
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 971
  pid: 15f4c35889ccc1ae258b680c2ca2fcbfe1e260f7
  title: Gaussian Error Linear Units (GELUs)
  year: 2016
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 209
  pid: e03d300581e16f6664157d2c1c6ceec33ec528ce
  title: Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object
    Classification, and Recognising Tectual Entailment
  year: 2006
slug: BART:-Denoising-Sequence-to-Sequence-Pre-training-Lewis-Liu
title: 'BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation,
  Translation, and Comprehension'
url: https://www.semanticscholar.org/paper/BART:-Denoising-Sequence-to-Sequence-Pre-training-Lewis-Liu/395de0bd3837fdf4b4b5e5f04835bcc69c279481?sort=total-citations
venue: ACL
year: 2020
