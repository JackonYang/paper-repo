authors:
- M. Lewis
- Yinhan Liu
- Naman Goyal
- Marjan Ghazvininejad
- Abdelrahman Mohamed
- Omer Levy
- Veselin Stoyanov
- Luke Zettlemoyer
badges:
- id: OPEN_ACCESS
corpusId: 204960716
fieldsOfStudy:
- Computer Science
numCitedBy: 2422
numCiting: 34
paperAbstract: We present BART, a denoising autoencoder for pretraining sequence-to-sequence
  models. BART is trained by (1) corrupting text with an arbitrary noising function,
  and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based
  neural machine translation architecture which, despite its simplicity, can be seen
  as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right
  decoder), and other recent pretraining schemes. We evaluate a number of noising
  approaches, finding the best performance by both randomly shuffling the order of
  sentences and using a novel in-filling scheme, where spans of text are replaced
  with a single mask token. BART is particularly effective when fine tuned for text
  generation but also works well for comprehension tasks. It matches the performance
  of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range
  of abstractive dialogue, question answering, and summarization tasks, with gains
  of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation
  system for machine translation, with only target language pretraining. We also replicate
  other pretraining schemes within the BART framework, to understand their effect
  on end-task performance.
ref_count: 34
references:
- pid: 145b8b5d99a2beba6029418ca043585b90138d12
  title: 'MASS: Masked Sequence to Sequence Pre-training for Language Generation'
- pid: df2b0e26d0599ce3e70df8a9da02e51594e0e992
  title: 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'
- pid: 1c71771c701aadfd72c5866170a9f5d71464bb88
  title: Unified Language Model Pre-training for Natural Language Understanding and
    Generation
- pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  title: Attention is All you Need
- pid: 3cfb319689f06bf04c2e28399361f414ca32c4b3
  title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer
- pid: 668db48c6a79826456341680ee1175dfc4cced71
  title: 'Get To The Point: Summarization with Pointer-Generator Networks'
- pid: e0c6abdbdecf04ffac65c440da77fb9d66bb474c
  title: 'XLNet: Generalized Autoregressive Pretraining for Language Understanding'
- pid: 9405cc0d6169988371b2755e573cc28650d14dfe
  title: Language Models are Unsupervised Multitask Learners
- pid: cd18800a0fe0b668a1cc19f2ec95b5003d0a5035
  title: Improving Language Understanding by Generative Pre-Training
- pid: 7a064df1aeada7e69e5173f7d4c8606f4470365b
  title: 'ALBERT: A Lite BERT for Self-supervised Learning of Language Representations'
- pid: ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc
  title: Cross-lingual Language Model Pretraining
- pid: 81f5810fbbab9b7203b9556f4ce3c741875407bc
  title: 'SpanBERT: Improving Pre-training by Representing and Predicting Spans'
- pid: 93b8da28d006415866bf48f9a6e06b5242129195
  title: 'GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language
    Understanding'
- pid: 3febb2bed8865945e7fddc99efd791887bb7e14f
  title: Deep Contextualized Word Representations
- pid: d1505c6123c102e53eb19dff312cb25cea840b72
  title: Teaching Machines to Read and Comprehend
- pid: 687bac2d3320083eb4530bf18bb8f8f721477600
  title: Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank
- pid: 077f8329a7b6fa3b7c877a57b81eb6c18b5f87de
  title: 'RoBERTa: A Robustly Optimized BERT Pretraining Approach'
- pid: 5ded2b8c64491b4a67f6d39ce473d4b9347a672e
  title: A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference
- pid: 05dd7254b632376973f3a1b4d39485da17814df5
  title: 'SQuAD: 100,000+ Questions for Machine Comprehension of Text'
- pid: 330da625c15427c6e42ccfa3b747fb29e5835bf0
  title: Efficient Estimation of Word Representations in Vector Space
- pid: cb0f3ee1e98faf92429d601cdcd76c69c1e484eb
  title: Neural Network Acceptability Judgments
- pid: 475354f10798f110d34792b6d88f31d6d5cb099e
  title: Automatically Constructing a Corpus of Sentential Paraphrases
- pid: 128cb6b891aee1b5df099acb48e2efecfcff689f
  title: The Winograd Schema Challenge
- pid: 15f4c35889ccc1ae258b680c2ca2fcbfe1e260f7
  title: Gaussian Error Linear Units (GELUs)
- pid: e03d300581e16f6664157d2c1c6ceec33ec528ce
  title: Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object
    Classification, and Recognising Tectual Entailment
slug: BART:-Denoising-Sequence-to-Sequence-Pre-training-Lewis-Liu
title: 'BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation,
  Translation, and Comprehension'
url: https://www.semanticscholar.org/paper/BART:-Denoising-Sequence-to-Sequence-Pre-training-Lewis-Liu/395de0bd3837fdf4b4b5e5f04835bcc69c279481?sort=total-citations
venue: ACL
year: 2020
