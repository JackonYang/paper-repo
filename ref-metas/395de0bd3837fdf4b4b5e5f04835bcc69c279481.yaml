authors:
- M. Lewis
- Yinhan Liu
- Naman Goyal
- Marjan Ghazvininejad
- Abdelrahman Mohamed
- Omer Levy
- Veselin Stoyanov
- Luke Zettlemoyer
badges:
- id: OPEN_ACCESS
corpusId: 204960716
fieldsOfStudy:
- Computer Science
meta_key: bart-denoising-sequence-to-sequence-pre-training-for-natural-language-generation-translation-and-comprehension
numCitedBy: 2422
numCiting: 34
paperAbstract: We present BART, a denoising autoencoder for pretraining sequence-to-sequence
  models. BART is trained by (1) corrupting text with an arbitrary noising function,
  and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based
  neural machine translation architecture which, despite its simplicity, can be seen
  as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right
  decoder), and other recent pretraining schemes. We evaluate a number of noising
  approaches, finding the best performance by both randomly shuffling the order of
  sentences and using a novel in-filling scheme, where spans of text are replaced
  with a single mask token. BART is particularly effective when fine tuned for text
  generation but also works well for comprehension tasks. It matches the performance
  of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range
  of abstractive dialogue, question answering, and summarization tasks, with gains
  of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation
  system for machine translation, with only target language pretraining. We also replicate
  other pretraining schemes within the BART framework, to understand their effect
  on end-task performance.
ref_count: 34
references:
- fieldsOfStudy:
  - Computer Science
  meta_key: mass-masked-sequence-to-sequence-pre-training-for-language-generation
  numCitedBy: 599
  pid: 145b8b5d99a2beba6029418ca043585b90138d12
  show_ref_link: true
  title: 'MASS: Masked Sequence to Sequence Pre-training for Language Generation'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding
  numCitedBy: 33768
  pid: df2b0e26d0599ce3e70df8a9da02e51594e0e992
  show_ref_link: true
  title: 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: unified-language-model-pre-training-for-natural-language-understanding-and-generation
  numCitedBy: 732
  pid: 1c71771c701aadfd72c5866170a9f5d71464bb88
  show_ref_link: true
  title: Unified Language Model Pre-training for Natural Language Understanding and
    Generation
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: text-summarization-with-pretrained-encoders
  numCitedBy: 642
  pid: 63748e59f4e106cbda6b65939b77589f40e48fcb
  show_ref_link: false
  title: Text Summarization with Pretrained Encoders
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: attention-is-all-you-need
  numCitedBy: 35164
  pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  show_ref_link: true
  title: Attention is All you Need
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: pre-trained-language-model-representations-for-language-generation
  numCitedBy: 89
  pid: b5aa927c906101b3f8854a29f374551e3ea64474
  show_ref_link: false
  title: Pre-trained language model representations for language generation
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: exploring-the-limits-of-transfer-learning-with-a-unified-text-to-text-transformer
  numCitedBy: 3766
  pid: 3cfb319689f06bf04c2e28399361f414ca32c4b3
  show_ref_link: true
  title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: get-to-the-point-summarization-with-pointer-generator-networks
  numCitedBy: 2464
  pid: 668db48c6a79826456341680ee1175dfc4cced71
  show_ref_link: true
  title: 'Get To The Point: Summarization with Pointer-Generator Networks'
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: xlnet-generalized-autoregressive-pretraining-for-language-understanding
  numCitedBy: 4228
  pid: e0c6abdbdecf04ffac65c440da77fb9d66bb474c
  show_ref_link: true
  title: 'XLNet: Generalized Autoregressive Pretraining for Language Understanding'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: language-models-are-unsupervised-multitask-learners
  numCitedBy: 6288
  pid: 9405cc0d6169988371b2755e573cc28650d14dfe
  show_ref_link: true
  title: Language Models are Unsupervised Multitask Learners
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: improving-language-understanding-by-generative-pre-training
  numCitedBy: 3535
  pid: cd18800a0fe0b668a1cc19f2ec95b5003d0a5035
  show_ref_link: true
  title: Improving Language Understanding by Generative Pre-Training
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: leveraging-pre-trained-checkpoints-for-sequence-generation-tasks
  numCitedBy: 187
  pid: 947471a42f81f2249b3bc0cb2a8f2ab69e888c9c
  show_ref_link: false
  title: Leveraging Pre-trained Checkpoints for Sequence Generation Tasks
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: albert-a-lite-bert-for-self-supervised-learning-of-language-representations
  numCitedBy: 2708
  pid: 7a064df1aeada7e69e5173f7d4c8606f4470365b
  show_ref_link: true
  title: 'ALBERT: A Lite BERT for Self-supervised Learning of Language Representations'
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: cross-lingual-language-model-pretraining
  numCitedBy: 1510
  pid: ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc
  show_ref_link: true
  title: Cross-lingual Language Model Pretraining
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: spanbert-improving-pre-training-by-representing-and-predicting-spans
  numCitedBy: 879
  pid: 81f5810fbbab9b7203b9556f4ce3c741875407bc
  show_ref_link: true
  title: 'SpanBERT: Improving Pre-training by Representing and Predicting Spans'
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: edinburgh-neural-machine-translation-systems-for-wmt-16
  numCitedBy: 449
  pid: 1a5ea605111eb3403868d4b679315e944beee8c6
  show_ref_link: false
  title: Edinburgh Neural Machine Translation Systems for WMT 16
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: glue-a-multi-task-benchmark-and-analysis-platform-for-natural-language-understanding
  numCitedBy: 2636
  pid: 93b8da28d006415866bf48f9a6e06b5242129195
  show_ref_link: true
  title: 'GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language
    Understanding'
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: deep-contextualized-word-representations
  numCitedBy: 7987
  pid: 3febb2bed8865945e7fddc99efd791887bb7e14f
  show_ref_link: true
  title: Deep Contextualized Word Representations
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: teaching-machines-to-read-and-comprehend
  numCitedBy: 2432
  pid: d1505c6123c102e53eb19dff312cb25cea840b72
  show_ref_link: true
  title: Teaching Machines to Read and Comprehend
  year: 2015
- fieldsOfStudy:
  - Computer Science
  meta_key: recursive-deep-models-for-semantic-compositionality-over-a-sentiment-treebank
  numCitedBy: 5367
  pid: 687bac2d3320083eb4530bf18bb8f8f721477600
  show_ref_link: true
  title: Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank
  year: 2013
- fieldsOfStudy:
  - Computer Science
  meta_key: roberta-a-robustly-optimized-bert-pretraining-approach
  numCitedBy: 7268
  pid: 077f8329a7b6fa3b7c877a57b81eb6c18b5f87de
  show_ref_link: true
  title: 'RoBERTa: A Robustly Optimized BERT Pretraining Approach'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: regularizing-neural-networks-by-penalizing-confident-output-distributions
  numCitedBy: 704
  pid: 6ce1922802169f757bbafc6e087cc274a867c763
  show_ref_link: false
  title: Regularizing Neural Networks by Penalizing Confident Output Distributions
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: the-second-conversational-intelligence-challenge-convai2
  numCitedBy: 199
  pid: 9ae17b09c59f06f02ef824b856a440de663471d0
  show_ref_link: false
  title: The Second Conversational Intelligence Challenge (ConvAI2)
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: a-broad-coverage-challenge-corpus-for-sentence-understanding-through-inference
  numCitedBy: 2037
  pid: 5ded2b8c64491b4a67f6d39ce473d4b9347a672e
  show_ref_link: true
  title: A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: don-t-give-me-the-details-just-the-summary-topic-aware-convolutional-neural-networks-for-extreme-summarization
  numCitedBy: 484
  pid: 305b2cf37e5dece81e95c92883d5a6e28ac93b22
  show_ref_link: false
  title: Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural
    Networks for Extreme Summarization
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: eli5-long-form-question-answering
  numCitedBy: 129
  pid: ebf59587f8f170ff4241c42263bbfb9da5bd2135
  show_ref_link: false
  title: 'ELI5: Long Form Question Answering'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: squad-100-000-questions-for-machine-comprehension-of-text
  numCitedBy: 4265
  pid: 05dd7254b632376973f3a1b4d39485da17814df5
  show_ref_link: true
  title: 'SQuAD: 100,000+ Questions for Machine Comprehension of Text'
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: controllable-abstractive-summarization
  numCitedBy: 194
  pid: 9b4a861151fabae1dfd61c917d031c86d26be704
  show_ref_link: false
  title: Controllable Abstractive Summarization
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: efficient-estimation-of-word-representations-in-vector-space
  numCitedBy: 21886
  pid: 330da625c15427c6e42ccfa3b747fb29e5835bf0
  show_ref_link: true
  title: Efficient Estimation of Word Representations in Vector Space
  year: 2013
- fieldsOfStudy:
  - Computer Science
  - Linguistics
  meta_key: neural-network-acceptability-judgments
  numCitedBy: 546
  pid: cb0f3ee1e98faf92429d601cdcd76c69c1e484eb
  show_ref_link: true
  title: Neural Network Acceptability Judgments
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: automatically-constructing-a-corpus-of-sentential-paraphrases
  numCitedBy: 834
  pid: 475354f10798f110d34792b6d88f31d6d5cb099e
  show_ref_link: false
  title: Automatically Constructing a Corpus of Sentential Paraphrases
  year: 2005
- fieldsOfStudy:
  - Linguistics
  meta_key: the-winograd-schema-challenge
  numCitedBy: 691
  pid: 128cb6b891aee1b5df099acb48e2efecfcff689f
  show_ref_link: false
  title: The Winograd Schema Challenge
  year: 2011
- fieldsOfStudy:
  - Computer Science
  meta_key: gaussian-error-linear-units-gelus
  numCitedBy: 971
  pid: 15f4c35889ccc1ae258b680c2ca2fcbfe1e260f7
  show_ref_link: true
  title: Gaussian Error Linear Units (GELUs)
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: machine-learning-challenges-evaluating-predictive-uncertainty-visual-object-classification-and-recognising-tectual-entailment
  numCitedBy: 209
  pid: e03d300581e16f6664157d2c1c6ceec33ec528ce
  show_ref_link: false
  title: Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object
    Classification, and Recognising Tectual Entailment
  year: 2006
slug: BART:-Denoising-Sequence-to-Sequence-Pre-training-Lewis-Liu
title: 'BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation,
  Translation, and Comprehension'
url: https://www.semanticscholar.org/paper/BART:-Denoising-Sequence-to-Sequence-Pre-training-Lewis-Liu/395de0bd3837fdf4b4b5e5f04835bcc69c279481?sort=total-citations
venue: ACL
year: 2020
