authors:
- B. Hassibi
- D. Stork
badges:
- id: OPEN_ACCESS
corpusId: 7057040
fieldsOfStudy:
- Computer Science
numCitedBy: 1586
numCiting: 21
paperAbstract: We investigate the use of information from all second order derivatives
  of the error function to perform network pruning (i.e., removing unimportant weights
  from a trained network) in order to improve generalization, simplify networks, reduce
  hardware or storage requirements, increase the speed of further training, and in
  some cases enable rule extraction. Our method, Optimal Brain Surgeon (OBS), is Significantly
  better than magnitude-based methods and Optimal Brain Damage [Le Cun, Denker and
  Solla, 1990], which often remove the wrong weights. OBS permits the pruning of more
  weights than other methods (for the same error on the training set), and thus yields
  better generalization on test data. Crucial to OBS is a recursion relation for calculating
  the inverse Hessian matrix H-1 from training data and structural information of
  the net. OBS permits a 90%, a 76%, and a 62% reduction in weights over backpropagation
  with weight decay on three benchmark MONK's problems [Thrun et al., 1991]. Of OBS,
  Optimal Brain Damage, and magnitude-based methods, only OBS deletes the correct
  weights from a trained XOR network in every case. Finally, whereas Sejnowski and
  Rosenberg [1987] used 18,000 weights in their NETtalk network, we used OBS to prune
  a network to just 1560 weights, yielding better generalization.
ref_count: 21
references:
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 3493
  pid: e7297db245c3feb1897720b173a59fe7e36babb7
  title: Optimal Brain Damage
  year: 1989
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1885
  pid: de996c32045df6f7b404dda2a753b6a9becf3c08
  title: Parallel Networks that Learn to Pronounce English Text
  year: 1987
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 6517
  pid: 6c0cbbd275bb43e09f0527a31ddd61824eca295b
  title: Introduction to the theory of neural computation
  year: 1991
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 6261
  pid: d382b9c11e5c6a8e173fbeb442545e3be8d3e3a5
  title: Modeling By Shortest Data Description*
  year: 1978
- fieldsOfStudy:
  - Biology
  numCitedBy: 19355
  pid: 111fd833a4ae576cfdbb27d87d2f8fc0640af355
  title: Learning internal representations by error propagation
  year: 1986
slug: Second-Order-Derivatives-for-Network-Pruning:-Brain-Hassibi-Stork
title: 'Second Order Derivatives for Network Pruning: Optimal Brain Surgeon'
url: https://www.semanticscholar.org/paper/Second-Order-Derivatives-for-Network-Pruning:-Brain-Hassibi-Stork/a42954d4b9d0ccdf1036e0af46d87a01b94c3516?sort=total-citations
venue: NIPS
year: 1992
