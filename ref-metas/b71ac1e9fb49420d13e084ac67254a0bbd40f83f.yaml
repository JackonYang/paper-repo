authors:
- Xavier Glorot
- Yoshua Bengio
badges:
- id: OPEN_ACCESS
corpusId: 5575601
fieldsOfStudy:
- Computer Science
meta_key: understanding-the-difficulty-of-training-deep-feedforward-neural-networks
numCitedBy: 12433
numCiting: 26
paperAbstract: "Whereas before 2006 it appears that deep multilayer neural networks\
  \ were not successfully trained, since then several algorithms have been shown to\
  \ successfully train them, with experimental results showing the superiority of\
  \ deeper vs less deep architectures. All these experimental results were obtained\
  \ with new initialization or training mechanisms. Our objective here is to understand\
  \ better why standard gradient descent from random initialization is doing so poorly\
  \ with deep neural networks, to better understand these recent relative successes\
  \ and help design better algorithms in the future. We first observe the influence\
  \ of the non-linear activations functions. We find that the logistic sigmoid activation\
  \ is unsuited for deep networks with random initialization because of its mean value,\
  \ which can drive especially the top hidden layer into saturation. Surprisingly,\
  \ we find that saturated units can move out of saturation by themselves, albeit\
  \ slowly, and explaining the plateaus sometimes seen when training neural networks.\
  \ We find that a new non-linearity that saturates less can often be beneficial.\
  \ Finally, we study how activations and gradients vary across layers and during\
  \ training, with the idea that training may be more difficult when the singular\
  \ values of the Jacobian associated with each layer are far from 1. Based on these\
  \ considerations, we propose a new initialization scheme that brings substantially\
  \ faster convergence. 1 Deep Neural Networks Deep learning methods aim at learning\
  \ feature hierarchies with features from higher levels of the hierarchy formed by\
  \ the composition of lower level features. They include Appearing in Proceedings\
  \ of the 13 International Conference on Artificial Intelligence and Statistics (AISTATS)\
  \ 2010, Chia Laguna Resort, Sardinia, Italy. Volume 9 of JMLR: WC Weston et al.,\
  \ 2008). Much attention has recently been devoted to them (see (Bengio, 2009) for\
  \ a review), because of their theoretical appeal, inspiration from biology and human\
  \ cognition, and because of empirical success in vision (Ranzato et al., 2007; Larochelle\
  \ et al., 2007; Vincent et al., 2008) and natural language processing (NLP) (Collobert\
  \ & Weston, 2008; Mnih & Hinton, 2009). Theoretical results reviewed and discussed\
  \ by Bengio (2009), suggest that in order to learn the kind of complicated functions\
  \ that can represent high-level abstractions (e.g. in vision, language, and other\
  \ AI-level tasks), one may need deep architectures. Most of the recent experimental\
  \ results with deep architecture are obtained with models that can be turned into\
  \ deep supervised neural networks, but with initialization or training schemes different\
  \ from the classical feedforward neural networks (Rumelhart et al., 1986). Why are\
  \ these new algorithms working so much better than the standard random initialization\
  \ and gradient-based optimization of a supervised training criterion? Part of the\
  \ answer may be found in recent analyses of the effect of unsupervised pretraining\
  \ (Erhan et al., 2009), showing that it acts as a regularizer that initializes the\
  \ parameters in a \u201Cbetter\u201D basin of attraction of the optimization procedure,\
  \ corresponding to an apparent local minimum associated with better generalization.\
  \ But earlier work (Bengio et al., 2007) had shown that even a purely supervised\
  \ but greedy layer-wise procedure would give better results. So here instead of\
  \ focusing on what unsupervised pre-training or semi-supervised criteria bring to\
  \ deep architectures, we focus on analyzing what may be going wrong with good old\
  \ (but deep) multilayer neural networks. Our analysis is driven by investigative\
  \ experiments to monitor activations (watching for saturation of hidden units) and\
  \ gradients, across layers and across training iterations. We also evaluate the\
  \ effects on these of choices of activation function (with the idea that it might\
  \ affect saturation) and initialization procedure (since unsupervised pretraining\
  \ is a particular form of initialization and it has a drastic impact)."
ref_count: 26
references:
- fieldsOfStudy:
  - Computer Science
  meta_key: exploring-strategies-for-training-deep-neural-networks
  numCitedBy: 1027
  pid: 05fd1da7b2e34f86ec7f010bef068717ae964332
  show_ref_link: false
  title: Exploring Strategies for Training Deep Neural Networks
  year: 2009
- fieldsOfStudy:
  - Computer Science
  meta_key: greedy-layer-wise-training-of-deep-networks
  numCitedBy: 3427
  pid: 355d44f53428b1ac4fb2ab468d593c720640e5bd
  show_ref_link: true
  title: Greedy Layer-Wise Training of Deep Networks
  year: 2006
- fieldsOfStudy:
  - Computer Science
  meta_key: the-difficulty-of-training-deep-architectures-and-the-effect-of-unsupervised-pre-training
  numCitedBy: 395
  pid: ccf415df5a83b343dae261286d29a40e8b80e6c6
  show_ref_link: false
  title: The Difficulty of Training Deep Architectures and the Effect of Unsupervised
    Pre-Training
  year: 2009
- fieldsOfStudy:
  - Computer Science
  meta_key: a-fast-learning-algorithm-for-deep-belief-nets
  numCitedBy: 13407
  pid: 8978cf7574ceb35f4c3096be768c7547b28a35d0
  show_ref_link: true
  title: A Fast Learning Algorithm for Deep Belief Nets
  year: 2006
- fieldsOfStudy:
  - Computer Science
  meta_key: learning-multiple-layers-of-features-from-tiny-images
  numCitedBy: 17080
  pid: 5d90f06bb70a0a3dced62413346235c02b1aa086
  show_ref_link: true
  title: Learning Multiple Layers of Features from Tiny Images
  year: 2009
- fieldsOfStudy:
  - Computer Science
  meta_key: learning-deep-architectures-for-ai
  numCitedBy: 7558
  pid: e60ff004dde5c13ec53087872cfcdd12e85beb57
  show_ref_link: true
  title: Learning Deep Architectures for AI
  year: 2007
- fieldsOfStudy:
  - Computer Science
  meta_key: extracting-and-composing-robust-features-with-denoising-autoencoders
  numCitedBy: 5467
  pid: 843959ffdccf31c6694d135fad07425924f785b1
  show_ref_link: true
  title: Extracting and composing robust features with denoising autoencoders
  year: 2008
- fieldsOfStudy:
  - Computer Science
  meta_key: an-empirical-evaluation-of-deep-architectures-on-problems-with-many-factors-of-variation
  numCitedBy: 973
  pid: b8012351bc5ebce4a4b3039bbbba3ce393bc3315
  show_ref_link: false
  title: An empirical evaluation of deep architectures on problems with many factors
    of variation
  year: 2007
- fieldsOfStudy:
  - Computer Science
  meta_key: learning-long-term-dependencies-with-gradient-descent-is-difficult
  numCitedBy: 6142
  pid: d0be39ee052d246ae99c082a565aba25b811be2d
  show_ref_link: false
  title: Learning long-term dependencies with gradient descent is difficult
  year: 1994
- fieldsOfStudy:
  - Computer Science
  meta_key: learning-representations-by-back-propagating-errors
  numCitedBy: 20330
  pid: 052b1d8ce63b07fec3de9dbb583772d860b7c769
  show_ref_link: false
  title: Learning representations by back-propagating errors
  year: 1986
- fieldsOfStudy:
  - Computer Science
  meta_key: efficient-learning-of-sparse-representations-with-an-energy-based-model
  numCitedBy: 1182
  pid: 932c2a02d462abd75af018125413b1ceaa1ee3f4
  show_ref_link: false
  title: Efficient Learning of Sparse Representations with an Energy-Based Model
  year: 2006
- fieldsOfStudy:
  - Computer Science
  meta_key: gradient-based-learning-applied-to-document-recognition
  numCitedBy: 35241
  pid: 162d958ff885f1462aeda91cd72582323fd6a1f4
  show_ref_link: true
  title: Gradient-based learning applied to document recognition
  year: 1998
- fieldsOfStudy:
  - Computer Science
  meta_key: accelerated-learning-in-layered-neural-networks
  numCitedBy: 246
  pid: 247698d0a716f0d99c0645050d049525e0b08ec2
  show_ref_link: false
  title: Accelerated Learning in Layered Neural Networks
  year: 1988
- fieldsOfStudy:
  - Computer Science
  meta_key: a-unified-architecture-for-natural-language-processing-deep-neural-networks-with-multitask-learning
  numCitedBy: 5023
  pid: 57458bc1cffe5caa45a885af986d70f723f406b4
  show_ref_link: true
  title: A unified architecture for natural language processing - deep neural networks
    with multitask learning
  year: 2008
- fieldsOfStudy:
  - Computer Science
  meta_key: a-scalable-hierarchical-distributed-language-model
  numCitedBy: 938
  pid: a9fc84f8abe740cdc7ee82e69444d1d00dbe0ceb
  show_ref_link: false
  title: A Scalable Hierarchical Distributed Language Model
  year: 2008
- fieldsOfStudy:
  - Computer Science
  meta_key: unsupervised-learning-of-probabilistic-grammar-markov-models-for-object-categories
  numCitedBy: 74
  pid: 8dccc6dc3c69d4078c3c60bf4d6a25798467f4a1
  show_ref_link: false
  title: Unsupervised Learning of Probabilistic Grammar-Markov Models for Object Categories
  year: 2009
- fieldsOfStudy:
  - Computer Science
  meta_key: deep-learning-via-semi-supervised-embedding
  numCitedBy: 607
  pid: 7ee368e60d0b826e78f965aad8d6c7d406127104
  show_ref_link: false
  title: Deep learning via semi-supervised embedding
  year: 2008
- fieldsOfStudy:
  - Computer Science
  meta_key: learning-in-modular-systems
  numCitedBy: 32
  pid: bfa588775eaebcaa94bdcb488393accc8347f039
  show_ref_link: false
  title: Learning in modular systems
  year: 2010
- fieldsOfStudy:
  - Computer Science
  meta_key: efficient-backprop
  numCitedBy: 2630
  pid: b87274e6d9aa4e6ba5148898aa92941617d2b6ed
  show_ref_link: true
  title: Efficient BackProp
  year: 2012
slug: Understanding-the-difficulty-of-training-deep-Glorot-Bengio
title: Understanding the difficulty of training deep feedforward neural networks
url: https://www.semanticscholar.org/paper/Understanding-the-difficulty-of-training-deep-Glorot-Bengio/b71ac1e9fb49420d13e084ac67254a0bbd40f83f?sort=total-citations
venue: AISTATS
year: 2010
