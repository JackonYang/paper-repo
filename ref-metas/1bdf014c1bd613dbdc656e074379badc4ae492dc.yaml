authors:
- Yann Dauphin
- Harm de Vries
- Junyoung Chung
- Yoshua Bengio
badges: []
corpusId: 209841442
fieldsOfStudy:
- Computer Science
numCitedBy: 234
numCiting: 18
paperAbstract: Parameter-specific adaptive learning rate methods are computationally
  efficient ways to reduce the ill-conditioning problems encountered when training
  large deep networks. Following recent work that strongly suggests that most of the
  critical points encountered when training such networks are saddle points, we find
  how considering the presence of negative eigenvalues of the Hessian could help us
  design better suited adaptive learning rate schemes, i.e., diagonal preconditioners.
  We show that the optimal preconditioner is based on taking the absolute value of
  the Hessian's eigenvalues, which is not what Newton and classical preconditioners
  like Jacobi's do. In this paper, we propose a novel adaptive learning rate scheme
  based on the equilibration preconditioner and show that RMSProp approximates it,
  which may explain some of its success in the presence of saddle points. Whereas
  RMSProp is a biased estimator of the equilibration preconditioner, the proposed
  stochastic estimator, ESGD, is unbiased and only adds a small percentage to computing
  time. We find that both schemes yield very similar step directions but that ESGD
  sometimes surpasses RMSProp in terms of convergence speed, always clearly improving
  over plain stochastic gradient descent.
ref_count: 18
references:
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 8025
  pid: 413c1142de9d91804d6d11c67ff3fed59c9fc279
  title: Adaptive Subgradient Methods for Online Learning and Stochastic Optimization
  year: 2010
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 119
  pid: a98483785378bde7e2384a3035b2b501ee03654b
  title: Krylov Subspace Descent for Deep Learning
  year: 2012
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1075
  pid: 981ce6b655cc06416ff6bf7fac8c6c2076fd7fac
  title: Identifying and attacking the saddle point problem in high-dimensional non-convex
    optimization
  year: 2014
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 3557
  pid: aa7bfd2304201afbb19971ebde87b17e40242e91
  title: On the importance of initialization and momentum in deep learning
  year: 2013
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 280
  pid: e8f95ccfd13689f672c39dca3eccf1c484533bcc
  title: Revisiting Natural Gradient for Deep Networks
  year: 2014
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 844
  pid: 4c46347fbc272b21468efe3d9af34b4b2bad6684
  title: Deep learning via Hessian-free optimization
  year: 2010
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 5464
  pid: 8729441d734782c3ed532a7d2d9611b438c0a09a
  title: 'ADADELTA: An Adaptive Learning Rate Method'
  year: 2012
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 275
  pid: ffa94bba647817fa5e8f8d3250fc977435b5ca76
  title: Fast Curvature Matrix-Vector Products for Second-Order Gradient Descent
  year: 2002
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1374
  pid: 855d0f722d75cc56a66a00ede18ace96bafee6bd
  title: 'Theano: new features and speed improvements'
  year: 2012
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 2630
  pid: b87274e6d9aa4e6ba5148898aa92941617d2b6ed
  title: Efficient BackProp
  year: 2012
slug: RMSProp-and-equilibrated-adaptive-learning-rates-Dauphin-Vries
title: RMSProp and equilibrated adaptive learning rates for non-convex optimization.
url: https://www.semanticscholar.org/paper/RMSProp-and-equilibrated-adaptive-learning-rates-Dauphin-Vries/1bdf014c1bd613dbdc656e074379badc4ae492dc?sort=total-citations
venue: ''
year: 2015
