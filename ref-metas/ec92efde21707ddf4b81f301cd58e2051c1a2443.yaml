authors:
- Sida I. Wang
- Christopher D. Manning
badges:
- id: OPEN_ACCESS
corpusId: 10357959
fieldsOfStudy:
- Computer Science
numCitedBy: 377
numCiting: 19
paperAbstract: Preventing feature co-adaptation by encouraging independent contributions
  from different features often improves classification and regression performance.
  Dropout training (Hinton et al., 2012) does this by randomly dropping out (zeroing)
  hidden units and input features during training of neural networks. However, repeatedly
  sampling a random subset of input features makes training much slower. Based on
  an examination of the implied objective function of dropout training, we show how
  to do fast dropout training by sampling from or integrating a Gaussian approximation,
  instead of doing Monte Carlo optimization of this objective. This approximation,
  justified by the central limit theorem and empirical evidence, gives an order of
  magnitude speedup and more stability. We show how to do fast dropout training for
  classification, regression, and multilayer neural networks. Beyond dropout, our
  technique is extended to integrate out other types of noise and small image transformations.
ref_count: 19
references:
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 6191
  pid: 1366de5bb112746a555e9c0cd00de3ad8628aea8
  title: Improving neural networks by preventing co-adaptation of feature detectors
  year: 2012
- fieldsOfStudy:
  - Mathematics
  numCitedBy: 993
  pid: c3ecd8e19e016d15670c8953b4b9afaa5186b0f3
  title: Training with Noise is Equivalent to Tikhonov Regularization
  year: 1995
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1822
  pid: b7b915d508987b73b61eccd2b237e7ed099a2d29
  title: Maxout Networks
  year: 2013
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 80961
  pid: abd1c342495432171beb7ca8fd9551ef13cbd0ff
  title: ImageNet classification with deep convolutional neural networks
  year: 2012
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 2230
  pid: 90929a6aa901ba958eb4960aeeb594c752e08369
  title: 'On Discriminative vs. Generative Classifiers: A comparison of logistic regression
    and naive Bayes'
  year: 2001
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 385
  pid: 153f64ab7c1c24b1b136d8da2f36c6333b8dbfdd
  title: Transformation Invariance in Pattern Recognition-Tangent Distance and Tangent
    Propagation
  year: 1996
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 792
  pid: 7abda1941534d3bb558dd959025d67f1df526303
  title: The Evidence Framework Applied to Classification Networks
  year: 1992
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1245
  pid: cfa2646776405d50533055ceb1b7f050e9014dcb
  title: Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions
  year: 2011
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1090
  pid: dc0975ae518a5b30e60fde23a41c74bafd7c6f8c
  title: 'Baselines and Bigrams: Simple, Good Sentiment and Topic Classification'
  year: 2012
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 3009
  pid: 649d03490ef72c5274e3bccd03d7a299d2f8da91
  title: Learning Word Vectors for Sentiment Analysis
  year: 2011
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 362
  pid: da5cd00115f7ec108de8eebf071c5f3f19807df4
  title: Dependency Tree-based Sentiment Classification using CRFs with Hidden Variables
  year: 2010
slug: Fast-dropout-training-Wang-Manning
title: Fast dropout training
url: https://www.semanticscholar.org/paper/Fast-dropout-training-Wang-Manning/ec92efde21707ddf4b81f301cd58e2051c1a2443?sort=total-citations
venue: ICML
year: 2013
