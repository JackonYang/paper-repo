authors:
- H. Schwenk
badges:
- id: OPEN_ACCESS
corpusId: 16930073
fieldsOfStudy:
- Computer Science
numCitedBy: 38
numCiting: 20
paperAbstract: Recently there has been increasing interest in using neural networks
  for language modeling. In contrast to the well-known backoff n-gram language models,
  the neural network approach tries to limit the data sparseness problem by performing
  the estimation in a continuous space, allowing by this means smooth interpolations.
  The complexity to train such a model and to calculate one n-gram probability is
  however several orders of magnitude higher than for the backoff models, making the
  new approach difficult to use in real applications. In this paper several techniques
  are presented that allow the use of a neural network language model in a large vocabulary
  speech recognition system, in particular very, fast lattice rescoring and efficient
  training of large neural networks on training corpora of over 10 million words.
  The described approach achieves significant word error reductions with respect to
  a carefully tuned 4-gram backoff language model in a state of the art conversational
  speech recognizer for the DARPA rich transcriptions evaluations.
ref_count: 20
references:
- pid: e41498c05d4c68e4750fb84a380317a112d97b01
  title: Connectionist language modeling for large vocabulary continuous speech recognition
- pid: 6c2b28f9354f667cd5bd07afc0471d8334430da7
  title: A Neural Probabilistic Language Model
- pid: 3d6036af971c1f11ab712cc41487376a94e63673
  title: Using a connectionist model in a syntactical based language model
- pid: 399da68d3b97218b6c80262df7963baa89dcc71b
  title: SRILM - an extensible language modeling toolkit
- pid: d4e8bed3b50a035e1eabad614fe4218a34b3b178
  title: An empirical study of smoothing techniques for language modeling
- pid: 6b388f0151ab37adb3d57738b8f52a3f943f86c8
  title: Quick Training of Probabilistic Neural Nets by Importance Sampling
slug: Efficient-training-of-large-neural-networks-for-Schwenk
title: Efficient training of large neural networks for language modeling
url: https://www.semanticscholar.org/paper/Efficient-training-of-large-neural-networks-for-Schwenk/d6fb7546a29320eadad868af66835059db93d99f?sort=total-citations
venue: 2004 IEEE International Joint Conference on Neural Networks (IEEE Cat. No.04CH37541)
year: 2004
