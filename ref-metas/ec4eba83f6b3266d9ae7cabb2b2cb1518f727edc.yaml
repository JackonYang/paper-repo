authors:
- Guillaume Lample
- A. Conneau
badges:
- id: OPEN_ACCESS
corpusId: 58981712
fieldsOfStudy:
- Computer Science
numCitedBy: 1509
numCiting: 52
paperAbstract: "Recent studies have demonstrated the efficiency of generative pretraining\
  \ for English natural language understanding. In this work, we extend this approach\
  \ to multiple languages and show the effectiveness of cross-lingual pretraining.\
  \ We propose two methods to learn cross-lingual language models (XLMs): one unsupervised\
  \ that only relies on monolingual data, and one supervised that leverages parallel\
  \ data with a new cross-lingual language model objective. We obtain state-of-the-art\
  \ results on cross-lingual classification, unsupervised and supervised machine translation.\
  \ On XNLI, our approach pushes the state of the art by an absolute gain of 4.9%\
  \ accuracy. On unsupervised machine translation, we obtain 34.3 BLEU on WMT\u2019\
  16 German-English, improving the previous state of the art by more than 9 BLEU.\
  \ On supervised machine translation, we obtain a new state of the art of 38.5 BLEU\
  \ on WMT\u201916 Romanian-English, outperforming the previous best approach by more\
  \ than 4 BLEU. Our code and pretrained models will be made publicly available."
ref_count: 52
references:
- pid: 85f94d8098322f8130512b4c6c4627548ce4a6cc
  title: Unsupervised Pretraining for Sequence to Sequence Learning
- pid: cd18800a0fe0b668a1cc19f2ec95b5003d0a5035
  title: Improving Language Understanding by Generative Pre-Training
- pid: 1af68821518f03568f913ab03fc02080247a27ff
  title: Neural Machine Translation of Rare Words with Subword Units
- pid: 93b8da28d006415866bf48f9a6e06b5242129195
  title: 'GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language
    Understanding'
- pid: 5ded2b8c64491b4a67f6d39ce473d4b9347a672e
  title: A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference
- pid: 1e077413b25c4d34945cc2707e17e46ed4fe784a
  title: Universal Language Model Fine-tuning for Text Classification
- pid: df2b0e26d0599ce3e70df8a9da02e51594e0e992
  title: 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'
- pid: 2f2d8f8072e5cc9b296fad551f65f183bdbff7aa
  title: Exploring the Limits of Language Modeling
- pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  title: Attention is All you Need
- pid: 687bac2d3320083eb4530bf18bb8f8f721477600
  title: Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank
- pid: f04df4e20a18358ea2f689b4c129781628ef7fc1
  title: A large annotated corpus for learning natural language inference
- pid: e2dba792360873aef125572812f3673b1a85d850
  title: Enriching Word Vectors with Subword Information
- pid: 9819b600a828a57e1cde047bbe710d3446b30da5
  title: Recurrent neural network based language model
- pid: b9de9599d7241459db9213b5cdd7059696f5ef8d
  title: Character-Level Language Modeling with Deeper Self-Attention
- pid: 7113bd87c3e6f727efae24ee52f20c81358da761
  title: 'SentEval: An Evaluation Toolkit for Universal Sentence Representations'
- pid: 87f40e6f3022adbc1f1905e3e506abad05a9964f
  title: Distributed Representations of Words and Phrases and their Compositionality
- pid: 4ee2eab4c298c1824a9fb8799ad8eed21be38d21
  title: 'Moses: Open Source Toolkit for Statistical Machine Translation'
- pid: a6cb366736791bcccc5c8639de5a8f9636bf87e8
  title: 'Adam: A Method for Stochastic Optimization'
- pid: 44d2abe2175df8153f465f6c39b68b76a0d40ab9
  title: Long Short-Term Memory
- pid: b36a5bb1707bb9c70025294b3a310138aae8327a
  title: Automatic differentiation in PyTorch
- pid: 1a3d22599028a05669e884f3eaf19a342e190a87
  title: 'Backpropagation Through Time: What It Does and How to Do It'
- pid: 4361e64f2d12d63476fdc88faf72a0f70d9a2ffb
  title: Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear
    Units
- pid: 766ce989b8b8b984f7a4691fd8c9af4bdb2b74cd
  title: "\u201CCloze Procedure\u201D: A New Tool for Measuring Readability"
- pid: 99e8d34817ae10d7304521e89c5fbf908b9d856b
  title: 'Open Source Toolkit for Statistical Machine Translation: Factored Translation
    Models and Lattice Decoding'
slug: Cross-lingual-Language-Model-Pretraining-Lample-Conneau
title: Cross-lingual Language Model Pretraining
url: https://www.semanticscholar.org/paper/Cross-lingual-Language-Model-Pretraining-Lample-Conneau/ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc?sort=total-citations
venue: NeurIPS
year: 2019
