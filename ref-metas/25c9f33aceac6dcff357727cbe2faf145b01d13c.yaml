authors:
- Geoffrey E. Hinton
- D. Camp
badges:
- id: OPEN_ACCESS
corpusId: 9346534
fieldsOfStudy:
- Computer Science
numCitedBy: 934
numCiting: 15
paperAbstract: Supervised neural networks generalize well if there is much less information
  in the weights than there is in the output vectors of the training cases. So during
  learning, it is important to keep the weights simple by penalizing the amount of
  information they contain. The amount of information in a weight can be controlled
  by adding Gaussian noise and the noise level can be adapted during learning to optimize
  the trade-o between the expected squared error of the network and the amount of
  information in the weights. We describe a method of computing the derivatives of
  the expected squared error and of the amount of information in the noisy weights
  in a network that contains a layer of non-linear hidden units. Provided the output
  units are linear, the exact derivatives can be computed e ciently without time-consuming
  Monte Carlo simulations. The idea of minimizing the amount of information that is
  required to communicate the weights of a neural network leads to a number of interesting
  schemes for encoding the weights.
ref_count: 15
references:
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 644
  pid: de75e4e15e22d4376300e5c968e2db44be29ac9e
  title: Simplifying Neural Networks by Soft Weight-Sharing
  year: 1992
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 2590
  pid: b959164d1efca4b73986ba5d21e664aadbbc0457
  title: A Practical Bayesian Framework for Backpropagation Networks
  year: 1992
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 213
  pid: d275cf94e620bf5b3776bba8a88acccdcfcd9a19
  title: Bayesian Learning via Stochastic Dynamics
  year: 1992
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 205
  pid: 3e6bea2649298c68d17b9421fc7dd19eeacc935e
  title: Learning Translation Invariant Recognition in Massively Parallel Networks
  year: 1987
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 640
  pid: e08d090d1e586610d636a46004876e9f3ded8209
  title: A time-delay neural network architecture for isolated word recognition
  year: 1990
- fieldsOfStudy:
  - Mathematics
  numCitedBy: 1036
  pid: 7e1ca8d081fc07e6190a3bf5e3156569d8e9c96b
  title: Stochastic Complexity and Modeling
  year: 1986
slug: Keeping-the-neural-networks-simple-by-minimizing-of-Hinton-Camp
title: Keeping the neural networks simple by minimizing the description length of
  the weights
url: https://www.semanticscholar.org/paper/Keeping-the-neural-networks-simple-by-minimizing-of-Hinton-Camp/25c9f33aceac6dcff357727cbe2faf145b01d13c?sort=total-citations
venue: COLT '93
year: 1993
