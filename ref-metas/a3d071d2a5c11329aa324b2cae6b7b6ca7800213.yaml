authors:
- Aishwarya Agrawal
- Aniruddha Kembhavi
- Dhruv Batra
- Devi Parikh
badges:
- id: OPEN_ACCESS
corpusId: 29665343
fieldsOfStudy:
- Computer Science
numCitedBy: 59
numCiting: 39
paperAbstract: Visual Question Answering (VQA) has received a lot of attention over
  the past couple of years. A number of deep learning models have been proposed for
  this task. However, it has been shown that these models are heavily driven by superficial
  correlations in the training data and lack compositionality -- the ability to answer
  questions about unseen compositions of seen concepts. This compositionality is desirable
  and central to intelligence. In this paper, we propose a new setting for Visual
  Question Answering where the test question-answer pairs are compositionally novel
  compared to training question-answer pairs. To facilitate developing models under
  this setting, we present a new compositional split of the VQA v1.0 dataset, which
  we call Compositional VQA (C-VQA). We analyze the distribution of questions and
  answers in the C-VQA splits. Finally, we evaluate several existing VQA models under
  this new setting and show that the performances of these models degrade by a significant
  amount compared to the original VQA setting.
ref_count: 40
references:
- pid: a27ed1310b9c0832bde8f906e0fcd23d1f3aa79c
  title: 'Making the V in VQA Matter: Elevating the Role of Image Understanding in
    Visual Question Answering'
- pid: 8e759195eb4b4f0f480a8a2cf1c629bfd881d4e5
  title: Analyzing the Behavior of Visual Question Answering Models
- pid: 3d1382fa43c31e594ed2d84dda9984b1db047b0e
  title: Compositional Memory for Visual Question Answering
- pid: ebe5081b8a24b4740db929b6eae75f28f8edbc58
  title: Answer-Type Prediction for Visual Question Answering
- pid: fb9d253258d6b3beceb9d6cd7bba6e0a29ab875b
  title: Hierarchical Question-Image Co-Attention for Visual Question Answering
- pid: def584565d05d6a8ba94de6621adab9e301d375d
  title: 'Visual7W: Grounded Question Answering in Images'
- pid: 5fa973b8d284145bf0ced9acf2913a74674260f6
  title: 'Yin and Yang: Balancing and Answering Binary Visual Questions'
- pid: 2fcd5cff2b4743ea640c4af68bf4143f4a2cccb1
  title: Are You Talking to a Machine? Dataset and Methods for Multilingual Image
    Question
- pid: 62a956d7600b10ca455076cd56e604dfd106072a
  title: Exploring Models and Data for Image Question Answering
- pid: b196bc11ad516c8e6ff96f83acfc443fd7161730
  title: 'ABC-CNN: An Attention Based Convolutional Neural Network for Visual Question
    Answering'
- pid: 0ac8f1a3c679b90d22c1f840cdc8d61ffef750ac
  title: Deep Compositional Question Answering with Neural Module Networks
- pid: 1cf6bc0866226c1f8e282463adc8b75d92fba9bb
  title: 'Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for
    Visual Question Answering'
- pid: fddc15480d086629b960be5bff96232f967f2252
  title: Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual
    Grounding
- pid: 7214daf035ab005b3d1e739750dd597b4f4513fa
  title: A Focused Dynamic Attention Model for Visual Question Answering
- pid: 20dbdf02497aa84510970d0f5e8b599073bca1bc
  title: 'Ask Me Anything: Free-Form Visual Question Answering Based on Knowledge
    from External Sources'
- pid: b58e08741fb9803fa2a870eee139137d3bade332
  title: Training Recurrent Answering Units with Joint Loss Minimization for VQA
- pid: f96898d15a1bf1fa8925b1280d0e07a7a8e72194
  title: Dynamic Memory Networks for Visual and Textual Question Answering
- pid: 2c1890864c1c2b750f48316dc8b650ba4772adc5
  title: Stacked Attention Networks for Image Question Answering
- pid: 121a9a160f1f2819a01edbe522024b58dbfee798
  title: 'DualNet: Domain-invariant network for visual question answering'
- pid: 75ddc7ee15be14013a3462c01b38b0548486fbcb
  title: Learning to Compose Neural Networks for Question Answering
- pid: 175e9bb50cc062c6c1742a5d90c8dfe31d2e4e22
  title: 'Where to Look: Focus Regions for Visual Question Answering'
- pid: 0b0a1cd432413978e4ef3d0418ebf3bb07af6c7a
  title: Explicit Knowledge-based Reasoning for Visual Question Answering
- pid: 1afb710a5b35a2352a6495e4bf6eef66808daf1b
  title: Multimodal Residual Learning for Visual QA
- pid: 03eb382e04cca8cca743f7799070869954f1402a
  title: 'CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual
    Reasoning'
- pid: afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d
  title: 'Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image
    Annotations'
- pid: ac64fb7e6d2ddf236332ec9f371fe85d308c114d
  title: A Multi-World Approach to Question Answering about Real-World Scenes based
    on Uncertain Input
- pid: eb42cf88027de515750f230b23b1a057dc782108
  title: Very Deep Convolutional Networks for Large-Scale Image Recognition
- pid: 050da5d159fb0dd96143948e1cffeb3dec814673
  title: Visual Turing test for computer vision systems
- pid: 0566bf06a0368b518b8b474166f7b1dfef3f9283
  title: Learning to detect unseen object classes by between-class attribute transfer
- pid: 784da2a7b53a16d2243f747e14946cc5e3476af0
  title: 'VQA: Visual Question Answering'
slug: C-VQA:-A-Compositional-Split-of-the-Visual-Question-Agrawal-Kembhavi
title: 'C-VQA: A Compositional Split of the Visual Question Answering (VQA) v1.0 Dataset'
url: https://www.semanticscholar.org/paper/C-VQA:-A-Compositional-Split-of-the-Visual-Question-Agrawal-Kembhavi/a3d071d2a5c11329aa324b2cae6b7b6ca7800213?sort=total-citations
venue: ArXiv
year: 2017
