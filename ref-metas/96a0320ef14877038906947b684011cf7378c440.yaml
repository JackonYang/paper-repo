authors:
- Haonan Yu
- J. Siskind
badges:
- id: OPEN_ACCESS
corpusId: 5061155
fieldsOfStudy:
- Computer Science
numCitedBy: 138
numCiting: 27
paperAbstract: We present a method that learns representations for word meanings from
  short video clips paired with sentences. Unlike prior work on learning language
  from symbolic input, our input consists of video of people interacting with multiple
  complex objects in outdoor environments. Unlike prior computer-vision approaches
  that learn from videos with verb labels or images with noun labels, our labels are
  sentences containing nouns, verbs, prepositions, adjectives, and adverbs. The correspondence
  between words and concepts in the video is learned in an unsupervised fashion, even
  when the video depicts simultaneous events described by multiple sentences or when
  different aspects of a single event are described with multiple sentences. The learned
  word meanings can be subsequently used to automatically generate description of
  new video.
ref_count: 27
references:
- pid: 793c1c908672ea71aef9e1b41a46272aa27598f7
  title: Video In Sentences Out
- pid: 169b847e69c35cfd475eb4dcc561a24de11762ca
  title: 'Baby talk: Understanding and generating simple image descriptions'
- pid: 05e074abddd3fe987b9bebd46f6cf4bf8465c37e
  title: 'I2T: Image Parsing to Text Description'
- pid: 2a0d0f6c5a69b264710df0230696f47c5918e2f2
  title: Collective Generation of Natural Image Descriptions
- pid: 0008076a1968d9fb590c9013ab27b824849a4e80
  title: 'Learning to sportscast: a test of grounded language acquisition'
- pid: 45336e96c04ea005b203ff3fc84aa4f4159e8cb0
  title: Recognizing human action in time-sequential images using hidden Markov model
- pid: 2854f6721708f1f3d5fe746e11efe0c866f83c19
  title: A computational study of cross-situational techniques for learning word-to-meaning
    mappings
- pid: d90cb88d89408daf4a0fe5ac341a6b9db747a556
  title: 'Action bank: A high-level representation of activity in video'
- pid: 8e080b98efbe65c02a116439205ca2344b9f7cd4
  title: 'Im2Text: Describing Images Using 1 Million Captioned Photographs'
- pid: e79272fe3d65197100eae8be9fec6469107969ae
  title: Object Detection with Discriminatively Trained Part Based Models
- pid: 657a403fa4d37ef13493ec88276ea5c5017cda2f
  title: Cascade object detection with deformable part models
- pid: 145c0b53514b02bdc3dadfb2e1cea124f2abd99b
  title: Error bounds for convolutional codes and an asymptotically optimum decoding
    algorithm
- pid: 3092a4929bdb3d6a8fe53f162586b7431b5ff8a4
  title: A Maximization Technique Occurring in the Statistical Analysis of Probabilistic
    Functions of Markov Chains
- pid: 603bdbb17ba1f909280405a076455ac4f878fbf3
  title: Statistical Inference for Probabilistic Functions of Finite State Markov
    Chains
- pid: 539036ab9e8f038c8a948596e77cc0dfcfa91fb3
  title: An inequality and associated maximization technique in statistical estimation
    of probabilistic functions of a Markov process
slug: Grounded-Language-Learning-from-Video-Described-Yu-Siskind
title: Grounded Language Learning from Video Described with Sentences
url: https://www.semanticscholar.org/paper/Grounded-Language-Learning-from-Video-Described-Yu-Siskind/96a0320ef14877038906947b684011cf7378c440?sort=total-citations
venue: ACL
year: 2013
