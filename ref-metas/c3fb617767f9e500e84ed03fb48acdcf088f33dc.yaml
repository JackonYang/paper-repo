authors:
- H. H. Thodberg
badges: []
corpusId: 15593225
fieldsOfStudy:
- Computer Science
numCitedBy: 49
numCiting: 20
paperAbstract: 'MacKay''s Bayesian framework for backpropagation is a practical and
  powerful means of improving the generalisation ability of neural networks. The framework
  is reviewed and extended in a pedagogical way. The notation is simpliied using the
  ordinary weight decay parameter, and the noise parameter is shown to be nothing
  more than an overall scale. A detailed and explicit procedure for adjusting several
  weight decay parameters is given. Pruning is incorporated into the Bayesian framework.
  Appropriate symmetry factors on sparse architectures are deduced. Bayesian weight
  decay is demonstrated using artiicial data generated by a sparsely connected network.
  Pruning yields computational advantages: by removing unimportant weights the posterior
  weight distribution becomes Gaussian, and pruning removes zero-modes of the Hessian
  and redundant hidden units. In addition, pruning improves generalisation. The Bayesian
  evidence is used as a stop criterion for pruning. Bayesian backprop is applied in
  the prediction of fat content in minced meat from near infrared spectra. It outperforms
  \early stopping" as well as quadratic regression. The evidence of a committee of
  diierently trained networks is computed and the corresponding improved generalisation
  is veriied. The error bars on the predictions of the fat content are computed. There
  are three contributors: The random noise, the uncertainty in the weights, and the
  deviation among the committee members. Finally the Bayesian framework is compared
  to Moody''s GPE.'
ref_count: 20
references:
- pid: b959164d1efca4b73986ba5d21e664aadbbc0457
  title: A Practical Bayesian Framework for Backpropagation Networks
- pid: 7abda1941534d3bb558dd959025d67f1df526303
  title: The Evidence Framework Applied to Classification Networks
- pid: a42954d4b9d0ccdf1036e0af46d87a01b94c3516
  title: 'Second Order Derivatives for Network Pruning: Optimal Brain Surgeon'
- pid: de75e4e15e22d4376300e5c968e2db44be29ac9e
  title: Simplifying Neural Networks by Soft Weight-Sharing
- pid: 2a1e1da81b535e1bead3fc2ab6af8b07877823b9
  title: Exact Calculation of the Hessian Matrix for the Multilayer Perceptron
- pid: 7e0dab4fe4299bc2f8b4b18f82702af717cf3924
  title: 'The Effective Number of Parameters: An Analysis of Generalization and Regularization
    in Nonlinear Learning Systems'
- pid: 2cee043045b529fceda7964a70e626d45657245a
  title: 'Predicting the Future: a Connectionist Approach'
- pid: a205103d4f25ae39f417bac7bd5142302d7f448c
  title: Bayesian inference in statistical analysis
- pid: 2046412fecff64e095cc5190b69172055afd2094
  title: Information-Based Objective Functions for Active Data Selection
- pid: b9dd05b69d6906fff6ea6c4ba3609a6d97c9b8a3
  title: Statistical Decision Theory and Bayesian Analysis
slug: Ace-of-Bayes-:-Application-of-Neural-Thodberg
title: 'Ace of Bayes : Application of Neural'
url: https://www.semanticscholar.org/paper/Ace-of-Bayes-:-Application-of-Neural-Thodberg/c3fb617767f9e500e84ed03fb48acdcf088f33dc?sort=total-citations
venue: ''
year: 1993
