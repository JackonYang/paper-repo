authors:
- Victor Sanh
- Lysandre Debut
- Julien Chaumond
- Thomas Wolf
badges:
- id: OPEN_ACCESS
corpusId: 203626972
fieldsOfStudy:
- Computer Science
meta_key: distilbert-a-distilled-version-of-bert-smaller-faster-cheaper-and-lighter
numCitedBy: 2086
numCiting: 23
paperAbstract: As Transfer Learning from large-scale pre-trained models becomes more
  prevalent in Natural Language Processing (NLP), operating these large models in
  on-the-edge and/or under constrained computational training or inference budgets
  remains challenging. In this work, we propose a method to pre-train a smaller general-purpose
  language representation model, called DistilBERT, which can then be fine-tuned with
  good performances on a wide range of tasks like its larger counterparts. While most
  prior work investigated the use of distillation for building task-specific models,
  we leverage knowledge distillation during the pre-training phase and show that it
  is possible to reduce the size of a BERT model by 40%, while retaining 97% of its
  language understanding capabilities and being 60% faster. To leverage the inductive
  biases learned by larger models during pre-training, we introduce a triple loss
  combining language modeling, distillation and cosine-distance losses. Our smaller,
  faster and lighter model is cheaper to pre-train and we demonstrate its capabilities
  for on-device computations in a proof-of-concept experiment and a comparative on-device
  study.
ref_count: 23
references:
- fieldsOfStudy:
  - Computer Science
  meta_key: distilling-task-specific-knowledge-from-bert-into-simple-neural-networks
  numCitedBy: 218
  pid: a08293b2c9c5bcddb023cc7eb3354d4d86bfae89
  show_ref_link: false
  title: Distilling Task-Specific Knowledge from BERT into Simple Neural Networks
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding
  numCitedBy: 33777
  pid: df2b0e26d0599ce3e70df8a9da02e51594e0e992
  show_ref_link: true
  title: BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: well-read-students-learn-better-the-impact-of-student-initialization-on-knowledge-distillation
  numCitedBy: 87
  pid: 93ad19fbc85360043988fa9ea7932b7fdf1fa948
  show_ref_link: false
  title: Well-Read Students Learn Better - The Impact of Student Initialization on
    Knowledge Distillation
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: language-models-are-unsupervised-multitask-learners
  numCitedBy: 6289
  pid: 9405cc0d6169988371b2755e573cc28650d14dfe
  show_ref_link: true
  title: Language Models are Unsupervised Multitask Learners
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: model-compression-with-multi-task-knowledge-distillation-for-web-scale-question-answering-system
  numCitedBy: 11
  pid: 5fd20d2fd9a5408221a76c37675c329a9310963d
  show_ref_link: false
  title: Model Compression with Multi-Task Knowledge Distillation for Web-scale Question
    Answering System
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: distilling-the-knowledge-in-a-neural-network
  numCitedBy: 8705
  pid: 0c908739fbff75f03469d13d4a1a07de3414ee19
  show_ref_link: true
  title: Distilling the Knowledge in a Neural Network
  year: 2015
- fieldsOfStudy:
  - Computer Science
  meta_key: are-sixteen-heads-really-better-than-one
  numCitedBy: 404
  pid: b03c7ff961822183bab66b2e594415e585d3fd09
  show_ref_link: false
  title: Are Sixteen Heads Really Better than One?
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: roberta-a-robustly-optimized-bert-pretraining-approach
  numCitedBy: 7271
  pid: 077f8329a7b6fa3b7c877a57b81eb6c18b5f87de
  show_ref_link: true
  title: RoBERTa - A Robustly Optimized BERT Pretraining Approach
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: attention-is-all-you-need
  numCitedBy: 35186
  pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  show_ref_link: true
  title: Attention is All you Need
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: transformers-state-of-the-art-natural-language-processing
  numCitedBy: 2303
  pid: af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2
  show_ref_link: true
  title: Transformers - State-of-the-Art Natural Language Processing
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: glue-a-multi-task-benchmark-and-analysis-platform-for-natural-language-understanding
  numCitedBy: 2637
  pid: 93b8da28d006415866bf48f9a6e06b5242129195
  show_ref_link: true
  title: GLUE - A Multi-Task Benchmark and Analysis Platform for Natural Language
    Understanding
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: small-and-practical-bert-models-for-sequence-labeling
  numCitedBy: 75
  pid: 2f9d4887d0022400fc40c774c4c78350c3bc5390
  show_ref_link: false
  title: Small and Practical BERT Models for Sequence Labeling
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: energy-and-policy-considerations-for-deep-learning-in-nlp
  numCitedBy: 1166
  pid: d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea
  show_ref_link: false
  title: Energy and Policy Considerations for Deep Learning in NLP
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: model-compression
  numCitedBy: 1452
  pid: 30c9bb327b7f2b9f1d1e5b69b9d0c97b410948d9
  show_ref_link: false
  title: Model compression
  year: 2006
- fieldsOfStudy:
  - Computer Science
  meta_key: deep-contextualized-word-representations
  numCitedBy: 7988
  pid: 3febb2bed8865945e7fddc99efd791887bb7e14f
  show_ref_link: true
  title: Deep Contextualized Word Representations
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: jiant-a-software-toolkit-for-research-on-general-purpose-text-understanding-models
  numCitedBy: 52
  pid: 00b30ed463625da04166eb78ca617539b41a9846
  show_ref_link: false
  title: jiant - A Software Toolkit for Research on General-Purpose Text Understanding
    Models
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: squad-100-000-questions-for-machine-comprehension-of-text
  numCitedBy: 4266
  pid: 05dd7254b632376973f3a1b4d39485da17814df5
  show_ref_link: true
  title: SQuAD - 100,000+ Questions for Machine Comprehension of Text
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: aligning-books-and-movies-towards-story-like-visual-explanations-by-watching-movies-and-reading-books
  numCitedBy: 1419
  pid: 0e6824e137847be0599bb0032e37042ed2ef5045
  show_ref_link: true
  title: Aligning Books and Movies - Towards Story-Like Visual Explanations by Watching
    Movies and Reading Books
  year: 2015
- fieldsOfStudy:
  - Computer Science
  meta_key: deep-learning-with-limited-numerical-precision
  numCitedBy: 1491
  pid: b7cf49e30355633af2db19f35189410c8515e91f
  show_ref_link: false
  title: Deep Learning with Limited Numerical Precision
  year: 2015
- fieldsOfStudy:
  - Computer Science
  meta_key: making-neural-machine-reading-comprehension-faster
  numCitedBy: 7
  pid: 690772647d0d235f14c4d41ef2df643018d20d83
  show_ref_link: false
  title: Making Neural Machine Reading Comprehension Faster
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: learning-word-vectors-for-sentiment-analysis
  numCitedBy: 3009
  pid: 649d03490ef72c5274e3bccd03d7a299d2f8da91
  show_ref_link: true
  title: Learning Word Vectors for Sentiment Analysis
  year: 2011
slug: DistilBERT,-a-distilled-version-of-BERT:-smaller,-Sanh-Debut
title: DistilBERT, a distilled version of BERT - smaller, faster, cheaper and lighter
url: https://www.semanticscholar.org/paper/DistilBERT,-a-distilled-version-of-BERT:-smaller,-Sanh-Debut/a54b56af24bb4873ed0163b77df63b92bd018ddc?sort=total-citations
venue: ArXiv
year: 2019
