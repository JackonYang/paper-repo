authors:
- Victor Sanh
- Lysandre Debut
- Julien Chaumond
- Thomas Wolf
badges:
- id: OPEN_ACCESS
corpusId: 203626972
fieldsOfStudy:
- Computer Science
numCitedBy: 2086
numCiting: 23
paperAbstract: As Transfer Learning from large-scale pre-trained models becomes more
  prevalent in Natural Language Processing (NLP), operating these large models in
  on-the-edge and/or under constrained computational training or inference budgets
  remains challenging. In this work, we propose a method to pre-train a smaller general-purpose
  language representation model, called DistilBERT, which can then be fine-tuned with
  good performances on a wide range of tasks like its larger counterparts. While most
  prior work investigated the use of distillation for building task-specific models,
  we leverage knowledge distillation during the pre-training phase and show that it
  is possible to reduce the size of a BERT model by 40%, while retaining 97% of its
  language understanding capabilities and being 60% faster. To leverage the inductive
  biases learned by larger models during pre-training, we introduce a triple loss
  combining language modeling, distillation and cosine-distance losses. Our smaller,
  faster and lighter model is cheaper to pre-train and we demonstrate its capabilities
  for on-device computations in a proof-of-concept experiment and a comparative on-device
  study.
ref_count: 23
references:
- pid: df2b0e26d0599ce3e70df8a9da02e51594e0e992
  title: 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'
- pid: 9405cc0d6169988371b2755e573cc28650d14dfe
  title: Language Models are Unsupervised Multitask Learners
- pid: 0c908739fbff75f03469d13d4a1a07de3414ee19
  title: Distilling the Knowledge in a Neural Network
- pid: 077f8329a7b6fa3b7c877a57b81eb6c18b5f87de
  title: 'RoBERTa: A Robustly Optimized BERT Pretraining Approach'
- pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  title: Attention is All you Need
- pid: af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2
  title: 'Transformers: State-of-the-Art Natural Language Processing'
- pid: 93b8da28d006415866bf48f9a6e06b5242129195
  title: 'GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language
    Understanding'
- pid: 30c9bb327b7f2b9f1d1e5b69b9d0c97b410948d9
  title: Model compression
- pid: 3febb2bed8865945e7fddc99efd791887bb7e14f
  title: Deep Contextualized Word Representations
- pid: 05dd7254b632376973f3a1b4d39485da17814df5
  title: 'SQuAD: 100,000+ Questions for Machine Comprehension of Text'
- pid: 0e6824e137847be0599bb0032e37042ed2ef5045
  title: 'Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching
    Movies and Reading Books'
- pid: 649d03490ef72c5274e3bccd03d7a299d2f8da91
  title: Learning Word Vectors for Sentiment Analysis
slug: DistilBERT,-a-distilled-version-of-BERT:-smaller,-Sanh-Debut
title: 'DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter'
url: https://www.semanticscholar.org/paper/DistilBERT,-a-distilled-version-of-BERT:-smaller,-Sanh-Debut/a54b56af24bb4873ed0163b77df63b92bd018ddc?sort=total-citations
venue: ArXiv
year: 2019
