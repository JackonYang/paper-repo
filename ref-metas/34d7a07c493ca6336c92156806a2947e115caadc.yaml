authors:
- A. Lavie
- Abhaya Agarwal
badges:
- id: UNPAYWALL
- id: OPEN_ACCESS
corpusId: 16289845
fieldsOfStudy:
- Computer Science
numCitedBy: 809
numCiting: 16
paperAbstract: Meteor is an automatic metric for Machine Translation evaluation which
  has been demonstrated to have high levels of correlation with human judgments of
  translation quality, significantly outperforming the more commonly used Bleu metric.
  It is one of several automatic metrics used in this year's shared task within the
  ACL WMT-07 workshop. This paper recaps the technical details underlying the metric
  and describes recent improvements in the metric. The latest release includes improved
  metric parameters and extends the metric to support evaluation of MT output in Spanish,
  French and German, in addition to English.
ref_count: 16
references:
- pid: 0cd18e4400ff75b2f8b58d60ddb9b0bc12f489e7
  title: 'METEOR: An Automatic Metric for MT Evaluation with Improved Correlation
    with Human Judgments'
- pid: d7da009f457917aa381619facfa5ffae9329a6e9
  title: 'Bleu: a Method for Automatic Evaluation of Machine Translation'
- pid: 1f12451245667a85d0ee225a80880fc93c71cc8b
  title: Minimum Error Rate Training in Statistical Machine Translation
slug: METEOR:-An-Automatic-Metric-for-MT-Evaluation-with-Lavie-Agarwal
title: 'METEOR: An Automatic Metric for MT Evaluation with High Levels of Correlation
  with Human Judgments'
url: https://www.semanticscholar.org/paper/METEOR:-An-Automatic-Metric-for-MT-Evaluation-with-Lavie-Agarwal/34d7a07c493ca6336c92156806a2947e115caadc?sort=total-citations
venue: WMT@ACL
year: 2007
