authors:
- Axel Cleeremans
- D. Servan-Schreiber
- James L. McClelland
badges:
- id: OPEN_ACCESS
corpusId: 7741931
fieldsOfStudy:
- Computer Science
numCitedBy: 513
numCiting: 13
paperAbstract: We explore a network architecture introduced by Elman (1988) for predicting
  successive elements of a sequence. The network uses the pattern of activation over
  a set of hidden units from time-step t1, together with element t, to predict element
  t 1. When the network is trained with strings from a particular finite-state grammar,
  it can learn to be a perfect finite-state recognizer for the grammar. When the network
  has a minimal number of hidden units, patterns on the hidden units come to correspond
  to the nodes of the grammar, although this correspondence is not necessary for the
  network to act as a perfect finite-state recognizer. We explore the conditions under
  which the network can carry information about distant sequential contingencies across
  intervening elements. Such information is maintained with relative ease if it is
  relevant at each intermediate step; it tends to be lost when intervening elements
  do not depend on it. At first glance this may suggest that such networks are not
  relevant to natural language, in which dependencies may span indefinite distances.
  However, embeddings in natural language are not completely independent of earlier
  information. The final simulation shows that long distance sequential contingencies
  can be encoded by the network even if only subtle statistical properties of embedded
  strings depend on the early information.
ref_count: 13
references:
- pid: d0d7f8b5d54e3d68fd45a70d4a0d13f42e8d71ff
  title: Learning Subsequential Structure in Simple Recurrent Networks
- pid: 668087f0ae7ce1de6e0bd0965dbb480c08103260
  title: Finding Structure in Time
- pid: ce9a21b93ba29d4145a8ef6bf401e77f261848de
  title: A Learning Algorithm for Continually Running Fully Recurrent Neural Networks
- pid: 052b1d8ce63b07fec3de9dbb583772d860b7c769
  title: Learning representations by back-propagating errors
- pid: 26bc0449360d7016f684eafae5b5d2feded32041
  title: An Efficient Gradient-Based Algorithm for On-Line Training of Recurrent Network
    Trajectories
- pid: 406033f22b6a671b94bcbdfaf63070b7ce6f3e48
  title: 'NETtalk: a parallel network that learns to read aloud'
- pid: 1d453386011ef21285fa81fb4f87fdf811c6ad7a
  title: Learning internal representations by back-propagating errors
slug: Finite-State-Automata-and-Simple-Recurrent-Networks-Cleeremans-Servan-Schreiber
title: Finite State Automata and Simple Recurrent Networks
url: https://www.semanticscholar.org/paper/Finite-State-Automata-and-Simple-Recurrent-Networks-Cleeremans-Servan-Schreiber/bd46c1b5948abe04e565a8bae6454da63a1b021e?sort=total-citations
venue: Neural Computation
year: 1989
