authors:
- J. Schmidhuber
badges: []
corpusId: 18271205
fieldsOfStudy:
- Computer Science
numCitedBy: 428
numCiting: 20
paperAbstract: Previous neural network learning algorithms for sequence processing
  are computationally expensive and perform poorly when it comes to long time lags.
  This paper first introduces a simple principle for reducing the descriptions of
  event sequences without loss of information. A consequence of this principle is
  that only unexpected inputs can be relevant. This insight leads to the construction
  of neural architectures that learn to divide and conquer by recursively decomposing
  sequences. I describe two architectures. The first functions as a self-organizing
  multilevel hierarchy of recurrent networks. The second, involving only two recurrent
  networks, tries to collapse a multilevel predictor hierarchy into a single recurrent
  net. Experiments show that the system can require less computation per time step
  and many fewer training sequences than conventional training algorithms for recurrent
  nets.
ref_count: 20
references:
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 336
  pid: 424710825d726e10b016204ed2bc979e2a342d10
  title: Experimental Analysis of the Real-time Recurrent Learning Algorithm
  year: 1989
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 136
  pid: 89b9a181801f32bf62c4237c4265ba036a79f9dc
  title: A Fixed Size Storage O(n3) Time Complexity Learning Algorithm for Fully Recurrent
    Continually Running Networks
  year: 1992
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 634
  pid: 26bc0449360d7016f684eafae5b5d2feded32041
  title: An Efficient Gradient-Based Algorithm for On-Line Training of Recurrent Network
    Trajectories
  year: 1990
slug: Learning-Complex,-Extended-Sequences-Using-the-of-Schmidhuber
title: Learning Complex, Extended Sequences Using the Principle of History Compression
url: https://www.semanticscholar.org/paper/Learning-Complex,-Extended-Sequences-Using-the-of-Schmidhuber/50c770b425a5bb25c77387f687a9910a9d130722?sort=total-citations
venue: Neural Computation
year: 1992
