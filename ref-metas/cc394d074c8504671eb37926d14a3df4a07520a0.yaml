authors:
- L. Breiman
badges:
- id: UNPAYWALL
corpusId: 123349680
fieldsOfStudy:
- Computer Science
numCitedBy: 933
numCiting: 0
paperAbstract: Recent work has shown that combining multiple versions of unstable
  classifiers such as trees or neural nets results in reduced test set error. One
  of the more effective is bagging. Here, modified training sets are formed by resampling
  from the original training set, classifiers constructed using these training sets
  and then combined by voting. Freund and Schapire propose an algorithm the basis
  of which is to adaptively resample and combine (hence the acronym arcing) so that
  the weights in the resampling are increased for those cases most often misclassified
  and the combining is done by weighted voting. Arcing is more successful than bagging
  in test set error reduction. We explore two arcing algorithms, compare them to each
  other and to bagging, and try to understand how arcing works. We introduce the definitions
  of bias and variance for a classifier as components of the test set error. Unstable
  classifiers can have low bias on a large range of data sets. Their problem is high
  variance. Combining multiple versions either through bagging or arcing reduces variance
  significantly.
ref_count: 0
references: []
slug: Arcing-classifier-(with-discussion-and-a-rejoinder-Breiman
title: Arcing classifier (with discussion and a rejoinder by the author)
url: https://www.semanticscholar.org/paper/Arcing-classifier-(with-discussion-and-a-rejoinder-Breiman/cc394d074c8504671eb37926d14a3df4a07520a0?sort=total-citations
venue: ''
year: 1998
