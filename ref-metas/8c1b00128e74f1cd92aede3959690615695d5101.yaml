authors:
- Adams Wei Yu
- David Dohan
- Minh-Thang Luong
- Rui Zhao
- Kai Chen
- Mohammad Norouzi
- Quoc V. Le
badges:
- id: OPEN_ACCESS
corpusId: 4842909
fieldsOfStudy:
- Computer Science
numCitedBy: 800
numCiting: 56
paperAbstract: 'Current end-to-end machine reading and question answering (Q\&A) models
  are primarily based on recurrent neural networks (RNNs) with attention. Despite
  their success, these models are often slow for both training and inference due to
  the sequential nature of RNNs. We propose a new Q\&A architecture called QANet,
  which does not require recurrent networks: Its encoder consists exclusively of convolution
  and self-attention, where convolution models local interactions and self-attention
  models global interactions. On the SQuAD dataset, our model is 3x to 13x faster
  in training and 4x to 9x faster in inference, while achieving equivalent accuracy
  to recurrent models. The speed-up gain allows us to train the model with much more
  data. We hence combine our model with data generated by backtranslation from a neural
  machine translation model. On the SQuAD dataset, our single model, trained with
  augmented data, achieves 84.6 F1 score on the test set, which is significantly better
  than the best published F1 score of 81.8.'
ref_count: 56
references:
- pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  title: Attention is All you Need
- pid: 93499a7c7f699b6630a86fad964536f9423bb6d0
  title: Effective Approaches to Attention-based Neural Machine Translation
- pid: b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f
  title: Gated Self-Matching Networks for Reading Comprehension and Question Answering
- pid: ff1861b71eaedba46cb679bbe2c585dbe18f9b19
  title: Machine Comprehension Using Match-LSTM and Answer Pointer
- pid: 3a7b63b50c64f4ec3358477790e84cbd6be2a0b4
  title: Bidirectional Attention Flow for Machine Comprehension
- pid: f010affab57b5fcf1cd6be23df79d8ec98c7289c
  title: 'TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading
    Comprehension'
- pid: dbde7dfa6cae81df8ac19ef500c42db96c3d1edd
  title: 'Google''s Neural Machine Translation System: Bridging the Gap between Human
    and Machine Translation'
- pid: 3c78c6df5eb1695b6a399e346dde880af27d1016
  title: Simple and Effective Multi-Paragraph Reading Comprehension
- pid: fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5
  title: Neural Machine Translation by Jointly Learning to Align and Translate
- pid: e978d832a4d86571e1b52aa1685dc32ccb250f50
  title: Dynamic Coattention Networks For Question Answering
- pid: 05dd7254b632376973f3a1b4d39485da17814df5
  title: 'SQuAD: 100,000+ Questions for Machine Comprehension of Text'
- pid: d1505c6123c102e53eb19dff312cb25cea840b72
  title: Teaching Machines to Read and Comprehend
- pid: 832fc9327695f7425d8759c6aaeec0fa2d7b0a90
  title: 'WikiReading: A Novel Large-scale Language Understanding Task over Wikipedia'
- pid: 35b91b365ceb016fb3e022577cec96fb9b445dc5
  title: 'The Goldilocks Principle: Reading Children''s Books with Explicit Memory
    Representations'
- pid: 43428880d75b3a14257c3ee9bda054e61eb869c0
  title: Convolutional Sequence to Sequence Learning
- pid: 44d2abe2175df8153f465f6c39b68b76a0d40ab9
  title: Long Short-Term Memory
- pid: ffb949d3493c3b2f3c9acf9c75cb03938933ddf0
  title: Adversarial Examples for Evaluating Reading Comprehension Systems
- pid: adfcf065e15fd3bc9badf6145034c84dfb08f204
  title: Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling
- pid: 51db1f3c8dfc7d4077da39c96bb90a6358128111
  title: Deep Networks with Stochastic Depth
- pid: 5b6ec746d309b165f9f9def873a2375b6fb40f3d
  title: 'Xception: Deep Learning with Depthwise Separable Convolutions'
- pid: f37e1b62a767a307c046404ca96bc140b3e68cb5
  title: 'GloVe: Global Vectors for Word Representation'
- pid: 1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba
  title: Convolutional Neural Networks for Sentence Classification
- pid: 97fb4e3d45bb098e27e0071448b6152217bd35a5
  title: Layer Normalization
- pid: a6cb366736791bcccc5c8639de5a8f9636bf87e8
  title: 'Adam: A Method for Stochastic Optimization'
- pid: 9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d
  title: 'TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems'
- pid: 51a55df1f023571a7e07e338ee45a3e3d66ef73e
  title: Character-level Convolutional Networks for Text Classification
- pid: e0945081b5b87187a53d4329cf77cd8bff635795
  title: Highway Networks
- pid: a8c33413a626bafc67d46029ed28c2a28cc08899
  title: End-to-End Reading Comprehension with Dynamic Answer Chunk Ranking
slug: QANet:-Combining-Local-Convolution-with-Global-for-Yu-Dohan
title: 'QANet: Combining Local Convolution with Global Self-Attention for Reading
  Comprehension'
url: https://www.semanticscholar.org/paper/QANet:-Combining-Local-Convolution-with-Global-for-Yu-Dohan/8c1b00128e74f1cd92aede3959690615695d5101?sort=total-citations
venue: ICLR
year: 2018
