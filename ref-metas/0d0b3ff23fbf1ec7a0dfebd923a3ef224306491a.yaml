authors:
- Wojciech Zaremba
- Ilya Sutskever
badges:
- id: OPEN_ACCESS
corpusId: 12730022
fieldsOfStudy:
- Computer Science
numCitedBy: 460
numCiting: 32
paperAbstract: Recurrent Neural Networks (RNNs) with Long Short-Term Memory units
  (LSTM) are widely used because they are expressive and are easy to train. Our interest
  lies in empirically evaluating the expressiveness and the learnability of LSTMs
  in the sequence-to-sequence regime by training them to evaluate short computer programs,
  a domain that has traditionally been seen as too complex for neural networks. We
  consider a simple class of programs that can be evaluated with a single left-to-right
  pass using constant memory. Our main result is that LSTMs can learn to map the character-level
  representations of such programs to their correct outputs. Notably, it was necessary
  to use curriculum learning, and while conventional curriculum learning proved ineffective,
  we developed a new variant of curriculum learning that improved our networks' performance
  in all experimental conditions. The improved curriculum had a dramatic impact on
  an addition problem, making it possible to train an LSTM to add two 9-digit numbers
  with 99% accuracy.
ref_count: 32
references:
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 14881
  pid: cea967b59209c6be22829699f05b8b1ac4dc092d
  title: Sequence to Sequence Learning with Neural Networks
  year: 2014
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 6900
  pid: 4177ec52d1b80ed57f2e72b0f9a42365f1a8598d
  title: Speech recognition with deep recurrent neural networks
  year: 2013
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 376
  pid: 5522764282c85aea422f1c4dc92ff7e0ca6987bc
  title: A Clockwork RNN
  year: 2014
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 792
  pid: 533ee188324b833e059cb59b654e6160776d5812
  title: How to Construct Deep Recurrent Neural Networks
  year: 2014
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 51704
  pid: 44d2abe2175df8153f465f6c39b68b76a0d40ab9
  title: Long Short-Term Memory
  year: 1997
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 4902
  pid: 9819b600a828a57e1cde047bbe710d3446b30da5
  title: Recurrent neural network based language model
  year: 2010
- fieldsOfStudy:
  - Education
  numCitedBy: 3193
  pid: 8de174ab5419b9d3127695405efd079808e956e8
  title: Curriculum learning
  year: 2009
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 466
  pid: c0b624c46b51920dfec5aa02cc86323c0beb0df5
  title: Dropout Improves Recurrent Neural Networks for Handwriting Recognition
  year: 2014
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1970
  pid: f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97
  title: Recurrent Neural Network Regularization
  year: 2014
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 3153
  pid: 89b1f4740ae37fd04f6ac007577bdd34621f0861
  title: Generating Sequences With Recurrent Neural Networks
  year: 2013
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 575
  pid: 96364af2d208ea75ca3aeb71892d2f7ce7326b55
  title: Statistical Language Models Based on Neural Networks
  year: 2012
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 15052
  pid: 0b544dfe355a5070b60986319a3f51fb45d1348e
  title: "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical\
    \ Machine Translation"
  year: 2014
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 845
  pid: 4c46347fbc272b21468efe3d9af34b4b2bad6684
  title: Deep learning via Hessian-free optimization
  year: 2010
slug: Learning-to-Execute-Zaremba-Sutskever
title: Learning to Execute
url: https://www.semanticscholar.org/paper/Learning-to-Execute-Zaremba-Sutskever/0d0b3ff23fbf1ec7a0dfebd923a3ef224306491a?sort=total-citations
venue: ArXiv
year: 2014
