authors:
- Jiasen Lu
- Caiming Xiong
- Devi Parikh
- R. Socher
badges:
- id: UNPAYWALL
- id: OPEN_ACCESS
corpusId: 18347865
fieldsOfStudy:
- Computer Science
numCitedBy: 961
numCiting: 38
paperAbstract: Attention-based neural encoder-decoder frameworks have been widely
  adopted for image captioning. Most methods force visual attention to be active for
  every generated word. However, the decoder likely requires little to no visual information
  from the image to predict non-visual words such as the and of. Other words that
  may seem visual can often be predicted reliably just from the language model e.g.,
  sign after behind a red stop or phone following talking on a cell. In this paper,
  we propose a novel adaptive attention model with a visual sentinel. At each time
  step, our model decides whether to attend to the image (and if so, to which regions)
  or to the visual sentinel. The model decides whether to attend to the image and
  where, in order to extract meaningful information for sequential word generation.
  We test our method on the COCO image captioning 2015 challenge dataset and Flickr30K.
  Our approach sets the new state-of-the-art by a significant margin.
ref_count: 38
references:
- pid: fb9d253258d6b3beceb9d6cd7bba6e0a29ab875b
  title: Hierarchical Question-Image Co-Attention for Visual Question Answering
- pid: a72b8bbd039989db39769da836cdb287737deb92
  title: 'Mind''s eye: A recurrent visual representation for image caption generation'
- pid: 15f102c3c9f4d4fe6ba105e221df48c6e8902b3b
  title: From captions to visual concepts and back
- pid: d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0
  title: 'Show and tell: A neural image caption generator'
- pid: 4d8f2d14af5991d4f0d050d22216825cac3157bd
  title: 'Show, Attend and Tell: Neural Image Caption Generation with Visual Attention'
- pid: bf55591e09b58ea9ce8d66110d6d3000ee804bdd
  title: Image Captioning with Semantic Attention
- pid: 00fe3d95d0fd5f1433d81405bee772c4fe9af9c6
  title: What Value Do Explicit High Level Concepts Have in Vision to Language Problems?
- pid: 5785466bc14529e94e54baa4ed051f7037f3b1d3
  title: Boosting Image Captioning with Attributes
- pid: 54b2b6f35f1b5704dddfaa3a137a2f4ad3dfe745
  title: Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)
- pid: 55e022fb7581bb9e1fce678d21fb25ffbb3fbb88
  title: Deep Visual-Semantic Alignments for Generating Image Descriptions
- pid: f96898d15a1bf1fa8925b1280d0e07a7a8e72194
  title: Dynamic Memory Networks for Visual and Textual Question Answering
- pid: 2c1890864c1c2b750f48316dc8b650ba4772adc5
  title: Stacked Attention Networks for Image Question Answering
- pid: f01fc808592ea7c473a69a6e7484040a435f36d9
  title: Long-term recurrent convolutional networks for visual recognition and description
- pid: eaaed23a2d94feb2f1c3ff22a25777c7a78f3141
  title: 'Every Picture Tells a Story: Generating Sentences from Images'
- pid: 23ffaa0fe06eae05817f527a47ac3291077f9e58
  title: Rethinking the Inception Architecture for Computer Vision
- pid: 5cb6700d94c6118ee13f4f4fecac99f111189812
  title: 'BabyTalk: Understanding and Generating Simple Image Descriptions'
- pid: cea967b59209c6be22829699f05b8b1ac4dc092d
  title: Sequence to Sequence Learning with Neural Networks
- pid: 0ca7d208ff8d81377e0eaa9723820aeae7a7322d
  title: Grounded Compositional Semantics for Finding and Describing Images with Sentences
- pid: 2c03df8b48bf3fa39054345bafabfeff15bfd11d
  title: Deep Residual Learning for Image Recognition
- pid: 258986132bf17755fe8263e42429fe73218c1534
  title: 'CIDEr: Consensus-based image description evaluation'
- pid: 1c54acd7d9ed8017acdc5674c9b7faac738fd651
  title: 'SPICE: Semantic Propositional Image Caption Evaluation'
- pid: fad611e35b3731740b4d8b754241e77add5a70b9
  title: Multimodal Neural Language Models
- pid: 71b7178df5d2b112d07e45038cb5637208659ff7
  title: 'Microsoft COCO: Common Objects in Context'
- pid: 169b847e69c35cfd475eb4dcc561a24de11762ca
  title: 'Baby talk: Understanding and generating simple image descriptions'
- pid: fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5
  title: Neural Machine Translation by Jointly Learning to Align and Translate
- pid: 2a0d0f6c5a69b264710df0230696f47c5918e2f2
  title: Collective Generation of Natural Image Descriptions
- pid: 3ca194773fe583661b988fbdf33f7680764438b3
  title: Exploring Nearest Neighbor Approaches for Image Captioning
- pid: efbd381493bb9636f489b965a2034d529cd56bcd
  title: Pointer Sentinel Mixture Models
- pid: 0b544dfe355a5070b60986319a3f51fb45d1348e
  title: "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical\
    \ Machine Translation"
- pid: 355de7460120ddc1150d9ce3756f9848983f7ff4
  title: 'Midge: Generating Image Descriptions From Computer Vision Detections'
- pid: 44040913380206991b1991daf1192942e038fe31
  title: 'From image descriptions to visual denotations: New similarity metrics for
    semantic inference over event descriptions'
- pid: 60b05f32c32519a809f21642ef1eb3eaf3848008
  title: 'ROUGE: A Package for Automatic Evaluation of Summaries'
- pid: 26adb749fc5d80502a6d889966e50b31391560d3
  title: 'Meteor Universal: Language Specific Translation Evaluation for Any Target
    Language'
- pid: d7da009f457917aa381619facfa5ffae9329a6e9
  title: 'Bleu: a Method for Automatic Evaluation of Machine Translation'
slug: Knowing-When-to-Look:-Adaptive-Attention-via-a-for-Lu-Xiong
title: 'Knowing When to Look: Adaptive Attention via a Visual Sentinel for Image Captioning'
url: https://www.semanticscholar.org/paper/Knowing-When-to-Look:-Adaptive-Attention-via-a-for-Lu-Xiong/9f4d7d622d1f7319cc511bfef661cd973e881a4c?sort=total-citations
venue: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
year: 2017
