authors:
- Yonghui Wu
- M. Schuster
- Z. Chen
- Quoc V. Le
- Mohammad Norouzi
- Wolfgang Macherey
- M. Krikun
- Yuan Cao
- Qin Gao
- Klaus Macherey
- J. Klingner
- Apurva Shah
- Melvin Johnson
- Xiaobing Liu
- Lukasz Kaiser
- Stephan Gouws
- Y. Kato
- Taku Kudo
- H. Kazawa
- K. Stevens
- George Kurian
- Nishant Patil
- Wei Wang
- C. Young
- Jason R. Smith
- Jason Riesa
- Alex Rudnick
- Oriol Vinyals
- G. Corrado
- Macduff Hughes
- J. Dean
badges:
- id: OPEN_ACCESS
corpusId: 3603249
fieldsOfStudy:
- Computer Science
numCitedBy: 4645
numCiting: 48
paperAbstract: Neural Machine Translation (NMT) is an end-to-end learning approach
  for automated translation, with the potential to overcome many of the weaknesses
  of conventional phrase-based translation systems. Unfortunately, NMT systems are
  known to be computationally expensive both in training and in translation inference.
  Also, most NMT systems have difficulty with rare words. These issues have hindered
  NMT's use in practical deployments and services, where both accuracy and speed are
  essential. In this work, we present GNMT, Google's Neural Machine Translation system,
  which attempts to address many of these issues. Our model consists of a deep LSTM
  network with 8 encoder and 8 decoder layers using attention and residual connections.
  To improve parallelism and therefore decrease training time, our attention mechanism
  connects the bottom layer of the decoder to the top layer of the encoder. To accelerate
  the final translation speed, we employ low-precision arithmetic during inference
  computations. To improve handling of rare words, we divide words into a limited
  set of common sub-word units ("wordpieces") for both input and output. This method
  provides a good balance between the flexibility of "character"-delimited models
  and the efficiency of "word"-delimited models, naturally handles translation of
  rare words, and ultimately improves the overall accuracy of the system. Our beam
  search technique employs a length-normalization procedure and uses a coverage penalty,
  which encourages generation of an output sentence that is most likely to cover all
  the words in the source sentence. On the WMT'14 English-to-French and English-to-German
  benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human
  side-by-side evaluation on a set of isolated simple sentences, it reduces translation
  errors by an average of 60% compared to Google's phrase-based production system.
ref_count: 48
references:
- pid: 1956c239b3552e030db1b78951f64781101125ed
  title: Addressing the Rare Word Problem in Neural Machine Translation
- pid: fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5
  title: Neural Machine Translation by Jointly Learning to Align and Translate
- pid: b60abe57bc195616063be10638c6437358c81d1e
  title: Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation
- pid: 1af68821518f03568f913ab03fc02080247a27ff
  title: Neural Machine Translation of Rare Words with Subword Units
- pid: 93499a7c7f699b6630a86fad964536f9423bb6d0
  title: Effective Approaches to Attention-based Neural Machine Translation
- pid: 1938624bb9b0f999536dcc8d8f519810bb4e1b3b
  title: On Using Very Large Target Vocabulary for Neural Machine Translation
- pid: cea967b59209c6be22829699f05b8b1ac4dc092d
  title: Sequence to Sequence Learning with Neural Networks
- pid: d76c07211479e233f7c6a6f32d5346c983c5598f
  title: Multi-task Sequence to Sequence Learning
- pid: 944a1cfd79dbfb6fef460360a0765ba790f4027a
  title: Recurrent Continuous Translation Models
- pid: 97cedf99252026f58e8154bc61d49cf885d42030
  title: "Edinburgh\u2019s Phrase-based Machine Translation Systems for WMT-14"
- pid: 35c1668dc64d24a28c6041978e5fcca754eb2f4b
  title: Sequence Level Training with Recurrent Neural Networks
- pid: 0b544dfe355a5070b60986319a3f51fb45d1348e
  title: "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical\
    \ Machine Translation"
- pid: 2166fa493a8c6e40f7f8562d15712dd3c75f03df
  title: A Statistical Approach to Language Translation
- pid: 0894b06cff1cd0903574acaa7fcf071b144ae775
  title: Fast and Robust Neural Network Joint Models for Statistical Machine Translation
- pid: a4b828609b60b06e61bea7a4029cc9e1cad5df87
  title: Statistical Phrase-Based Translation
- pid: 46200b99c40e8586c8a0f588488ab6414119fb28
  title: 'TensorFlow: A system for large-scale machine learning'
- pid: 8e4fb17fff38a7834af5b4eaafcbbde02bf00975
  title: N-gram Counts and Language Models from the Common Crawl
- pid: 11540131eae85b2e11d53df7f1360eeb6476e7f4
  title: 'Learning to Forget: Continual Prediction with LSTM'
- pid: ab7b5917515c460b90451e67852171a531671ab8
  title: 'The Mathematics of Statistical Machine Translation: Parameter Estimation'
- pid: 44d2abe2175df8153f465f6c39b68b76a0d40ab9
  title: Long Short-Term Memory
- pid: a6cb366736791bcccc5c8639de5a8f9636bf87e8
  title: 'Adam: A Method for Stochastic Optimization'
- pid: e23c34414e66118ecd9b08cf0cd4d016f59b0b85
  title: Bidirectional recurrent neural networks
- pid: 2c03df8b48bf3fa39054345bafabfeff15bfd11d
  title: Deep Residual Learning for Image Recognition
- pid: aed054834e2c696807cc8b227ac7a4197196e211
  title: 'Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies'
- pid: 3127190433230b3dc1abd0680bb58dced4bcd90e
  title: Large Scale Distributed Deep Networks
- pid: 995a3b11cc8a4751d8e167abc4aa937abc934df0
  title: The Cascade-Correlation Learning Architecture
- pid: c5145b1d15fea9340840cc8bb6f0e46e8934827f
  title: Understanding the exploding gradient problem
- pid: a1066659ec1afee9dce586f6f49b7d44527827e1
  title: A Statistical Approach to Machine Translation
slug: Google's-Neural-Machine-Translation-System:-the-Gap-Wu-Schuster
title: 'Google''s Neural Machine Translation System: Bridging the Gap between Human
  and Machine Translation'
url: https://www.semanticscholar.org/paper/Google's-Neural-Machine-Translation-System:-the-Gap-Wu-Schuster/dbde7dfa6cae81df8ac19ef500c42db96c3d1edd?sort=total-citations
venue: ArXiv
year: 2016
