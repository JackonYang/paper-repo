authors:
- Tomas Mikolov
- Ilya Sutskever
- Kai Chen
- G. Corrado
- J. Dean
badges:
- id: OPEN_ACCESS
corpusId: 16447573
fieldsOfStudy:
- Computer Science
numCitedBy: 26053
numCiting: 32
paperAbstract: "The recently introduced continuous Skip-gram model is an efficient\
  \ method for learning high-quality distributed vector representations that capture\
  \ a large number of precise syntactic and semantic word relationships. In this paper\
  \ we present several extensions that improve both the quality of the vectors and\
  \ the training speed. By subsampling of the frequent words we obtain significant\
  \ speedup and also learn more regular word representations. We also describe a simple\
  \ alternative to the hierarchical softmax called negative sampling. \n \nAn inherent\
  \ limitation of word representations is their indifference to word order and their\
  \ inability to represent idiomatic phrases. For example, the meanings of \"Canada\"\
  \ and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this\
  \ example, we present a simple method for finding phrases in text, and show that\
  \ learning good vector representations for millions of phrases is possible."
ref_count: 32
references:
- pid: 27e38351e48fe4b7da2775bf94341738bc4da07e
  title: Semantic Compositionality through Recursive Matrix-Vector Spaces
- pid: c4fd9c86b2b41df51a6fe212406dda81b1997fd4
  title: Linguistic Regularities in Continuous Space Word Representations
- pid: 330da625c15427c6e42ccfa3b747fb29e5835bf0
  title: Efficient Estimation of Word Representations in Vector Space
- pid: 6c2b28f9354f667cd5bd07afc0471d8334430da7
  title: A Neural Probabilistic Language Model
- pid: 5b0d644f5c4b9880cbaf79932c0a4fa98996f068
  title: A fast and simple algorithm for training neural probabilistic language models
- pid: c19fbefdeead6a4154a22a9c8551a18b1530033a
  title: Hierarchical Probabilistic Neural Network Language Model
- pid: dac72f2c509aee67524d3321f77e97e8eff51de6
  title: 'Word Representations: A Simple and General Method for Semi-Supervised Learning'
- pid: a9fc84f8abe740cdc7ee82e69444d1d00dbe0ceb
  title: A Scalable Hierarchical Distributed Language Model
- pid: 0fcc184b3b90405ec3ceafd6a4007c749df7c363
  title: Continuous space language models
- pid: 3a0e788268fafb23ab20da0e98bb578b06830f7d
  title: 'From Frequency to Meaning: Vector Space Models of Semantics'
- pid: 57458bc1cffe5caa45a885af986d70f723f406b4
  title: 'A unified architecture for natural language processing: deep neural networks
    with multitask learning'
- pid: 9c0ddf74f87d154db88d79c640578c1610451eec
  title: Parsing Natural Scenes and Natural Language with Recursive Neural Networks
- pid: cb45e9217fe323fbc199d820e7735488fca2a9b3
  title: Strategies for training large scale neural network language models
- pid: 96364af2d208ea75ca3aeb71892d2f7ce7326b55
  title: Statistical Language Models Based on Neural Networks
- pid: 052b1d8ce63b07fec3de9dbb583772d860b7c769
  title: Learning representations by back-propagating errors
- pid: 07ca885cb5cc4328895bfaec9ab752d5801b14cd
  title: Extensions of recurrent neural network language model
- pid: ae3fe34be9230c98b04d68b4621c89b7dbc2d717
  title: Learning representations by backpropagating errors
- pid: 6f4065f0cc99a0839b0248ffb4457e5f0277b30d
  title: 'Domain Adaptation for Large-Scale Sentiment Classification: A Deep Learning
    Approach'
- pid: 51480ee8f067453c2878f0148ffcfa3a856a02dc
  title: 'WSABIE: Scaling Up to Large Vocabulary Image Annotation'
- pid: dfbfcd288ed9b37106564bf6dc95041f6d33b6b2
  title: and
slug: Distributed-Representations-of-Words-and-Phrases-Mikolov-Sutskever
title: Distributed Representations of Words and Phrases and their Compositionality
url: https://www.semanticscholar.org/paper/Distributed-Representations-of-Words-and-Phrases-Mikolov-Sutskever/87f40e6f3022adbc1f1905e3e506abad05a9964f?sort=total-citations
venue: NIPS
year: 2013
