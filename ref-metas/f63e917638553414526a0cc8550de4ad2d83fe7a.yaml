authors:
- "Djork-Arn\xE9 Clevert"
- Thomas Unterthiner
- S. Hochreiter
badges:
- id: OPEN_ACCESS
corpusId: 5273326
fieldsOfStudy:
- Computer Science
numCitedBy: 3669
numCiting: 74
paperAbstract: We introduce the "exponential linear unit" (ELU) which speeds up learning
  in deep neural networks and leads to higher classification accuracies. Like rectified
  linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PReLUs), ELUs
  alleviate the vanishing gradient problem via the identity for positive values. However,
  ELUs have improved learning characteristics compared to the units with other activation
  functions. In contrast to ReLUs, ELUs have negative values which allows them to
  push mean unit activations closer to zero like batch normalization but with lower
  computational complexity. Mean shifts toward zero speed up learning by bringing
  the normal gradient closer to the unit natural gradient because of a reduced bias
  shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure
  a noise-robust deactivation state. ELUs saturate to a negative value with smaller
  inputs and thereby decrease the forward propagated variation and information. Therefore,
  ELUs code the degree of presence of particular phenomena in the input, while they
  do not quantitatively model the degree of their absence. In experiments, ELUs lead
  not only to faster learning, but also to significantly better generalization performance
  than ReLUs and LReLUs on networks with more than 5 layers. On CIFAR-100 ELUs networks
  significantly outperform ReLU networks with batch normalization while batch normalization
  does not improve ELU networks. ELU networks are among the top 10 reported CIFAR-10
  results and yield the best published result on CIFAR-100, without resorting to multi-view
  evaluation or model averaging. On ImageNet, ELU networks considerably speed up learning
  compared to a ReLU network with the same architecture, obtaining less than 10% classification
  error for a single crop, single model network.
ref_count: 74
references:
- pid: b8ef1230a5cc9ea7cd8358f1ae7d1af97813ba14
  title: Deep Learning Made Easier by Linear Transformations in Perceptrons
- pid: 4d376d6978dad0374edfa6709c9556b42d3594d3
  title: 'Batch Normalization: Accelerating Deep Network Training by Reducing Internal
    Covariate Shift'
- pid: d6f2f611da110b5b5061731be3fc4c7f45d8ee23
  title: 'Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet
    Classification'
- pid: 1a3c74c7b11ad5635570932577cdde2a3f7a6a5c
  title: Improving deep neural networks for LVCSR using rectified linear units and
    dropout
- pid: 5e83ab70d0cbc003471e87ec306d27d9c80ecb16
  title: Network In Network
- pid: 64da1980714cfc130632c5b92b9d98c2f6763de6
  title: On rectified linear units for speech processing
- pid: fb91db6aa4f710814f8aec28a7f3ecbc4e5ad4fd
  title: Deeply-Supervised Nets
- pid: 0d6203718c15f137fda2f295c96269bc2b254644
  title: Learning Recurrent Neural Networks with Hessian-Free Optimization
- pid: 44d2abe2175df8153f465f6c39b68b76a0d40ab9
  title: Long Short-Term Memory
- pid: e8f95ccfd13689f672c39dca3eccf1c484533bcc
  title: Revisiting Natural Gradient for Deep Networks
- pid: b1a5961609c623fc816aaa77565ba38b25531a8e
  title: 'Neural Networks: Tricks of the Trade'
- pid: 367f2c63a6f6a10b3b64b8729d601e69337ee3cc
  title: Rectifier Nonlinearities Improve Neural Network Acoustic Models
- pid: abd1c342495432171beb7ca8fd9551ef13cbd0ff
  title: ImageNet classification with deep convolutional neural networks
- pid: b92aa7024b87f50737b372e5df31ef091ab54e62
  title: Training Very Deep Networks
- pid: 0f84a81f431b18a78bd97f59ed4b9d8eda390970
  title: 'Striving for Simplicity: The All Convolutional Net'
- pid: a98483785378bde7e2384a3035b2b501ee03654b
  title: Krylov Subspace Descent for Deep Learning
- pid: 4c46347fbc272b21468efe3d9af34b4b2bad6684
  title: Deep learning via Hessian-free optimization
- pid: 55dda8f230566867acbfaa7bdd08fd8c7b8721ed
  title: Fractional Max-Pooling
- pid: 5a767a341364de1f75bea85e0b12ba7d3586a461
  title: Natural Gradient Works Efficiently in Learning
- pid: b7b915d508987b73b61eccd2b237e7ed099a2d29
  title: Maxout Networks
- pid: a538b05ebb01a40323997629e171c91aa28b8e2f
  title: Rectified Linear Units Improve Restricted Boltzmann Machines
- pid: 94e3e7bc3d23276f0ee2d1cb8f9d14aa19668d5f
  title: Under Review as a Conference Paper at Iclr 2017 Delving into Transferable
    Adversarial Ex- Amples and Black-box Attacks
- pid: 6ed460701019072ee2e364a1a491f73dd931f27f
  title: Topmoumoute Online Natural Gradient Algorithm
- pid: aed054834e2c696807cc8b227ac7a4197196e211
  title: 'Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies'
- pid: fac0e753905d1498e0b3debf01431696e1f0c645
  title: A New Learning Algorithm for Blind Signal Separation
- pid: 67107f78a84bdb2411053cb54e94fa226eea6d8e
  title: Deep Sparse Rectifier Neural Networks
- pid: bcd857d75841aa3e92cd4284a8818aba9f6c0c3f
  title: Published as a conference paper at ICLR 2018 S IMULATING A CTION D YNAMICS
    WITH N EURAL P ROCESS N ETWORKS
- pid: 3f3d13e95c25a8f6a753e38dfce88885097cbd43
  title: Untersuchungen zu dynamischen neuronalen Netzen
- pid: b87274e6d9aa4e6ba5148898aa92941617d2b6ed
  title: Efficient BackProp
- pid: 420322994c59e9081786b46b31e2c82a9753e23a
  title: Differential-geometrical methods in statistics
slug: Fast-and-Accurate-Deep-Network-Learning-by-Linear-Clevert-Unterthiner
title: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)
url: https://www.semanticscholar.org/paper/Fast-and-Accurate-Deep-Network-Learning-by-Linear-Clevert-Unterthiner/f63e917638553414526a0cc8550de4ad2d83fe7a?sort=total-citations
venue: ICLR
year: 2016
