authors:
- Frederic Morin
- Yoshua Bengio
badges:
- id: OPEN_ACCESS
corpusId: 1326925
fieldsOfStudy:
- Computer Science
numCitedBy: 942
numCiting: 32
paperAbstract: In recent years, variants of a neural network architecture for statistical
  language modeling have been proposed and successfully applied, e.g. in the language
  modeling component of speech recognizers. The main advantage of these architectures
  is that they learn an embedding for words (or other symbols) in a continuous space
  that helps to smooth the language model and provide good generalization even when
  the number of training examples is insufficient. However, these models are extremely
  slow in comparison to the more commonly used n-gram models, both for training and
  recognition. As an alternative to an importance sampling method proposed to speed-up
  training, we introduce a hierarchical decomposition of the conditional probabilities
  that yields a speed-up of about 200 both during training and recognition. The hierarchical
  decomposition is a binary hierarchical clustering constrained by the prior knowledge
  extracted from the WordNet semantic hierarchy.
ref_count: 32
references:
- pid: d6fb7546a29320eadad868af66835059db93d99f
  title: Efficient training of large neural networks for language modeling
- pid: 6c2b28f9354f667cd5bd07afc0471d8334430da7
  title: A Neural Probabilistic Language Model
- pid: e41498c05d4c68e4750fb84a380317a112d97b01
  title: Connectionist language modeling for large vocabulary continuous speech recognition
- pid: bfab4ffa229c8af0174a683ff1eda524c4f59d00
  title: Can artificial neural networks learn language models?
- pid: b0130277677e5b915d5cd86b3afafd77fd08eb2e
  title: Estimation of probabilities from sparse data for the language model component
    of a speech recognizer
- pid: 4af41f4d838daa7ca6995aeb4918b61989d1ed80
  title: Classes for fast maximum entropy training
- pid: 09c76da2361d46689825c4efc37ad862347ca577
  title: A bit of progress in language modeling
- pid: 3de5d40b60742e3dfa86b19e7f660962298492af
  title: Class-Based n-gram Models of Natural Language
- pid: 399da68d3b97218b6c80262df7963baa89dcc71b
  title: SRILM - an extensible language modeling toolkit
- pid: 5eb328cf7e94995199e4c82a1f4d0696430a80b5
  title: Distributional Clustering of English Words
- pid: fb486e03369a64de2d5b0df86ec0a7b55d3907db
  title: A Maximum Entropy Approach to Natural Language Processing
- pid: 668087f0ae7ce1de6e0bd0965dbb480c08103260
  title: Finding Structure in Time
- pid: e733226b881f11f25c87e8bac8d602ba3d9c220e
  title: Distributional clustering of words for text classification
- pid: 5d24afe3a62331ebfad400c3fec77c836d2b99db
  title: Word Space
- pid: 9360e5ce9c98166bb179ad479a9d2919ff13d022
  title: Training Products of Experts by Minimizing Contrastive Divergence
- pid: 20a80a7356859daa4170fb4da6b87b84adbb547f
  title: Indexing by Latent Semantic Analysis
- pid: d87ceda3042f781c341ac17109d1e94a717f5f60
  title: 'WordNet : an electronic lexical database'
- pid: e50a316f97c9a405aa000d883a633bd5707f1a34
  title: Term-Weighting Approaches in Automatic Text Retrieval
- pid: 6a923c9f89ed53b6e835b3807c0c1bd8d532687b
  title: Interpolated estimation of Markov source parameters from sparse data
- pid: 4ade4934db522fe6d634ff6f48887da46eedb4d1
  title: Learning distributed representations of concepts.
- pid: b9ed0b35c9eaba0328492de65c4cdc5545094df4
  title: Improved clustering techniques for class-based statistical language modelling
- pid: 6b388f0151ab37adb3d57738b8f52a3f943f86c8
  title: Quick Training of Probabilistic Neural Nets by Importance Sampling
slug: Hierarchical-Probabilistic-Neural-Network-Language-Morin-Bengio
title: Hierarchical Probabilistic Neural Network Language Model
url: https://www.semanticscholar.org/paper/Hierarchical-Probabilistic-Neural-Network-Language-Morin-Bengio/c19fbefdeead6a4154a22a9c8551a18b1530033a?sort=total-citations
venue: AISTATS
year: 2005
