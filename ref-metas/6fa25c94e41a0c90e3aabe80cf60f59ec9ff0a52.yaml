authors:
- Weituo Hao
- Chunyuan Li
- Xiujun Li
- L. Carin
- Jianfeng Gao
badges:
- id: UNPAYWALL
- id: OPEN_ACCESS
corpusId: 211296530
fieldsOfStudy:
- Computer Science
numCitedBy: 95
numCiting: 38
paperAbstract: Learning to navigate in a visual environment following natural-language
  instructions is a challenging task, because the multimodal inputs to the agent are
  highly variable, and the training data on a new task is often limited. In this paper,
  we present the first pre-training and fine-tuning paradigm for vision-and-language
  navigation (VLN) tasks. By training on a large amount of image-text-action triplets
  in a self-supervised learning manner, the pre-trained model provides generic representations
  of visual environments and language instructions. It can be easily used as a drop-in
  for existing VLN frameworks, leading to the proposed agent PREVALENT. It learns
  more effectively in new tasks and generalizes better in a previously unseen environment.
  The performance is validated on three VLN tasks. On the Room-to-Room benchmark,
  our model improves the state-of-the-art from 47\% to 51\% on success rate weighted
  by path length. Further, the learned representation is transferable to other VLN
  tasks. On two recent tasks, vision-and-dialog navigation and ``Help, Anna!'', the
  proposed PREVALENT leads to significant improvement over existing methods, achieving
  a new state of the art.
ref_count: 38
references:
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 564
  pid: 6bd9642470ff8c2089427f7a6392cd17d213a334
  title: 'Vision-and-Language Navigation: Interpreting Visually-Grounded Navigation
    Instructions in Real Environments'
  year: 2018
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1266
  pid: 65a9c7b0800c86a196bc14e7621ff895cc6ab287
  title: 'ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language
    Tasks'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 355
  pid: 6648b4db5f12c30941ea78c695e77aded19672bb
  title: Unified Vision-Language Pre-Training for Image Captioning and VQA
  year: 2020
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 707
  pid: 2527626c11a84f15709e943fbfa2356e19930e3b
  title: 'VL-BERT: Pre-training of Generic Visual-Linguistic Representations'
  year: 2020
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 382
  pid: 2bc1c8bd00bbf7401afcb5460277840fd8bab029
  title: 'Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal
    Pre-training'
  year: 2020
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 361
  pid: e5790afc079c6f36d6fe9235d6d253f3da631f51
  title: Embodied Question Answering
  year: 2018
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 569
  pid: c41a11c0e9b8b92b4faaf97749841170b760760a
  title: 'VideoBERT: A Joint Model for Video and Language Representation Learning'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 916
  pid: 79c93274429d6355959f1e4374c2147bb81ea649
  title: 'LXMERT: Learning Cross-Modality Encoder Representations from Transformers'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 35157
  pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  title: Attention is All you Need
  year: 2017
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 33754
  pid: df2b0e26d0599ce3e70df8a9da02e51594e0e992
  title: 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 14881
  pid: cea967b59209c6be22829699f05b8b1ac4dc092d
  title: Sequence to Sequence Learning with Neural Networks
  year: 2014
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 2275
  pid: a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8
  title: Bottom-Up and Top-Down Attention for Image Captioning and Visual Question
    Answering
  year: 2018
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 95324
  pid: 2c03df8b48bf3fa39054345bafabfeff15bfd11d
  title: Deep Residual Learning for Image Recognition
  year: 2016
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 90063
  pid: a6cb366736791bcccc5c8639de5a8f9636bf87e8
  title: 'Adam: A Method for Stochastic Optimization'
  year: 2015
- fieldsOfStudy:
  - Computer Science
  - Environmental Science
  numCitedBy: 14073
  pid: 7ffdbc358b63378f07311e883dddacc9faeeaf4b
  title: Fast R-CNN
  year: 2015
slug: Towards-Learning-a-Generic-Agent-for-Navigation-via-Hao-Li
title: Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-Training
url: https://www.semanticscholar.org/paper/Towards-Learning-a-Generic-Agent-for-Navigation-via-Hao-Li/6fa25c94e41a0c90e3aabe80cf60f59ec9ff0a52?sort=total-citations
venue: 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
year: 2020
