authors:
- Geoffrey E. Hinton
- Oriol Vinyals
- J. Dean
badges:
- id: OPEN_ACCESS
corpusId: 7200347
fieldsOfStudy:
- Computer Science
numCitedBy: 8702
numCiting: 14
paperAbstract: A very simple way to improve the performance of almost any machine
  learning algorithm is to train many different models on the same data and then to
  average their predictions. Unfortunately, making predictions using a whole ensemble
  of models is cumbersome and may be too computationally expensive to allow deployment
  to a large number of users, especially if the individual models are large neural
  nets. Caruana and his collaborators have shown that it is possible to compress the
  knowledge in an ensemble into a single model which is much easier to deploy and
  we develop this approach further using a different compression technique. We achieve
  some surprising results on MNIST and we show that we can significantly improve the
  acoustic model of a heavily used commercial system by distilling the knowledge in
  an ensemble of models into a single model. We also introduce a new type of ensemble
  composed of one or more full models and many specialist models which learn to distinguish
  fine-grained classes that the full models confuse. Unlike a mixture of experts,
  these specialist models can be trained rapidly and in parallel.
ref_count: 14
references:
- pid: 34f25a8704614163c4095b3ee2fc969b60de4698
  title: 'Dropout: a simple way to prevent neural networks from overfitting'
- pid: abd1c342495432171beb7ca8fd9551ef13cbd0ff
  title: ImageNet classification with deep convolutional neural networks
- pid: 1366de5bb112746a555e9c0cd00de3ad8628aea8
  title: Improving neural networks by preventing co-adaptation of feature detectors
- pid: 3127190433230b3dc1abd0680bb58dced4bcd90e
  title: Large Scale Distributed Deep Networks
- pid: 30c9bb327b7f2b9f1d1e5b69b9d0c97b410948d9
  title: Model compression
- pid: c8d90974c3f3b40fa05e322df2905fc16204aa56
  title: Adaptive Mixtures of Local Experts
- pid: 31868290adf1c000c611dfc966b514d5a34e8d23
  title: 'Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared
    Views of Four Research Groups'
slug: Distilling-the-Knowledge-in-a-Neural-Network-Hinton-Vinyals
title: Distilling the Knowledge in a Neural Network
url: https://www.semanticscholar.org/paper/Distilling-the-Knowledge-in-a-Neural-Network-Hinton-Vinyals/0c908739fbff75f03469d13d4a1a07de3414ee19?sort=total-citations
venue: ArXiv
year: 2015
