authors:
- Geoffrey E. Hinton
- Oriol Vinyals
- J. Dean
badges:
- id: OPEN_ACCESS
corpusId: 7200347
fieldsOfStudy:
- Computer Science
meta_key: distilling-the-knowledge-in-a-neural-network
numCitedBy: 8699
numCiting: 14
paperAbstract: A very simple way to improve the performance of almost any machine
  learning algorithm is to train many different models on the same data and then to
  average their predictions. Unfortunately, making predictions using a whole ensemble
  of models is cumbersome and may be too computationally expensive to allow deployment
  to a large number of users, especially if the individual models are large neural
  nets. Caruana and his collaborators have shown that it is possible to compress the
  knowledge in an ensemble into a single model which is much easier to deploy and
  we develop this approach further using a different compression technique. We achieve
  some surprising results on MNIST and we show that we can significantly improve the
  acoustic model of a heavily used commercial system by distilling the knowledge in
  an ensemble of models into a single model. We also introduce a new type of ensemble
  composed of one or more full models and many specialist models which learn to distinguish
  fine-grained classes that the full models confuse. Unlike a mixture of experts,
  these specialist models can be trained rapidly and in parallel.
ref_count: 14
references:
- fieldsOfStudy:
  - Computer Science
  meta_key: dropout-a-simple-way-to-prevent-neural-networks-from-overfitting
  numCitedBy: 28149
  pid: 34f25a8704614163c4095b3ee2fc969b60de4698
  show_ref_link: true
  title: Dropout - a simple way to prevent neural networks from overfitting
  year: 2014
- fieldsOfStudy:
  - Computer Science
  meta_key: learning-small-size-dnn-with-output-distribution-based-criteria
  numCitedBy: 236
  pid: 8d25d04051074be7590cbe5e4e34c45bb26674e1
  show_ref_link: false
  title: Learning small-size DNN with output-distribution-based criteria
  year: 2014
- fieldsOfStudy:
  - Computer Science
  meta_key: imagenet-classification-with-deep-convolutional-neural-networks
  numCitedBy: 80947
  pid: abd1c342495432171beb7ca8fd9551ef13cbd0ff
  show_ref_link: true
  title: ImageNet classification with deep convolutional neural networks
  year: 2012
- fieldsOfStudy:
  - Computer Science
  meta_key: improving-neural-networks-by-preventing-co-adaptation-of-feature-detectors
  numCitedBy: 6191
  pid: 1366de5bb112746a555e9c0cd00de3ad8628aea8
  show_ref_link: true
  title: Improving neural networks by preventing co-adaptation of feature detectors
  year: 2012
- fieldsOfStudy:
  - Computer Science
  meta_key: large-scale-distributed-deep-networks
  numCitedBy: 3026
  pid: 3127190433230b3dc1abd0680bb58dced4bcd90e
  show_ref_link: true
  title: Large Scale Distributed Deep Networks
  year: 2012
- fieldsOfStudy:
  - Computer Science
  meta_key: model-compression
  numCitedBy: 1451
  pid: 30c9bb327b7f2b9f1d1e5b69b9d0c97b410948d9
  show_ref_link: false
  title: Model compression
  year: 2006
- fieldsOfStudy:
  - Computer Science
  meta_key: adaptive-mixtures-of-local-experts
  numCitedBy: 4007
  pid: c8d90974c3f3b40fa05e322df2905fc16204aa56
  show_ref_link: false
  title: Adaptive Mixtures of Local Experts
  year: 1991
- fieldsOfStudy:
  - Computer Science
  meta_key: deep-neural-networks-for-acoustic-modeling-in-speech-recognition-the-shared-views-of-four-research-groups
  numCitedBy: 7452
  pid: 31868290adf1c000c611dfc966b514d5a34e8d23
  show_ref_link: true
  title: Deep Neural Networks for Acoustic Modeling in Speech Recognition - The Shared
    Views of Four Research Groups
  year: 2012
- fieldsOfStudy:
  - Computer Science
  meta_key: ensemble-methods-in-machine-learning
  numCitedBy: 6134
  pid: 3b900b13a8ae7814c1fb00960ef1da66c4580859
  show_ref_link: false
  title: Ensemble Methods in Machine Learning
  year: 2000
slug: Distilling-the-Knowledge-in-a-Neural-Network-Hinton-Vinyals
title: Distilling the Knowledge in a Neural Network
url: https://www.semanticscholar.org/paper/Distilling-the-Knowledge-in-a-Neural-Network-Hinton-Vinyals/0c908739fbff75f03469d13d4a1a07de3414ee19?sort=total-citations
venue: ArXiv
year: 2015
