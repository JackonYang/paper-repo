authors:
- James Martens
- Ilya Sutskever
badges:
- id: OPEN_ACCESS
corpusId: 9153163
fieldsOfStudy:
- Computer Science
numCitedBy: 585
numCiting: 21
paperAbstract: 'In this work we resolve the long-outstanding problem of how to effectively
  train recurrent neural networks (RNNs) on complex and difficult sequence modeling
  problems which may contain long-term data dependencies. Utilizing recent advances
  in the Hessian-free optimization approach (Martens, 2010), together with a novel
  damping scheme, we successfully train RNNs on two sets of challenging problems.
  First, a collection of pathological synthetic datasets which are known to be impossible
  for standard optimization approaches (due to their extremely long-term dependencies),
  and second, on three natural and highly complex real-world sequence datasets where
  we find that our method significantly outperforms the previous state-of-the-art
  method for training neural sequence models: the Long Short-term Memory approach
  of Hochreiter and Schmidhuber (1997). Additionally, we offer a new interpretation
  of the generalized Gauss-Newton matrix of Schraudolph (2002) which is used within
  the HF approach of Martens.'
ref_count: 21
references:
- pid: d0be39ee052d246ae99c082a565aba25b811be2d
  title: Learning long-term dependencies with gradient descent is difficult
- pid: 4c46347fbc272b21468efe3d9af34b4b2bad6684
  title: Deep learning via Hessian-free optimization
- pid: 44d2abe2175df8153f465f6c39b68b76a0d40ab9
  title: Long Short-Term Memory
- pid: 11540131eae85b2e11d53df7f1360eeb6476e7f4
  title: 'Learning to Forget: Continual Prediction with LSTM'
- pid: 052b1d8ce63b07fec3de9dbb583772d860b7c769
  title: Learning representations by back-propagating errors
- pid: c6867b6b564462d6b902f68e0bfa58f4717ca1cc
  title: Fast Exact Multiplication by the Hessian
- pid: 030ba5a03666bf4c3a17c64699f8de8ec13d623b
  title: Bridging Long Time Lags by Weight Guessing and \long Short Term Memory"
- pid: 46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e
  title: Reducing the Dimensionality of Data with Neural Networks
- pid: aed054834e2c696807cc8b227ac7a4197196e211
  title: 'Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies'
- pid: 2f83f6e1afadf0963153974968af6b8342775d82
  title: Framewise phoneme classification with bidirectional LSTM and other neural
    network architectures
- pid: ffa94bba647817fa5e8f8d3250fc977435b5ca76
  title: Fast Curvature Matrix-Vector Products for Second-Order Gradient Descent
- pid: c08d0525bd42fa1c24f9f5df72f4c8fcf7063b22
  title: Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks
- pid: 0d073966e48ffb6dccde1e4eb3f0380c10c6a766
  title: 'Harnessing Nonlinearity: Predicting Chaotic Systems and Saving Energy in
    Wireless Communication'
- pid: 43c3bfffdcd313c549b2045980855ea001d6f13b
  title: Numerical Optimization
- pid: 3f3d13e95c25a8f6a753e38dfce88885097cbd43
  title: Untersuchungen zu dynamischen neuronalen Netzen
slug: Learning-Recurrent-Neural-Networks-with-Martens-Sutskever
title: Learning Recurrent Neural Networks with Hessian-Free Optimization
url: https://www.semanticscholar.org/paper/Learning-Recurrent-Neural-Networks-with-Martens-Sutskever/0d6203718c15f137fda2f295c96269bc2b254644?sort=total-citations
venue: ICML
year: 2011
