authors:
- Yen-Chun Chen
- Linjie Li
- Licheng Yu
- Ahmed El Kholy
- Faisal Ahmed
- Zhe Gan
- Yu Cheng
- Jingjing Liu
badges:
- id: OPEN_ACCESS
corpusId: 202889174
fieldsOfStudy:
- Computer Science
meta_key: uniter-learning-universal-image-text-representations
numCitedBy: 282
numCiting: 56
paperAbstract: 'Joint image-text embedding is the bedrock for most Vision-and-Language
  (V+L) tasks, where multimodality inputs are jointly processed for visual and textual
  understanding. In this paper, we introduce UNITER, a UNiversal Image-TExt Representation,
  learned through large-scale pre-training over four image-text datasets (COCO, Visual
  Genome, Conceptual Captions, and SBU Captions), which can power heterogeneous downstream
  V+L tasks with joint multimodal embeddings. We design three pre-training tasks:
  Masked Language Modeling (MLM), Image-Text Matching (ITM), and Masked Region Modeling
  (MRM, with three variants). Different from concurrent work on multimodal pre-training
  that apply joint random masking to both modalities, we use conditioned masking on
  pre-training tasks (i.e., masked language/region modeling is conditioned on full
  observation of image/text). Comprehensive analysis shows that conditioned masking
  yields better performance than unconditioned masking. We also conduct a thorough
  ablation study to find an optimal setting for the combination of pre-training tasks.
  Extensive experiments show that UNITER achieves new state of the art across six
  V+L tasks (over nine datasets), including Visual Question Answering, Image-Text
  Retrieval, Referring Expression Comprehension, Visual Commonsense Reasoning, Visual
  Entailment, and NLVR2.'
ref_count: 57
references:
- fieldsOfStudy:
  - Computer Science
  meta_key: unicoder-vl-a-universal-encoder-for-vision-and-language-by-cross-modal-pre-training
  numCitedBy: 382
  pid: 2bc1c8bd00bbf7401afcb5460277840fd8bab029
  show_ref_link: true
  title: 'Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal
    Pre-training'
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: learning-deep-structure-preserving-image-text-embeddings
  numCitedBy: 582
  pid: b27e791e843c924ef052981b79490ab59fc0433d
  show_ref_link: true
  title: Learning Deep Structure-Preserving Image-Text Embeddings
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: stacked-cross-attention-for-image-text-matching
  numCitedBy: 481
  pid: 45dd2a3cd7c27f2e9509b023d702408f5ac11c9d
  show_ref_link: true
  title: Stacked Cross Attention for Image-Text Matching
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: vilbert-pretraining-task-agnostic-visiolinguistic-representations-for-vision-and-language-tasks
  numCitedBy: 1266
  pid: 65a9c7b0800c86a196bc14e7621ff895cc6ab287
  show_ref_link: true
  title: 'ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language
    Tasks'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: multimodal-compact-bilinear-pooling-for-visual-question-answering-and-visual-grounding
  numCitedBy: 1086
  pid: fddc15480d086629b960be5bff96232f967f2252
  show_ref_link: true
  title: Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual
    Grounding
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: videobert-a-joint-model-for-video-and-language-representation-learning
  numCitedBy: 569
  pid: c41a11c0e9b8b92b4faaf97749841170b760760a
  show_ref_link: true
  title: 'VideoBERT: A Joint Model for Video and Language Representation Learning'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: mattnet-modular-attention-network-for-referring-expression-comprehension
  numCitedBy: 369
  pid: fdce9cbe5c726201575b3c8a8c1af0752f1af53f
  show_ref_link: true
  title: 'MAttNet: Modular Attention Network for Referring Expression Comprehension'
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: context-encoders-feature-learning-by-inpainting
  numCitedBy: 3342
  pid: 7d0effebfa4bed19b6ba41f3af5b7e5b6890de87
  show_ref_link: true
  title: 'Context Encoders: Feature Learning by Inpainting'
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: deep-visual-semantic-alignments-for-generating-image-descriptions
  numCitedBy: 2575
  pid: 55e022fb7581bb9e1fce678d21fb25ffbb3fbb88
  show_ref_link: true
  title: Deep Visual-Semantic Alignments for Generating Image Descriptions
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: lxmert-learning-cross-modality-encoder-representations-from-transformers
  numCitedBy: 916
  pid: 79c93274429d6355959f1e4374c2147bb81ea649
  show_ref_link: true
  title: 'LXMERT: Learning Cross-Modality Encoder Representations from Transformers'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: 12-in-1-multi-task-vision-and-language-representation-learning
  numCitedBy: 227
  pid: 6548a60a6bcdf6c402d9de1c05ba7afe4f49fee9
  show_ref_link: true
  title: '12-in-1: Multi-Task Vision and Language Representation Learning'
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: deep-modular-co-attention-networks-for-visual-question-answering
  numCitedBy: 323
  pid: 8a1744da011375d711ed75fc2d160c6fdca2cf89
  show_ref_link: true
  title: Deep Modular Co-Attention Networks for Visual Question Answering
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: visualbert-a-simple-and-performant-baseline-for-vision-and-language
  numCitedBy: 630
  pid: 5aec474c31a2f4b74703c6f786c0a8ff85c450da
  show_ref_link: true
  title: 'VisualBERT: A Simple and Performant Baseline for Vision and Language'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: visual-entailment-a-novel-task-for-fine-grained-image-understanding
  numCitedBy: 99
  pid: 3c54b796cc10cb530f77caa4d18e1c80ac863822
  show_ref_link: false
  title: 'Visual Entailment: A Novel Task for Fine-Grained Image Understanding'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: vl-bert-pre-training-of-generic-visual-linguistic-representations
  numCitedBy: 707
  pid: 2527626c11a84f15709e943fbfa2356e19930e3b
  show_ref_link: true
  title: 'VL-BERT: Pre-training of Generic Visual-Linguistic Representations'
  year: 2020
- fieldsOfStudy:
  - Computer Science
  - Environmental Science
  meta_key: conceptual-captions-a-cleaned-hypernymed-image-alt-text-dataset-for-automatic-image-captioning
  numCitedBy: 632
  pid: b4df354db88a70183a64dbc9e56cf14e7669a6c0
  show_ref_link: true
  title: 'Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic
    Image Captioning'
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: unsupervised-visual-representation-learning-by-context-prediction
  numCitedBy: 1765
  pid: fc1b1c9364c58ec406f494dd944b609a6a038ba6
  show_ref_link: true
  title: Unsupervised Visual Representation Learning by Context Prediction
  year: 2015
- fieldsOfStudy:
  - Computer Science
  meta_key: unsupervised-representation-learning-by-predicting-image-rotations
  numCitedBy: 1664
  pid: aab368284210c1bb917ec2d31b84588e3d2d7eb4
  show_ref_link: true
  title: Unsupervised Representation Learning by Predicting Image Rotations
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: flickr30k-entities-collecting-region-to-phrase-correspondences-for-richer-image-to-sentence-models
  numCitedBy: 664
  pid: 0612745dbd292fc0a548a16d39cd73e127faedde
  show_ref_link: false
  title: 'Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer
    Image-to-Sentence Models'
  year: 2015
- fieldsOfStudy:
  - Computer Science
  meta_key: language-models-are-unsupervised-multitask-learners
  numCitedBy: 6284
  pid: 9405cc0d6169988371b2755e573cc28650d14dfe
  show_ref_link: true
  title: Language Models are Unsupervised Multitask Learners
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: attention-is-all-you-need
  numCitedBy: 35148
  pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  show_ref_link: true
  title: Attention is All you Need
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding
  numCitedBy: 33744
  pid: df2b0e26d0599ce3e70df8a9da02e51594e0e992
  show_ref_link: true
  title: 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: bottom-up-and-top-down-attention-for-image-captioning-and-visual-question-answering
  numCitedBy: 2275
  pid: a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8
  show_ref_link: true
  title: Bottom-Up and Top-Down Attention for Image Captioning and Visual Question
    Answering
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: unsupervised-learning-of-visual-representations-by-solving-jigsaw-puzzles
  numCitedBy: 1664
  pid: 2ec8f7e0257a07d3914322b36072d1bbcd58a1e0
  show_ref_link: true
  title: Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: fusion-of-detected-objects-in-text-for-visual-question-answering
  numCitedBy: 111
  pid: b82153bf85d5d1edd3f170aace830e5328ca9ed0
  show_ref_link: true
  title: Fusion of Detected Objects in Text for Visual Question Answering
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: albert-a-lite-bert-for-self-supervised-learning-of-language-representations
  numCitedBy: 2706
  pid: 7a064df1aeada7e69e5173f7d4c8606f4470365b
  show_ref_link: true
  title: 'ALBERT: A Lite BERT for Self-supervised Learning of Language Representations'
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: bilinear-attention-networks
  numCitedBy: 408
  pid: a5d10341717c0519cf63151b496a6d2ed67aa05f
  show_ref_link: true
  title: Bilinear Attention Networks
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: xlnet-generalized-autoregressive-pretraining-for-language-understanding
  numCitedBy: 4226
  pid: e0c6abdbdecf04ffac65c440da77fb9d66bb474c
  show_ref_link: true
  title: 'XLNet: Generalized Autoregressive Pretraining for Language Understanding'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: visual-genome-connecting-language-and-vision-using-crowdsourced-dense-image-annotations
  numCitedBy: 2772
  pid: afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d
  show_ref_link: true
  title: 'Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image
    Annotations'
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: from-recognition-to-cognition-visual-commonsense-reasoning
  numCitedBy: 372
  pid: 6dfc2ff03534a4325d06c6f88c3144831996629b
  show_ref_link: true
  title: 'From Recognition to Cognition: Visual Commonsense Reasoning'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: colorful-image-colorization
  numCitedBy: 2297
  pid: 8201e6e687f2de477258e9be53ba7b73ee30d7de
  show_ref_link: true
  title: Colorful Image Colorization
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: learning-video-representations-using-contrastive-bidirectional-transformer
  numCitedBy: 170
  pid: 025a0dc4a2a98742f1b410b6318a46de2c854b22
  show_ref_link: true
  title: Learning Video Representations using Contrastive Bidirectional Transformer
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: deep-contextualized-word-representations
  numCitedBy: 7987
  pid: 3febb2bed8865945e7fddc99efd791887bb7e14f
  show_ref_link: true
  title: Deep Contextualized Word Representations
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: learning-generative-models-with-sinkhorn-divergences
  numCitedBy: 393
  pid: a1bc7d90564c342beb75cedf36fd921de89d94ad
  show_ref_link: true
  title: Learning Generative Models with Sinkhorn Divergences
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: modeling-context-in-referring-expressions
  numCitedBy: 378
  pid: 29efbe391950ae438c63d86ad5c82b2942efb0b4
  show_ref_link: true
  title: Modeling Context in Referring Expressions
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: nlvr2-visual-bias-analysis
  numCitedBy: 5
  pid: 8e86dd59429e8b7fd34b6893c2dea3921974c328
  show_ref_link: false
  title: NLVR2 Visual Bias Analysis
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: gqa-a-new-dataset-for-real-world-visual-reasoning-and-compositional-question-answering
  numCitedBy: 448
  pid: 1ab7f7c1d328589f25c79515b9a5d824d7ffbbd1
  show_ref_link: true
  title: 'GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question
    Answering'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: microsoft-coco-common-objects-in-context
  numCitedBy: 19778
  pid: 71b7178df5d2b112d07e45038cb5637208659ff7
  show_ref_link: true
  title: 'Microsoft COCO: Common Objects in Context'
  year: 2014
- fieldsOfStudy:
  - Computer Science
  meta_key: scaling-neural-machine-translation
  numCitedBy: 474
  pid: bf8fe437f779f2098f9af82b534aa51dc9edb06f
  show_ref_link: true
  title: Scaling Neural Machine Translation
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: google-s-neural-machine-translation-system-bridging-the-gap-between-human-and-machine-translation
  numCitedBy: 4645
  pid: dbde7dfa6cae81df8ac19ef500c42db96c3d1edd
  show_ref_link: true
  title: 'Google''s Neural Machine Translation System: Bridging the Gap between Human
    and Machine Translation'
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: im2text-describing-images-using-1-million-captioned-photographs
  numCitedBy: 734
  pid: 8e080b98efbe65c02a116439205ca2344b9f7cd4
  show_ref_link: false
  title: 'Im2Text: Describing Images Using 1 Million Captioned Photographs'
  year: 2011
- fieldsOfStudy:
  - Computer Science
  meta_key: distilling-the-knowledge-in-a-neural-network
  numCitedBy: 8699
  pid: 0c908739fbff75f03469d13d4a1a07de3414ee19
  show_ref_link: true
  title: Distilling the Knowledge in a Neural Network
  year: 2015
- fieldsOfStudy:
  - Computer Science
  meta_key: roberta-a-robustly-optimized-bert-pretraining-approach
  numCitedBy: 7266
  pid: 077f8329a7b6fa3b7c877a57b81eb6c18b5f87de
  show_ref_link: true
  title: 'RoBERTa: A Robustly Optimized BERT Pretraining Approach'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: a-corpus-for-reasoning-about-natural-language-grounded-in-photographs
  numCitedBy: 211
  pid: cf336d272a30d6ad6141db67faa64deb8791cd61
  show_ref_link: true
  title: A Corpus for Reasoning about Natural Language Grounded in Photographs
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: revealing-the-dark-secrets-of-bert
  numCitedBy: 290
  pid: d78aed1dac6656affa4a04cbf225ced11a83d103
  show_ref_link: true
  title: Revealing the Dark Secrets of BERT
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: referitgame-referring-to-objects-in-photographs-of-natural-scenes
  numCitedBy: 563
  pid: 92c141447f51b6732242376164ff961e464731c8
  show_ref_link: false
  title: 'ReferItGame: Referring to Objects in Photographs of Natural Scenes'
  year: 2014
- fieldsOfStudy:
  - Computer Science
  meta_key: decoupled-weight-decay-regularization
  numCitedBy: 3475
  pid: d07284a6811f1b2745d91bdb06b040b57f226882
  show_ref_link: true
  title: Decoupled Weight Decay Regularization
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: improving-gans-using-optimal-transport
  numCitedBy: 207
  pid: 69902406e7d08f8865f02185699978db499d25e7
  show_ref_link: true
  title: Improving GANs Using Optimal Transport
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: automatic-differentiation-in-pytorch
  numCitedBy: 10323
  pid: b36a5bb1707bb9c70025294b3a310138aae8327a
  show_ref_link: true
  title: Automatic differentiation in PyTorch
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: a-fast-proximal-point-method-for-wasserstein-distance
  numCitedBy: 37
  pid: 7b54a851675cc73367cd28c296d393564ebe55f5
  show_ref_link: false
  title: A Fast Proximal Point Method for Wasserstein Distance
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: wasserstein-generative-adversarial-networks
  numCitedBy: 3929
  pid: acd87843a451d18b4dc6474ddce1ae946429eaf1
  show_ref_link: true
  title: Wasserstein Generative Adversarial Networks
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: a-fast-proximal-point-method-for-computing-wasserstein-distance
  numCitedBy: 16
  pid: a2aafb1cdfdf657b3ee7336071d5a4cbae6385f7
  show_ref_link: false
  title: A Fast Proximal Point Method for Computing Wasserstein Distance
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: sinkhorn-distances-lightspeed-computation-of-optimal-transport
  numCitedBy: 1873
  pid: 0080118b0eb02af581ff32b85a1bb6aed7081f45
  show_ref_link: true
  title: 'Sinkhorn Distances: Lightspeed Computation of Optimal Transport'
  year: 2013
- fieldsOfStudy:
  - Geology
  meta_key: computational-optimal-transport
  numCitedBy: 960
  pid: 8e51d68250db5637cd6bc1de98a99396441399b2
  show_ref_link: true
  title: Computational Optimal Transport
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: selfie-self-supervised-pretraining-for-image-embedding
  numCitedBy: 68
  pid: 5466ee5f16fc3c776fd1da667917592e5fd06720
  show_ref_link: false
  title: 'Selfie: Self-supervised Pretraining for Image Embedding'
  year: 2019
- fieldsOfStudy: []
  meta_key: vqa-visual-question-answering
  numCitedBy: 0
  pid: 784da2a7b53a16d2243f747e14946cc5e3476af0
  show_ref_link: false
  title: 'VQA: Visual Question Answering'
  year: 2015
slug: UNITER:-Learning-UNiversal-Image-TExt-Chen-Li
title: 'UNITER: Learning UNiversal Image-TExt Representations'
url: https://www.semanticscholar.org/paper/UNITER:-Learning-UNiversal-Image-TExt-Chen-Li/54416048772b921720f19869ed11c2a360589d03?sort=total-citations
venue: ECCV 2020
year: 2019
