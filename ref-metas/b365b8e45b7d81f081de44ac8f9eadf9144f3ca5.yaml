authors:
- R. Tibshirani
badges:
- id: OPEN_ACCESS
corpusId: 16162039
fieldsOfStudy:
- Computer Science
numCitedBy: 36493
numCiting: 27
paperAbstract: 'SUMMARY We propose a new method for estimation in linear models. The
  ''lasso'' minimizes the residual sum of squares subject to the sum of the absolute
  value of the coefficients being less than a constant. Because of the nature of this
  constraint it tends to produce some coefficients that are exactly 0 and hence gives
  interpretable models. Our simulation studies suggest that the lasso enjoys some
  of the favourable properties of both subset selection and ridge regression. It produces
  interpretable models like subset selection and exhibits the stability of ridge regression.
  There is also an interesting relationship with recent work in adaptive function
  estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied
  in a variety of statistical models: extensions to generalized regression models
  and tree-based models are briefly described.'
ref_count: 27
references:
- pid: 961e2156d523e3901c491cc2a1f65764c976fc44
  title: Reversible jump Markov chain Monte Carlo computation and Bayesian model determination
- pid: 8017699564136f93af21575810d557dba1ee6fc6
  title: Classification and Regression Trees
- pid: b8d9abd1c078573188b13d36c1b1efb7cb2fa865
  title: Practical optimization
slug: Regression-Shrinkage-and-Selection-via-the-Lasso-Tibshirani
title: Regression Shrinkage and Selection via the Lasso
url: https://www.semanticscholar.org/paper/Regression-Shrinkage-and-Selection-via-the-Lasso-Tibshirani/b365b8e45b7d81f081de44ac8f9eadf9144f3ca5?sort=total-citations
venue: ''
year: 1996
