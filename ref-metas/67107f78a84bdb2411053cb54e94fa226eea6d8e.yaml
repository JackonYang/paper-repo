authors:
- Xavier Glorot
- Antoine Bordes
- Yoshua Bengio
badges:
- id: OPEN_ACCESS
corpusId: 2239473
fieldsOfStudy:
- Computer Science
numCitedBy: 5919
numCiting: 40
paperAbstract: While logistic sigmoid neurons are more biologically plausible than
  hyperbolic tangent neurons, the latter work better for training multi-layer neural
  networks. This paper shows that rectifying neurons are an even better model of biological
  neurons and yield equal or better performance than hyperbolic tangent networks in
  spite of the hard non-linearity and non-dierentiabil ity
ref_count: 40
references:
- pid: 355d44f53428b1ac4fb2ab468d593c720640e5bd
  title: Greedy Layer-Wise Training of Deep Networks
- pid: b71ac1e9fb49420d13e084ac67254a0bbd40f83f
  title: Understanding the difficulty of training deep feedforward neural networks
- pid: a538b05ebb01a40323997629e171c91aa28b8e2f
  title: Rectified Linear Units Improve Restricted Boltzmann Machines
- pid: 8978cf7574ceb35f4c3096be768c7547b28a35d0
  title: A Fast Learning Algorithm for Deep Belief Nets
- pid: 41fef1a197fab9684a4608b725d3ae72e1ab4b39
  title: Sparse Feature Learning for Deep Belief Networks
- pid: e60ff004dde5c13ec53087872cfcdd12e85beb57
  title: Learning Deep Architectures for AI
- pid: 932c2a02d462abd75af018125413b1ceaa1ee3f4
  title: Efficient Learning of Sparse Representations with an Energy-Based Model
- pid: 202cbbf671743aefd380d2f23987bd46b9caaf97
  title: Sparse deep belief net model for visual area V2
- pid: 5d90f06bb70a0a3dced62413346235c02b1aa086
  title: Learning Multiple Layers of Features from Tiny Images
- pid: 2805537bec87a6177037b18f9a3a9d3f1038867b
  title: 'Sparse coding with an overcomplete basis set: A strategy employed by V1?'
- pid: e64a9960734215e2b1866ea3cb723ffa5585ac14
  title: Efficient sparse coding algorithms
- pid: 3137bc367c61c0e507a5e3c1f8caeb26f292d79f
  title: Measuring Invariances in Deep Networks
- pid: 843959ffdccf31c6694d135fad07425924f785b1
  title: Extracting and composing robust features with denoising autoencoders
- pid: 0d2336389dff3031910bd21dd1c44d1b4cd51725
  title: Why Does Unsupervised Pre-training Help Deep Learning?
- pid: 162d958ff885f1462aeda91cd72582323fd6a1f4
  title: Gradient-based learning applied to document recognition
- pid: 9d65ba8bb20ae6dd001b9833c525c279dfe18916
  title: Supervised Dictionary Learning
- pid: 1f88427d7aa8225e47f946ac41a0667d7b69ac52
  title: What is the best multi-stage architecture for object recognition?
- pid: f354310098e09c1e1dc88758fca36767fd9d084d
  title: Learning methods for generic object recognition with invariance to pose and
    lighting
- pid: b87274e6d9aa4e6ba5148898aa92941617d2b6ed
  title: Efficient BackProp
slug: Deep-Sparse-Rectifier-Neural-Networks-Glorot-Bordes
title: Deep Sparse Rectifier Neural Networks
url: https://www.semanticscholar.org/paper/Deep-Sparse-Rectifier-Neural-Networks-Glorot-Bordes/67107f78a84bdb2411053cb54e94fa226eea6d8e?sort=total-citations
venue: AISTATS
year: 2011
