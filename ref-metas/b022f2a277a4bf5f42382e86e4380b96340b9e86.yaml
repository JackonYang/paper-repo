authors:
- I. Loshchilov
- F. Hutter
badges:
- id: OPEN_ACCESS
corpusId: 14337532
fieldsOfStudy:
- Computer Science
numCitedBy: 2724
numCiting: 45
paperAbstract: Restart techniques are common in gradient-free optimization to deal
  with multimodal functions. Partial warm restarts are also gaining popularity in
  gradient-based optimization to improve the rate of convergence in accelerated gradient
  schemes to deal with ill-conditioned functions. In this paper, we propose a simple
  warm restart technique for stochastic gradient descent to improve its anytime performance
  when training deep neural networks. We empirically study its performance on the
  CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results
  at 3.14% and 16.21%, respectively. We also demonstrate its advantages on a dataset
  of EEG recordings and on a downsampled version of the ImageNet dataset. Our source
  code is available at this https URL
ref_count: 45
references:
- pid: 22ba26e56fc3e68f2e6a96c60d27d5f721ea00e9
  title: Equilibrated adaptive learning rates for non-convex optimization
- pid: a6cb366736791bcccc5c8639de5a8f9636bf87e8
  title: 'Adam: A Method for Stochastic Optimization'
- pid: 1bdf014c1bd613dbdc656e074379badc4ae492dc
  title: RMSProp and equilibrated adaptive learning rates for non-convex optimization.
- pid: b44ff78214ccd975ce16fbbc333423ca78d99141
  title: 'SGD-QN: Careful Quasi-Newton Stochastic Gradient Descent'
- pid: 51db1f3c8dfc7d4077da39c96bb90a6358128111
  title: Deep Networks with Stochastic Depth
- pid: 77f0a39b8e02686fd85b01971f8feb7f60971f80
  title: Identity Mappings in Deep Residual Networks
- pid: 8729441d734782c3ed532a7d2d9611b438c0a09a
  title: 'ADADELTA: An Adaptive Learning Rate Method'
- pid: 1c4e9156ca07705531e45960b7a919dc473abb51
  title: Wide Residual Networks
- pid: abd1c342495432171beb7ca8fd9551ef13cbd0ff
  title: ImageNet classification with deep convolutional neural networks
- pid: 2c03df8b48bf3fa39054345bafabfeff15bfd11d
  title: Deep Residual Learning for Image Recognition
- pid: b8de958fead0d8a9619b55c7299df3257c624a96
  title: 'DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition'
- pid: 5d90f06bb70a0a3dced62413346235c02b1aa086
  title: Learning Multiple Layers of Features from Tiny Images
- pid: 5694e46284460a648fe29117cbc55f6c9be3fa3c
  title: Densely Connected Convolutional Networks
- pid: 981ce6b655cc06416ff6bf7fac8c6c2076fd7fac
  title: Identifying and attacking the saddle point problem in high-dimensional non-convex
    optimization
- pid: 1267fe36b5ece49a9d8f913eb67716a040bbcced
  title: On the limited memory BFGS method for large scale optimization
- pid: bcd857d75841aa3e92cd4284a8818aba9f6c0c3f
  title: Published as a conference paper at ICLR 2018 S IMULATING A CTION D YNAMICS
    WITH N EURAL P ROCESS N ETWORKS
slug: SGDR:-Stochastic-Gradient-Descent-with-Warm-Loshchilov-Hutter
title: 'SGDR: Stochastic Gradient Descent with Warm Restarts'
url: https://www.semanticscholar.org/paper/SGDR:-Stochastic-Gradient-Descent-with-Warm-Loshchilov-Hutter/b022f2a277a4bf5f42382e86e4380b96340b9e86?sort=total-citations
venue: ICLR
year: 2017
