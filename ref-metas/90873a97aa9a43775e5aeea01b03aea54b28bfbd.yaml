authors:
- Aishwarya Agrawal
- Dhruv Batra
- Devi Parikh
- Aniruddha Kembhavi
badges:
- id: UNPAYWALL
- id: OPEN_ACCESS
corpusId: 19298149
fieldsOfStudy:
- Computer Science
numCitedBy: 336
numCiting: 45
paperAbstract: A number of studies have found that today's Visual Question Answering
  (VQA) models are heavily driven by superficial correlations in the training data
  and lack sufficient image grounding. To encourage development of models geared towards
  the latter, we propose a new setting for VQA where for every question type, train
  and test sets have different prior distributions of answers. Specifically, we present
  new splits of the VQA v1 and VQA v2 datasets, which we call Visual Question Answering
  under Changing Priors (VQA-CP v1 and VQA-CP v2 respectively). First, we evaluate
  several existing VQA models under this new setting and show that their performance
  degrades significantly compared to the original VQA setting. Second, we propose
  a novel Grounded Visual Question Answering model (GVQA) that contains inductive
  biases and restrictions in the architecture specifically designed to prevent the
  model from 'cheating' by primarily relying on priors in the training data. Specifically,
  GVQA explicitly disentangles the recognition of visual concepts present in the image
  from the identification of plausible answer space for a given question, enabling
  the model to more robustly generalize across different distributions of answers.
  GVQA is built off an existing VQA model - Stacked Attention Networks (SAN). Our
  experiments demonstrate that GVQA significantly outperforms SAN on both VQA-CP v1
  and VQA-CP v2 datasets. Interestingly, it also outperforms more powerful VQA models
  such as Multimodal Compact Bilinear Pooling (MCB) in several cases. GVQA offers
  strengths complementary to SAN when trained and evaluated on the original VQA v1
  and VQA v2 datasets. Finally, GVQA is more transparent and interpretable than existing
  VQA models.
ref_count: 45
references:
- pid: a3d071d2a5c11329aa324b2cae6b7b6ca7800213
  title: 'C-VQA: A Compositional Split of the Visual Question Answering (VQA) v1.0
    Dataset'
- pid: a27ed1310b9c0832bde8f906e0fcd23d1f3aa79c
  title: 'Making the V in VQA Matter: Elevating the Role of Image Understanding in
    Visual Question Answering'
- pid: 915b5b12f9bdebc321e970ecd713458c3479d70e
  title: An Analysis of Visual Question Answering Algorithms
- pid: 8e759195eb4b4f0f480a8a2cf1c629bfd881d4e5
  title: Analyzing the Behavior of Visual Question Answering Models
- pid: def584565d05d6a8ba94de6621adab9e301d375d
  title: 'Visual7W: Grounded Question Answering in Images'
- pid: ebe5081b8a24b4740db929b6eae75f28f8edbc58
  title: Answer-Type Prediction for Visual Question Answering
- pid: 97ad70a9fa3f99adf18030e5e38ebe3d90daa2db
  title: 'VQA: Visual Question Answering'
- pid: 5fa973b8d284145bf0ced9acf2913a74674260f6
  title: 'Yin and Yang: Balancing and Answering Binary Visual Questions'
- pid: 20dbdf02497aa84510970d0f5e8b599073bca1bc
  title: 'Ask Me Anything: Free-Form Visual Question Answering Based on Knowledge
    from External Sources'
- pid: 3d1382fa43c31e594ed2d84dda9984b1db047b0e
  title: Compositional Memory for Visual Question Answering
- pid: 7214daf035ab005b3d1e739750dd597b4f4513fa
  title: A Focused Dynamic Attention Model for Visual Question Answering
- pid: 1cf6bc0866226c1f8e282463adc8b75d92fba9bb
  title: 'Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for
    Visual Question Answering'
- pid: fddc15480d086629b960be5bff96232f967f2252
  title: Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual
    Grounding
- pid: fb9d253258d6b3beceb9d6cd7bba6e0a29ab875b
  title: Hierarchical Question-Image Co-Attention for Visual Question Answering
- pid: 2fcd5cff2b4743ea640c4af68bf4143f4a2cccb1
  title: Are You Talking to a Machine? Dataset and Methods for Multilingual Image
    Question
- pid: 175e9bb50cc062c6c1742a5d90c8dfe31d2e4e22
  title: 'Where to Look: Focus Regions for Visual Question Answering'
- pid: 62a956d7600b10ca455076cd56e604dfd106072a
  title: Exploring Models and Data for Image Question Answering
- pid: b196bc11ad516c8e6ff96f83acfc443fd7161730
  title: 'ABC-CNN: An Attention Based Convolutional Neural Network for Visual Question
    Answering'
- pid: 0ac8f1a3c679b90d22c1f840cdc8d61ffef750ac
  title: Deep Compositional Question Answering with Neural Module Networks
- pid: 121a9a160f1f2819a01edbe522024b58dbfee798
  title: 'DualNet: Domain-invariant network for visual question answering'
- pid: 2c1890864c1c2b750f48316dc8b650ba4772adc5
  title: Stacked Attention Networks for Image Question Answering
- pid: b58e08741fb9803fa2a870eee139137d3bade332
  title: Training Recurrent Answering Units with Joint Loss Minimization for VQA
- pid: f96898d15a1bf1fa8925b1280d0e07a7a8e72194
  title: Dynamic Memory Networks for Visual and Textual Question Answering
- pid: 0b0a1cd432413978e4ef3d0418ebf3bb07af6c7a
  title: Explicit Knowledge-based Reasoning for Visual Question Answering
- pid: afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d
  title: 'Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image
    Annotations'
- pid: 75ddc7ee15be14013a3462c01b38b0548486fbcb
  title: Learning to Compose Neural Networks for Question Answering
- pid: ac64fb7e6d2ddf236332ec9f371fe85d308c114d
  title: A Multi-World Approach to Question Answering about Real-World Scenes based
    on Uncertain Input
- pid: 03eb382e04cca8cca743f7799070869954f1402a
  title: 'CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual
    Reasoning'
- pid: 050da5d159fb0dd96143948e1cffeb3dec814673
  title: Visual Turing test for computer vision systems
- pid: 1afb710a5b35a2352a6495e4bf6eef66808daf1b
  title: Multimodal Residual Learning for Visual QA
- pid: 0566bf06a0368b518b8b474166f7b1dfef3f9283
  title: Learning to detect unseen object classes by between-class attribute transfer
- pid: f37e1b62a767a307c046404ca96bc140b3e68cb5
  title: 'GloVe: Global Vectors for Word Representation'
- pid: eb42cf88027de515750f230b23b1a057dc782108
  title: Very Deep Convolutional Networks for Large-Scale Image Recognition
- pid: 3449b65008b27f6e60a73d80c1fd990f0481126b
  title: 'Torch7: A Matlab-like Environment for Machine Learning'
slug: Don't-Just-Assume;-Look-and-Answer:-Overcoming-for-Agrawal-Batra
title: 'Don''t Just Assume; Look and Answer: Overcoming Priors for Visual Question
  Answering'
url: https://www.semanticscholar.org/paper/Don't-Just-Assume;-Look-and-Answer:-Overcoming-for-Agrawal-Batra/90873a97aa9a43775e5aeea01b03aea54b28bfbd?sort=total-citations
venue: 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition
year: 2018
