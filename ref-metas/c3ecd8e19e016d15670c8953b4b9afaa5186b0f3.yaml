authors:
- Charles M. Bishop
badges:
- id: OPEN_ACCESS
corpusId: 16096318
fieldsOfStudy:
- Mathematics
numCitedBy: 993
numCiting: 42
paperAbstract: It is well known that the addition of noise to the input data of a
  neural network during training can, in some circumstances, lead to significant improvements
  in generalization performance. Previous work has shown that such training with noise
  is equivalent to a form of regularization in which an extra term is added to the
  error function. However, the regularization term, which involves second derivatives
  of the error function, is not bounded below, and so can lead to difficulties if
  used directly in a learning algorithm based on error minimization. In this paper
  we show that for the purposes of network training, the regularization term can be
  reduced to a positive semi-definite form that involves only first derivatives of
  the network mapping. For a sum-of-squares error function, the regularization term
  belongs to the class of generalized Tikhonov regularizers. Direct minimization of
  the regularized error function provides a practical alternative to training with
  noise.
ref_count: 42
references:
- pid: 030a977bf32e81fb694117d78ac84a3fbe2a1d81
  title: 'An analysis of noise in recurrent neural networks: convergence and generalization'
- pid: 3c39ddfbd9822230b5375d581bf505ecf6255283
  title: 'Curvature-driven smoothing: a learning algorithm for feedforward networks'
- pid: 3d565fb42892f20c52b9fc615cc537835f30d094
  title: On the Use of Evidence in Neural Networks
- pid: a42954d4b9d0ccdf1036e0af46d87a01b94c3516
  title: 'Second Order Derivatives for Network Pruning: Optimal Brain Surgeon'
- pid: a34e35dbbc6911fa7b94894dffdc0076a261b6f0
  title: Neural Networks and the Bias/Variance Dilemma
- pid: 2a1e1da81b535e1bead3fc2ab6af8b07877823b9
  title: Exact Calculation of the Hessian Matrix for the Multilayer Perceptron
- pid: c3fb617767f9e500e84ed03fb48acdcf088f33dc
  title: 'Ace of Bayes : Application of Neural'
- pid: b959164d1efca4b73986ba5d21e664aadbbc0457
  title: A Practical Bayesian Framework for Backpropagation Networks
- pid: e354ec85b8287bf15ed596be16ef6e422ccc29e7
  title: Creating artificial neural networks that generalize
- pid: c6867b6b564462d6b902f68e0bfa58f4717ca1cc
  title: Fast Exact Multiplication by the Hessian
- pid: b0f2433c088591d265891231f1c22424047f1bc1
  title: A Practical Bayesian Framework for Backprop Networks
- pid: e7297db245c3feb1897720b173a59fe7e36babb7
  title: Optimal Brain Damage
- pid: 33fdc91c520b54e097f5e09fae1cfc94793fbfcf
  title: Large Automatic Learning, Rule Extraction, and Generalization
- pid: c83684f6207697c12850db423fd9747572cf1784
  title: Bayesian Back-Propagation
- pid: f707a81a278d1598cd0a4493ba73f22dcdf90639
  title: Generalization by Weight-Elimination with Application to Forecasting
- pid: d275cf94e620bf5b3776bba8a88acccdcfcd9a19
  title: Bayesian Learning via Stochastic Dynamics
- pid: b8d9abd1c078573188b13d36c1b1efb7cb2fa865
  title: Practical optimization
- pid: bc14819e745cd7af37efd09ea29773dc0065119e
  title: Solutions of ill-posed problems
- pid: 20b844e395355b40fa5940c61362ec40e56027aa
  title: Neural networks
slug: Training-with-Noise-is-Equivalent-to-Tikhonov-Bishop
title: Training with Noise is Equivalent to Tikhonov Regularization
url: https://www.semanticscholar.org/paper/Training-with-Noise-is-Equivalent-to-Tikhonov-Bishop/c3ecd8e19e016d15670c8953b4b9afaa5186b0f3?sort=total-citations
venue: Neural Computation
year: 1995
