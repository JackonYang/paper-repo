authors:
- Charles M. Bishop
badges:
- id: OPEN_ACCESS
corpusId: 16096318
fieldsOfStudy:
- Mathematics
numCitedBy: 993
numCiting: 42
paperAbstract: It is well known that the addition of noise to the input data of a
  neural network during training can, in some circumstances, lead to significant improvements
  in generalization performance. Previous work has shown that such training with noise
  is equivalent to a form of regularization in which an extra term is added to the
  error function. However, the regularization term, which involves second derivatives
  of the error function, is not bounded below, and so can lead to difficulties if
  used directly in a learning algorithm based on error minimization. In this paper
  we show that for the purposes of network training, the regularization term can be
  reduced to a positive semi-definite form that involves only first derivatives of
  the network mapping. For a sum-of-squares error function, the regularization term
  belongs to the class of generalized Tikhonov regularizers. Direct minimization of
  the regularized error function provides a practical alternative to training with
  noise.
ref_count: 42
references:
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 128
  pid: 030a977bf32e81fb694117d78ac84a3fbe2a1d81
  title: 'An analysis of noise in recurrent neural networks: convergence and generalization'
  year: 1996
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 102
  pid: 3c39ddfbd9822230b5375d581bf505ecf6255283
  title: 'Curvature-driven smoothing: a learning algorithm for feedforward networks'
  year: 1993
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 48
  pid: 3d565fb42892f20c52b9fc615cc537835f30d094
  title: On the Use of Evidence in Neural Networks
  year: 1992
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1586
  pid: a42954d4b9d0ccdf1036e0af46d87a01b94c3516
  title: 'Second Order Derivatives for Network Pruning: Optimal Brain Surgeon'
  year: 1992
- fieldsOfStudy:
  - Computer Science
  - Psychology
  numCitedBy: 3532
  pid: a34e35dbbc6911fa7b94894dffdc0076a261b6f0
  title: Neural Networks and the Bias/Variance Dilemma
  year: 1992
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 163
  pid: 2a1e1da81b535e1bead3fc2ab6af8b07877823b9
  title: Exact Calculation of the Hessian Matrix for the Multilayer Perceptron
  year: 1992
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 49
  pid: c3fb617767f9e500e84ed03fb48acdcf088f33dc
  title: 'Ace of Bayes : Application of Neural'
  year: 1993
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 2590
  pid: b959164d1efca4b73986ba5d21e664aadbbc0457
  title: A Practical Bayesian Framework for Backpropagation Networks
  year: 1992
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 580
  pid: e354ec85b8287bf15ed596be16ef6e422ccc29e7
  title: Creating artificial neural networks that generalize
  year: 1991
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 586
  pid: c6867b6b564462d6b902f68e0bfa58f4717ca1cc
  title: Fast Exact Multiplication by the Hessian
  year: 1994
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 254
  pid: b0f2433c088591d265891231f1c22424047f1bc1
  title: A Practical Bayesian Framework for Backprop Networks
  year: 1991
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 3492
  pid: e7297db245c3feb1897720b173a59fe7e36babb7
  title: Optimal Brain Damage
  year: 1989
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 317
  pid: 33fdc91c520b54e097f5e09fae1cfc94793fbfcf
  title: Large Automatic Learning, Rule Extraction, and Generalization
  year: 1987
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 376
  pid: c83684f6207697c12850db423fd9747572cf1784
  title: Bayesian Back-Propagation
  year: 1991
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 655
  pid: f707a81a278d1598cd0a4493ba73f22dcdf90639
  title: Generalization by Weight-Elimination with Application to Forecasting
  year: 1990
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 213
  pid: d275cf94e620bf5b3776bba8a88acccdcfcd9a19
  title: Bayesian Learning via Stochastic Dynamics
  year: 1992
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 7627
  pid: b8d9abd1c078573188b13d36c1b1efb7cb2fa865
  title: Practical optimization
  year: 1981
- fieldsOfStudy:
  - Mathematics
  numCitedBy: 7884
  pid: bc14819e745cd7af37efd09ea29773dc0065119e
  title: Solutions of ill-posed problems
  year: 1977
- fieldsOfStudy:
  - Medicine
  numCitedBy: 4706
  pid: 20b844e395355b40fa5940c61362ec40e56027aa
  title: Neural networks
  year: 1995
slug: Training-with-Noise-is-Equivalent-to-Tikhonov-Bishop
title: Training with Noise is Equivalent to Tikhonov Regularization
url: https://www.semanticscholar.org/paper/Training-with-Noise-is-Equivalent-to-Tikhonov-Bishop/c3ecd8e19e016d15670c8953b4b9afaa5186b0f3?sort=total-citations
venue: Neural Computation
year: 1995
