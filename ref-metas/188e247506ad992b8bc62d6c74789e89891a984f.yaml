authors:
- J. Bergstra
- Yoshua Bengio
badges:
- id: OPEN_ACCESS
corpusId: 15700257
fieldsOfStudy:
- Computer Science
numCitedBy: 5671
numCiting: 39
paperAbstract: Grid search and manual search are the most widely used strategies for
  hyper-parameter optimization. This paper shows empirically and theoretically that
  randomly chosen trials are more efficient for hyper-parameter optimization than
  trials on a grid. Empirical evidence comes from a comparison with a large previous
  study that used grid search and manual search to configure neural networks and deep
  belief networks. Compared with neural networks configured by a pure grid search,
  we find that random search over the same domain is able to find models that are
  as good or better within a small fraction of the computation time. Granting random
  search the same computational budget, random search finds better models by effectively
  searching a larger, less promising configuration space. Compared with deep belief
  networks configured by a thoughtful combination of manual search and grid search,
  purely random search over the same 32-dimensional configuration space found statistically
  equal performance on four of seven data sets, and superior performance on one of
  seven. A Gaussian process analysis of the function from hyper-parameters to validation
  set performance reveals that for most data sets only a few of the hyper-parameters
  really matter, but that different hyper-parameters are important on different data
  sets. This phenomenon makes grid search a poor choice for configuring algorithms
  for new data sets. Our analysis casts some light on why recent "High Throughput"
  methods achieve surprising success--they appear to search through a large number
  of hyper-parameters because most hyper-parameters do not matter much. We anticipate
  that growing interest in large hierarchical models will place an increasing burden
  on techniques for hyper-parameter optimization; this work shows that random search
  is a natural baseline against which to judge progress in the development of adaptive
  (sequential) hyper-parameter optimization algorithms.
ref_count: 39
references:
- pid: 8978cf7574ceb35f4c3096be768c7547b28a35d0
  title: A Fast Learning Algorithm for Deep Belief Nets
- pid: 82266f6103bade9005ec555ed06ba20b5210ff22
  title: Gaussian Processes for Machine Learning
- pid: b71ac1e9fb49420d13e084ac67254a0bbd40f83f
  title: Understanding the difficulty of training deep feedforward neural networks
- pid: 017ddb7e815236defd0566bc46f6ed8401cc6ba6
  title: A Simplex Method for Function Minimization
- pid: e60ff004dde5c13ec53087872cfcdd12e85beb57
  title: Learning Deep Architectures for AI
- pid: dd5061631a4d11fa394f4421700ebf7e78dcbc59
  title: Optimization by Simulated Annealing
- pid: 0d2336389dff3031910bd21dd1c44d1b4cd51725
  title: Why Does Unsupervised Pre-training Help Deep Learning?
- pid: dbc0a468ab103ae29717703d4aa9f682f6a2b664
  title: Neural Networks for Pattern Recognition
- pid: e95d3934e51107da7610acd0b1bcb6551671f9f1
  title: A Practical Guide to Training Restricted Boltzmann Machines
- pid: 162d958ff885f1462aeda91cd72582323fd6a1f4
  title: Gradient-based learning applied to document recognition
- pid: b8012351bc5ebce4a4b3039bbbba3ce393bc3315
  title: An empirical evaluation of deep architectures on problems with many factors
    of variation
- pid: 273dfbcb68080251f5e9ff38b4413d7bd84b10a1
  title: 'LIBSVM: A library for support vector machines'
- pid: 843959ffdccf31c6694d135fad07425924f785b1
  title: Extracting and composing robust features with denoising autoencoders
- pid: 1729f731482a628177a0fb81050966514c385e5e
  title: 'Adaptive Control Processes: A Guided Tour.'
- pid: b87274e6d9aa4e6ba5148898aa92941617d2b6ed
  title: Efficient BackProp
slug: Random-Search-for-Hyper-Parameter-Optimization-Bergstra-Bengio
title: Random Search for Hyper-Parameter Optimization
url: https://www.semanticscholar.org/paper/Random-Search-for-Hyper-Parameter-Optimization-Bergstra-Bengio/188e247506ad992b8bc62d6c74789e89891a984f?sort=total-citations
venue: J. Mach. Learn. Res.
year: 2012
