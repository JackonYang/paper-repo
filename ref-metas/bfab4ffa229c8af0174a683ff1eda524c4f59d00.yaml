authors:
- W. Xu
- A. Rudnicky
badges:
- id: OPEN_ACCESS
corpusId: 14974472
fieldsOfStudy:
- Computer Science
numCitedBy: 86
numCiting: 8
paperAbstract: Currently, N-gram models are the most common and widely used models
  for statistical language modeling. In this paper, we investigated an alternative
  way to build language models, i.e., using artificial neural networks to learn the
  language model. Our experiment result shows that the neural network can learn a
  language model that has performance even better than standard statistical methods.
ref_count: 8
references:
- pid: d4e8bed3b50a035e1eabad614fe4218a34b3b178
  title: An empirical study of smoothing techniques for language modeling
- pid: 9548ac30c113562a51e603dbbc8e9fa651cfd3ab
  title: Improved backing-off for M-gram language modeling
- pid: 231f6de83cfa4d641da1681e97a11b689a48e3aa
  title: Statistical methods for speech recognition
- pid: 1f462943c8d0af69c12a09058251848324135e5a
  title: Probabilistic Interpretation of Feedforward Classification Network Outputs,
    with Relationships to Statistical Pattern Recognition
- pid: d36efb9ad91e00faa334b549ce989bfae7e2907a
  title: Maximum likelihood from incomplete data via the EM - algorithm plus discussions
    on the paper
slug: Can-artificial-neural-networks-learn-language-Xu-Rudnicky
title: Can artificial neural networks learn language models?
url: https://www.semanticscholar.org/paper/Can-artificial-neural-networks-learn-language-Xu-Rudnicky/bfab4ffa229c8af0174a683ff1eda524c4f59d00?sort=total-citations
venue: INTERSPEECH
year: 2000
