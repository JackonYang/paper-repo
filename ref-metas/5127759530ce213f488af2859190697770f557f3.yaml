authors:
- Laurenz Wiskott
- T. Sejnowski
badges:
- id: OPEN_ACCESS
corpusId: 12366835
fieldsOfStudy:
- Computer Science
numCitedBy: 1189
numCiting: 50
paperAbstract: Invariant features of temporally varying signals are useful for analysis
  and classification. Slow feature analysis (SFA) is a new method for learning invariant
  or slowly varying features from a vectorial input signal. It is based on a nonlinear
  expansion of the input signal and application of principal component analysis to
  this expanded signal and its time derivative. It is guaranteed to find the optimal
  solution within a family of functions directly and can learn to extract a large
  number of decor-related features, which are ordered by their degree of invariance.
  SFA can be applied hierarchically to process high-dimensional input signals and
  extract complex features. SFA is applied first to complex cell tuning properties
  based on simple cell output, including disparity and motion. Then more complicated
  input-output functions are learned by repeated application of SFA. Finally, a hierarchical
  network of SFA modules is presented as a simple model of the visual system. The
  same unstructured network can learn translation, size, rotation, contrast, or, to
  a lesser degree, illumination invariance for one-dimensional objects, depending
  on only the training stimulus. Surprisingly, only a few training objects suffice
  to achieve good generalization to new objects. The generated representation is suitable
  for object recognition. Performance degrades if the network is trained to learn
  multiple invariances simultaneously.
ref_count: 50
references:
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 698
  pid: 2da4e9984a75ffe28c5364662807996ac5bb2662
  title: Learning Invariance from Transformation Sequences
  year: 1991
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 408
  pid: 2c85b7fe70dda0adbbd7630e2a341a904c74fbd2
  title: Self-organizing neural network that discovers surfaces in random-dot stereograms
  year: 1992
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 8757
  pid: 1d7d0e8c4791700defd4b0df82a26b50055346e0
  title: An Information-Maximization Approach to Blind Separation and Blind Deconvolution
  year: 1995
- fieldsOfStudy:
  - Environmental Science
  numCitedBy: 1709
  pid: 0dbf797d5b34f40d16eeadfa7a5b4543c2af2c11
  title: An evaluation of the two-dimensional Gabor filter model of simple receptive
    fields in cat striate cortex.
  year: 1987
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 15338
  pid: dbc0a468ab103ae29717703d4aa9f682f6a2b664
  title: Neural Networks for Pattern Recognition
  year: 1993
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 755
  pid: 71ea46c9266f5104f79ea27fdfb4c5686677695a
  title: 'Neocognitron: A neural network model for a mechanism of visual pattern recognition'
  year: 1983
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 7830
  pid: a8e8f3c8d4418c8d62e306538c9c1292635e9d27
  title: Backpropagation Applied to Handwritten Zip Code Recognition
  year: 1989
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 38756
  pid: 8213dbed4db44e113af3ed17d6dad57471a0c048
  title: The Nature of Statistical Learning Theory
  year: 2000
slug: Slow-Feature-Analysis:-Unsupervised-Learning-of-Wiskott-Sejnowski
title: 'Slow Feature Analysis: Unsupervised Learning of Invariances'
url: https://www.semanticscholar.org/paper/Slow-Feature-Analysis:-Unsupervised-Learning-of-Wiskott-Sejnowski/5127759530ce213f488af2859190697770f557f3?sort=total-citations
venue: Neural Computation
year: 2002
