authors:
- T. Cover
- R. C. King
badges:
- id: OPEN_ACCESS
corpusId: 12493947
fieldsOfStudy:
- Computer Science
numCitedBy: 208
numCiting: 134
paperAbstract: In his original paper on the subject, Shannon found upper and lower
  bounds for the entropy of printed English based on the number of trials required
  for a subject to guess subsequent symbols in a given text. The guessing approach
  precludes asymptotic consistency of either the upper or lower bounds except for
  degenerate ergodic processes. Shannon's technique of guessing the next symbol is
  altered by having the subject place sequential bets on the next symbol of text.
  If S_{n} denotes the subject's capital after n bets at 27 for 1 odds, and if it
  is assumed that the subject knows the underlying probability distribution for the
  process X , then the entropy estimate is \hat{H}_{n}(X)=(1-(1/n) \log_{27}S_{n})
  \log_{2} 27 bits/symbol. If the subject does not know the true probability distribution
  for the stochastic process, then \hat{H}_{n}(X) is an asymptotic upper bound for
  the true entropy. If X is stationary, E\hat{H}_{n}(X)\rightarrowH(X), H(X) being
  the true entropy of the process. Moreover, if X is ergodic, then by the Shannon-McMillan-Breiman
  theorem \hat{H}_{n}(X)\rightarrowH(X) with probability one. Preliminary indications
  are that English text has an entropy of approximately 1.3 bits/symbol, which agrees
  well with Shannon's estimate. In his original paper on the subject, Shannon found
  upper and lower bounds for the entropy of printed English based on the number of
  trials required for a subject to guess subsequent symbols in a given text. The guessing
  approach precludes asymptotic consistency of either the upper or lower bounds except
  for degenerate ergodic processes. Shannon's technique of guessing the next symbol
  is altered by having the subject place sequential bets on the next symbol of text.
  If S_{n} denotes the subject's capital after n bets at 27 for 1 odds, and if it
  is assumed that the subject knows the underlying probability distribution for the
  process X , then the entropy estimate is \hat{H}_{n}(X)=(1-(1/n) \log_{27}S_{n})
  \log_{2} 27 bits/symbol. If the subject does not know the true probability distribution
  for the stochastic process, then \hat{H}_{n}(X) is an asymptotic upper bound for
  the true entropy. If X is stationary, E\hat{H}_{n}(X)\rightarrowH(X), H(X) being
  the true entropy of the process. Moreover, if X is ergodic, then by the Shannon-McMillan-Breiman
  theorem \hat{H}_{n}(X)\rightarrowH(X) with probability one. Preliminary indications
  are that English text has an entropy of approximately 1.3 bits/symbol, which agrees
  well with Shannon's estimate. In his original paper on the subject, Shannon found
  upper and lower bounds for the entropy of printed English based on the number of
  trials required for a subject to guess subsequent symbols in a given text. The guessing
  approach precludes asymptotic consistency of either the upper or lower bounds except
  for degenerate ergodic processes. Shannon's technique of guessing the next symbol
  is altered by having the subject place sequential bets on the next symbol of text.
  If S_{n} denotes the subject's capital after n bets at 27 for 1 odds, and if it
  is assumed that the subject knows the underlying probability distribution for the
  process X , then the entropy estimate is \hat{H}_{n}(X)=(1-(1/n) \log_{27}S_{n})
  \log_{2} 27 bits/symbol. If the subject does not know the true probability distribution
  for the stochastic process, then \hat{H}_{n}(X) is an asymptotic upper bound for
  the true entropy. If X is stationary, E\hat{H}_{n}(X)\rightarrowH(X), H(X) being
  the true entropy of the process.Moreover, if X is ergodic, then by the Shannon-McMillan-Breiman
  theorem \hat{H}_{n}(X)\rightarrowH(X) with probability one. Preliminary indications
  are that English text has an entropy of approximately 1.3 bits/symbol, which agrees
  well with Shannon's estimate. In his original paper on the subject, Shannon found
  upper and lower bounds for the entropy of printed English based on the number of
  trials required for a subject to guess subsequent symbols in a given text. The guessing
  approach precludes asymptotic consistency of either the upper or lower bounds except
  for degenerate ergodic processes. Shannon's technique of guessing the next symbol
  is altered by having the subject place sequential bets on the next symbol of text.
  If S_{n} denotes the subject's capital after n bets at 27 for 1 odds, and if it
  is assumed that the subject knows the underlying probability distribution for the
  process X , then the entropy estimate is \hat{H}_{n}(X)=(1-(1/n) \log_{27}S_{n})
  \log_{2} 27 bits/symbol. If the subject does not know the true probability distribution
  for the stochastic process, then \hat{H}_{n}(X) is an asymptotic upper bound for
  the true entropy. If X is stationary, E\hat{H}_{n}(X)\rightarrowH(X), H(X) being
  the true entropy of the process. Moreover, if X is ergodic, then by the Shannon-McMillan-Breiman
  theorem \hat{H}_{n}(X)\rightarrowH(X) with probability one. Preliminary indications
  are that English text has an entropy of approximately 1.3 bits/symbol, which agrees
  well with Shannon's estimate.
ref_count: 134
references:
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 2530
  pid: c1e3f2d537e50e0d5263e4731ab6c7983acd6687
  title: Prediction and entropy of printed English
  year: 1951
slug: A-convergent-gambling-estimate-of-the-entropy-of-Cover-King
title: A convergent gambling estimate of the entropy of English
url: https://www.semanticscholar.org/paper/A-convergent-gambling-estimate-of-the-entropy-of-Cover-King/130b1c7786328bf8f4ebea56e6d2f1cb992404ab?sort=total-citations
venue: IEEE Trans. Inf. Theory
year: 1978
