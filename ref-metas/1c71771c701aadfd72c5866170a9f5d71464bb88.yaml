authors:
- Li Dong
- Nan Yang
- Wenhui Wang
- Furu Wei
- Xiaodong Liu
- Yu Wang
- Jianfeng Gao
- M. Zhou
- H. Hon
badges:
- id: OPEN_ACCESS
corpusId: 147704286
fieldsOfStudy:
- Computer Science
numCitedBy: 732
numCiting: 61
paperAbstract: 'This paper presents a new Unified pre-trained Language Model (UniLM)
  that can be fine-tuned for both natural language understanding and generation tasks.
  The model is pre-trained using three types of language modeling tasks: unidirectional,
  bidirectional, and sequence-to-sequence prediction. The unified modeling is achieved
  by employing a shared Transformer network and utilizing specific self-attention
  masks to control what context the prediction conditions on. UniLM compares favorably
  with BERT on the GLUE benchmark, and the SQuAD 2.0 and CoQA question answering tasks.
  Moreover, UniLM achieves new state-of-the-art results on five natural language generation
  datasets, including improving the CNN/DailyMail abstractive summarization ROUGE-L
  to 40.51 (2.04 absolute improvement), the Gigaword abstractive summarization ROUGE-L
  to 35.75 (0.86 absolute improvement), the CoQA generative question answering F1
  score to 82.5 (37.1 absolute improvement), the SQuAD question generation BLEU-4
  to 22.12 (3.75 absolute improvement), and the DSTC7 document-grounded dialog response
  generation NIST-4 to 2.67 (human performance is 2.65). The code and pre-trained
  models are available at this https URL.'
ref_count: 61
references:
- pid: 658721bc13b0fa97366d38c05a96bf0a9f4bb0ac
  title: Multi-Task Deep Neural Networks for Natural Language Understanding
- pid: 9405cc0d6169988371b2755e573cc28650d14dfe
  title: Language Models are Unsupervised Multitask Learners
- pid: df2b0e26d0599ce3e70df8a9da02e51594e0e992
  title: 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'
- pid: 145b8b5d99a2beba6029418ca043585b90138d12
  title: 'MASS: Masked Sequence to Sequence Pre-training for Language Generation'
- pid: cd18800a0fe0b668a1cc19f2ec95b5003d0a5035
  title: Improving Language Understanding by Generative Pre-Training
- pid: 93b8da28d006415866bf48f9a6e06b5242129195
  title: 'GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language
    Understanding'
- pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  title: Attention is All you Need
- pid: 032274e57f7d8b456bd255fe76b909b2c1d7458e
  title: A Deep Reinforced Model for Abstractive Summarization
- pid: 1e077413b25c4d34945cc2707e17e46ed4fe784a
  title: Universal Language Model Fine-tuning for Text Classification
- pid: 687bac2d3320083eb4530bf18bb8f8f721477600
  title: Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank
- pid: 3febb2bed8865945e7fddc99efd791887bb7e14f
  title: Deep Contextualized Word Representations
- pid: dbde7dfa6cae81df8ac19ef500c42db96c3d1edd
  title: 'Google''s Neural Machine Translation System: Bridging the Gap between Human
    and Machine Translation'
- pid: 5ded2b8c64491b4a67f6d39ce473d4b9347a672e
  title: A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference
- pid: 668db48c6a79826456341680ee1175dfc4cced71
  title: 'Get To The Point: Summarization with Pointer-Generator Networks'
- pid: 5082a1a13daea5c7026706738f8528391a1e6d59
  title: A Neural Attention Model for Abstractive Sentence Summarization
- pid: a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096
  title: 'SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual
    Focused Evaluation'
- pid: 4aa9f5150b46320f534de4747a2dd0cd7f3fe292
  title: Semi-supervised Sequence Learning
- pid: 05dd7254b632376973f3a1b4d39485da17814df5
  title: 'SQuAD: 100,000+ Questions for Machine Comprehension of Text'
- pid: 4d1c856275744c0284312a3a50efb6ca9dc4cd4c
  title: "Know What You Don\u2019t Know: Unanswerable Questions for SQuAD"
- pid: db8885a0037fe47d973ade79d696586453710233
  title: The Sixth PASCAL Recognizing Textual Entailment Challenge
- pid: 23ffaa0fe06eae05817f527a47ac3291077f9e58
  title: Rethinking the Inception Architecture for Computer Vision
- pid: aab5002a22b9b4244a8329b140bd0a86021aa2d1
  title: 'OpenNMT: Open-Source Toolkit for Neural Machine Translation'
- pid: 0f8468de03ee9f12d693237bec87916311bf1c24
  title: The Seventh PASCAL Recognizing Textual Entailment Challenge
- pid: cb0f3ee1e98faf92429d601cdcd76c69c1e484eb
  title: Neural Network Acceptability Judgments
- pid: 0e6824e137847be0599bb0032e37042ed2ef5045
  title: 'Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching
    Movies and Reading Books'
- pid: 136326377c122560768db674e35f5bcd6de3bc40
  title: The Second PASCAL Recognising Textual Entailment Challenge
- pid: a6cb366736791bcccc5c8639de5a8f9636bf87e8
  title: 'Adam: A Method for Stochastic Optimization'
- pid: 44d2abe2175df8153f465f6c39b68b76a0d40ab9
  title: Long Short-Term Memory
- pid: 60b05f32c32519a809f21642ef1eb3eaf3848008
  title: 'ROUGE: A Package for Automatic Evaluation of Summaries'
- pid: 475354f10798f110d34792b6d88f31d6d5cb099e
  title: Automatically Constructing a Corpus of Sentential Paraphrases
- pid: de794d50713ea5f91a7c9da3d72041e2f5ef8452
  title: The PASCAL Recognising Textual Entailment Challenge
- pid: d79ac7a7bafdc9a782fb8c53285ca11c7f2e3f18
  title: 'BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language
    Model'
- pid: 128cb6b891aee1b5df099acb48e2efecfcff689f
  title: The Winograd Schema Challenge
- pid: 15f4c35889ccc1ae258b680c2ca2fcbfe1e260f7
  title: Gaussian Error Linear Units (GELUs)
- pid: 69d7086300e7f5322c06f2f242a565b3a182efb5
  title: In Advances in Neural Information Processing Systems
- pid: 766ce989b8b8b984f7a4691fd8c9af4bdb2b74cd
  title: "\u201CCloze Procedure\u201D: A New Tool for Measuring Readability"
slug: Unified-Language-Model-Pre-training-for-Natural-and-Dong-Yang
title: Unified Language Model Pre-training for Natural Language Understanding and
  Generation
url: https://www.semanticscholar.org/paper/Unified-Language-Model-Pre-training-for-Natural-and-Dong-Yang/1c71771c701aadfd72c5866170a9f5d71464bb88?sort=total-citations
venue: NeurIPS
year: 2019
