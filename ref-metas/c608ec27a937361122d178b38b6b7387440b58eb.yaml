authors:
- David J. Miller
- H. S. Uyar
badges:
- id: OPEN_ACCESS
corpusId: 17425751
fieldsOfStudy:
- Computer Science
numCitedBy: 346
numCiting: 14
paperAbstract: We address statistical classifier design given a mixed training set
  consisting of a small labelled feature set and a (generally larger) set of unlabelled
  features. This situation arises, e.g., for medical images, where although training
  features may be plentiful, expensive expertise is required to extract their class
  labels. We propose a classifier structure and learning algorithm that make effective
  use of unlabelled data to improve performance. The learning is based on maximization
  of the total data likelihood, i.e. over both the labelled and unlabelled data subsets.
  Two distinct EM learning algorithms are proposed, differing in the EM formalism
  applied for unlabelled data. The classifier, based on a joint probability model
  for features and labels, is a "mixture of experts" structure that is equivalent
  to the radial basis function (RBF) classifier, but unlike RBFs, is amenable to likelihood-based
  training. The scope of application for the new method is greatly extended by the
  observation that test data, or any new data to classify, is in fact additional,
  unlabelled data - thus, a combined learning/classification operation - much akin
  to what is done in image segmentation - can be invoked whenever there is new data
  to classify. Experiments with data sets from the UC Irvine database demonstrate
  that the new learning algorithms and structure achieve substantial performance gains
  over alternative approaches.
ref_count: 14
references:
- pid: 5db7dc2239f820eae498b07a955f31b3d113179f
  title: Supervised learning from incomplete data via an EM approach
- pid: f67e9a6bd7c688f1c9c653584a4fa1f9c7fda2a6
  title: On the exponential value of labeled samples
- pid: 334867ed99a0af07d8a53dae4f7fdeffffdecc09
  title: The effect of unlabeled samples in reducing the small sample size problem
    and mitigating the Hughes phenomenon
- pid: f6d8a7fc2e2d53923832f9404376512068ca2a57
  title: Hierarchical mixtures of experts and the EM algorithm
- pid: 1e7c4f513f24c3b82a1138b9f22ed87ed00cbe76
  title: Fast Learning in Networks of Locally-Tuned Processing Units
- pid: d36efb9ad91e00faa334b549ce989bfae7e2907a
  title: Maximum likelihood from incomplete data via the EM - algorithm plus discussions
    on the paper
slug: A-Mixture-of-Experts-Classifier-with-Learning-Based-Miller-Uyar
title: A Mixture of Experts Classifier with Learning Based on Both Labelled and Unlabelled
  Data
url: https://www.semanticscholar.org/paper/A-Mixture-of-Experts-Classifier-with-Learning-Based-Miller-Uyar/c608ec27a937361122d178b38b6b7387440b58eb?sort=total-citations
venue: NIPS
year: 1996
