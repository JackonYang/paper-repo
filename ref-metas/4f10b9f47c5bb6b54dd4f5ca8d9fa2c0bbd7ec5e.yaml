authors:
- Sainbayar Sukhbaatar
- Arthur D. Szlam
- J. Weston
- R. Fergus
badges:
- id: OPEN_ACCESS
corpusId: 1399322
fieldsOfStudy:
- Computer Science
numCitedBy: 1990
numCiting: 27
paperAbstract: We introduce a neural network with a recurrent attention model over
  a possibly large external memory. The architecture is a form of Memory Network [23]
  but unlike the model in that work, it is trained end-to-end, and hence requires
  significantly less supervision during training, making it more generally applicable
  in realistic settings. It can also be seen as an extension of RNNsearch [2] to the
  case where multiple computational steps (hops) are performed per output symbol.
  The flexibility of the model allows us to apply it to tasks as diverse as (synthetic)
  question answering [22] and to language modeling. For the former our approach is
  competitive with Memory Networks, but with less supervision. For the latter, on
  the Penn TreeBank and Text8 datasets our approach demonstrates comparable performance
  to RNNs and LSTMs. In both cases we show that the key concept of multiple computational
  hops yields improved results.
ref_count: 27
references:
- pid: f9a1b3850dfd837793743565a8af95973d395a4e
  title: LSTM Neural Networks for Language Modeling
- pid: 71ae756c75ac89e2d731c9c79649562b5768ff39
  title: Memory Networks
- pid: 5522764282c85aea422f1c4dc92ff7e0ca6987bc
  title: A Clockwork RNN
- pid: 9665247ea3421929f9b6ad721f139f11edb1dbb8
  title: Learning Longer Memory in Recurrent Neural Networks
- pid: 44d2abe2175df8153f465f6c39b68b76a0d40ab9
  title: Long Short-Term Memory
- pid: adfcf065e15fd3bc9badf6145034c84dfb08f204
  title: Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling
- pid: d38e8631bba0720becdaf7b89f79d9f9dca45d82
  title: Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets
- pid: fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5
  title: Neural Machine Translation by Jointly Learning to Align and Translate
- pid: f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97
  title: Recurrent Neural Network Regularization
- pid: 30110856f45fde473f1903f686aa365cf70ed4c7
  title: 'Learning Context-free Grammars: Capabilities and Limitations of a Recurrent
    Neural Network with an External Stack Memory'
- pid: abb33d75dc297993fcc3fb75e0f4498f413eb4f6
  title: 'Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks'
- pid: 96364af2d208ea75ca3aeb71892d2f7ce7326b55
  title: Statistical Language Models Based on Neural Networks
- pid: 89b1f4740ae37fd04f6ac007577bdd34621f0861
  title: Generating Sequences With Recurrent Neural Networks
- pid: a2785f66c20fbdf30ec26c0931584c6d6a0f4fca
  title: 'DRAW: A Recurrent Neural Network For Image Generation'
- pid: 6c2b28f9354f667cd5bd07afc0471d8334430da7
  title: A Neural Probabilistic Language Model
- pid: 4d8f2d14af5991d4f0d050d22216825cac3157bd
  title: 'Show, Attend and Tell: Neural Image Caption Generation with Visual Attention'
- pid: 09c76da2361d46689825c4efc37ad862347ca577
  title: A bit of progress in language modeling
- pid: c3823aacea60bc1f2cabb9283144690a3d015db5
  title: Neural Turing Machines
- pid: 0b44fcbeea9415d400c5f5789d6b892b6f98daff
  title: 'Building a Large Annotated Corpus of English: The Penn Treebank'
slug: End-To-End-Memory-Networks-Sukhbaatar-Szlam
title: End-To-End Memory Networks
url: https://www.semanticscholar.org/paper/End-To-End-Memory-Networks-Sukhbaatar-Szlam/4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e?sort=total-citations
venue: NIPS
year: 2015
