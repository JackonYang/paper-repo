authors:
- Yoshua Bengio
- "Nicholas L\xE9onard"
- Aaron C. Courville
badges:
- id: OPEN_ACCESS
corpusId: 18406556
fieldsOfStudy:
- Computer Science
numCitedBy: 1508
numCiting: 21
paperAbstract: 'Stochastic neurons and hard non-linearities can be useful for a number
  of reasons in deep learning models, but in many cases they pose a challenging problem:
  how to estimate the gradient of a loss function with respect to the input of such
  stochastic or non-smooth neurons? I.e., can we "back-propagate" through these stochastic
  neurons? We examine this question, existing approaches, and compare four families
  of solutions, applicable in different settings. One of them is the minimum variance
  unbiased gradient estimator for stochatic binary neurons (a special case of the
  REINFORCE algorithm). A second approach, introduced here, decomposes the operation
  of a binary stochastic neuron into a stochastic binary part and a smooth differentiable
  part, which approximates the expected effect of the pure stochatic binary neuron
  to first order. A third approach involves the injection of additive or multiplicative
  noise in a computational graph that is otherwise differentiable. A fourth approach
  heuristically copies the gradient with respect to the stochastic output directly
  as an estimator of the gradient with respect to the sigmoid argument (we call this
  the straight-through estimator). To explore a context where these estimators are
  useful, we consider a small-scale version of {\em conditional computation}, where
  sparse stochastic units form a distributed representation of gaters that can turn
  off in combinatorially many ways large chunks of the computation performed in the
  rest of the neural network. In this case, it is important that the gating units
  produce an actual 0 most of the time. The resulting sparsity can be potentially
  be exploited to greatly reduce the computational cost of large deep networks for
  which conditional computation would be useful.'
ref_count: 22
references:
- pid: 4c915c1eecb217c123a36dc6d3ce52d12c742614
  title: Simple statistical gradient-following algorithms for connectionist reinforcement
    learning
- pid: 052b1d8ce63b07fec3de9dbb583772d860b7c769
  title: Learning representations by back-propagating errors
- pid: 843959ffdccf31c6694d135fad07425924f785b1
  title: Extracting and composing robust features with denoising autoencoders
- pid: 1366de5bb112746a555e9c0cd00de3ad8628aea8
  title: Improving neural networks by preventing co-adaptation of feature detectors
- pid: abd1c342495432171beb7ca8fd9551ef13cbd0ff
  title: ImageNet classification with deep convolutional neural networks
- pid: b7b915d508987b73b61eccd2b237e7ed099a2d29
  title: Maxout Networks
- pid: ae3fe34be9230c98b04d68b4621c89b7dbc2d717
  title: Learning representations by backpropagating errors
- pid: a538b05ebb01a40323997629e171c91aa28b8e2f
  title: Rectified Linear Units Improve Restricted Boltzmann Machines
- pid: b13813b49f160e1a2010c44bd4fb3d09a28446e3
  title: Hierarchical Recurrent Neural Networks for Long-Term Dependencies
- pid: 67107f78a84bdb2411053cb54e94fa226eea6d8e
  title: Deep Sparse Rectifier Neural Networks
- pid: cd5af41a81e7fc9588dc74f3831fb14daf2f8e2a
  title: Semantic hashing
slug: "Estimating-or-Propagating-Gradients-Through-Neurons-Bengio-L\xE9onard"
title: Estimating or Propagating Gradients Through Stochastic Neurons for Conditional
  Computation
url: "https://www.semanticscholar.org/paper/Estimating-or-Propagating-Gradients-Through-Neurons-Bengio-L\xE9\
  onard/62c76ca0b2790c34e85ba1cce09d47be317c7235?sort=total-citations"
venue: ArXiv
year: 2013
