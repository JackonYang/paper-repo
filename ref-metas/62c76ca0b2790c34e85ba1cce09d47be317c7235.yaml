authors:
- Yoshua Bengio
- "Nicholas L\xE9onard"
- Aaron C. Courville
badges:
- id: OPEN_ACCESS
corpusId: 18406556
fieldsOfStudy:
- Computer Science
meta_key: estimating-or-propagating-gradients-through-stochastic-neurons-for-conditional-computation
numCitedBy: 1505
numCiting: 21
paperAbstract: 'Stochastic neurons and hard non-linearities can be useful for a number
  of reasons in deep learning models, but in many cases they pose a challenging problem:
  how to estimate the gradient of a loss function with respect to the input of such
  stochastic or non-smooth neurons? I.e., can we "back-propagate" through these stochastic
  neurons? We examine this question, existing approaches, and compare four families
  of solutions, applicable in different settings. One of them is the minimum variance
  unbiased gradient estimator for stochatic binary neurons (a special case of the
  REINFORCE algorithm). A second approach, introduced here, decomposes the operation
  of a binary stochastic neuron into a stochastic binary part and a smooth differentiable
  part, which approximates the expected effect of the pure stochatic binary neuron
  to first order. A third approach involves the injection of additive or multiplicative
  noise in a computational graph that is otherwise differentiable. A fourth approach
  heuristically copies the gradient with respect to the stochastic output directly
  as an estimator of the gradient with respect to the sigmoid argument (we call this
  the straight-through estimator). To explore a context where these estimators are
  useful, we consider a small-scale version of {\em conditional computation}, where
  sparse stochastic units form a distributed representation of gaters that can turn
  off in combinatorially many ways large chunks of the computation performed in the
  rest of the neural network. In this case, it is important that the gating units
  produce an actual 0 most of the time. The resulting sparsity can be potentially
  be exploited to greatly reduce the computational cost of large deep networks for
  which conditional computation would be useful.'
ref_count: 22
references:
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 5181
  pid: 4c915c1eecb217c123a36dc6d3ce52d12c742614
  show_ref_link: true
  title: Simple statistical gradient-following algorithms for connectionist reinforcement
    learning
  year: 2004
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 20335
  pid: 052b1d8ce63b07fec3de9dbb583772d860b7c769
  show_ref_link: false
  title: Learning representations by back-propagating errors
  year: 1986
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 191
  pid: 4d92df4a844c94fbb31b95157488e4b562b4f681
  show_ref_link: false
  title: The Optimal Reward Baseline for Gradient-Based Reinforcement Learning
  year: 2001
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 5471
  pid: 843959ffdccf31c6694d135fad07425924f785b1
  show_ref_link: true
  title: Extracting and composing robust features with denoising autoencoders
  year: 2008
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 483
  pid: f8c8619ea7d68e604e40b814b40c72888a755e95
  show_ref_link: false
  title: 'Unsupervised Feature Learning and Deep Learning: A Review and New Perspectives'
  year: 2012
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 6191
  pid: 1366de5bb112746a555e9c0cd00de3ad8628aea8
  show_ref_link: true
  title: Improving neural networks by preventing co-adaptation of feature detectors
  year: 2012
- fieldsOfStudy:
  - Biology
  - Computer Science
  numCitedBy: 104
  pid: 0f6089fb276a8ab926b735b9043263362bf19985
  show_ref_link: false
  title: Gradient learning in spiking neural networks by dynamic perturbation of conductances.
  year: 2006
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 80978
  pid: abd1c342495432171beb7ca8fd9551ef13cbd0ff
  show_ref_link: true
  title: ImageNet classification with deep convolutional neural networks
  year: 2012
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 541
  pid: 72d32c986b47d6b880dad0c3f155fe23d2939038
  show_ref_link: false
  title: 'Deep Learning of Representations: Looking Forward'
  year: 2013
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1822
  pid: b7b915d508987b73b61eccd2b237e7ed099a2d29
  show_ref_link: true
  title: Maxout Networks
  year: 2013
- fieldsOfStudy:
  - Environmental Science
  numCitedBy: 1037
  pid: ae3fe34be9230c98b04d68b4621c89b7dbc2d717
  show_ref_link: false
  title: Learning representations by backpropagating errors
  year: 2004
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 12813
  pid: a538b05ebb01a40323997629e171c91aa28b8e2f
  show_ref_link: true
  title: Rectified Linear Units Improve Restricted Boltzmann Machines
  year: 2010
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 332
  pid: b13813b49f160e1a2010c44bd4fb3d09a28446e3
  show_ref_link: false
  title: Hierarchical Recurrent Neural Networks for Long-Term Dependencies
  year: 1995
- fieldsOfStudy:
  - Computer Science
  - Mathematics
  numCitedBy: 1897
  pid: f7410cd1afeba276f4479e8b5f04f12530b48d83
  show_ref_link: false
  title: Multivariate stochastic approximation using a simultaneous perturbation gradient
    approximation
  year: 1992
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 5919
  pid: 67107f78a84bdb2411053cb54e94fa226eea6d8e
  show_ref_link: true
  title: Deep Sparse Rectifier Neural Networks
  year: 2011
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1260
  pid: cd5af41a81e7fc9588dc74f3831fb14daf2f8e2a
  show_ref_link: false
  title: Semantic hashing
  year: 2009
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 281
  pid: 5df0a0e9ceec70a9321b0555288222bf53216342
  show_ref_link: false
  title: Neural Networks in Machine Learning
  year: 2014
slug: "Estimating-or-Propagating-Gradients-Through-Neurons-Bengio-L\xE9onard"
title: Estimating or Propagating Gradients Through Stochastic Neurons for Conditional
  Computation
url: "https://www.semanticscholar.org/paper/Estimating-or-Propagating-Gradients-Through-Neurons-Bengio-L\xE9\
  onard/62c76ca0b2790c34e85ba1cce09d47be317c7235?sort=total-citations"
venue: ArXiv
year: 2013
