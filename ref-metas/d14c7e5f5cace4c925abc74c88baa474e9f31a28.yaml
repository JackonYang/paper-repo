authors:
- Junyoung Chung
- "\xC7aglar G\xFCl\xE7ehre"
- Kyunghyun Cho
- Yoshua Bengio
badges:
- id: OPEN_ACCESS
corpusId: 8577750
fieldsOfStudy:
- Computer Science
numCitedBy: 647
numCiting: 41
paperAbstract: In this work, we propose a novel recurrent neural network (RNN) architecture.
  The proposed RNN, gated-feedback RNN (GF-RNN), extends the existing approach of
  stacking multiple recurrent layers by allowing and controlling signals flowing from
  upper recurrent layers to lower layers using a global gating unit for each pair
  of layers. The recurrent signals exchanged between layers are gated adaptively based
  on the previous hidden states and the current input. We evaluated the proposed GF-RNN
  with different types of recurrent units, such as tanh, long short-term memory and
  gated recurrent units, on the tasks of character-level language modeling and Python
  program evaluation. Our empirical evaluation of different RNN units, revealed that
  in both tasks, the GF-RNN outperforms the conventional approaches to build deep
  stacked RNNs. We suggest that the improvement arises because the GFRNN can adaptively
  assign different layers to different timescales and layer-to-layer interactions
  (including the top-down ones which are not usually present in a stacked RNN) by
  learning to gate these interactions.
ref_count: 41
references:
- pid: 5522764282c85aea422f1c4dc92ff7e0ca6987bc
  title: A Clockwork RNN
- pid: e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de
  title: Generating Text with Recurrent Neural Networks
- pid: b0b33aaed1d408d04fadf9ff2a080e47ef8cb7b1
  title: Training and Analysing Deep Recurrent Neural Networks
- pid: 70155488a49d51755c1dfea728e03a6dd72703a1
  title: Deep Networks with Internal Selective Attention through Feedback Connections
- pid: 11540131eae85b2e11d53df7f1360eeb6476e7f4
  title: 'Learning to Forget: Continual Prediction with LSTM'
- pid: 0d0b3ff23fbf1ec7a0dfebd923a3ef224306491a
  title: Learning to Execute
- pid: f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97
  title: Recurrent Neural Network Regularization
- pid: 44d2abe2175df8153f465f6c39b68b76a0d40ab9
  title: Long Short-Term Memory
- pid: b13813b49f160e1a2010c44bd4fb3d09a28446e3
  title: Hierarchical Recurrent Neural Networks for Long-Term Dependencies
- pid: cea967b59209c6be22829699f05b8b1ac4dc092d
  title: Sequence to Sequence Learning with Neural Networks
- pid: d0be39ee052d246ae99c082a565aba25b811be2d
  title: Learning long-term dependencies with gradient descent is difficult
- pid: 0b544dfe355a5070b60986319a3f51fb45d1348e
  title: "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical\
    \ Machine Translation"
- pid: 1fd7fc06653723b05abe5f3d1de393ddcf6bdddb
  title: SUBWORD LANGUAGE MODELING WITH NEURAL NETWORKS
- pid: 50c770b425a5bb25c77387f687a9910a9d130722
  title: Learning Complex, Extended Sequences Using the Principle of History Compression
- pid: a6cb366736791bcccc5c8639de5a8f9636bf87e8
  title: 'Adam: A Method for Stochastic Optimization'
- pid: 96364af2d208ea75ca3aeb71892d2f7ce7326b55
  title: Statistical Language Models Based on Neural Networks
- pid: fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5
  title: Neural Machine Translation by Jointly Learning to Align and Translate
- pid: 89b1f4740ae37fd04f6ac007577bdd34621f0861
  title: Generating Sequences With Recurrent Neural Networks
- pid: 6c2b28f9354f667cd5bd07afc0471d8334430da7
  title: A Neural Probabilistic Language Model
- pid: 855d0f722d75cc56a66a00ede18ace96bafee6bd
  title: 'Theano: new features and speed improvements'
- pid: 3f3d13e95c25a8f6a753e38dfce88885097cbd43
  title: Untersuchungen zu dynamischen neuronalen Netzen
- pid: 836acf6fc99ebf81d219e2b67f7ab25efc29a6a4
  title: 'Pylearn2: a machine learning research library'
slug: "Gated-Feedback-Recurrent-Neural-Networks-Chung-G\xFCl\xE7ehre"
title: Gated Feedback Recurrent Neural Networks
url: "https://www.semanticscholar.org/paper/Gated-Feedback-Recurrent-Neural-Networks-Chung-G\xFC\
  l\xE7ehre/d14c7e5f5cace4c925abc74c88baa474e9f31a28?sort=total-citations"
venue: ICML
year: 2015
