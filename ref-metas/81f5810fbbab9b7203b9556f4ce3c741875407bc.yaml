authors:
- Mandar Joshi
- Danqi Chen
- Yinhan Liu
- Daniel S. Weld
- Luke Zettlemoyer
- Omer Levy
badges:
- id: OPEN_ACCESS
corpusId: 198229624
fieldsOfStudy:
- Computer Science
numCitedBy: 879
numCiting: 120
paperAbstract: We present SpanBERT, a pre-training method that is designed to better
  represent and predict spans of text. Our approach extends BERT by (1) masking contiguous
  random spans, rather than random tokens, and (2) training the span boundary representations
  to predict the entire content of the masked span, without relying on the individual
  token representations within it. SpanBERT consistently outperforms BERT and our
  better-tuned baselines, with substantial gains on span selection tasks such as question
  answering and coreference resolution. In particular, with the same training data
  and model size as BERTlarge, our single model obtains 94.6% and 88.7% F1 on SQuAD
  1.1 and 2.0 respectively. We also achieve a new state of the art on the OntoNotes
  coreference resolution task (79.6% F1), strong performance on the TACRED relation
  extraction benchmark, and even gains on GLUE.1
ref_count: 68
references:
- pid: df2b0e26d0599ce3e70df8a9da02e51594e0e992
  title: 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'
- pid: 1c71771c701aadfd72c5866170a9f5d71464bb88
  title: Unified Language Model Pre-training for Natural Language Understanding and
    Generation
- pid: 145b8b5d99a2beba6029418ca043585b90138d12
  title: 'MASS: Masked Sequence to Sequence Pre-training for Language Generation'
- pid: c4744a7c2bb298e4a52289a1e085c71cc3d37bc6
  title: 'Transformer-XL: Attentive Language Models beyond a Fixed-Length Context'
- pid: 93b8da28d006415866bf48f9a6e06b5242129195
  title: 'GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language
    Understanding'
- pid: 077f8329a7b6fa3b7c877a57b81eb6c18b5f87de
  title: 'RoBERTa: A Robustly Optimized BERT Pretraining Approach'
- pid: 6e795c6e9916174ae12349f5dc3f516570c17ce8
  title: Skip-Thought Vectors
- pid: 05dd7254b632376973f3a1b4d39485da17814df5
  title: 'SQuAD: 100,000+ Questions for Machine Comprehension of Text'
- pid: ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc
  title: Cross-lingual Language Model Pretraining
- pid: e0c6abdbdecf04ffac65c440da77fb9d66bb474c
  title: 'XLNet: Generalized Autoregressive Pretraining for Language Understanding'
- pid: f010affab57b5fcf1cd6be23df79d8ec98c7289c
  title: 'TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading
    Comprehension'
- pid: 5ded2b8c64491b4a67f6d39ce473d4b9347a672e
  title: A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference
- pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  title: Attention is All you Need
- pid: 1fa9ed2bea208511ae698a967875e943049f16b6
  title: 'HuggingFace''s Transformers: State-of-the-art Natural Language Processing'
- pid: 1e077413b25c4d34945cc2707e17e46ed4fe784a
  title: Universal Language Model Fine-tuning for Text Classification
- pid: 658721bc13b0fa97366d38c05a96bf0a9f4bb0ac
  title: Multi-Task Deep Neural Networks for Natural Language Understanding
- pid: 3eda43078ae1f4741f09be08c4ecab6229046a5c
  title: 'NewsQA: A Machine Comprehension Dataset'
- pid: 17dbd7b72029181327732e4d11b52a08ed4630d0
  title: 'Natural Questions: A Benchmark for Question Answering Research'
- pid: 4d1c856275744c0284312a3a50efb6ca9dc4cd4c
  title: "Know What You Don\u2019t Know: Unanswerable Questions for SQuAD"
- pid: 687bac2d3320083eb4530bf18bb8f8f721477600
  title: Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank
- pid: a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096
  title: 'SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual
    Focused Evaluation'
- pid: bc1d609520290e0460c49b685675eb5a57fa5935
  title: An efficient framework for learning sentence representations
- pid: 0f8468de03ee9f12d693237bec87916311bf1c24
  title: The Seventh PASCAL Recognizing Textual Entailment Challenge
- pid: 3febb2bed8865945e7fddc99efd791887bb7e14f
  title: Deep Contextualized Word Representations
- pid: 128cb6b891aee1b5df099acb48e2efecfcff689f
  title: The Winograd Schema Challenge
- pid: 136326377c122560768db674e35f5bcd6de3bc40
  title: The Second PASCAL Recognising Textual Entailment Challenge
- pid: 63e39cdf1ad884da6bc69096bb3413b5b1100559
  title: Using the Output Embedding to Improve Language Models
- pid: 475354f10798f110d34792b6d88f31d6d5cb099e
  title: Automatically Constructing a Corpus of Sentential Paraphrases
- pid: cb0f3ee1e98faf92429d601cdcd76c69c1e484eb
  title: Neural Network Acceptability Judgments
- pid: de794d50713ea5f91a7c9da3d72041e2f5ef8452
  title: The PASCAL Recognising Textual Entailment Challenge
- pid: a6cb366736791bcccc5c8639de5a8f9636bf87e8
  title: 'Adam: A Method for Stochastic Optimization'
- pid: d07284a6811f1b2745d91bdb06b040b57f226882
  title: Decoupled Weight Decay Regularization
- pid: 97fb4e3d45bb098e27e0071448b6152217bd35a5
  title: Layer Normalization
- pid: 15f4c35889ccc1ae258b680c2ca2fcbfe1e260f7
  title: Gaussian Error Linear Units (GELUs)
slug: SpanBERT:-Improving-Pre-training-by-Representing-Joshi-Chen
title: 'SpanBERT: Improving Pre-training by Representing and Predicting Spans'
url: https://www.semanticscholar.org/paper/SpanBERT:-Improving-Pre-training-by-Representing-Joshi-Chen/81f5810fbbab9b7203b9556f4ce3c741875407bc?sort=total-citations
venue: TACL
year: 2020
