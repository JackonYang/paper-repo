authors:
- Shuohang Wang
- Jing Jiang
badges:
- id: OPEN_ACCESS
corpusId: 11004224
fieldsOfStudy:
- Computer Science
numCitedBy: 357
numCiting: 28
paperAbstract: Natural language inference (NLI) is a fundamentally important task
  in natural language processing that has many applications. The recently released
  Stanford Natural Language Inference (SNLI) corpus has made it possible to develop
  and evaluate learning-centered methods such as deep neural networks for natural
  language inference (NLI). In this paper, we propose a special long short-term memory
  (LSTM) architecture for NLI. Our model builds on top of a recently proposed neural
  attention model for NLI but is based on a significantly different idea. Instead
  of deriving sentence embeddings for the premise and the hypothesis to be used for
  classification, our solution uses a match-LSTM to perform word-by-word matching
  of the hypothesis with the premise. This LSTM is able to place more emphasis on
  important word-level matching results. In particular, we observe that this LSTM
  remembers important mismatches that are critical for predicting the contradiction
  or the neutral relationship label. On the SNLI corpus, our model achieves an accuracy
  of 86.1%, outperforming the state of the art.
ref_count: 28
references:
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 2521
  pid: f04df4e20a18358ea2f689b4c129781628ef7fc1
  title: A large annotated corpus for learning natural language inference
  year: 2015
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 669
  pid: 2846e83d405cbe3bf2f0f3b5f635dd8b3c680c45
  title: Reasoning about Entailment with Neural Attention
  year: 2016
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 19347
  pid: fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5
  title: Neural Machine Translation by Jointly Learning to Align and Translate
  year: 2015
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1928
  pid: 6e795c6e9916174ae12349f5dc3f516570c17ce8
  title: Skip-Thought Vectors
  year: 2015
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 22548
  pid: f37e1b62a767a307c046404ca96bc140b3e68cb5
  title: 'GloVe: Global Vectors for Word Representation'
  year: 2014
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 2108
  pid: 5082a1a13daea5c7026706738f8528391a1e6d59
  title: A Neural Attention Model for Abstractive Sentence Summarization
  year: 2015
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 496
  pid: c333778104f648c385b4631f7b4a859787e9d3d3
  title: A SICK cure for the evaluation of compositional distributional semantic models
  year: 2014
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 2433
  pid: d1505c6123c102e53eb19dff312cb25cea840b72
  title: Teaching Machines to Read and Comprehend
  year: 2015
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 51714
  pid: 44d2abe2175df8153f465f6c39b68b76a0d40ab9
  title: Long Short-Term Memory
  year: 1997
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 691
  pid: ef12383f516840ec1ec998cd5921dfc6e197c9b2
  title: 'PPDB: The Paraphrase Database'
  year: 2013
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 434
  pid: 46b8cbcdff87b842c2c1d4a003c831f845096ba7
  title: Order-Embeddings of Images and Language
  year: 2016
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1748
  pid: a97b5db17acc731ef67321832dbbaf5766153135
  title: Supervised Sequence Labelling with Recurrent Neural Networks
  year: 2008
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 90126
  pid: a6cb366736791bcccc5c8639de5a8f9636bf87e8
  title: 'Adam: A Method for Stochastic Optimization'
  year: 2015
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1763
  pid: de794d50713ea5f91a7c9da3d72041e2f5ef8452
  title: The PASCAL Recognising Textual Entailment Challenge
  year: 2005
slug: Learning-Natural-Language-Inference-with-LSTM-Wang-Jiang
title: Learning Natural Language Inference with LSTM
url: https://www.semanticscholar.org/paper/Learning-Natural-Language-Inference-with-LSTM-Wang-Jiang/596c882de006e4bb4a93f1fa08a5dd467bee060a?sort=total-citations
venue: NAACL
year: 2016
