authors:
- Nicolas Le Roux
- Pierre-Antoine Manzagol
- Yoshua Bengio
badges:
- id: OPEN_ACCESS
corpusId: 9666804
fieldsOfStudy:
- Computer Science
numCitedBy: 179
numCiting: 9
paperAbstract: Guided by the goal of obtaining an optimization algorithm that is both
  fast and yields good generalization, we study the descent direction maximizing the
  decrease in generalization error or the probability of not increasing generalization
  error. The surprising result is that from both the Bayesian and frequentist perspectives
  this can yield the natural gradient direction. Although that direction can be very
  expensive to compute we develop an efficient, general, online approximation to the
  natural gradient descent which is suited to large scale problems. We report experimental
  results showing much faster convergence in computation time and in number of iterations
  with TONGA (Topmoumoute Online natural Gradient Algorithm) than with stochastic
  gradient descent, even on very large datasets.
ref_count: 9
references:
- pid: 5a767a341364de1f75bea85e0b12ba7d3586a461
  title: Natural Gradient Works Efficiently in Learning
- pid: ffa94bba647817fa5e8f8d3250fc977435b5ca76
  title: Fast Curvature Matrix-Vector Products for Second-Order Gradient Descent
- pid: b8012351bc5ebce4a4b3039bbbba3ce393bc3315
  title: An empirical evaluation of deep architectures on problems with many factors
    of variation
- pid: 493cd5e15d6b8726ce27ed0595f1993c27525670
  title: Numerical recipes
slug: Topmoumoute-Online-Natural-Gradient-Algorithm-Roux-Manzagol
title: Topmoumoute Online Natural Gradient Algorithm
url: https://www.semanticscholar.org/paper/Topmoumoute-Online-Natural-Gradient-Algorithm-Roux-Manzagol/6ed460701019072ee2e364a1a491f73dd931f27f?sort=total-citations
venue: NIPS
year: 2007
