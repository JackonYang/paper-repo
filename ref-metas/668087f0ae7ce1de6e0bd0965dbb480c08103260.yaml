authors:
- J. Elman
badges:
- id: UNPAYWALL
- id: OPEN_ACCESS
corpusId: 2763403
fieldsOfStudy:
- Psychology
numCitedBy: 9863
numCiting: 111
paperAbstract: Time underlies many interesting human behaviors. Thus, the question
  of how to represent time in connectionist models is very important. One approach
  is to represent time implicitly by its effects on processing rather than explicitly
  (as in a spatial representation). The current report develops a proposal along these
  lines first described by Jordan (1986) which involves the use of recurrent links
  in order to provide networks with a dynamic memory. In this approach, hidden unit
  patterns are fed back to themselves; the internal representations which develop
  thus reflect task demands in the context of prior internal states. A set of simulations
  is reported which range from relatively simple problems (temporal version of XOR)
  to discovering syntactic/semantic features for words. The networks are able to learn
  interesting internal representations which incorporate task demands with memory
  demands; indeed, in this approach the notion of memory is inextricably bound up
  with task processing. These representations reveal a rich structure, which allows
  them to be highly context-dependent while also expressing generalizations across
  classes of items. These representations suggest a method for representing lexical
  categories and the type/token distinction.
ref_count: 112
references:
- pid: d0d7f8b5d54e3d68fd45a70d4a0d13f42e8d71ff
  title: Learning Subsequential Structure in Simple Recurrent Networks
- pid: 9430105f125277f3b32380ec85c965fc796dd580
  title: On the proper treatment of connectionism
- pid: 3106e66537a0c8f53278e553bcb38f0b0992ec0e
  title: Distributed Representations
- pid: 56cbfcbfffd8c54bd8477d10b6e0e17e097b97c7
  title: 'Connectionism and cognitive architecture: A critical analysis'
- pid: de996c32045df6f7b404dda2a753b6a9becf3c08
  title: Parallel Networks that Learn to Pronounce English Text
- pid: e69606729837aa1d0168c47f812cbccaba09dc83
  title: Neural computation by concentrating information in time.
- pid: c86590e947c28e8791d1e8bab8fc8ab53302341f
  title: Learning the hidden structure of speech.
- pid: ff2c2e3e83d1e8828695484728393c76ee07a101
  title: 'Parallel distributed processing: explorations in the microstructure of cognition,
    vol. 1: foundations'
- pid: cd62c9976534a6a2096a38244f6cbb03635a127e
  title: Phoneme recognition using time-delay neural networks
- pid: f1e40283ecd4633c36c70fbc8dbb14e9a4afb37f
  title: Learning Phonetic Features Using Connectionist Networks
- pid: ce9a21b93ba29d4145a8ef6bf401e77f261848de
  title: A Learning Algorithm for Continually Running Fully Recurrent Neural Networks
- pid: 16c762445f11fa2020994918dc4f93e76264df17
  title: "\u0935\u093E\u0915\u094D\u092F\u0935\u093F\u0928\u094D\u092F\u093E\u0938\
    \ \u0915\u093E \u0938\u0948\u0926\u094D\u0927\u093E\u0928\u094D\u0924\u093F\u0915\
    \ \u092A\u0915\u094D\u0937 = Aspects of the theory of syntax"
- pid: bd459cc59b09e612eeec5327d0690d1508ffe362
  title: A theory of syntactic recognition for natural language
- pid: 111fd833a4ae576cfdbb27d87d2f8fc0640af355
  title: Learning internal representations by error propagation
slug: Finding-Structure-in-Time-Elman
title: Finding Structure in Time
url: https://www.semanticscholar.org/paper/Finding-Structure-in-Time-Elman/668087f0ae7ce1de6e0bd0965dbb480c08103260?sort=total-citations
venue: Cogn. Sci.
year: 1990
