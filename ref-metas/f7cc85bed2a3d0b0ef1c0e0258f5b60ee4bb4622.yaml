authors:
- Duy-Kien Nguyen
- Takayuki Okatani
badges:
- id: UNPAYWALL
- id: OPEN_ACCESS
corpusId: 4625261
fieldsOfStudy:
- Computer Science
numCitedBy: 171
numCiting: 38
paperAbstract: A key solution to visual question answering (VQA) exists in how to
  fuse visual and language features extracted from an input image and question. We
  show that an attention mechanism that enables dense, bi-directional interactions
  between the two modalities contributes to boost accuracy of prediction of answers.
  Specifically, we present a simple architecture that is fully symmetric between visual
  and language representations, in which each question word attends on image regions
  and each image region attends on question words. It can be stacked to form a hierarchy
  for multi-step interactions between an image-question pair. We show through experiments
  that the proposed architecture achieves a new state-of-the-art on VQA and VQA 2.0
  despite its small size. We also present qualitative evaluation, demonstrating how
  the proposed attention mechanism can generate reasonable attention maps on images
  and questions, which leads to the correct answer prediction.
ref_count: 38
references:
- pid: fb9d253258d6b3beceb9d6cd7bba6e0a29ab875b
  title: Hierarchical Question-Image Co-Attention for Visual Question Answering
- pid: d740d0a960368633ed32fc84877b8391993acdca
  title: Multi-level Attention Networks for Visual Question Answering
- pid: 1cf6bc0866226c1f8e282463adc8b75d92fba9bb
  title: 'Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for
    Visual Question Answering'
- pid: 5823d18cd378898b12de537862d996443ce9c9e8
  title: Structured Attentions for Visual Question Answering
- pid: fddc15480d086629b960be5bff96232f967f2252
  title: Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual
    Grounding
- pid: 7214daf035ab005b3d1e739750dd597b4f4513fa
  title: A Focused Dynamic Attention Model for Visual Question Answering
- pid: f651593fa6c83d717fc961482696a53b6fca5ab5
  title: Dual Attention Networks for Multimodal Reasoning and Matching
- pid: a27ed1310b9c0832bde8f906e0fcd23d1f3aa79c
  title: 'Making the V in VQA Matter: Elevating the Role of Image Understanding in
    Visual Question Answering'
- pid: 8e9ad6f8b2bc97f0412fa0cc243ac6975864534a
  title: Multi-modal Factorized Bilinear Pooling with Co-attention Learning for Visual
    Question Answering
- pid: 2c1890864c1c2b750f48316dc8b650ba4772adc5
  title: Stacked Attention Networks for Image Question Answering
- pid: d674b540dcd968bc302ea4360df3f4e85e994b55
  title: 'Show, Ask, Attend, and Answer: A Strong Baseline For Visual Question Answering'
- pid: 97ad70a9fa3f99adf18030e5e38ebe3d90daa2db
  title: 'VQA: Visual Question Answering'
- pid: fe466e84fa2e838adc3c37ee327cd68004ae08fe
  title: 'MUTAN: Multimodal Tucker Fusion for Visual Question Answering'
- pid: 175e9bb50cc062c6c1742a5d90c8dfe31d2e4e22
  title: 'Where to Look: Focus Regions for Visual Question Answering'
- pid: 1afb710a5b35a2352a6495e4bf6eef66808daf1b
  title: Multimodal Residual Learning for Visual QA
- pid: b58e08741fb9803fa2a870eee139137d3bade332
  title: Training Recurrent Answering Units with Joint Loss Minimization for VQA
- pid: afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d
  title: 'Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image
    Annotations'
- pid: b14a60a1c3e6bb45baddd754a1cfe83ffc1bbb81
  title: 'Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge'
- pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  title: Attention is All you Need
- pid: a396a6febdacb84340d139096455e67049ac1e22
  title: 'Learning to Reason: End-to-End Module Networks for Visual Question Answering'
- pid: 6e795c6e9916174ae12349f5dc3f516570c17ce8
  title: Skip-Thought Vectors
- pid: 75ddc7ee15be14013a3462c01b38b0548486fbcb
  title: Learning to Compose Neural Networks for Question Answering
- pid: e978d832a4d86571e1b52aa1685dc32ccb250f50
  title: Dynamic Coattention Networks For Question Answering
- pid: 081651b38ff7533550a3adfc1c00da333a8fe86c
  title: How transferable are features in deep neural networks?
- pid: 1a2a770d23b4a171fa81de62a78a3deb0588f238
  title: Visualizing and Understanding Convolutional Networks
- pid: f37e1b62a767a307c046404ca96bc140b3e68cb5
  title: 'GloVe: Global Vectors for Word Representation'
- pid: bc8fa64625d9189f5801837e7b133e7fe3c581f7
  title: 'Learned in Translation: Contextualized Word Vectors'
- pid: 2c03df8b48bf3fa39054345bafabfeff15bfd11d
  title: Deep Residual Learning for Image Recognition
- pid: 327dc2fd203a7049f3409479ab68e5e2a83cd352
  title: Compact Bilinear Pooling
- pid: 71b7178df5d2b112d07e45038cb5637208659ff7
  title: 'Microsoft COCO: Common Objects in Context'
- pid: fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5
  title: Neural Machine Translation by Jointly Learning to Align and Translate
- pid: 5b8364c21155d3d2cd38ea4c8b8580beba9a3250
  title: An Empirical Exploration of Recurrent Network Architectures
- pid: a6cb366736791bcccc5c8639de5a8f9636bf87e8
  title: 'Adam: A Method for Stochastic Optimization'
- pid: 34f25a8704614163c4095b3ee2fc969b60de4698
  title: 'Dropout: a simple way to prevent neural networks from overfitting'
- pid: c3823aacea60bc1f2cabb9283144690a3d015db5
  title: Neural Turing Machines
slug: Improved-Fusion-of-Visual-and-Language-by-Dense-for-Nguyen-Okatani
title: Improved Fusion of Visual and Language Representations by Dense Symmetric Co-attention
  for Visual Question Answering
url: https://www.semanticscholar.org/paper/Improved-Fusion-of-Visual-and-Language-by-Dense-for-Nguyen-Okatani/f7cc85bed2a3d0b0ef1c0e0258f5b60ee4bb4622?sort=total-citations
venue: 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition
year: 2018
