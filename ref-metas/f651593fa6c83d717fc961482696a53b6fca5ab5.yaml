authors:
- Hyeonseob Nam
- Jung-Woo Ha
- Jeonghee Kim
badges:
- id: UNPAYWALL
- id: OPEN_ACCESS
corpusId: 945386
fieldsOfStudy:
- Computer Science
numCitedBy: 469
numCiting: 38
paperAbstract: We propose Dual Attention Networks (DANs) which jointly leverage visual
  and textual attention mechanisms to capture fine-grained interplay between vision
  and language. DANs attend to specific regions in images and words in text through
  multiple steps and gather essential information from both modalities. Based on this
  framework, we introduce two types of DANs for multimodal reasoning and matching,
  respectively. The reasoning model allows visual and textual attentions to steer
  each other during collaborative inference, which is useful for tasks such as Visual
  Question Answering (VQA). In addition, the matching model exploits the two attention
  mechanisms to estimate the similarity between images and sentences by focusing on
  their shared semantics. Our extensive experiments validate the effectiveness of
  DANs in combining vision and language, achieving the state-of-the-art performance
  on public benchmarks for VQA and image-text matching.
ref_count: 38
references:
- pid: fb9d253258d6b3beceb9d6cd7bba6e0a29ab875b
  title: Hierarchical Question-Image Co-Attention for Visual Question Answering
- pid: f96898d15a1bf1fa8925b1280d0e07a7a8e72194
  title: Dynamic Memory Networks for Visual and Textual Question Answering
- pid: fddc15480d086629b960be5bff96232f967f2252
  title: Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual
    Grounding
- pid: 2c1890864c1c2b750f48316dc8b650ba4772adc5
  title: Stacked Attention Networks for Image Question Answering
- pid: 7f1b111f0bb703b0bd97aba505728a9b0d9b2a54
  title: Deep Fragment Embeddings for Bidirectional Image Sentence Mapping
- pid: 1afb710a5b35a2352a6495e4bf6eef66808daf1b
  title: Multimodal Residual Learning for Visual QA
- pid: 21c99706bb26e9012bfb4d8d48009a3d45af59b2
  title: Neural Module Networks
- pid: 2e36ea91a3c8fbff92be2989325531b4002e2afc
  title: Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models
- pid: 153d6feb7149e063b33e8ee437b74e4a2def8057
  title: Multimodal Convolutional Neural Networks for Matching Image and Sentence
- pid: b58e08741fb9803fa2a870eee139137d3bade332
  title: Training Recurrent Answering Units with Joint Loss Minimization for VQA
- pid: 55e022fb7581bb9e1fce678d21fb25ffbb3fbb88
  title: Deep Visual-Semantic Alignments for Generating Image Descriptions
- pid: 20dbdf02497aa84510970d0f5e8b599073bca1bc
  title: 'Ask Me Anything: Free-Form Visual Question Answering Based on Knowledge
    from External Sources'
- pid: 175e9bb50cc062c6c1742a5d90c8dfe31d2e4e22
  title: 'Where to Look: Focus Regions for Visual Question Answering'
- pid: 4d8f2d14af5991d4f0d050d22216825cac3157bd
  title: 'Show, Attend and Tell: Neural Image Caption Generation with Visual Attention'
- pid: 9814df8bd00ba999c4d1e305a7e9bca579dc7c75
  title: 'Framing Image Description as a Ranking Task: Data, Models and Evaluation
    Metrics (Extended Abstract)'
- pid: 0612745dbd292fc0a548a16d39cd73e127faedde
  title: 'Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer
    Image-to-Sentence Models'
- pid: 97ad70a9fa3f99adf18030e5e38ebe3d90daa2db
  title: 'VQA: Visual Question Answering'
- pid: e516d22697bad6d0f7956b0e8bfa93d6eb0b2f17
  title: 'Deep Compositional Captioning: Describing Novel Object Categories without
    Paired Training Data'
- pid: d696a1923288e6c15422660de9553f6fdb6a4fae
  title: Natural Language Object Retrieval
- pid: d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0
  title: 'Show and tell: A neural image caption generator'
- pid: b27e791e843c924ef052981b79490ab59fc0433d
  title: Learning Deep Structure-Preserving Image-Text Embeddings
- pid: 54b2b6f35f1b5704dddfaa3a137a2f4ad3dfe745
  title: Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)
- pid: 70155488a49d51755c1dfea728e03a6dd72703a1
  title: Deep Networks with Internal Selective Attention through Feedback Connections
- pid: a2785f66c20fbdf30ec26c0931584c6d6a0f4fca
  title: 'DRAW: A Recurrent Neural Network For Image Generation'
- pid: 5082a1a13daea5c7026706738f8528391a1e6d59
  title: A Neural Attention Model for Abstractive Sentence Summarization
- pid: 267fb4ac632449dbe84f7acf17c8c7527cb25b0d
  title: Simple Baseline for Visual Question Answering
- pid: 8a756d4d25511d92a45d0f4545fa819de993851d
  title: Recurrent Models of Visual Attention
- pid: eb42cf88027de515750f230b23b1a057dc782108
  title: Very Deep Convolutional Networks for Large-Scale Image Recognition
- pid: 51239b320c73f3f2219286bf62f24d6763379328
  title: Associating neural word embeddings with deep image representations using
    Fisher Vectors
- pid: efb0e69bc640171d1f115bb286d865bec6f21a7f
  title: Deep correlation for matching images and text
- pid: 452059171226626718eb677358836328f884298e
  title: 'Ask Me Anything: Dynamic Memory Networks for Natural Language Processing'
- pid: 385c18cc4024a3b3206c508c512e037b9c00b8f3
  title: Image Question Answering Using Convolutional Neural Network with Dynamic
    Parameter Prediction
- pid: b21c78a62fbb945a19ae9a8935933711647e7d70
  title: A Hierarchical Neural Autoencoder for Paragraphs and Documents
- pid: 71b7178df5d2b112d07e45038cb5637208659ff7
  title: 'Microsoft COCO: Common Objects in Context'
- pid: 44040913380206991b1991daf1192942e038fe31
  title: 'From image descriptions to visual denotations: New similarity metrics for
    semantic inference over event descriptions'
- pid: 2c03df8b48bf3fa39054345bafabfeff15bfd11d
  title: Deep Residual Learning for Image Recognition
- pid: fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5
  title: Neural Machine Translation by Jointly Learning to Align and Translate
slug: Dual-Attention-Networks-for-Multimodal-Reasoning-Nam-Ha
title: Dual Attention Networks for Multimodal Reasoning and Matching
url: https://www.semanticscholar.org/paper/Dual-Attention-Networks-for-Multimodal-Reasoning-Nam-Ha/f651593fa6c83d717fc961482696a53b6fca5ab5?sort=total-citations
venue: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
year: 2017
