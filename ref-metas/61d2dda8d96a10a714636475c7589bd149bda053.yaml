authors:
- Zhilin Yang
- Ye Yuan
- Yuexin Wu
- William W. Cohen
- R. Salakhutdinov
badges:
- id: OPEN_ACCESS
corpusId: 17369385
fieldsOfStudy:
- Computer Science
numCitedBy: 210
numCiting: 23
paperAbstract: 'We propose a novel extension of the encoder-decoder framework, called
  a review network. The review network is generic and can enhance any existing encoder-
  decoder model: in this paper, we consider RNN decoders with both CNN and RNN encoders.
  The review network performs a number of review steps with attention mechanism on
  the encoder hidden states, and outputs a thought vector after each review step;
  the thought vectors are used as the input of the attention mechanism in the decoder.
  We show that conventional encoder-decoders are a special case of our framework.
  Empirically, we show that our framework improves over state-of- the-art encoder-decoder
  systems on the tasks of image captioning and source code captioning.'
ref_count: 23
references:
- pid: 0b544dfe355a5070b60986319a3f51fb45d1348e
  title: "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical\
    \ Machine Translation"
- pid: d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0
  title: 'Show and tell: A neural image caption generator'
- pid: 4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e
  title: End-To-End Memory Networks
- pid: 15f102c3c9f4d4fe6ba105e221df48c6e8902b3b
  title: From captions to visual concepts and back
- pid: bf55591e09b58ea9ce8d66110d6d3000ee804bdd
  title: Image Captioning with Semantic Attention
- pid: 5082a1a13daea5c7026706738f8528391a1e6d59
  title: A Neural Attention Model for Abstractive Sentence Summarization
- pid: cea967b59209c6be22829699f05b8b1ac4dc092d
  title: Sequence to Sequence Learning with Neural Networks
- pid: 4d8f2d14af5991d4f0d050d22216825cac3157bd
  title: 'Show, Attend and Tell: Neural Image Caption Generation with Visual Attention'
- pid: eb42cf88027de515750f230b23b1a057dc782108
  title: Very Deep Convolutional Networks for Large-Scale Image Recognition
- pid: 93499a7c7f699b6630a86fad964536f9423bb6d0
  title: Effective Approaches to Attention-based Neural Machine Translation
- pid: 23ffaa0fe06eae05817f527a47ac3291077f9e58
  title: Rethinking the Inception Architecture for Computer Vision
- pid: fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5
  title: Neural Machine Translation by Jointly Learning to Align and Translate
- pid: 452059171226626718eb677358836328f884298e
  title: 'Ask Me Anything: Dynamic Memory Networks for Natural Language Processing'
- pid: d01379ebb53c66a4ccf5f4959d904dcf9e161e41
  title: 'Order Matters: Sequence to sequence for sets'
- pid: 696ca58d93f6404fea0fc75c62d1d7b378f47628
  title: 'Microsoft COCO Captions: Data Collection and Evaluation Server'
- pid: 71ae756c75ac89e2d731c9c79649562b5768ff39
  title: Memory Networks
- pid: 413c1142de9d91804d6d11c67ff3fed59c9fc279
  title: Adaptive Subgradient Methods for Online Learning and Stochastic Optimization
slug: Review-Networks-for-Caption-Generation-Yang-Yuan
title: Review Networks for Caption Generation
url: https://www.semanticscholar.org/paper/Review-Networks-for-Caption-Generation-Yang-Yuan/61d2dda8d96a10a714636475c7589bd149bda053?sort=total-citations
venue: NIPS
year: 2016
