authors:
- S. Nowlan
- Geoffrey E. Hinton
badges: []
corpusId: 5597033
fieldsOfStudy:
- Computer Science
numCitedBy: 644
numCiting: 50
paperAbstract: One way of simplifying neural networks so they generalize better is
  to add an extra term to the error function that will penalize complexity. Simple
  versions of this approach include penalizing the sum of the squares of the weights
  or penalizing the number of nonzero weights. We propose a more complicated penalty
  term in which the distribution of weight values is modeled as a mixture of multiple
  gaussians. A set of weights is simple if the weights have high probability density
  under the mixture model. This can be achieved by clustering the weights into subsets
  with the weights in each cluster having very similar values. Since we do not know
  the appropriate means or variances of the clusters in advance, we allow the parameters
  of the mixture model to adapt at the same time as the network learns. Simulations
  on two different problems demonstrate that this complexity term is more effective
  than previous complexity terms.
ref_count: 51
references:
- pid: 59fa47fc237a0781b4bf1c84fedb728d20db26a1
  title: 'Soft competitive adaptation: neural network learning algorithms based on
    fitting statistical mixtures'
- pid: 25406e6733a698bfc4ac836f8e74f458e75dad4f
  title: What Size Net Gives Valid Generalization?
- pid: f707a81a278d1598cd0a4493ba73f22dcdf90639
  title: Generalization by Weight-Elimination with Application to Forecasting
- pid: 2cee043045b529fceda7964a70e626d45657245a
  title: 'Predicting the Future: a Connectionist Approach'
- pid: 6f3175b3930d0c71495a52a7bccb3889e5f33520
  title: 'Generalization and Parameter Estimation in Feedforward Netws: Some Experiments'
- pid: e7297db245c3feb1897720b173a59fe7e36babb7
  title: Optimal Brain Damage
- pid: c83684f6207697c12850db423fd9747572cf1784
  title: Bayesian Back-Propagation
- pid: 82fa37d5be8e747131a5857992cc33bb95469ce3
  title: Developments in Maximum Entropy Data Analysis
- pid: 3e6bea2649298c68d17b9421fc7dd19eeacc935e
  title: Learning Translation Invariant Recognition in Massively Parallel Networks
- pid: 6272baf82e2e442edab4fb613ef2b7186bf5f1fb
  title: Bayesian Inductive Inference and Maximum Entropy
- pid: e08d090d1e586610d636a46004876e9f3ded8209
  title: A time-delay neural network architecture for isolated word recognition
- pid: d382b9c11e5c6a8e173fbeb442545e3be8d3e3a5
  title: Modeling By Shortest Data Description*
- pid: 86ab4cae682fbd49c5a5bedb630e5a40fa7529f6
  title: Handwritten Digit Recognition with a Back-Propagation Network
- pid: f3a217c11175f2cf904b2f7f6378b7ade176f2d0
  title: Associative memory. A system-theoretical approach
- pid: d36efb9ad91e00faa334b549ce989bfae7e2907a
  title: Maximum likelihood from incomplete data via the EM - algorithm plus discussions
    on the paper
- pid: 400b45a803d642b752a84147ef547af7811e8f3f
  title: Information Theory and an Extension of the Maximum Likelihood Principle
- pid: acb94e335c64a4bbda6a493d6386e1138bcd109c
  title: Theory of probability
- pid: 01b6affe3ea4eae1978aec54e87087feb76d9215
  title: Generalization and network design strategies
- pid: 4ade4934db522fe6d634ff6f48887da46eedb4d1
  title: Learning distributed representations of concepts.
- pid: f1f4386524be3ed96caaf05f661aacb94db1e566
  title: Theory of Probability
slug: Simplifying-Neural-Networks-by-Soft-Weight-Sharing-Nowlan-Hinton
title: Simplifying Neural Networks by Soft Weight-Sharing
url: https://www.semanticscholar.org/paper/Simplifying-Neural-Networks-by-Soft-Weight-Sharing-Nowlan-Hinton/de75e4e15e22d4376300e5c968e2db44be29ac9e?sort=total-citations
venue: Neural Computation
year: 1992
