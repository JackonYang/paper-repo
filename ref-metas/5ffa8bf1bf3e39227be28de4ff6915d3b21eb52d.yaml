authors:
- Yoshua Bengio
- Eric Thibodeau-Laufer
- Guillaume Alain
- J. Yosinski
badges:
- id: OPEN_ACCESS
corpusId: 9494295
fieldsOfStudy:
- Computer Science
numCitedBy: 354
numCiting: 44
paperAbstract: We introduce a novel training principle for probabilistic models that
  is an alternative to maximum likelihood. The proposed Generative Stochastic Networks
  (GSN) framework is based on learning the transition operator of a Markov chain whose
  stationary distribution estimates the data distribution. The transition distribution
  of the Markov chain is conditional on the previous state, generally involving a
  small move, so this conditional distribution has fewer dominant modes, being unimodal
  in the limit of small moves. Thus, it is easier to learn because it is easier to
  approximate its partition function, more like learning to perform supervised function
  approximation, with gradients that can be obtained by backprop. We provide theorems
  that generalize recent work on the probabilistic interpretation of denoising autoencoders
  and obtain along the way an interesting justification for dependency networks and
  generalized pseudolikelihood, along with a definition of an appropriate joint distribution
  and sampling mechanism even when the conditionals are not consistent. GSNs can be
  used with missing inputs and can be used to sample subsets of variables given the
  rest. We validate these theoretical results with experiments on two image datasets
  using an architecture that mimics the Deep Boltzmann Machine Gibbs sampler but allows
  training to proceed with simple backprop, without the need for layerwise pretraining.
ref_count: 44
references:
- pid: aaaea06da21f22221d5fbfd61bb3a02439f0fe02
  title: A Generative Process for sampling Contractive Auto-Encoders
- pid: d9704f8119d6ba748230b4f2ad59f0e8c64fdfb0
  title: Generalized Denoising Auto-Encoders as Generative Models
- pid: 484ad17c926292fbe0d5211540832a8c8a8e958b
  title: Stochastic Backpropagation and Approximate Inference in Deep Generative Models
- pid: 355d44f53428b1ac4fb2ab468d593c720640e5bd
  title: Greedy Layer-Wise Training of Deep Networks
- pid: 8978cf7574ceb35f4c3096be768c7547b28a35d0
  title: A Fast Learning Algorithm for Deep Belief Nets
- pid: 85021c84383d18a7a4434d76dc8135fc6bdc0aa6
  title: Deep Boltzmann Machines
- pid: 325ea1f2022ee3886a5810df76dcfbe4010ad439
  title: Structured Learning with Approximate Inference
- pid: 843959ffdccf31c6694d135fad07425924f785b1
  title: Extracting and composing robust features with denoising autoencoders
- pid: 413c1142de9d91804d6d11c67ff3fed59c9fc279
  title: Adaptive Subgradient Methods for Online Learning and Stochastic Optimization
- pid: e60ff004dde5c13ec53087872cfcdd12e85beb57
  title: Learning Deep Architectures for AI
- pid: 5656fa5aa6e1beeb98703fc53ec112ad227c49ca
  title: Multi-Prediction Deep Boltzmann Machines
- pid: 90b63e917d5737b06357d50aa729619e933d9614
  title: Phone Recognition with the Mean-Covariance Restricted Boltzmann Machine
- pid: e3ce36b9deb47aa6bb2aa19c4bfa71283b505025
  title: 'Noise-contrastive estimation: A new estimation principle for unnormalized
    statistical models'
- pid: 7fc604e1a3e45cd2d2742f96d62741930a363efa
  title: A Tutorial on Energy-Based Learning
- pid: e9a54374aec5c92296c7b24436f08934643829ae
  title: Exploiting Tractable Substructures in Intractable Networks
- pid: 932c2a02d462abd75af018125413b1ceaa1ee3f4
  title: Efficient Learning of Sparse Representations with an Energy-Based Model
- pid: 1366de5bb112746a555e9c0cd00de3ad8628aea8
  title: Improving neural networks by preventing co-adaptation of feature detectors
- pid: 9b20ad513361a26e98289e5a517291c6ff49960d
  title: Learning Continuous Attractors in Recurrent Networks
- pid: 57458bc1cffe5caa45a885af986d70f723f406b4
  title: 'A unified architecture for natural language processing: deep neural networks
    with multitask learning'
- pid: abd1c342495432171beb7ca8fd9551ef13cbd0ff
  title: ImageNet classification with deep convolutional neural networks
- pid: d0965d8f9842f2db960b36b528107ca362c00d1a
  title: Better Mixing via Deep Representations
- pid: e64a9960734215e2b1866ea3cb723ffa5585ac14
  title: Efficient sparse coding algorithms
- pid: 30afca3a4056bc54deadc1c5794048436d1c9eb4
  title: Dependency Networks for Inference, Collaborative Filtering, and Data Visualization
- pid: 473f0739666af2791ad6592822118240ed968b70
  title: Conversational Speech Transcription Using Context-Dependent Deep Neural Networks
slug: Deep-Generative-Stochastic-Networks-Trainable-by-Bengio-Thibodeau-Laufer
title: Deep Generative Stochastic Networks Trainable by Backprop
url: https://www.semanticscholar.org/paper/Deep-Generative-Stochastic-Networks-Trainable-by-Bengio-Thibodeau-Laufer/5ffa8bf1bf3e39227be28de4ff6915d3b21eb52d?sort=total-citations
venue: ICML
year: 2014
