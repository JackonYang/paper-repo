authors:
- G. Doddington
badges:
- id: UNPAYWALL
- id: OPEN_ACCESS
corpusId: 14067706
fieldsOfStudy:
- Psychology
numCitedBy: 1570
numCiting: 1
paperAbstract: Evaluation is recognized as an extremely helpful forcing function in
  Human Language Technology R&D. Unfortunately, evaluation has not been a very powerful
  tool in machine translation (MT) research because it requires human judgments and
  is thus expensive and time-consuming and not easily factored into the MT research
  agenda. However, at the July 2001 TIDES PI meeting in Philadelphia, IBM described
  an automatic MT evaluation technique that can provide immediate feedback and guidance
  in MT research. Their idea, which they call an "evaluation understudy", compares
  MT output with expert reference translations in terms of the statistics of short
  sequences of words (word N-grams). The more of these N-grams that a translation
  shares with the reference translations, the better the translation is judged to
  be. The idea is elegant in its simplicity. But far more important, IBM showed a
  strong correlation between these automatically generated scores and human judgments
  of translation quality. As a result, DARPA commissioned NIST to develop an MT evaluation
  facility based on the IBM work. This utility is now available from NIST and serves
  as the primary evaluation measure for TIDES MT research.
ref_count: 1
references:
- pid: d7da009f457917aa381619facfa5ffae9329a6e9
  title: 'Bleu: a Method for Automatic Evaluation of Machine Translation'
slug: Automatic-evaluation-of-machine-translation-quality-Doddington
title: Automatic evaluation of machine translation quality using n-gram co-occurrence
  statistics
url: https://www.semanticscholar.org/paper/Automatic-evaluation-of-machine-translation-quality-Doddington/417f9ce1b1cb3c98e5c2a66d586c7a2eb7438a9f?sort=total-citations
venue: ''
year: 2002
