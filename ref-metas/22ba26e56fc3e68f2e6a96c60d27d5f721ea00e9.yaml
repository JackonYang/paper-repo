authors:
- Yann Dauphin
- Harm de Vries
- Yoshua Bengio
badges:
- id: OPEN_ACCESS
corpusId: 9440787
fieldsOfStudy:
- Computer Science
numCitedBy: 236
numCiting: 29
paperAbstract: Parameter-specific adaptive learning rate methods are computationally
  efficient ways to reduce the ill-conditioning problems encountered when training
  large deep networks. Following recent work that strongly suggests that most of the
  critical points encountered when training such networks are saddle points, we find
  how considering the presence of negative eigenvalues of the Hessian could help us
  design better suited adaptive learning rate schemes. We show that the popular Jacobi
  preconditioner has undesirable behavior in the presence of both positive and negative
  curvature, and present theoretical and empirical evidence that the so-called equilibration
  preconditioner is comparatively better suited to non-convex problems. We introduce
  a novel adaptive learning rate scheme, called ESGD, based on the equilibration preconditioner.
  Our experiments show that ESGD performs as well or better than RMSProp in terms
  of convergence speed, always clearly improving over plain stochastic gradient descent.
ref_count: 29
references:
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 8025
  pid: 413c1142de9d91804d6d11c67ff3fed59c9fc279
  title: Adaptive Subgradient Methods for Online Learning and Stochastic Optimization
  year: 2010
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1075
  pid: 981ce6b655cc06416ff6bf7fac8c6c2076fd7fac
  title: Identifying and attacking the saddle point problem in high-dimensional non-convex
    optimization
  year: 2014
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 119
  pid: a98483785378bde7e2384a3035b2b501ee03654b
  title: Krylov Subspace Descent for Deep Learning
  year: 2012
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 3557
  pid: aa7bfd2304201afbb19971ebde87b17e40242e91
  title: On the importance of initialization and momentum in deep learning
  year: 2013
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 844
  pid: 4c46347fbc272b21468efe3d9af34b4b2bad6684
  title: Deep learning via Hessian-free optimization
  year: 2010
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 280
  pid: e8f95ccfd13689f672c39dca3eccf1c484533bcc
  title: Revisiting Natural Gradient for Deep Networks
  year: 2014
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 5464
  pid: 8729441d734782c3ed532a7d2d9611b438c0a09a
  title: 'ADADELTA: An Adaptive Learning Rate Method'
  year: 2012
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 275
  pid: ffa94bba647817fa5e8f8d3250fc977435b5ca76
  title: Fast Curvature Matrix-Vector Products for Second-Order Gradient Descent
  year: 2002
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1374
  pid: 855d0f722d75cc56a66a00ede18ace96bafee6bd
  title: 'Theano: new features and speed improvements'
  year: 2012
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 2630
  pid: b87274e6d9aa4e6ba5148898aa92941617d2b6ed
  title: Efficient BackProp
  year: 2012
slug: Equilibrated-adaptive-learning-rates-for-non-convex-Dauphin-Vries
title: Equilibrated adaptive learning rates for non-convex optimization
url: https://www.semanticscholar.org/paper/Equilibrated-adaptive-learning-rates-for-non-convex-Dauphin-Vries/22ba26e56fc3e68f2e6a96c60d27d5f721ea00e9?sort=total-citations
venue: NIPS
year: 2015
