authors:
- R. Sutton
- David A. McAllester
- Satinder Singh
- Y. Mansour
badges:
- id: OPEN_ACCESS
corpusId: 1211821
fieldsOfStudy:
- Computer Science
numCitedBy: 4404
numCiting: 32
paperAbstract: Function approximation is essential to reinforcement learning, but
  the standard approach of approximating a value function and determining a policy
  from it has so far proven theoretically intractable. In this paper we explore an
  alternative approach in which the policy is explicitly represented by its own function
  approximator, independent of the value function, and is updated according to the
  gradient of expected reward with respect to the policy parameters. Williams's REINFORCE
  method and actor-critic methods are examples of this approach. Our main new result
  is to show that the gradient can be written in a form suitable for estimation from
  experience aided by an approximate action-value or advantage function. Using this
  result, we prove for the first time that a version of policy iteration with arbitrary
  differentiable function approximation is convergent to a locally optimal policy.
ref_count: 32
references:
- pid: 8a7acaf6469c06ae5876d92f013184db5897bb13
  title: Neuronlike adaptive elements that can solve difficult learning control problems
- pid: b1362879e77efef96ab552f5cb1198c2a67204d6
  title: Introduction to Reinforcement Learning
slug: Policy-Gradient-Methods-for-Reinforcement-Learning-Sutton-McAllester
title: Policy Gradient Methods for Reinforcement Learning with Function Approximation
url: https://www.semanticscholar.org/paper/Policy-Gradient-Methods-for-Reinforcement-Learning-Sutton-McAllester/a20f0ce0616def7cc9a87446c228906cd5da093b?sort=total-citations
venue: NIPS
year: 1999
