authors:
- Pedro M. Domingos
- M. Pazzani
badges:
- id: OPEN_ACCESS
corpusId: 77139
fieldsOfStudy:
- Computer Science
numCitedBy: 3124
numCiting: 59
paperAbstract: The simple Bayesian classifier is known to be optimal when attributes
  are independent given the class, but the question of whether other sufficient conditions
  for its optimality exist has so far not been explored. Empirical results showing
  that it performs surprisingly well in many domains containing clear attribute dependences
  suggest that the answer to this question may be positive. This article shows that,
  although the Bayesian classifier's probability estimates are only optimal under
  quadratic loss if the independence assumption holds, the classifier itself can be
  optimal under zero-one loss (misclassification rate) even when this assumption is
  violated by a wide margin. The region of quadratic-loss optimality of the Bayesian
  classifier is in fact a second-order infinitesimal fraction of the region of zero-one
  optimality. This implies that the Bayesian classifier has a much greater range of
  applicability than previously thought. For example, in this article it is shown
  to be optimal for learning conjunctions and disjunctions, even though they violate
  the independence assumption. Further, studies in artificial domains show that it
  will often outperform more powerful classifiers for common training set sizes and
  numbers of attributes, even if its bias is a priori much less appropriate to the
  domain. This article's results also imply that detecting attribute dependence is
  not necessarily the best way to extend the Bayesian classifier, and this is also
  verified empirically.
ref_count: 59
references:
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 428
  pid: 0d136cd362fb9d38cec1b6dbbf41c3d693c2cec1
  title: Learning Limited Dependence Bayesian Classifiers
  year: 1996
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 637
  pid: d414438926b73bde0313948d8b074cb5360a0e6f
  title: Bias, Variance , And Arcing Classifiers
  year: 1996
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 551
  pid: 3b814ad3055d6bfd7828effdbfbf1372646b7c22
  title: 'Quantifying Inductive Bias: AI Learning Algorithms and Valiant''s Learning
    Framework'
  year: 1988
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 21898
  pid: 807c1f19047f96083e13614f7ce20f2ac98c239a
  title: 'C4.5: Programs for Machine Learning'
  year: 1992
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 443
  pid: 25744dbb4294fe7abb2d9b1b0d39006482ebb4ab
  title: Error-Correcting Output Coding Corrects Bias and Variance
  year: 1995
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 16927
  pid: b07ce649d6f6eb636872527104b0209d3edc8188
  title: Pattern classification and scene analysis
  year: 1973
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 13446
  pid: e068be31ded63600aea068eacd12931efd2a1029
  title: UCI Repository of machine learning databases
  year: 1998
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 580
  pid: ad58594194155af8b48eaf3ab9525a1fafd24e58
  title: 'Estimating Probabilities: A Crucial Task in Machine Learning'
  year: 1990
slug: On-the-Optimality-of-the-Simple-Bayesian-Classifier-Domingos-Pazzani
title: On the Optimality of the Simple Bayesian Classifier under Zero-One Loss
url: https://www.semanticscholar.org/paper/On-the-Optimality-of-the-Simple-Bayesian-Classifier-Domingos-Pazzani/700666f0c59a4fedc8b08294424c47cb99a8e2ff?sort=total-citations
venue: Machine Learning
year: 2004
