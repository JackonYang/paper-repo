authors:
- B. Bakker
- T. Heskes
badges:
- id: OPEN_ACCESS
corpusId: 10436583
fieldsOfStudy:
- Computer Science
numCitedBy: 594
numCiting: 26
paperAbstract: Modeling a collection of similar regression or classification tasks
  can be improved by making the tasks 'learn from each other'. In machine learning,
  this subject is approached through 'multitask learning', where parallel tasks are
  modeled as multiple outputs of the same network. In multilevel analysis this is
  generally implemented through the mixed-effects linear model where a distinction
  is made between 'fixed effects', which are the same for all tasks, and 'random effects',
  which may vary between tasks. In the present article we will adopt a Bayesian approach
  in which some of the model parameters are shared (the same for all tasks) and others
  more loosely connected through a joint prior distribution that can be learned from
  the data. We seek in this way to combine the best parts of both the statistical
  multilevel approach and the neural network machinery. The standard assumption expressed
  in both approaches is that each task can learn equally well from any other task.
  In this article we extend the model by allowing more differentiation in the similarities
  between tasks. One such extension is to make the prior mean depend on higher-level
  task characteristics. More unsupervised clustering of tasks is obtained if we go
  from a single Gaussian prior to a mixture of Gaussians. This can be further generalized
  to a mixture of experts architecture with the gates depending on task characteristics.
  All three extensions are demonstrated through application both on an artificial
  data set and on two real-world problems, one a school problem and the other involving
  single-copy newspaper sales.
ref_count: 26
references:
- pid: 1bd6e929ed8384ea2212d50ab3c103ec018cc9fd
  title: A Bayesian/Information Theoretic Model of Learning to Learn via Multiple
    Task Sampling
- pid: 47aaeb6dc682162dfe5659c2cad64e5d825ad910
  title: Multitask Learning
- pid: 3ce9da2d2182a2fbc4b460bdb56d3c34110b3e39
  title: Probable networks and plausible predictions - a review of practical Bayesian
    methods for supervised neural networks
- pid: f6d8a7fc2e2d53923832f9404376512068ca2a57
  title: Hierarchical mixtures of experts and the EM algorithm
- pid: 072d756c8b17a78018298e67ff29e6d3a4fe5770
  title: 'Overfitting in Neural Nets: Backpropagation, Conjugate Gradient, and Early
    Stopping'
- pid: dbc0a468ab103ae29717703d4aa9f682f6a2b664
  title: Neural Networks for Pattern Recognition
- pid: d36efb9ad91e00faa334b549ce989bfae7e2907a
  title: Maximum likelihood from incomplete data via the EM - algorithm plus discussions
    on the paper
slug: Task-Clustering-and-Gating-for-Bayesian-Multitask-Bakker-Heskes
title: Task Clustering and Gating for Bayesian Multitask Learning
url: https://www.semanticscholar.org/paper/Task-Clustering-and-Gating-for-Bayesian-Multitask-Bakker-Heskes/a43d7b8e5e1bcb7c3fbf82164cfc9d12737176e8?sort=total-citations
venue: J. Mach. Learn. Res.
year: 2003
