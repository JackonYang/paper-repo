authors:
- Danqi Chen
- Jason Bolton
- Christopher D. Manning
badges:
- id: UNPAYWALL
- id: OPEN_ACCESS
corpusId: 6360322
fieldsOfStudy:
- Computer Science
numCitedBy: 493
numCiting: 22
paperAbstract: Enabling a computer to understand a document so that it can answer
  comprehension questions is a central, yet unsolved goal of NLP. A key factor impeding
  its solution by machine learned systems is the limited availability of human-annotated
  data. Hermann et al. (2015) seek to solve this problem by creating over a million
  training examples by pairing CNN and Daily Mail news articles with their summarized
  bullet points, and show that a neural network can then be trained to give good performance
  on this task. In this paper, we conduct a thorough examination of this new reading
  comprehension task. Our primary aim is to understand what depth of language understanding
  is required to do well on this task. We approach this from one side by doing a careful
  hand-analysis of a small subset of the problems and from the other by showing that
  simple, carefully designed systems can obtain accuracies of 73.6% and 76.6% on these
  two datasets, exceeding current state-of-the-art results by 7-10% and approaching
  what we believe is the ceiling for performance on this task.
ref_count: 22
references:
- pid: abb33d75dc297993fcc3fb75e0f4498f413eb4f6
  title: 'Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks'
- pid: f2e50e2ee4021f199877c8920f1f984481c723aa
  title: Text Understanding with the Attention Sum Reader Network
- pid: 564257469fa44cdb57e4272f85253efb9acfd69d
  title: 'MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of
    Text'
- pid: d1505c6123c102e53eb19dff312cb25cea840b72
  title: Teaching Machines to Read and Comprehend
- pid: 93499a7c7f699b6630a86fad964536f9423bb6d0
  title: Effective Approaches to Attention-based Neural Machine Translation
- pid: 452059171226626718eb677358836328f884298e
  title: 'Ask Me Anything: Dynamic Memory Networks for Natural Language Processing'
- pid: 35b91b365ceb016fb3e022577cec96fb9b445dc5
  title: 'The Goldilocks Principle: Reading Children''s Books with Explicit Memory
    Representations'
- pid: 4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e
  title: End-To-End Memory Networks
- pid: 6396ab37641d36be4c26420e58adeb8665914c3b
  title: Modeling Biological Processes for Reading Comprehension
- pid: 71ae756c75ac89e2d731c9c79649562b5768ff39
  title: Memory Networks
- pid: f37e1b62a767a307c046404ca96bc140b3e68cb5
  title: 'GloVe: Global Vectors for Word Representation'
- pid: 999f0acfac28215db2e4c69ff42711fd4f56511d
  title: Machine Comprehension with Syntax, Frames, and Semantics
- pid: a14045a751f5d8ed387c8630a86a3a2861b90643
  title: A Fast and Accurate Dependency Parser using Neural Networks
- pid: 0b544dfe355a5070b60986319a3f51fb45d1348e
  title: "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical\
    \ Machine Translation"
slug: A-Thorough-Examination-of-the-CNN/Daily-Mail-Task-Chen-Bolton
title: A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task
url: https://www.semanticscholar.org/paper/A-Thorough-Examination-of-the-CNN/Daily-Mail-Task-Chen-Bolton/b1e20420982a4f923c08652941666b189b11b7fe?sort=total-citations
venue: ACL
year: 2016
