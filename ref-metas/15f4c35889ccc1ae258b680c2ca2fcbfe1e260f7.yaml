authors:
- Dan Hendrycks
- Kevin Gimpel
badges: []
corpusId: 125617073
fieldsOfStudy:
- Computer Science
numCitedBy: 971
numCiting: 26
paperAbstract: We propose the Gaussian Error Linear Unit (GELU), a high-performing
  neural network activation function. The GELU activation function is $x\Phi(x)$,
  where $\Phi(x)$ the standard Gaussian cumulative distribution function. The GELU
  nonlinearity weights inputs by their value, rather than gates inputs by their sign
  as in ReLUs ($x\mathbf{1}_{x>0}$). We perform an empirical evaluation of the GELU
  nonlinearity against the ReLU and ELU activations and find performance improvements
  across all considered computer vision, natural language processing, and speech tasks.
ref_count: 26
references:
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 3669
  pid: f63e917638553414526a0cc8550de4ad2d83fe7a
  title: Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)
  year: 2016
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 12809
  pid: a538b05ebb01a40323997629e171c91aa28b8e2f
  title: Rectified Linear Units Improve Restricted Boltzmann Machines
  year: 2010
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 266
  pid: 9f0687bcd0a7d7fc91b8c5d36c003a38b8853105
  title: 'Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations'
  year: 2017
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 28158
  pid: 34f25a8704614163c4095b3ee2fc969b60de4698
  title: 'Dropout: a simple way to prevent neural networks from overfitting'
  year: 2014
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 16693
  pid: 98b4d4e24aab57ab4e1124ff8106909050645cfa
  title: Neural networks and physical systems with emergent collective computational
    abilities.
  year: 1982
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1265
  pid: 99c970348b8f70ce23d6641e201904ea49266b6e
  title: Exact solutions to the nonlinear dynamics of learning in deep linear neural
    networks
  year: 2014
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 489
  pid: 97dc8df45972e4ed7423fc992a5092ba25b33411
  title: All you need is a good init
  year: 2016
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 90110
  pid: a6cb366736791bcccc5c8639de5a8f9636bf87e8
  title: 'Adam: A Method for Stochastic Optimization'
  year: 2015
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1290
  pid: 3d2c6941a9b4608ba52b328369a3352db2092ae0
  title: 'Weight Normalization: A Simple Reparameterization to Accelerate Training
    of Deep Neural Networks'
  year: 2016
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 236
  pid: 5d5d4f49d6443c8529a6f5ebef5c499d47a869da
  title: Improving Neural Networks with Dropout
  year: 2013
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1641
  pid: d2b62f77cb2864e465aa60bca6c26bb1d2f84963
  title: Acoustic Modeling Using Deep Belief Networks
  year: 2012
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 2729
  pid: b022f2a277a4bf5f42382e86e4380b96340b9e86
  title: 'SGDR: Stochastic Gradient Descent with Warm Restarts'
  year: 2017
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 4266
  pid: 1c4e9156ca07705531e45960b7a919dc473abb51
  title: Wide Residual Networks
  year: 2016
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 62232
  pid: eb42cf88027de515750f230b23b1a057dc782108
  title: Very Deep Convolutional Networks for Large-Scale Image Recognition
  year: 2015
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1426
  pid: 51db1f3c8dfc7d4077da39c96bb90a6358128111
  title: Deep Networks with Stochastic Depth
  year: 2016
slug: Gaussian-Error-Linear-Units-(GELUs)-Hendrycks-Gimpel
title: Gaussian Error Linear Units (GELUs)
url: https://www.semanticscholar.org/paper/Gaussian-Error-Linear-Units-(GELUs)-Hendrycks-Gimpel/15f4c35889ccc1ae258b680c2ca2fcbfe1e260f7?sort=total-citations
venue: ''
year: 2016
