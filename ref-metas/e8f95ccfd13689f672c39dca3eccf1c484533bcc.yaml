authors:
- Razvan Pascanu
- Yoshua Bengio
badges:
- id: OPEN_ACCESS
corpusId: 15085443
fieldsOfStudy:
- Computer Science
numCitedBy: 280
numCiting: 44
paperAbstract: 'We evaluate natural gradient, an algorithm originally proposed in
  Amari (1997), for learning deep models. The contributions of this paper are as follows.
  We show the connection between natural gradient and three other recently proposed
  methods for training deep models: Hessian-Free (Martens, 2010), Krylov Subspace
  Descent (Vinyals and Povey, 2012) and TONGA (Le Roux et al., 2008). We describe
  how one can use unlabeled data to improve the generalization error obtained by natural
  gradient and empirically evaluate the robustness of the algorithm to the ordering
  of the training set compared to stochastic gradient descent. Finally we extend natural
  gradient to incorporate second order information alongside the manifold information
  and provide a benchmark of the new algorithm using a truncated Newton approach for
  inverting the metric matrix instead of using a diagonal approximation of it.'
ref_count: 44
references:
- pid: a98483785378bde7e2384a3035b2b501ee03654b
  title: Krylov Subspace Descent for Deep Learning
- pid: 0d6203718c15f137fda2f295c96269bc2b254644
  title: Learning Recurrent Neural Networks with Hessian-Free Optimization
- pid: 4c46347fbc272b21468efe3d9af34b4b2bad6684
  title: Deep learning via Hessian-free optimization
- pid: 0d2336389dff3031910bd21dd1c44d1b4cd51725
  title: Why Does Unsupervised Pre-training Help Deep Learning?
- pid: 6ed460701019072ee2e364a1a491f73dd931f27f
  title: Topmoumoute Online Natural Gradient Algorithm
- pid: 5a767a341364de1f75bea85e0b12ba7d3586a461
  title: Natural Gradient Works Efficiently in Learning
- pid: e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de
  title: Generating Text with Recurrent Neural Networks
- pid: c6867b6b564462d6b902f68e0bfa58f4717ca1cc
  title: Fast Exact Multiplication by the Hessian
- pid: ffa94bba647817fa5e8f8d3250fc977435b5ca76
  title: Fast Curvature Matrix-Vector Products for Second-Order Gradient Descent
- pid: 855d0f722d75cc56a66a00ede18ace96bafee6bd
  title: 'Theano: new features and speed improvements'
- pid: 932a106c21a1db1e1876459c1521d27fd152caac
  title: Pattern Recognition and Machine Learning (Information Science and Statistics)
- pid: bf86896c23300a46b7fc76298e365984c0b05105
  title: Numerical Optimization
- pid: 420322994c59e9081786b46b31e2c82a9753e23a
  title: Differential-geometrical methods in statistics
slug: Revisiting-Natural-Gradient-for-Deep-Networks-Pascanu-Bengio
title: Revisiting Natural Gradient for Deep Networks
url: https://www.semanticscholar.org/paper/Revisiting-Natural-Gradient-for-Deep-Networks-Pascanu-Bengio/e8f95ccfd13689f672c39dca3eccf1c484533bcc?sort=total-citations
venue: ICLR
year: 2014
