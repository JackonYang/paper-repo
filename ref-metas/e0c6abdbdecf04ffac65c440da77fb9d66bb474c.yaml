authors:
- Zhilin Yang
- Zihang Dai
- Yiming Yang
- J. Carbonell
- R. Salakhutdinov
- Quoc V. Le
badges:
- id: OPEN_ACCESS
corpusId: 195069387
fieldsOfStudy:
- Computer Science
numCitedBy: 4226
numCiting: 48
paperAbstract: With the capability of modeling bidirectional contexts, denoising autoencoding
  based pretraining like BERT achieves better performance than pretraining approaches
  based on autoregressive language modeling. However, relying on corrupting the input
  with masks, BERT neglects dependency between the masked positions and suffers from
  a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet,
  a generalized autoregressive pretraining method that (1) enables learning bidirectional
  contexts by maximizing the expected likelihood over all permutations of the factorization
  order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation.
  Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive
  model, into pretraining. Empirically, under comparable experiment settings, XLNet
  outperforms BERT on 20 tasks, often by a large margin, including question answering,
  natural language inference, sentiment analysis, and document ranking.
ref_count: 48
references:
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 33781
  pid: df2b0e26d0599ce3e70df8a9da02e51594e0e992
  title: 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 2709
  pid: 7a064df1aeada7e69e5173f7d4c8606f4470365b
  title: 'ALBERT: A Lite BERT for Self-supervised Learning of Language Representations'
  year: 2020
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 216
  pid: b9de9599d7241459db9213b5cdd7059696f5ef8d
  title: Character-Level Language Modeling with Deeper Self-Attention
  year: 2019
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 732
  pid: 658721bc13b0fa97366d38c05a96bf0a9f4bb0ac
  title: Multi-Task Deep Neural Networks for Natural Language Understanding
  year: 2019
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 710
  pid: bc8fa64625d9189f5801837e7b133e7fe3c581f7
  title: 'Learned in Translation: Contextualized Word Vectors'
  year: 2017
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1772
  pid: c4744a7c2bb298e4a52289a1e085c71cc3d37bc6
  title: 'Transformer-XL: Attentive Language Models beyond a Fixed-Length Context'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 35186
  pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  title: Attention is All you Need
  year: 2017
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 3536
  pid: cd18800a0fe0b668a1cc19f2ec95b5003d0a5035
  title: Improving Language Understanding by Generative Pre-Training
  year: 2018
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 7273
  pid: 077f8329a7b6fa3b7c877a57b81eb6c18b5f87de
  title: 'RoBERTa: A Robustly Optimized BERT Pretraining Approach'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 881
  pid: 4aa9f5150b46320f534de4747a2dd0cd7f3fe292
  title: Semi-supervised Sequence Learning
  year: 2015
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 7988
  pid: 3febb2bed8865945e7fddc99efd791887bb7e14f
  title: Deep Contextualized Word Representations
  year: 2018
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 2638
  pid: 93b8da28d006415866bf48f9a6e06b5242129195
  title: 'GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language
    Understanding'
  year: 2018
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 139
  pid: ef6948edae12eba6f1d486b8600108b9762f36ab
  title: BAM! Born-Again Multi-Task Networks for Natural Language Understanding
  year: 2019
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 2252
  pid: 1e077413b25c4d34945cc2707e17e46ed4fe784a
  title: Universal Language Model Fine-tuning for Text Classification
  year: 2018
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 580
  pid: 2cd55ded95d5d13430edfa223ba591b514ebe8a5
  title: Adversarial Training Methods for Semi-Supervised Text Classification
  year: 2017
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 586
  pid: 3eda43078ae1f4741f09be08c4ecab6229046a5c
  title: 'NewsQA: A Machine Comprehension Dataset'
  year: 2017
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 4266
  pid: 05dd7254b632376973f3a1b4d39485da17814df5
  title: 'SQuAD: 100,000+ Questions for Machine Comprehension of Text'
  year: 2016
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1419
  pid: 0e6824e137847be0599bb0032e37042ed2ef5045
  title: 'Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching
    Movies and Reading Books'
  year: 2015
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1400
  pid: 4d1c856275744c0284312a3a50efb6ca9dc4cd4c
  title: "Know What You Don\u2019t Know: Unanswerable Questions for SQuAD"
  year: 2018
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1749
  pid: 41f1d50c85d3180476c4c7b3eea121278b0d8474
  title: Pixel Recurrent Neural Networks
  year: 2016
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 3478
  pid: 51a55df1f023571a7e07e338ee45a3e3d66ef73e
  title: Character-level Convolutional Networks for Text Classification
  year: 2015
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 123
  pid: 190e4800c67ef445e4bd0944a55debaccebcf43f
  title: Modeling High-Dimensional Discrete Data with Multi-Layer Neural Networks
  year: 1999
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 699
  pid: 636a79420d838eabe4af7fb25d6437de45ab64e8
  title: 'RACE: Large-scale ReAding Comprehension Dataset From Examinations'
  year: 2017
slug: XLNet:-Generalized-Autoregressive-Pretraining-for-Yang-Dai
title: 'XLNet: Generalized Autoregressive Pretraining for Language Understanding'
url: https://www.semanticscholar.org/paper/XLNet:-Generalized-Autoregressive-Pretraining-for-Yang-Dai/e0c6abdbdecf04ffac65c440da77fb9d66bb474c?sort=total-citations
venue: NeurIPS
year: 2019
