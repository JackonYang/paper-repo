authors:
- Fei Yu
- Jiji Tang
- Weichong Yin
- Yu Sun
- Hao Tian
- Hua Wu
- Haifeng Wang
badges:
- id: OPEN_ACCESS
corpusId: 220265934
fieldsOfStudy:
- Computer Science
numCitedBy: 125
numCiting: 45
paperAbstract: We propose a knowledge-enhanced approach, ERNIE-ViL, to learn joint
  representations of vision and language. ERNIE-ViL tries to construct the detailed
  semantic connections (objects, attributes of objects and relationships between objects
  in visual scenes) across vision and language, which are essential to vision-language
  cross-modal tasks. Incorporating knowledge from scene graphs, ERNIE-ViL constructs
  Scene Graph Prediction tasks, i.e., Object Prediction, Attribute Prediction and
  Relationship Prediction in the pre-training phase. More specifically, these prediction
  tasks are implemented by predicting nodes of different types in the scene graph
  parsed from the sentence. Thus, ERNIE-ViL can model the joint representation characterizing
  the alignments of the detailed semantics across vision and language. Pre-trained
  on two large image-text alignment datasets (Conceptual Captions and SBU), ERNIE-ViL
  learns better and more robust joint representations. It achieves state-of-the-art
  performance on 5 vision-language downstream tasks after fine-tuning ERNIE-ViL. Furthermore,
  it ranked the 1st place on the VCR leader-board with an absolute improvement of
  3.7%.
ref_count: 45
references:
- pid: 65a9c7b0800c86a196bc14e7621ff895cc6ab287
  title: 'ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language
    Tasks'
- pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  title: Attention is All you Need
- pid: 1c54acd7d9ed8017acdc5674c9b7faac738fd651
  title: 'SPICE: Semantic Propositional Image Caption Evaluation'
- pid: b4df354db88a70183a64dbc9e56cf14e7669a6c0
  title: 'Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic
    Image Captioning'
- pid: afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d
  title: 'Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image
    Annotations'
- pid: 8e080b98efbe65c02a116439205ca2344b9f7cd4
  title: 'Im2Text: Describing Images Using 1 Million Captioned Photographs'
- pid: 2f5f81bc516a6d085d39479378af1fc27104f91e
  title: Large-Scale Adversarial Training for Vision-and-Language Representation Learning
- pid: 818e5cbc337e4e1b98e65a2d7c2d6d2a0318cd57
  title: 'Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks'
- pid: 598a2ee223e2949c3b28389e922c1892b4717d2a
  title: 'Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers'
- pid: a9fd5511b42206a27748f373e0fdb7eb76a23055
  title: 'ImageBERT: Cross-modal Pre-training with Large-scale Weak-supervised Image-Text
    Data'
- pid: 6548a60a6bcdf6c402d9de1c05ba7afe4f49fee9
  title: '12-in-1: Multi-Task Vision and Language Representation Learning'
- pid: 54416048772b921720f19869ed11c2a360589d03
  title: 'UNITER: Learning UNiversal Image-TExt Representations'
- pid: 6648b4db5f12c30941ea78c695e77aded19672bb
  title: Unified Vision-Language Pre-Training for Image Captioning and VQA
- pid: 2527626c11a84f15709e943fbfa2356e19930e3b
  title: 'VL-BERT: Pre-training of Generic Visual-Linguistic Representations'
- pid: 79c93274429d6355959f1e4374c2147bb81ea649
  title: 'LXMERT: Learning Cross-Modality Encoder Representations from Transformers'
- pid: 2bc1c8bd00bbf7401afcb5460277840fd8bab029
  title: 'Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal
    Pre-training'
- pid: 5aec474c31a2f4b74703c6f786c0a8ff85c450da
  title: 'VisualBERT: A Simple and Performant Baseline for Vision and Language'
- pid: c41a11c0e9b8b92b4faaf97749841170b760760a
  title: 'VideoBERT: A Joint Model for Video and Language Representation Learning'
- pid: f6feb1af1809dfd872d868dfcc13021cc42f496c
  title: Auto-Encoding Scene Graphs for Image Captioning
- pid: 6dfc2ff03534a4325d06c6f88c3144831996629b
  title: 'From Recognition to Cognition: Visual Commonsense Reasoning'
- pid: df2b0e26d0599ce3e70df8a9da02e51594e0e992
  title: 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'
- pid: cd18800a0fe0b668a1cc19f2ec95b5003d0a5035
  title: Improving Language Understanding by Generative Pre-Training
- pid: 46b5d408d950287637dd21ce04772d9b2bacfd14
  title: Image Generation from Scene Graphs
- pid: 45dd2a3cd7c27f2e9509b023d702408f5ac11c9d
  title: Stacked Cross Attention for Image-Text Matching
- pid: fdce9cbe5c726201575b3c8a8c1af0752f1af53f
  title: 'MAttNet: Modular Attention Network for Referring Expression Comprehension'
- pid: 0da8af8d81e84381ffe656a0bbf2f3937ffac618
  title: 'Neural Motifs: Scene Graph Parsing with Global Context'
- pid: a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8
  title: Bottom-Up and Top-Down Attention for Image Captioning and Visual Question
    Answering
- pid: 34b73c1aa158b892bbe41705b4ae5bf01ecaea86
  title: Scene Graph Generation by Iterative Message Passing
- pid: 2c03df8b48bf3fa39054345bafabfeff15bfd11d
  title: Deep Residual Learning for Image Recognition
- pid: 2606e6a5759c030e259ebf3f4261b9c04a36a609
  title: Generating Semantically Precise Scene Graphs from Textual Descriptions for
    Improved Image Retrieval
- pid: 85ae705ef4353c6854f5be4a4664269d6317c66b
  title: Image retrieval using scene graphs
- pid: 44040913380206991b1991daf1192942e038fe31
  title: 'From image descriptions to visual denotations: New similarity metrics for
    semantic inference over event descriptions'
- pid: 97ad70a9fa3f99adf18030e5e38ebe3d90daa2db
  title: 'VQA: Visual Question Answering'
- pid: 424561d8585ff8ebce7d5d07de8dbf7aae5e7270
  title: 'Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks'
- pid: 92c141447f51b6732242376164ff961e464731c8
  title: 'ReferItGame: Referring to Objects in Photographs of Natural Scenes'
- pid: 71b7178df5d2b112d07e45038cb5637208659ff7
  title: 'Microsoft COCO: Common Objects in Context'
slug: ERNIE-ViL:-Knowledge-Enhanced-Vision-Language-Scene-Yu-Tang
title: 'ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through Scene
  Graph'
url: https://www.semanticscholar.org/paper/ERNIE-ViL:-Knowledge-Enhanced-Vision-Language-Scene-Yu-Tang/bc996a4dbf9d4234eacdd0b930a94de1d158e256?sort=total-citations
venue: AAAI
year: 2021
