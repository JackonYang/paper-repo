authors:
- C. Hilborn
- D. Lainiotis
badges:
- id: OPEN_ACCESS
corpusId: 9664198
fieldsOfStudy:
- Computer Science
numCitedBy: 985
numCiting: 6
paperAbstract: "Since, by (8) pertaining to the nearest neighbor decision rule (NN\
  \ rule). We briefly review the NN rule and then describe the CNN rule. The NN rule['l-[\
  \ \" I assigns an unclassified sample to the same class as the nearest of n stored,\
  \ correctly classified samples. In other words, given a collection of n reference\
  \ points, each classified by some external source, a new point is assigned to the\
  \ same class as its nearest neighbor. The most interesting t)heoretical property\
  \ of the NN rule is that under very mild regularity assumptions on the underlying\
  \ statistics, for any metric, and for a variety of loss functions , the large-sample\
  \ risk incurred is less than twice the Bayes risk. (The Bayes decision rule achieves\
  \ minimum risk but ,requires complete knowledge of the underlying statistics.) From\
  \ a practical point of view, however, the NN rule is not a prime candidate for many\
  \ applications because of the storage requirements it imposes. The CNN rule is suggested\
  \ as a rule which retains the basic approach of the NN rule without imposing such\
  \ stringent storage requirements. Before describing the CNN rule we first define\
  \ the notion of a consistent subset of a sample set. This is a subset which, when\
  \ used as a stored reference set for the NN rule, correctly classifies all of the\
  \ remaining points in the sample set. A minimal consistent subset is a consistent\
  \ subset with a minimum number of elements. Every set has a consistent subset, since\
  \ every set is trivially a consistent subset of itself. Obviously, every finite\
  \ set has a minimal consistent subset, although the minimum size is not, in general,\
  \ achieved uniquely. The CNN rule uses the following algorithm to determine a consistent\
  \ subset of the original sample set. In general, however, the algorithm will not\
  \ find a minimal consistent subset. We assume that the original sample set is arranged\
  \ in some order; then we set up bins called STORE and GRABHAG and proceed as follows.\
  \ 1) The first sample is placed in STORE. 2) The second sample is classified by\
  \ the NN rule, using as a reference set the current contents of STORE. (Since STORE\
  \ has only one point, the classification is trivial at this stage.) If the second\
  \ sample is classified correctly it is placed in GRABBAG; otherwise it is placed\
  \ in STORE. 3) Proceeding inductively, the ith sample is classified by the current\
  \ contents of \u2026"
ref_count: 6
references: []
slug: The-Condensed-Nearest-Neighbor-Rule-Hilborn-Lainiotis
title: The Condensed Nearest Neighbor Rule
url: https://www.semanticscholar.org/paper/The-Condensed-Nearest-Neighbor-Rule-Hilborn-Lainiotis/7c3771fd6829630cf450af853df728ecd8da4ab2?sort=total-citations
venue: ''
year: 1967
