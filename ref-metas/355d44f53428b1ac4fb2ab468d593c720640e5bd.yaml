authors:
- Yoshua Bengio
- Pascal Lamblin
- D. Popovici
- H. Larochelle
badges:
- id: OPEN_ACCESS
corpusId: 14201947
fieldsOfStudy:
- Computer Science
numCitedBy: 3434
numCiting: 25
paperAbstract: Complexity theory of circuits strongly suggests that deep architectures
  can be much more efficient (sometimes exponentially) than shallow architectures,
  in terms of computational elements required to represent some functions. Deep multi-layer
  neural networks have many levels of non-linearities allowing them to compactly represent
  highly non-linear and highly-varying functions. However, until recently it was not
  clear how to train such deep networks, since gradient-based optimization starting
  from random initialization appears to often get stuck in poor solutions. Hinton
  et al. recently introduced a greedy layer-wise unsupervised learning algorithm for
  Deep Belief Networks (DBN), a generative model with many layers of hidden causal
  variables. In the context of the above optimization problem, we study this algorithm
  empirically and explore variants to better understand its success and extend it
  to cases where the inputs are continuous or where the structure of the input distribution
  is not revealing enough about the variable to be predicted in a supervised task.
  Our experiments also confirm the hypothesis that the greedy layer-wise unsupervised
  training strategy mostly helps the optimization, by initializing weights in a region
  near a good local minimum, giving rise to internal distributed representations that
  are high-level abstractions of the input, bringing better generalization.
ref_count: 25
references:
- pid: 8978cf7574ceb35f4c3096be768c7547b28a35d0
  title: A Fast Learning Algorithm for Deep Belief Nets
- pid: 6fdb77260fc83dff91c44fea0f31a2cb8ed13d04
  title: Scaling learning algorithms towards AI
- pid: b7d471970467a99bec4bce34c7dba5ef6745ad06
  title: The Curse of Highly Variable Functions for Local Kernel Machines
- pid: 995a3b11cc8a4751d8e167abc4aa937abc934df0
  title: The Cascade-Correlation Learning Architecture
- pid: 46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e
  title: Reducing the Dimensionality of Data with Neural Networks
- pid: 9360e5ce9c98166bb179ad479a9d2919ff13d022
  title: Training Products of Experts by Minimizing Contrastive Divergence
- pid: 6dd01cd9c17d1491ead8c9f97597fbc61dead8ea
  title: The "wake-sleep" algorithm for unsupervised neural networks.
- pid: 2184fb6d32bc46f252b940035029273563c4fc82
  title: Exponential Family Harmoniums with an Application to Information Retrieval
- pid: 3067cab09b04637260b85716c605ba578dafec54
  title: Computational limitations of small-depth circuits
slug: Greedy-Layer-Wise-Training-of-Deep-Networks-Bengio-Lamblin
title: Greedy Layer-Wise Training of Deep Networks
url: https://www.semanticscholar.org/paper/Greedy-Layer-Wise-Training-of-Deep-Networks-Bengio-Lamblin/355d44f53428b1ac4fb2ab468d593c720640e5bd?sort=total-citations
venue: NIPS
year: 2006
