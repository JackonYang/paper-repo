authors:
- Yoshua Bengio
- Pascal Lamblin
- D. Popovici
- H. Larochelle
badges:
- id: OPEN_ACCESS
corpusId: 14201947
fieldsOfStudy:
- Computer Science
meta_key: greedy-layer-wise-training-of-deep-networks
numCitedBy: 3434
numCiting: 25
paperAbstract: Complexity theory of circuits strongly suggests that deep architectures
  can be much more efficient (sometimes exponentially) than shallow architectures,
  in terms of computational elements required to represent some functions. Deep multi-layer
  neural networks have many levels of non-linearities allowing them to compactly represent
  highly non-linear and highly-varying functions. However, until recently it was not
  clear how to train such deep networks, since gradient-based optimization starting
  from random initialization appears to often get stuck in poor solutions. Hinton
  et al. recently introduced a greedy layer-wise unsupervised learning algorithm for
  Deep Belief Networks (DBN), a generative model with many layers of hidden causal
  variables. In the context of the above optimization problem, we study this algorithm
  empirically and explore variants to better understand its success and extend it
  to cases where the inputs are continuous or where the structure of the input distribution
  is not revealing enough about the variable to be predicted in a supervised task.
  Our experiments also confirm the hypothesis that the greedy layer-wise unsupervised
  training strategy mostly helps the optimization, by initializing weights in a region
  near a good local minimum, giving rise to internal distributed representations that
  are high-level abstractions of the input, bringing better generalization.
ref_count: 25
references:
- fieldsOfStudy:
  - Computer Science
  meta_key: a-fast-learning-algorithm-for-deep-belief-nets
  numCitedBy: 13407
  pid: 8978cf7574ceb35f4c3096be768c7547b28a35d0
  show_ref_link: true
  title: A Fast Learning Algorithm for Deep Belief Nets
  year: 2006
- fieldsOfStudy:
  - Computer Science
  meta_key: scaling-learning-algorithms-towards-ai
  numCitedBy: 1116
  pid: 6fdb77260fc83dff91c44fea0f31a2cb8ed13d04
  show_ref_link: false
  title: Scaling learning algorithms towards AI
  year: 2007
- fieldsOfStudy:
  - Computer Science
  meta_key: training-mlps-layer-by-layer-using-an-objective-function-for-internal-representations
  numCitedBy: 54
  pid: 29a37cc8b6866e5809074cac2f7ce134aa763c4b
  show_ref_link: false
  title: Training MLPs layer by layer using an objective function for internal representations
  year: 1996
- fieldsOfStudy:
  - Computer Science
  meta_key: convex-neural-networks
  numCitedBy: 166
  pid: 758b1d823ac975720e6e81e375cd4432009e5bca
  show_ref_link: false
  title: Convex Neural Networks
  year: 2005
- fieldsOfStudy:
  - Computer Science
  meta_key: the-curse-of-highly-variable-functions-for-local-kernel-machines
  numCitedBy: 194
  pid: b7d471970467a99bec4bce34c7dba5ef6745ad06
  show_ref_link: false
  title: The Curse of Highly Variable Functions for Local Kernel Machines
  year: 2005
- fieldsOfStudy:
  - Computer Science
  meta_key: a-monte-carlo-em-approach-for-partially-observable-diffusion-processes-theory-and-applications-to-neural-networks
  numCitedBy: 32
  pid: 19bb461ebc18b43d44b3589659a2e450fff74c32
  show_ref_link: false
  title: A Monte Carlo EM Approach for Partially Observable Diffusion Processes -
    Theory and Applications to Neural Networks
  year: 2002
- fieldsOfStudy:
  - Computer Science
  meta_key: the-cascade-correlation-learning-architecture
  numCitedBy: 2937
  pid: 995a3b11cc8a4751d8e167abc4aa937abc934df0
  show_ref_link: false
  title: The Cascade-Correlation Learning Architecture
  year: 1989
- fieldsOfStudy:
  - Computer Science
  meta_key: reducing-the-dimensionality-of-data-with-neural-networks
  numCitedBy: 14638
  pid: 46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e
  show_ref_link: true
  title: Reducing the Dimensionality of Data with Neural Networks
  year: 2006
- fieldsOfStudy:
  - Computer Science
  meta_key: training-products-of-experts-by-minimizing-contrastive-divergence
  numCitedBy: 4567
  pid: 9360e5ce9c98166bb179ad479a9d2919ff13d022
  show_ref_link: true
  title: Training Products of Experts by Minimizing Contrastive Divergence
  year: 2002
- fieldsOfStudy:
  - Computer Science
  meta_key: many-layered-learning
  numCitedBy: 103
  pid: 398c477f674b228fec7f3f418a8cec047e2dafe5
  show_ref_link: false
  title: Many-Layered Learning
  year: 2002
- fieldsOfStudy:
  - Computer Science
  meta_key: the-wake-sleep-algorithm-for-unsupervised-neural-networks
  numCitedBy: 1001
  pid: 6dd01cd9c17d1491ead8c9f97597fbc61dead8ea
  show_ref_link: false
  title: The wake-sleep algorithm for unsupervised neural networks.
  year: 1995
- fieldsOfStudy:
  - Computer Science
  meta_key: exponential-family-harmoniums-with-an-application-to-information-retrieval
  numCitedBy: 502
  pid: 2184fb6d32bc46f252b940035029273563c4fc82
  show_ref_link: false
  title: Exponential Family Harmoniums with an Application to Information Retrieval
  year: 2004
- fieldsOfStudy:
  - Computer Science
  meta_key: continuous-restricted-boltzmann-machine-with-an-implementable-training-algorithm
  numCitedBy: 168
  pid: 24b2cc86a03203452bc5a1a7c318cb5178a5d961
  show_ref_link: false
  title: Continuous restricted Boltzmann machine with an implementable training algorithm
  year: 2003
- fieldsOfStudy:
  - Computer Science
  meta_key: computational-limitations-of-small-depth-circuits
  numCitedBy: 523
  pid: 3067cab09b04637260b85716c605ba578dafec54
  show_ref_link: false
  title: Computational limitations of small-depth circuits
  year: 1987
- fieldsOfStudy:
  - Computer Science
  meta_key: circuit-complexity-before-the-dawn-of-the-new-millennium
  numCitedBy: 52
  pid: 5e1b26ed51ece1f83e00dd18ae2454ef9fd50bb6
  show_ref_link: false
  title: Circuit Complexity before the Dawn of the New Millennium
  year: 1996
- fieldsOfStudy:
  - Computer Science
  meta_key: practical-issues-in-temporal-difference-learning
  numCitedBy: 333
  pid: 646ff15fbd38f8c4e2b099ad09e4570179709c73
  show_ref_link: false
  title: Practical issues in temporal difference learning
  year: 2004
slug: Greedy-Layer-Wise-Training-of-Deep-Networks-Bengio-Lamblin
title: Greedy Layer-Wise Training of Deep Networks
url: https://www.semanticscholar.org/paper/Greedy-Layer-Wise-Training-of-Deep-Networks-Bengio-Lamblin/355d44f53428b1ac4fb2ab468d593c720640e5bd?sort=total-citations
venue: NIPS
year: 2006
