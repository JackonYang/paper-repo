authors:
- Olga Kovaleva
- Alexey Romanov
- Anna Rogers
- Anna Rumshisky
badges:
- id: OPEN_ACCESS
corpusId: 201645145
fieldsOfStudy:
- Computer Science
numCitedBy: 290
numCiting: 28
paperAbstract: "BERT-based architectures currently give state-of-the-art performance\
  \ on many NLP tasks, but little is known about the exact mechanisms that contribute\
  \ to its success. In the current work, we focus on the interpretation of self-attention,\
  \ which is one of the fundamental underlying components of BERT. Using a subset\
  \ of GLUE tasks and a set of handcrafted features-of-interest, we propose the methodology\
  \ and carry out a qualitative and quantitative analysis of the information encoded\
  \ by the individual BERT\u2019s heads. Our findings suggest that there is a limited\
  \ set of attention patterns that are repeated across different heads, indicating\
  \ the overall model overparametrization. While different heads consistently use\
  \ the same attention patterns, they have varying impact on performance across different\
  \ tasks. We show that manually disabling attention in certain heads leads to a performance\
  \ improvement over the regular fine-tuned BERT models."
ref_count: 28
references:
- pid: df2b0e26d0599ce3e70df8a9da02e51594e0e992
  title: 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'
- pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  title: Attention is All you Need
- pid: d9f6ada77448664b71128bb19df15765336974a6
  title: 'SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding
    Systems'
- pid: 081651b38ff7533550a3adfc1c00da333a8fe86c
  title: How transferable are features in deep neural networks?
- pid: 93b8da28d006415866bf48f9a6e06b5242129195
  title: 'GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language
    Understanding'
- pid: 05dd7254b632376973f3a1b4d39485da17814df5
  title: 'SQuAD: 100,000+ Questions for Machine Comprehension of Text'
- pid: 687bac2d3320083eb4530bf18bb8f8f721477600
  title: Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank
- pid: a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096
  title: 'SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual
    Focused Evaluation'
- pid: 5ded2b8c64491b4a67f6d39ce473d4b9347a672e
  title: A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference
- pid: 547f23597f9ec8a93f66cedaa6fbfb73960426b1
  title: The Berkeley FrameNet Project
- pid: 475354f10798f110d34792b6d88f31d6d5cb099e
  title: Automatically Constructing a Corpus of Sentential Paraphrases
slug: Revealing-the-Dark-Secrets-of-BERT-Kovaleva-Romanov
title: Revealing the Dark Secrets of BERT
url: https://www.semanticscholar.org/paper/Revealing-the-Dark-Secrets-of-BERT-Kovaleva-Romanov/d78aed1dac6656affa4a04cbf225ced11a83d103?sort=total-citations
venue: EMNLP
year: 2019
