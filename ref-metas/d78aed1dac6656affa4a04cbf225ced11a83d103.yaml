authors:
- Olga Kovaleva
- Alexey Romanov
- Anna Rogers
- Anna Rumshisky
badges:
- id: OPEN_ACCESS
corpusId: 201645145
fieldsOfStudy:
- Computer Science
meta_key: revealing-the-dark-secrets-of-bert
numCitedBy: 290
numCiting: 28
paperAbstract: "BERT-based architectures currently give state-of-the-art performance\
  \ on many NLP tasks, but little is known about the exact mechanisms that contribute\
  \ to its success. In the current work, we focus on the interpretation of self-attention,\
  \ which is one of the fundamental underlying components of BERT. Using a subset\
  \ of GLUE tasks and a set of handcrafted features-of-interest, we propose the methodology\
  \ and carry out a qualitative and quantitative analysis of the information encoded\
  \ by the individual BERT\u2019s heads. Our findings suggest that there is a limited\
  \ set of attention patterns that are repeated across different heads, indicating\
  \ the overall model overparametrization. While different heads consistently use\
  \ the same attention patterns, they have varying impact on performance across different\
  \ tasks. We show that manually disabling attention in certain heads leads to a performance\
  \ improvement over the regular fine-tuned BERT models."
ref_count: 28
references:
- fieldsOfStudy:
  - Computer Science
  meta_key: are-sixteen-heads-really-better-than-one
  numCitedBy: 403
  pid: b03c7ff961822183bab66b2e594415e585d3fd09
  show_ref_link: false
  title: Are Sixteen Heads Really Better than One?
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: analyzing-multi-head-self-attention-specialized-heads-do-the-heavy-lifting-the-rest-can-be-pruned
  numCitedBy: 438
  pid: 07a64686ce8e43ac475a8d820a8a9f1d87989583
  show_ref_link: false
  title: Analyzing Multi-Head Self-Attention - Specialized Heads Do the Heavy Lifting,
    the Rest Can Be Pruned
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding
  numCitedBy: 33754
  pid: df2b0e26d0599ce3e70df8a9da02e51594e0e992
  show_ref_link: true
  title: BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding
  year: 2019
- fieldsOfStudy:
  - Computer Science
  - Linguistics
  meta_key: what-does-bert-learn-about-the-structure-of-language
  numCitedBy: 562
  pid: 335613303ebc5eac98de757ed02a56377d99e03a
  show_ref_link: false
  title: What Does BERT Learn about the Structure of Language?
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: attention-is-all-you-need
  numCitedBy: 35157
  pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  show_ref_link: true
  title: Attention is All you Need
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: superglue-a-stickier-benchmark-for-general-purpose-language-understanding-systems
  numCitedBy: 823
  pid: d9f6ada77448664b71128bb19df15765336974a6
  show_ref_link: true
  title: SuperGLUE - A Stickier Benchmark for General-Purpose Language Understanding
    Systems
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: rethinking-complex-neural-network-architectures-for-document-classification
  numCitedBy: 49
  pid: b7bdf98ef84909d4ec0b2ebd5157ee3cb38522b8
  show_ref_link: false
  title: Rethinking Complex Neural Network Architectures for Document Classification
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: how-transferable-are-features-in-deep-neural-networks
  numCitedBy: 5776
  pid: 081651b38ff7533550a3adfc1c00da333a8fe86c
  show_ref_link: true
  title: How transferable are features in deep neural networks?
  year: 2014
- fieldsOfStudy:
  - Computer Science
  meta_key: the-lottery-ticket-hypothesis-finding-sparse-trainable-neural-networks
  numCitedBy: 1428
  pid: 21937ecd9d66567184b83eca3d3e09eb4e6fbd60
  show_ref_link: false
  title: The Lottery Ticket Hypothesis - Finding Sparse, Trainable Neural Networks
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: linguistic-knowledge-and-transferability-of-contextual-representations
  numCitedBy: 441
  pid: f6fbb6809374ca57205bd2cf1421d4f4fa04f975
  show_ref_link: false
  title: Linguistic Knowledge and Transferability of Contextual Representations
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: the-importance-of-being-recurrent-for-modeling-hierarchical-structure
  numCitedBy: 114
  pid: 997c55547aeca733dfc5dfebd12412612ecba022
  show_ref_link: false
  title: The Importance of Being Recurrent for Modeling Hierarchical Structure
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: glue-a-multi-task-benchmark-and-analysis-platform-for-natural-language-understanding
  numCitedBy: 2635
  pid: 93b8da28d006415866bf48f9a6e06b5242129195
  show_ref_link: true
  title: GLUE - A Multi-Task Benchmark and Analysis Platform for Natural Language
    Understanding
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: why-self-attention-a-targeted-evaluation-of-neural-machine-translation-architectures
  numCitedBy: 173
  pid: e3ee61f49cd2639c15c8662a45f1d0c2b83a60c1
  show_ref_link: false
  title: Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: squad-100-000-questions-for-machine-comprehension-of-text
  numCitedBy: 4263
  pid: 05dd7254b632376973f3a1b4d39485da17814df5
  show_ref_link: true
  title: SQuAD - 100,000+ Questions for Machine Comprehension of Text
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: lessons-from-natural-language-inference-in-the-clinical-domain
  numCitedBy: 111
  pid: f2588de5173fb047192dbb93d62ce6636bdf46bd
  show_ref_link: false
  title: Lessons from Natural Language Inference in the Clinical Domain
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: recursive-deep-models-for-semantic-compositionality-over-a-sentiment-treebank
  numCitedBy: 5366
  pid: 687bac2d3320083eb4530bf18bb8f8f721477600
  show_ref_link: true
  title: Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank
  year: 2013
- fieldsOfStudy:
  - Computer Science
  - Psychology
  meta_key: semeval-2017-task-1-semantic-textual-similarity-multilingual-and-crosslingual-focused-evaluation
  numCitedBy: 934
  pid: a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096
  show_ref_link: true
  title: SemEval-2017 Task 1 - Semantic Textual Similarity Multilingual and Crosslingual
    Focused Evaluation
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: a-broad-coverage-challenge-corpus-for-sentence-understanding-through-inference
  numCitedBy: 2037
  pid: 5ded2b8c64491b4a67f6d39ce473d4b9347a672e
  show_ref_link: true
  title: A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference
  year: 2018
- fieldsOfStudy:
  - Linguistics
  meta_key: assessing-bert-s-syntactic-abilities
  numCitedBy: 314
  pid: efeab0dcdb4c1cce5e537e57745d84774be99b9a
  show_ref_link: false
  title: Assessing BERT's Syntactic Abilities
  year: 2019
- fieldsOfStudy:
  - Linguistics
  meta_key: the-berkeley-framenet-project
  numCitedBy: 2882
  pid: 547f23597f9ec8a93f66cedaa6fbfb73960426b1
  show_ref_link: false
  title: The Berkeley FrameNet Project
  year: 1998
- fieldsOfStudy:
  - Computer Science
  meta_key: automatically-constructing-a-corpus-of-sentential-paraphrases
  numCitedBy: 834
  pid: 475354f10798f110d34792b6d88f31d6d5cb099e
  show_ref_link: false
  title: Automatically Constructing a Corpus of Sentential Paraphrases
  year: 2005
- fieldsOfStudy:
  - Linguistics
  - Psychology
  meta_key: the-empirical-base-of-linguistics-grammaticality-judgments-and-linguistic-methodology
  numCitedBy: 595
  pid: 9eb31f967e7503280834b5cbe8fbfa0f952d8f6d
  show_ref_link: false
  title: The empirical base of linguistics - Grammaticality judgments and linguistic
    methodology
  year: 1998
- fieldsOfStudy:
  - Computer Science
  meta_key: pay-less-attention-with-lightweight-and-dynamic-convolutions
  numCitedBy: 393
  pid: ef523bb9437178c50d1b1e3e3ca5fb230ab37e3f
  show_ref_link: false
  title: Pay Less Attention with Lightweight and Dynamic Convolutions
  year: 2019
slug: Revealing-the-Dark-Secrets-of-BERT-Kovaleva-Romanov
title: Revealing the Dark Secrets of BERT
url: https://www.semanticscholar.org/paper/Revealing-the-Dark-Secrets-of-BERT-Kovaleva-Romanov/d78aed1dac6656affa4a04cbf225ced11a83d103?sort=total-citations
venue: EMNLP
year: 2019
