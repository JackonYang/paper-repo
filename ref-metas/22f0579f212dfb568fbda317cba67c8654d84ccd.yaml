authors:
- Thomas G. Dietterich
badges:
- id: OPEN_ACCESS
corpusId: 683036
fieldsOfStudy:
- Computer Science
numCitedBy: 3143
numCiting: 28
paperAbstract: 'This article reviews five approximate statistical tests for determining
  whether one learning algorithm outperforms another on a particular learning task.
  These test sare compared experimentally to determine their probability of incorrectly
  detecting a difference when no difference exists (type I error). Two widely used
  statistical tests are shown to have high probability of type I error in certain
  situations and should never be used: a test for the difference of two proportions
  and a paired-differences t test based on taking several random train-test splits.
  A third test, a paired-differences t test based on 10-fold cross-validation, exhibits
  somewhat elevated probability of type I error. A fourth test, McNemar''s test, is
  shown to have low type I error. The fifth test is a new test, 5 2 cv, based on five
  iterations of twofold cross-validation. Experiments show that this test also has
  acceptable type I error. The article also measures the power (ability to detect
  algorithm differences when they do exist) of these tests. The cross-validated t
  test is the most powerful. The 52 cv test is shown to be slightly more powerful
  than McNemar''s test. The choice of the best test is determined by the computational
  cost of running the learning algorithm. For algorithms that can be executed only
  once, Mc-Nemar''s test is the only test with acceptable type I error. For algorithms
  that can be executed 10 times, the 5 2 cv test is recommended, because it is slightly
  more powerful and because it directly measures variation due to the choice of training
  set.'
ref_count: 28
references:
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 495
  pid: 5f49a73c42be6dbd851af4599d9911ea1d6ac7f4
  title: Evaluation of gaussian processes and other methods for non-linear regression
  year: 1997
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 443
  pid: 25744dbb4294fe7abb2d9b1b0d39006482ebb4ab
  title: Error-Correcting Output Coding Corrects Bias and Variance
  year: 1995
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 13446
  pid: e068be31ded63600aea068eacd12931efd2a1029
  title: UCI Repository of machine learning databases
  year: 1998
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1939
  pid: 0b1d3ec2e6fe49aaf8dc068b8a812e9ef3f163fa
  title: 'Nearest neighbor (NN) norms: NN pattern classification techniques'
  year: 1991
slug: Approximate-Statistical-Tests-for-Comparing-Dietterich
title: Approximate Statistical Tests for Comparing Supervised Classification Learning
  Algorithms
url: https://www.semanticscholar.org/paper/Approximate-Statistical-Tests-for-Comparing-Dietterich/22f0579f212dfb568fbda317cba67c8654d84ccd?sort=total-citations
venue: Neural Computation
year: 1998
