authors:
- Ali Farhadi
- Mohsen Hejrati
- M. Sadeghi
- Peter Young
- Cyrus Rashtchian
- J. Hockenmaier
- D. Forsyth
badges:
- id: OPEN_ACCESS
corpusId: 13272863
fieldsOfStudy:
- Computer Science
numCitedBy: 986
numCiting: 40
paperAbstract: Humans can prepare concise descriptions of pictures, focusing on what
  they find important. We demonstrate that automatic methods can do so too. We describe
  a system that can compute a score linking an image to a sentence. This score can
  be used to attach a descriptive sentence to a given image, or to obtain images that
  illustrate a given sentence. The score is obtained by comparing an estimate of meaning
  obtained from the image to one obtained from the sentence. Each estimate of meaning
  comes from a discriminative procedure that is learned us-ingdata. We evaluate on
  a novel dataset consisting of human-annotated images. While our underlying estimate
  of meaning is impoverished, it is sufficient to produce very good quantitative results,
  evaluated with a novel score that can account for synecdoche.
ref_count: 40
references:
- pid: df70db146b07ce173476be3877a5a3ae3ca06aa5
  title: Clustering art
- pid: 49927656ede0c75af22ca73dcf4abba028839650
  title: Understanding videos, constructing plots learning a visually grounded storyline
    model from annotated videos
- pid: eed4e6967c7a96e4cc2c590db40269cd97c8c98e
  title: 'Towards total scene understanding: Classification, annotation and segmentation
    in an automatic framework'
- pid: 05e074abddd3fe987b9bebd46f6cf4bf8465c37e
  title: 'I2T: Image Parsing to Text Description'
- pid: 6d9f55b445f36578802e7eef4393cfa914b11620
  title: 'Object Recognition as Machine Translation: Learning a Lexicon for a Fixed
    Image Vocabulary'
- pid: 0f86767732f76f478d5845f2e59f99ba106e9265
  title: Learning realistic human actions from movies
- pid: bf60322f83714523e2d7c1d39983151fe9db7146
  title: "Collecting Image Annotations Using Amazon\u2019s Mechanical Turk"
- pid: 8e523721feebeaee18e487607b7d0920ac6cd3b4
  title: 'Beyond Nouns: Exploiting Prepositions and Comparative Adjectives for Learning
    Visual Classifiers'
- pid: 8b29ffb4207435540ddecf4b14a8a32106b33830
  title: Image-to-word transformation based on dividing
- pid: 869171b2f56cfeaa9b81b2626cb4956fea590a57
  title: 'Modeling the Shape of the Scene: A Holistic Representation of the Spatial
    Envelope'
- pid: 3a8da6accff92f915c1b8ac26d8176308c425b61
  title: 'Observing Human-Object Interactions: Using Spatial and Functional Compatibility
    for Recognition'
- pid: ef4209ed288ef38fecdfae2409bce78633386c10
  title: What, where and who? Classifying events by scene and object recognition
- pid: 9d94fc289d82738a4d1071470b16ba861ea12169
  title: 'Building the gist of a scene: the role of global image features in recognition.'
- pid: 860a9d55d87663ca88e74b3ca357396cd51733d0
  title: A discriminatively trained, multiscale, deformable part model
- pid: 8d56b2a75aa5624660b60787e1f38ee2c70d493a
  title: 'Learning structured prediction models: a large margin approach'
- pid: 04554de05a3a9ebb1890d25aaa7e34544a0d32a7
  title: Active Matching
- pid: 927432c50d920e647260c67506859d7845c7f729
  title: Modeling mutual context of object and human pose in human-object interaction
    activities
- pid: 435ace68aa855103d76f869a88d34fee0771383b
  title: "Computer Vision \u2014 ECCV 2002"
- pid: 2788d0f1f7fbd44ebb5185e59c4aaf09aad97013
  title: What's in a picture?
- pid: cc0c3033ea7d4e19e1f5ac71934759507e126162
  title: An Information-Theoretic Definition of Similarity
slug: Every-Picture-Tells-a-Story:-Generating-Sentences-Farhadi-Hejrati
title: 'Every Picture Tells a Story: Generating Sentences from Images'
url: https://www.semanticscholar.org/paper/Every-Picture-Tells-a-Story:-Generating-Sentences-Farhadi-Hejrati/eaaed23a2d94feb2f1c3ff22a25777c7a78f3141?sort=total-citations
venue: ECCV
year: 2010
