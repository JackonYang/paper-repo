authors:
- H. Seung
badges:
- id: OPEN_ACCESS
corpusId: 8439071
fieldsOfStudy:
- Computer Science
numCitedBy: 94
numCiting: 17
paperAbstract: 'One approach to invariant object recognition employs a recurrent neural
  network as an associative memory. In the standard depiction of the network''s state
  space, memories of objects are stored as attractive fixed points of the dynamics.
  I argue for a modification of this picture: if an object has a continuous family
  of instantiations, it should be represented by a continuous attractor. This idea
  is illustrated with a network that learns to complete patterns. To perform the task
  of filling in missing information, the network develops a continuous attractor that
  models the manifold from which the patterns are drawn. From a statistical view-point,
  the pattern completion task allows a formulation of unsupervised learning in terms
  of regression rather than density estimation.'
ref_count: 17
references:
- pid: 9552ac39a57daacf3d75865a268935b5a0df9bbb
  title: 'Neural networks and principal component analysis: Learning from examples
    without local minima'
- pid: a0d16f0e99f7ce5e6fb70b1a68c685e9ad610657
  title: A Learning Algorithm for Boltzmann Machines
- pid: 842dd6d0f4b72ce0e8f3ac8e6861637c1f4645ea
  title: 'Learning algorithms for classification: A comparison on handwritten digit
    recognition'
- pid: 98b4d4e24aab57ab4e1124ff8106909050645cfa
  title: Neural networks and physical systems with emergent collective computational
    abilities.
- pid: 1ce38ff6a6801a0a7e0ec1fbd24503d7a2142fbf
  title: The self-organizing map
slug: Learning-Continuous-Attractors-in-Recurrent-Seung
title: Learning Continuous Attractors in Recurrent Networks
url: https://www.semanticscholar.org/paper/Learning-Continuous-Attractors-in-Recurrent-Seung/9b20ad513361a26e98289e5a517291c6ff49960d?sort=total-citations
venue: NIPS
year: 1997
