authors:
- Yoshua Bengio
- Holger Schwenk
- "Jean-S\xE9bastien Senecal"
- Frederic Morin
- J. Gauvain
badges: []
corpusId: 53821397
fieldsOfStudy:
- Computer Science
numCitedBy: 550
numCiting: 48
paperAbstract: 'A central goal of statistical language modeling is to learn the joint
  probability function of sequences of words in a language. This is intrinsically
  difficult because of the curse of dimensionality: a word sequence on which the model
  will be tested is likely to be different from all the word sequences seen during
  training. Traditional but very successful approaches based on n-grams obtain generalization
  by concatenating very short overlapping sequences seen in the training set. We propose
  to fight the curse of dimensionality by learning a distributed representation for
  words which allows each training sentence to inform the model about an exponential
  number of semantically neighboring sentences. Generalization is obtained because
  a sequence of words that has never been seen before gets high probability if it
  is made of words that are similar (in the sense of having a nearby representation)
  to words forming an already seen sentence. Training such large models (with millions
  of parameters) within a reasonable time is itself a significant challenge. We report
  on several methods to speed-up both training and probability computation, as well
  as comparative experiments to evaluate the improvements brought by these techniques.
  We finally describe the incorporation of this new language model into a state-of-the-art
  speech recognizer of conversational speech.'
ref_count: 49
references:
- pid: 6c2b28f9354f667cd5bd07afc0471d8334430da7
  title: A Neural Probabilistic Language Model
- pid: d6fb7546a29320eadad868af66835059db93d99f
  title: Efficient training of large neural networks for language modeling
- pid: e41498c05d4c68e4750fb84a380317a112d97b01
  title: Connectionist language modeling for large vocabulary continuous speech recognition
- pid: 4af41f4d838daa7ca6995aeb4918b61989d1ed80
  title: Classes for fast maximum entropy training
- pid: 3d6036af971c1f11ab712cc41487376a94e63673
  title: Using a connectionist model in a syntactical based language model
- pid: 09c76da2361d46689825c4efc37ad862347ca577
  title: A bit of progress in language modeling
- pid: 3de5d40b60742e3dfa86b19e7f660962298492af
  title: Class-Based n-gram Models of Natural Language
- pid: 5eb328cf7e94995199e4c82a1f4d0696430a80b5
  title: Distributional Clustering of English Words
- pid: b0130277677e5b915d5cd86b3afafd77fd08eb2e
  title: Estimation of probabilities from sparse data for the language model component
    of a speech recognizer
- pid: 14d46c6396837986bb4b9a14024cb64797b8c6c0
  title: Stochastic Neighbor Embedding
- pid: 668087f0ae7ce1de6e0bd0965dbb480c08103260
  title: Finding Structure in Time
- pid: bfab4ffa229c8af0174a683ff1eda524c4f59d00
  title: Can artificial neural networks learn language models?
- pid: 9548ac30c113562a51e603dbbc8e9fa651cfd3ab
  title: Improved backing-off for M-gram language modeling
- pid: 9360e5ce9c98166bb179ad479a9d2919ff13d022
  title: Training Products of Experts by Minimizing Contrastive Divergence
- pid: fb486e03369a64de2d5b0df86ec0a7b55d3907db
  title: A Maximum Entropy Approach to Natural Language Processing
- pid: 399da68d3b97218b6c80262df7963baa89dcc71b
  title: SRILM - an extensible language modeling toolkit
- pid: 121afb1502c90d510f64a0b3276a5454616a64e7
  title: Boosting a weak learning algorithm by majority
- pid: 190e4800c67ef445e4bd0944a55debaccebcf43f
  title: Modeling High-Dimensional Discrete Data with Multi-Layer Neural Networks
- pid: 5d24afe3a62331ebfad400c3fec77c836d2b99db
  title: Word Space
- pid: d87ceda3042f781c341ac17109d1e94a717f5f60
  title: 'WordNet : an electronic lexical database'
- pid: e733226b881f11f25c87e8bac8d602ba3d9c220e
  title: Distributional clustering of words for text classification
- pid: e50a316f97c9a405aa000d883a633bd5707f1a34
  title: Term-Weighting Approaches in Automatic Text Retrieval
- pid: b313a0581253191f291f923185a691b260d2bfee
  title: Editors. Advances in Neural Information Processing Systems
- pid: 769dbbe88801b57a9b44f89c5516264f16cbed60
  title: An empirical study of smoothing techniques for language modeling
- pid: 6a923c9f89ed53b6e835b3807c0c1bd8d532687b
  title: Interpolated estimation of Markov source parameters from sparse data
- pid: 4ade4934db522fe6d634ff6f48887da46eedb4d1
  title: Learning distributed representations of concepts.
- pid: b9ed0b35c9eaba0328492de65c4cdc5545094df4
  title: Improved clustering techniques for class-based statistical language modelling
- pid: 2928de5400a920a6a29af41821c680cef5d35f91
  title: A latent semantic analysis framework for large-Span language modeling
slug: Neural-Probabilistic-Language-Models-Bengio-Schwenk
title: Neural Probabilistic Language Models
url: https://www.semanticscholar.org/paper/Neural-Probabilistic-Language-Models-Bengio-Schwenk/5eb1a272f9933a11d113cf63fe659e073942bce5?sort=total-citations
venue: ''
year: 2006
