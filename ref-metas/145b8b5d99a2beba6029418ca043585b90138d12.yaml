authors:
- Kaitao Song
- Xu Tan
- Tao Qin
- Jianfeng Lu
- Tie-Yan Liu
badges:
- id: OPEN_ACCESS
corpusId: 146808476
fieldsOfStudy:
- Computer Science
numCitedBy: 599
numCiting: 59
paperAbstract: 'Pre-training and fine-tuning, e.g., BERT, have achieved great success
  in language understanding by transferring knowledge from rich-resource pre-training
  task to the low/zero-resource downstream tasks. Inspired by the success of BERT,
  we propose MAsked Sequence to Sequence pre-training (MASS) for the encoder-decoder
  based language generation tasks. MASS adopts the encoder-decoder framework to reconstruct
  a sentence fragment given the remaining part of the sentence: its encoder takes
  a sentence with randomly masked fragment (several consecutive tokens) as input,
  and its decoder tries to predict this masked fragment. In this way, MASS can jointly
  train the encoder and decoder to develop the capability of representation extraction
  and language modeling. By further fine-tuning on a variety of zero/low-resource
  language generation tasks, including neural machine translation, text summarization
  and conversational response generation (3 tasks and totally 8 datasets), MASS achieves
  significant improvements over the baselines without pre-training or with other pre-training
  methods. Specially, we achieve the state-of-the-art accuracy (37.5 in terms of BLEU
  score) on the unsupervised English-French translation, even beating the early attention-based
  supervised model.'
ref_count: 59
references:
- pid: df2b0e26d0599ce3e70df8a9da02e51594e0e992
  title: 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'
- pid: 85f94d8098322f8130512b4c6c4627548ce4a6cc
  title: Unsupervised Pretraining for Sequence to Sequence Learning
- pid: cd18800a0fe0b668a1cc19f2ec95b5003d0a5035
  title: Improving Language Understanding by Generative Pre-Training
- pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  title: Attention is All you Need
- pid: fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5
  title: Neural Machine Translation by Jointly Learning to Align and Translate
- pid: dbde7dfa6cae81df8ac19ef500c42db96c3d1edd
  title: 'Google''s Neural Machine Translation System: Bridging the Gap between Human
    and Machine Translation'
- pid: 6e795c6e9916174ae12349f5dc3f516570c17ce8
  title: Skip-Thought Vectors
- pid: 1e077413b25c4d34945cc2707e17e46ed4fe784a
  title: Universal Language Model Fine-tuning for Text Classification
- pid: 93b8da28d006415866bf48f9a6e06b5242129195
  title: 'GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language
    Understanding'
- pid: bc8fa64625d9189f5801837e7b133e7fe3c581f7
  title: 'Learned in Translation: Contextualized Word Vectors'
- pid: 1af68821518f03568f913ab03fc02080247a27ff
  title: Neural Machine Translation of Rare Words with Subword Units
- pid: 4aa9f5150b46320f534de4747a2dd0cd7f3fe292
  title: Semi-supervised Sequence Learning
- pid: 57458bc1cffe5caa45a885af986d70f723f406b4
  title: 'A unified architecture for natural language processing: deep neural networks
    with multitask learning'
- pid: ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc
  title: Cross-lingual Language Model Pretraining
- pid: 43428880d75b3a14257c3ee9bda054e61eb869c0
  title: Convolutional Sequence to Sequence Learning
- pid: 0b544dfe355a5070b60986319a3f51fb45d1348e
  title: "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical\
    \ Machine Translation"
- pid: 687bac2d3320083eb4530bf18bb8f8f721477600
  title: Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank
- pid: 3febb2bed8865945e7fddc99efd791887bb7e14f
  title: Deep Contextualized Word Representations
- pid: f527bcfb09f32e6a4a8afc0b37504941c1ba2cee
  title: Distributed Representations of Sentences and Documents
- pid: f04df4e20a18358ea2f689b4c129781628ef7fc1
  title: A large annotated corpus for learning natural language inference
- pid: bc1d609520290e0460c49b685675eb5a57fa5935
  title: An efficient framework for learning sentence representations
- pid: 6c2b28f9354f667cd5bd07afc0471d8334430da7
  title: A Neural Probabilistic Language Model
- pid: f37e1b62a767a307c046404ca96bc140b3e68cb5
  title: 'GloVe: Global Vectors for Word Representation'
- pid: 05dd7254b632376973f3a1b4d39485da17814df5
  title: 'SQuAD: 100,000+ Questions for Machine Comprehension of Text'
- pid: 87f40e6f3022adbc1f1905e3e506abad05a9964f
  title: Distributed Representations of Words and Phrases and their Compositionality
- pid: 2c03df8b48bf3fa39054345bafabfeff15bfd11d
  title: Deep Residual Learning for Image Recognition
- pid: 9819b600a828a57e1cde047bbe710d3446b30da5
  title: Recurrent neural network based language model
- pid: 2f4df08d9072fc2ac181b7fced6a245315ce05c8
  title: Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation
- pid: 944e1a7b2c5c62e952418d7684e3cade89c76f87
  title: A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled
    Data
- pid: a6cb366736791bcccc5c8639de5a8f9636bf87e8
  title: 'Adam: A Method for Stochastic Optimization'
- pid: 843959ffdccf31c6694d135fad07425924f785b1
  title: Extracting and composing robust features with denoising autoencoders
- pid: e15cf50aa89fee8535703b9f9512fca5bfc43327
  title: Going deeper with convolutions
- pid: 3de5d40b60742e3dfa86b19e7f660962298492af
  title: Class-Based n-gram Models of Natural Language
- pid: 10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb
  title: 'Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity
    Recognition'
slug: MASS:-Masked-Sequence-to-Sequence-Pre-training-for-Song-Tan
title: 'MASS: Masked Sequence to Sequence Pre-training for Language Generation'
url: https://www.semanticscholar.org/paper/MASS:-Masked-Sequence-to-Sequence-Pre-training-for-Song-Tan/145b8b5d99a2beba6029418ca043585b90138d12?sort=total-citations
venue: ICML
year: 2019
