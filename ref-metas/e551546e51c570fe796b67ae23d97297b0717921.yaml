authors:
- Lukasz Garncarek
- Rafal Powalski
- Tomasz Stanislawek
- Bartosz Topolski
- Piotr Halama
- Filip Grali'nski
badges: []
corpusId: 211171875
fieldsOfStudy:
- Computer Science
numCitedBy: 16
numCiting: 35
paperAbstract: In this paper we introduce a novel approach to the problem of understanding
  documents where the local semantics is influenced by non-trivial layout. Namely,
  we modify the Transformer architecture in a way that allows it to use the graphical
  features defined by the layout, without the need to re-learn the language semantics
  from scratch, thanks to starting the training process from a model pretrained on
  classical language modeling tasks.
ref_count: 35
references:
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 20
  pid: e2d2f64b3bb200c2c3db5ddc367b06311c369341
  title: 'Kleister: A novel task for Information Extraction involving Long Documents
    with Complex Layout'
  year: 2020
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 48
  pid: 2fbda89395f993040b7665730c64182ade3be195
  title: 'BERTgrid: Contextualized Embedding for 2D Document Representation and Understanding'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 100
  pid: 15aae08159856cdbf0ce539357d473a04dcbb7f3
  title: 'Chargrid: Towards Understanding 2D Documents'
  year: 2018
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 86
  pid: 04df8c70257b5280b9d303502c9d7ddf946f181b
  title: Graph Convolution for Multimodal Information Extraction from Visually Rich
    Documents
  year: 2019
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 3533
  pid: cd18800a0fe0b668a1cc19f2ec95b5003d0a5035
  title: Improving Language Understanding by Generative Pre-Training
  year: 2018
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 2634
  pid: 93b8da28d006415866bf48f9a6e06b5242129195
  title: 'GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language
    Understanding'
  year: 2018
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 6284
  pid: 9405cc0d6169988371b2755e573cc28650d14dfe
  title: Language Models are Unsupervised Multitask Learners
  year: 2019
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 39
  pid: 8f4e0912f9eb1fd2f87646906dfbf2deabc4b875
  title: A probabilistic approach to printed document understanding
  year: 2010
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 41
  pid: 3170d280095b2198570073eaa068d6b2946334e3
  title: Field Extraction from Administrative Documents by Incremental Structural
    Templates
  year: 2013
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 33744
  pid: df2b0e26d0599ce3e70df8a9da02e51594e0e992
  title: 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 2706
  pid: 7a064df1aeada7e69e5173f7d4c8606f4470365b
  title: 'ALBERT: A Lite BERT for Self-supervised Learning of Language Representations'
  year: 2020
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1771
  pid: c4744a7c2bb298e4a52289a1e085c71cc3d37bc6
  title: 'Transformer-XL: Attentive Language Models beyond a Fixed-Length Context'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 4226
  pid: e0c6abdbdecf04ffac65c440da77fb9d66bb474c
  title: 'XLNet: Generalized Autoregressive Pretraining for Language Understanding'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  - Sociology
  numCitedBy: 48
  pid: b0175f2602256e71c4f40b96b6997422db38f39c
  title: Analysis and understanding of multi-class invoices
  year: 2003
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 35148
  pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  title: Attention is All you Need
  year: 2017
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 174
  pid: bd86b4b551b9d3fb498f62008b037e7599365018
  title: Evaluation of deep convolutional nets for document image classification and
    retrieval
  year: 2015
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 7266
  pid: 077f8329a7b6fa3b7c877a57b81eb6c18b5f87de
  title: 'RoBERTa: A Robustly Optimized BERT Pretraining Approach'
  year: 2019
- fieldsOfStudy: []
  numCitedBy: 63
  pid: 47d79963ac69111d8dc82a228d26e6a746a4d087
  title: Transformers
  year: 2018
slug: LAMBERT:-Layout-Aware-language-Modeling-using-BERT-Garncarek-Powalski
title: 'LAMBERT: Layout-Aware language Modeling using BERT for information extraction'
url: https://www.semanticscholar.org/paper/LAMBERT:-Layout-Aware-language-Modeling-using-BERT-Garncarek-Powalski/e551546e51c570fe796b67ae23d97297b0717921?sort=total-citations
venue: ArXiv
year: 2020
