authors:
- Iz Beltagy
- Kyle Lo
- Arman Cohan
badges:
- id: OPEN_ACCESS
corpusId: 202558505
fieldsOfStudy:
- Computer Science
numCitedBy: 1019
numCiting: 32
paperAbstract: Obtaining large-scale annotated data for NLP tasks in the scientific
  domain is challenging and expensive. We release SciBERT, a pretrained language model
  based on BERT (Devlin et. al., 2018) to address the lack of high-quality, large-scale
  labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain
  corpus of scientific publications to improve performance on downstream scientific
  NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence
  classification and dependency parsing, with datasets from a variety of scientific
  domains. We demonstrate statistically significant improvements over BERT and achieve
  new state-of-the-art results on several of these tasks. The code and pretrained
  models are available at https://github.com/allenai/scibert/.
ref_count: 32
references:
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 2251
  pid: 1e077413b25c4d34945cc2707e17e46ed4fe784a
  title: Universal Language Model Fine-tuning for Text Classification
  year: 2018
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 893
  pid: 93b4cc549a1bc4bc112189da36c318193d05d806
  title: 'AllenNLP: A Deep Semantic Natural Language Processing Platform'
  year: 2018
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 3536
  pid: cd18800a0fe0b668a1cc19f2ec95b5003d0a5035
  title: Improving Language Understanding by Generative Pre-Training
  year: 2018
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 33777
  pid: df2b0e26d0599ce3e70df8a9da02e51594e0e992
  title: 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1994
  pid: 8dd6aae51e31a72752c4be5cddbdd76dfdc6cda4
  title: End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF
  year: 2016
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 795
  pid: 8cbef23c9ee2ae7c35cc691a0c1d713a6377c9f2
  title: Deep Biaffine Attention for Neural Dependency Parsing
  year: 2017
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 7988
  pid: 3febb2bed8865945e7fddc99efd791887bb7e14f
  title: Deep Contextualized Word Representations
  year: 2018
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 4645
  pid: dbde7dfa6cae81df8ac19ef500c42db96c3d1edd
  title: 'Google''s Neural Machine Translation System: Bridging the Gap between Human
    and Machine Translation'
  year: 2016
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 35186
  pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  title: Attention is All you Need
  year: 2017
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 90091
  pid: a6cb366736791bcccc5c8639de5a8f9636bf87e8
  title: 'Adam: A Method for Stochastic Optimization'
  year: 2015
slug: SciBERT:-A-Pretrained-Language-Model-for-Scientific-Beltagy-Lo
title: 'SciBERT: A Pretrained Language Model for Scientific Text'
url: https://www.semanticscholar.org/paper/SciBERT:-A-Pretrained-Language-Model-for-Scientific-Beltagy-Lo/5e98fe2163640da8ab9695b9ee9c433bb30f5353?sort=total-citations
venue: EMNLP
year: 2019
