authors:
- Zhicheng Huang
- Zhaoyang Zeng
- Bei Liu
- Dongmei Fu
- Jianlong Fu
badges:
- id: OPEN_ACCESS
corpusId: 214775221
fieldsOfStudy:
- Computer Science
numCitedBy: 148
numCiting: 40
paperAbstract: We propose Pixel-BERT to align image pixels with text by deep multi-modal
  transformers that jointly learn visual and language embedding in a unified end-to-end
  framework. We aim to build a more accurate and thorough connection between image
  pixels and language semantics directly from image and sentence pairs instead of
  using region-based image features as the most recent vision and language tasks.
  Our Pixel-BERT which aligns semantic connection in pixel and text level solves the
  limitation of task-specific visual representation for vision and language tasks.
  It also relieves the cost of bounding box annotations and overcomes the unbalance
  between semantic labels in visual task and language semantic. To provide a better
  representation for down-stream tasks, we pre-train a universal end-to-end model
  with image and sentence pairs from Visual Genome dataset and MS-COCO dataset. We
  propose to use a random pixel sampling mechanism to enhance the robustness of visual
  representation and to apply the Masked Language Model and Image-Text Matching as
  pre-training tasks. Extensive experiments on downstream tasks with our pre-trained
  model show that our approach makes the most state-of-the-arts in downstream tasks,
  including Visual Question Answering (VQA), image-text retrieval, Natural Language
  for Visual Reasoning for Real (NLVR). Particularly, we boost the performance of
  a single model in VQA task by 2.17 points compared with SOTA under fair comparison.
ref_count: 40
references:
- pid: 54416048772b921720f19869ed11c2a360589d03
  title: 'UNITER: Learning UNiversal Image-TExt Representations'
- pid: cf336d272a30d6ad6141db67faa64deb8791cd61
  title: A Corpus for Reasoning about Natural Language Grounded in Photographs
- pid: a27ed1310b9c0832bde8f906e0fcd23d1f3aa79c
  title: 'Making the V in VQA Matter: Elevating the Role of Image Understanding in
    Visual Question Answering'
- pid: 2c03df8b48bf3fa39054345bafabfeff15bfd11d
  title: Deep Residual Learning for Image Recognition
- pid: 2527626c11a84f15709e943fbfa2356e19930e3b
  title: 'VL-BERT: Pre-training of Generic Visual-Linguistic Representations'
- pid: 79c93274429d6355959f1e4374c2147bb81ea649
  title: 'LXMERT: Learning Cross-Modality Encoder Representations from Transformers'
- pid: 5aec474c31a2f4b74703c6f786c0a8ff85c450da
  title: 'VisualBERT: A Simple and Performant Baseline for Vision and Language'
- pid: 65a9c7b0800c86a196bc14e7621ff895cc6ab287
  title: 'ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language
    Tasks'
- pid: df2b0e26d0599ce3e70df8a9da02e51594e0e992
  title: 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'
- pid: a9fd5511b42206a27748f373e0fdb7eb76a23055
  title: 'ImageBERT: Cross-modal Pre-training with Large-scale Weak-supervised Image-Text
    Data'
- pid: 2bc1c8bd00bbf7401afcb5460277840fd8bab029
  title: 'Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal
    Pre-training'
- pid: a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8
  title: Bottom-Up and Top-Down Attention for Image Captioning and Visual Question
    Answering
- pid: f6e0856b4a9199fa968ac00da612a9407b5cb85c
  title: Aggregated Residual Transformations for Deep Neural Networks
- pid: 424561d8585ff8ebce7d5d07de8dbf7aae5e7270
  title: 'Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks'
- pid: 6648b4db5f12c30941ea78c695e77aded19672bb
  title: Unified Vision-Language Pre-Training for Image Captioning and VQA
- pid: b82153bf85d5d1edd3f170aace830e5328ca9ed0
  title: Fusion of Detected Objects in Text for Visual Question Answering
- pid: ad748d1772f893b3c8a3857a19292375be259daf
  title: Knowledge Aware Semantic Concept Expansion for Image-Text Matching
- pid: 48a7873681c6aa88b9e0e22a25c2a8245eaeb45f
  title: Position Focused Attention Network for Image-Text Matching
- pid: c41a11c0e9b8b92b4faaf97749841170b760760a
  title: 'VideoBERT: A Joint Model for Video and Language Representation Learning'
- pid: ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc
  title: Cross-lingual Language Model Pretraining
- pid: b4df354db88a70183a64dbc9e56cf14e7669a6c0
  title: 'Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic
    Image Captioning'
- pid: cd18800a0fe0b668a1cc19f2ec95b5003d0a5035
  title: Improving Language Understanding by Generative Pre-Training
- pid: a5d10341717c0519cf63151b496a6d2ed67aa05f
  title: Bilinear Attention Networks
- pid: 1b47265245e8db53a553049dcb27ed3e495fd625
  title: 'ImageNet: A large-scale hierarchical image database'
- pid: 45dd2a3cd7c27f2e9509b023d702408f5ac11c9d
  title: Stacked Cross Attention for Image-Text Matching
- pid: f7ab6c52be9351ac3f6cf8fe6ad5efba1c1595e8
  title: 'VSE++: Improving Visual-Semantic Embeddings with Hard Negatives'
- pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  title: Attention is All you Need
- pid: fe466e84fa2e838adc3c37ee327cd68004ae08fe
  title: 'MUTAN: Multimodal Tucker Fusion for Visual Question Answering'
- pid: dbde7dfa6cae81df8ac19ef500c42db96c3d1edd
  title: 'Google''s Neural Machine Translation System: Bridging the Gap between Human
    and Machine Translation'
- pid: 97fb4e3d45bb098e27e0071448b6152217bd35a5
  title: Layer Normalization
- pid: afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d
  title: 'Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image
    Annotations'
- pid: 97ad70a9fa3f99adf18030e5e38ebe3d90daa2db
  title: 'VQA: Visual Question Answering'
- pid: d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0
  title: 'Show and tell: A neural image caption generator'
- pid: 55e022fb7581bb9e1fce678d21fb25ffbb3fbb88
  title: Deep Visual-Semantic Alignments for Generating Image Descriptions
- pid: 34f25a8704614163c4095b3ee2fc969b60de4698
  title: 'Dropout: a simple way to prevent neural networks from overfitting'
- pid: 71b7178df5d2b112d07e45038cb5637208659ff7
  title: 'Microsoft COCO: Common Objects in Context'
- pid: 4d8f2d14af5991d4f0d050d22216825cac3157bd
  title: 'Show, Attend and Tell: Neural Image Caption Generation with Visual Attention'
- pid: e15cf50aa89fee8535703b9f9512fca5bfc43327
  title: Going deeper with convolutions
slug: Pixel-BERT:-Aligning-Image-Pixels-with-Text-by-Deep-Huang-Zeng
title: 'Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers'
url: https://www.semanticscholar.org/paper/Pixel-BERT:-Aligning-Image-Pixels-with-Text-by-Deep-Huang-Zeng/598a2ee223e2949c3b28389e922c1892b4717d2a?sort=total-citations
venue: ArXiv
year: 2020
