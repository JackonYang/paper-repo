authors:
- Jason Phang
- "Thibault F\xE9vry"
- Samuel R. Bowman
badges:
- id: OPEN_ACCESS
corpusId: 53221289
fieldsOfStudy:
- Computer Science
numCitedBy: 264
numCiting: 34
paperAbstract: Pretraining sentence encoders with language modeling and related unsupervised
  tasks has recently been shown to be very effective for language understanding tasks.
  By supplementing language model-style pretraining with further training on data-rich
  supervised tasks, such as natural language inference, we obtain additional performance
  improvements on the GLUE benchmark. Applying supplementary training on BERT (Devlin
  et al., 2018), we attain a GLUE score of 81.8---the state of the art (as of 02/24/2019)
  and a 1.4 point improvement over BERT. We also observe reduced variance across random
  restarts in this setting. Our approach yields similar improvements when applied
  to ELMo (Peters et al., 2018a) and Radford et al. (2018)'s model. In addition, the
  benefits of supplementary training are particularly pronounced in data-constrained
  regimes, as we show in experiments with artificially limited training data.
ref_count: 34
references:
- pid: df2b0e26d0599ce3e70df8a9da02e51594e0e992
  title: 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'
- pid: cd18800a0fe0b668a1cc19f2ec95b5003d0a5035
  title: Improving Language Understanding by Generative Pre-Training
- pid: 93b8da28d006415866bf48f9a6e06b5242129195
  title: 'GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language
    Understanding'
- pid: ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c
  title: Supervised Learning of Universal Sentence Representations from Natural Language
    Inference Data
- pid: 1e077413b25c4d34945cc2707e17e46ed4fe784a
  title: Universal Language Model Fine-tuning for Text Classification
- pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  title: Attention is All you Need
- pid: bc8fa64625d9189f5801837e7b133e7fe3c581f7
  title: 'Learned in Translation: Contextualized Word Vectors'
- pid: 5d833331b0e22ff359db05c62a8bca18c4f04b68
  title: One billion word benchmark for measuring progress in statistical language
    modeling
- pid: a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096
  title: 'SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual
    Focused Evaluation'
- pid: 5ded2b8c64491b4a67f6d39ce473d4b9347a672e
  title: A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference
- pid: 3febb2bed8865945e7fddc99efd791887bb7e14f
  title: Deep Contextualized Word Representations
- pid: 687bac2d3320083eb4530bf18bb8f8f721477600
  title: Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank
- pid: f04df4e20a18358ea2f689b4c129781628ef7fc1
  title: A large annotated corpus for learning natural language inference
- pid: 05dd7254b632376973f3a1b4d39485da17814df5
  title: 'SQuAD: 100,000+ Questions for Machine Comprehension of Text'
- pid: 93b4cc549a1bc4bc112189da36c318193d05d806
  title: 'AllenNLP: A Deep Semantic Natural Language Processing Platform'
- pid: cb0f3ee1e98faf92429d601cdcd76c69c1e484eb
  title: Neural Network Acceptability Judgments
- pid: 0e6824e137847be0599bb0032e37042ed2ef5045
  title: 'Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching
    Movies and Reading Books'
- pid: b36a5bb1707bb9c70025294b3a310138aae8327a
  title: Automatic differentiation in PyTorch
- pid: 475354f10798f110d34792b6d88f31d6d5cb099e
  title: Automatically Constructing a Corpus of Sentential Paraphrases
- pid: de794d50713ea5f91a7c9da3d72041e2f5ef8452
  title: The PASCAL Recognising Textual Entailment Challenge
- pid: 128cb6b891aee1b5df099acb48e2efecfcff689f
  title: The Winograd Schema Challenge
- pid: e03d300581e16f6664157d2c1c6ceec33ec528ce
  title: Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object
    Classification, and Recognising Tectual Entailment
slug: "Sentence-Encoders-on-STILTs:-Supplementary-Training-Phang-F\xE9vry"
title: 'Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data
  Tasks'
url: "https://www.semanticscholar.org/paper/Sentence-Encoders-on-STILTs:-Supplementary-Training-Phang-F\xE9\
  vry/b47381e04739ea3f392ba6c8faaf64105493c196?sort=total-citations"
venue: ArXiv
year: 2018
