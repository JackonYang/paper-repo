authors:
- Peter Anderson
- Xiaodong He
- Chris Buehler
- Damien Teney
- Mark Johnson
- Stephen Gould
- Lei Zhang
badges: []
corpusId: 195347831
fieldsOfStudy:
- Computer Science
numCitedBy: 292
numCiting: 58
paperAbstract: Top-down visual attention mechanisms have been used extensively in
  image captioning and visual question answering (VQA) to enable deeper image understanding
  through fine-grained analysis and even multiple steps of reasoning. In this work,
  we propose a combined bottom-up and topdown attention mechanism that enables attention
  to be calculated at the level of objects and other salient image regions. This is
  the natural basis for attention to be considered. Within our approach, the bottom-up
  mechanism (based on Faster R-CNN) proposes image regions, each with an associated
  feature vector, while the top-down mechanism determines feature weightings. Applying
  this approach to image captioning, our results on the MSCOCO test server establish
  a new state-of-the-art for the task, improving the best published result in terms
  of CIDEr score from 114.7 to 117.9 and BLEU-4 from 35.2 to 36.9. Demonstrating the
  broad applicability of the method, applying the same approach to VQA we obtain a
  new state-of-the-art on the VQA v2.0 dataset with 70.2% overall accuracy.
ref_count: 58
references:
- pid: fb9d253258d6b3beceb9d6cd7bba6e0a29ab875b
  title: Hierarchical Question-Image Co-Attention for Visual Question Answering
- pid: 9f4d7d622d1f7319cc511bfef661cd973e881a4c
  title: 'Knowing When to Look: Adaptive Attention via a Visual Sentinel for Image
    Captioning'
- pid: 00fe3d95d0fd5f1433d81405bee772c4fe9af9c6
  title: What Value Do Explicit High Level Concepts Have in Vision to Language Problems?
- pid: bf55591e09b58ea9ce8d66110d6d3000ee804bdd
  title: Image Captioning with Semantic Attention
- pid: 56ffece2817a0363f551210733a611830ba1155d
  title: 'Aligning where to see and what to tell: image caption with region-based
    attention and scene factorization'
- pid: def584565d05d6a8ba94de6621adab9e301d375d
  title: 'Visual7W: Grounded Question Answering in Images'
- pid: f77a604410d88307ec5c6331c8b6133272fbaa10
  title: Self-Critical Sequence Training for Image Captioning
- pid: a27ed1310b9c0832bde8f906e0fcd23d1f3aa79c
  title: 'Making the V in VQA Matter: Elevating the Role of Image Understanding in
    Visual Question Answering'
- pid: d674b540dcd968bc302ea4360df3f4e85e994b55
  title: 'Show, Ask, Attend, and Answer: A Strong Baseline For Visual Question Answering'
- pid: 3c1bbd2672c11a796f1e6e6aa787257498ec8bec
  title: Revisiting Visual Question Answering Baselines
- pid: 5785466bc14529e94e54baa4ed051f7037f3b1d3
  title: Boosting Image Captioning with Attributes
- pid: d7ce5665a72c0b607f484c1b448875f02ddfac3b
  title: 'DenseCap: Fully Convolutional Localization Networks for Dense Captioning'
- pid: 1cf6bc0866226c1f8e282463adc8b75d92fba9bb
  title: 'Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for
    Visual Question Answering'
- pid: 15f102c3c9f4d4fe6ba105e221df48c6e8902b3b
  title: From captions to visual concepts and back
- pid: d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0
  title: 'Show and tell: A neural image caption generator'
- pid: 54b2b6f35f1b5704dddfaa3a137a2f4ad3dfe745
  title: Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)
- pid: 2c1890864c1c2b750f48316dc8b650ba4772adc5
  title: Stacked Attention Networks for Image Question Answering
- pid: fddc15480d086629b960be5bff96232f967f2252
  title: Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual
    Grounding
- pid: 4d8f2d14af5991d4f0d050d22216825cac3157bd
  title: 'Show, Attend and Tell: Neural Image Caption Generation with Visual Attention'
- pid: 97ad70a9fa3f99adf18030e5e38ebe3d90daa2db
  title: 'VQA: Visual Question Answering'
- pid: 424561d8585ff8ebce7d5d07de8dbf7aae5e7270
  title: 'Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks'
- pid: 258986132bf17755fe8263e42429fe73218c1534
  title: 'CIDEr: Consensus-based image description evaluation'
- pid: 665a311c538fc021c27acd3953f171924cc5905c
  title: Optimization of image description metrics using policy gradient methods
- pid: 1c54acd7d9ed8017acdc5674c9b7faac738fd651
  title: 'SPICE: Semantic Propositional Image Caption Evaluation'
- pid: 4d7a9197433acbfb24ef0e9d0f33ed1699e4a5b0
  title: 'SSD: Single Shot MultiBox Detector'
- pid: 2c03df8b48bf3fa39054345bafabfeff15bfd11d
  title: Deep Residual Learning for Image Recognition
- pid: f8e79ac0ea341056ef20f2616628b3e964764cfd
  title: 'You Only Look Once: Unified, Real-Time Object Detection'
- pid: 71b7178df5d2b112d07e45038cb5637208659ff7
  title: 'Microsoft COCO: Common Objects in Context'
- pid: 61d2dda8d96a10a714636475c7589bd149bda053
  title: Review Networks for Caption Generation
- pid: f01fc808592ea7c473a69a6e7484040a435f36d9
  title: Long-term recurrent convolutional networks for visual recognition and description
- pid: 46b8cbcdff87b842c2c1d4a003c831f845096ba7
  title: Order-Embeddings of Images and Language
- pid: e74f9b7f8eec6ba4704c206b93bc8079af3da4bd
  title: ImageNet Large Scale Visual Recognition Challenge
- pid: 88caa4a0253a8b0076176745ebc072864eab66e1
  title: Language Modeling with Gated Convolutional Networks
- pid: 38b6540ddd5beebffd05047c78183f7575559fb2
  title: Selective Search for Object Recognition
- pid: f37e1b62a767a307c046404ca96bc140b3e68cb5
  title: 'GloVe: Global Vectors for Word Representation'
- pid: e543af6f1f29661a43a2cc7706e4a95639327d68
  title: Perceptual grouping and attention in visual search for features and for objects.
- pid: 77f0a39b8e02686fd85b01971f8feb7f60971f80
  title: Identity Mappings in Deep Residual Networks
- pid: 60b05f32c32519a809f21642ef1eb3eaf3848008
  title: 'ROUGE: A Package for Automatic Evaluation of Summaries'
- pid: 696ca58d93f6404fea0fc75c62d1d7b378f47628
  title: 'Microsoft COCO Captions: Data Collection and Evaluation Server'
- pid: fe87ea16d5eb1c7509da9a0314bbf4c7b0676506
  title: Spatial Transformer Networks
- pid: 44d2abe2175df8153f465f6c39b68b76a0d40ab9
  title: Long Short-Term Memory
- pid: e0945081b5b87187a53d4329cf77cd8bff635795
  title: Highway Networks
- pid: 76361a44e145732a39dbc68d9418871038c83be2
  title: A feature-integration theory of attention
- pid: 53e66b6934516a9859573f4866f81f04bce977ae
  title: Control of goal-directed and stimulus-driven attention in the brain
- pid: 0b544dfe355a5070b60986319a3f51fb45d1348e
  title: "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical\
    \ Machine Translation"
- pid: d7da009f457917aa381619facfa5ffae9329a6e9
  title: 'Bleu: a Method for Automatic Evaluation of Machine Translation'
- pid: 8729441d734782c3ed532a7d2d9611b438c0a09a
  title: 'ADADELTA: An Adaptive Learning Rate Method'
- pid: 26adb749fc5d80502a6d889966e50b31391560d3
  title: 'Meteor Universal: Language Specific Translation Evaluation for Any Target
    Language'
slug: Bottom-Up-and-Top-Down-Attention-for-Image-and-VQA-Anderson-He
title: Bottom-Up and Top-Down Attention for Image Captioning and VQA
url: https://www.semanticscholar.org/paper/Bottom-Up-and-Top-Down-Attention-for-Image-and-VQA-Anderson-He/a79b694bd4ef51207787da1948ed473903b751ef?sort=total-citations
venue: ArXiv
year: 2017
