authors:
- "R. J\xF3zefowicz"
- Wojciech Zaremba
- Ilya Sutskever
badges:
- id: OPEN_ACCESS
corpusId: 9668607
fieldsOfStudy:
- Computer Science
numCitedBy: 1383
numCiting: 32
paperAbstract: "The Recurrent Neural Network (RNN) is an extremely powerful sequence\
  \ model that is often difficult to train. The Long Short-Term Memory (LSTM) is a\
  \ specific RNN architecture whose design makes it much easier to train. While wildly\
  \ successful in practice, the LSTM's architecture appears to be ad-hoc so it is\
  \ not clear if it is optimal, and the significance of its individual components\
  \ is unclear. \n \nIn this work, we aim to determine whether the LSTM architecture\
  \ is optimal or whether much better architectures exist. We conducted a thorough\
  \ architecture search where we evaluated over ten thousand different RNN architectures,\
  \ and identified an architecture that outperforms both the LSTM and the recently-introduced\
  \ Gated Recurrent Unit (GRU) on some but not all tasks. We found that adding a bias\
  \ of 1 to the LSTM's forget gate closes the gap between the LSTM and the GRU."
ref_count: 32
references:
- pid: 0d0b3ff23fbf1ec7a0dfebd923a3ef224306491a
  title: Learning to Execute
- pid: 11540131eae85b2e11d53df7f1360eeb6476e7f4
  title: 'Learning to Forget: Continual Prediction with LSTM'
- pid: 9665247ea3421929f9b6ad721f139f11edb1dbb8
  title: Learning Longer Memory in Recurrent Neural Networks
- pid: a7976c2bacfbb194ddbe7fd10c2e50a545cf4081
  title: 'LSTM: A Search Space Odyssey'
- pid: adfcf065e15fd3bc9badf6145034c84dfb08f204
  title: Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling
- pid: cea967b59209c6be22829699f05b8b1ac4dc092d
  title: Sequence to Sequence Learning with Neural Networks
- pid: 0d6203718c15f137fda2f295c96269bc2b254644
  title: Learning Recurrent Neural Networks with Hessian-Free Optimization
- pid: aa7bfd2304201afbb19971ebde87b17e40242e91
  title: On the importance of initialization and momentum in deep learning
- pid: f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97
  title: Recurrent Neural Network Regularization
- pid: 44d2abe2175df8153f465f6c39b68b76a0d40ab9
  title: Long Short-Term Memory
- pid: d0be39ee052d246ae99c082a565aba25b811be2d
  title: Learning long-term dependencies with gradient descent is difficult
- pid: 9819b600a828a57e1cde047bbe710d3446b30da5
  title: Recurrent neural network based language model
- pid: 84069287da0a6b488b8c933f3cb5be759cb6237e
  title: On the difficulty of training recurrent neural networks
- pid: 0b544dfe355a5070b60986319a3f51fb45d1348e
  title: "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical\
    \ Machine Translation"
- pid: 96364af2d208ea75ca3aeb71892d2f7ce7326b55
  title: Statistical Language Models Based on Neural Networks
- pid: 89b1f4740ae37fd04f6ac007577bdd34621f0861
  title: Generating Sequences With Recurrent Neural Networks
- pid: 18c82d4b6cf94fb84ba6ea230e80cb07ed9a9cf8
  title: 'Modeling Temporal Dependencies in High-Dimensional Sequences: Application
    to Polyphonic Music Generation and Transcription'
- pid: 4c46347fbc272b21468efe3d9af34b4b2bad6684
  title: Deep learning via Hessian-free optimization
- pid: 0d073966e48ffb6dccde1e4eb3f0380c10c6a766
  title: 'Harnessing Nonlinearity: Predicting Chaotic Systems and Saving Energy in
    Wireless Communication'
- pid: 3449b65008b27f6e60a73d80c1fd990f0481126b
  title: 'Torch7: A Matlab-like Environment for Machine Learning'
- pid: 0b44fcbeea9415d400c5f5789d6b892b6f98daff
  title: 'Building a Large Annotated Corpus of English: The Penn Treebank'
- pid: 3f3d13e95c25a8f6a753e38dfce88885097cbd43
  title: Untersuchungen zu dynamischen neuronalen Netzen
slug: "An-Empirical-Exploration-of-Recurrent-Network-J\xF3zefowicz-Zaremba"
title: An Empirical Exploration of Recurrent Network Architectures
url: "https://www.semanticscholar.org/paper/An-Empirical-Exploration-of-Recurrent-Network-J\xF3\
  zefowicz-Zaremba/5b8364c21155d3d2cd38ea4c8b8580beba9a3250?sort=total-citations"
venue: ICML
year: 2015
