authors:
- Xiujun Li
- Xi Yin
- Chunyuan Li
- Xiaowei Hu
- Pengchuan Zhang
- Lei Zhang
- Lijuan Wang
- Houdong Hu
- Li Dong
- Furu Wei
- Yejin Choi
- Jianfeng Gao
badges:
- id: OPEN_ACCESS
corpusId: 215754208
fieldsOfStudy:
- Computer Science
numCitedBy: 534
numCiting: 54
paperAbstract: Large-scale pre-training methods of learning cross-modal representations
  on image-text pairs are becoming popular for vision-language tasks. While existing
  methods simply concatenate image region features and text features as input to the
  model to be pre-trained and use self-attention to learn image-text semantic alignments
  in a brute force manner, in this paper, we propose a new learning method Oscar (Object-Semantics
  Aligned Pre-training), which uses object tags detected in images as anchor points
  to significantly ease the learning of alignments. Our method is motivated by the
  observation that the salient objects in an image can be accurately detected, and
  are often mentioned in the paired text. We pre-train an Oscar model on the public
  corpus of 6.5 million text-image pairs, and fine-tune it on downstream tasks, creating
  new state-of-the-arts on six well-established vision-language understanding and
  generation tasks.
ref_count: 52
references:
- pid: 6648b4db5f12c30941ea78c695e77aded19672bb
  title: Unified Vision-Language Pre-Training for Image Captioning and VQA
- pid: d8a305b9366608d54452ac30459ee57b4f5cf1c9
  title: 'UNITER: UNiversal Image-TExt Representation Learning'
- pid: a9fd5511b42206a27748f373e0fdb7eb76a23055
  title: 'ImageBERT: Cross-modal Pre-training with Large-scale Weak-supervised Image-Text
    Data'
- pid: 2bc1c8bd00bbf7401afcb5460277840fd8bab029
  title: 'Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal
    Pre-training'
- pid: 54416048772b921720f19869ed11c2a360589d03
  title: 'UNITER: Learning UNiversal Image-TExt Representations'
- pid: 4aa4069693bee00d1b0759ca3df35e59284e9845
  title: 'DeViSE: A Deep Visual-Semantic Embedding Model'
- pid: 65a9c7b0800c86a196bc14e7621ff895cc6ab287
  title: 'ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language
    Tasks'
- pid: 45dd2a3cd7c27f2e9509b023d702408f5ac11c9d
  title: Stacked Cross Attention for Image-Text Matching
- pid: 6eb3a15108dfdec25b46522ed94b866aeb156de9
  title: 'Connecting modalities: Semi-supervised segmentation and annotation of images
    using unaligned text corpora'
- pid: 5ac18d505ed6d10e8692cbb7d33f6852e6782692
  title: The Open Images Dataset V4
- pid: 2e36ea91a3c8fbff92be2989325531b4002e2afc
  title: Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models
- pid: 79c93274429d6355959f1e4374c2147bb81ea649
  title: 'LXMERT: Learning Cross-Modality Encoder Representations from Transformers'
- pid: 6fa25c94e41a0c90e3aabe80cf60f59ec9ff0a52
  title: Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-Training
- pid: 58555c7d168d1f50422ed9435d31ecd28d66eaa8
  title: Dual-path Convolutional Image-Text Embeddings with Instance Loss
- pid: bf55591e09b58ea9ce8d66110d6d3000ee804bdd
  title: Image Captioning with Semantic Attention
- pid: 19c630ad5a9de227f6357479fc95c62667be17f6
  title: 'CAMP: Cross-Modal Adaptive Message Passing for Text-Image Retrieval'
- pid: c41a11c0e9b8b92b4faaf97749841170b760760a
  title: 'VideoBERT: A Joint Model for Video and Language Representation Learning'
- pid: 00fe3d95d0fd5f1433d81405bee772c4fe9af9c6
  title: What Value Do Explicit High Level Concepts Have in Vision to Language Problems?
- pid: 5ac18d505ed6d10e8692cbb7d33f6852e6782692
  title: The Open Images Dataset V4
- pid: 5aec474c31a2f4b74703c6f786c0a8ff85c450da
  title: 'VisualBERT: A Simple and Performant Baseline for Vision and Language'
- pid: 755e9f43ce398ae8737366720c5f82685b0c253e
  title: Zero-Shot Learning Through Cross-Modal Transfer
- pid: ad748d1772f893b3c8a3857a19292375be259daf
  title: Knowledge Aware Semantic Concept Expansion for Image-Text Matching
- pid: 086fa2fe3ee2a5b805aeaf9fbfe59ee8157dad5c
  title: Guided Open Vocabulary Image Captioning with Constrained Beam Search
- pid: 48a7873681c6aa88b9e0e22a25c2a8245eaeb45f
  title: Position Focused Attention Network for Image-Text Matching
- pid: afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d
  title: 'Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image
    Annotations'
- pid: 658721bc13b0fa97366d38c05a96bf0a9f4bb0ac
  title: Multi-Task Deep Neural Networks for Natural Language Understanding
- pid: a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8
  title: Bottom-Up and Top-Down Attention for Image Captioning and Visual Question
    Answering
- pid: 5ac18d505ed6d10e8692cbb7d33f6852e6782692
  title: The Open Images Dataset V4
- pid: df2b0e26d0599ce3e70df8a9da02e51594e0e992
  title: 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'
- pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  title: Attention is All you Need
- pid: 8b55402ffee2734bfc7d5d7595500916e1ef04e8
  title: 'nocaps: novel object captioning at scale'
- pid: a27ed1310b9c0832bde8f906e0fcd23d1f3aa79c
  title: 'Making the V in VQA Matter: Elevating the Role of Image Understanding in
    Visual Question Answering'
- pid: b4df354db88a70183a64dbc9e56cf14e7669a6c0
  title: 'Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic
    Image Captioning'
- pid: f7ab6c52be9351ac3f6cf8fe6ad5efba1c1595e8
  title: 'VSE++: Improving Visual-Semantic Embeddings with Hard Negatives'
- pid: 71b7178df5d2b112d07e45038cb5637208659ff7
  title: 'Microsoft COCO: Common Objects in Context'
- pid: 424561d8585ff8ebce7d5d07de8dbf7aae5e7270
  title: 'Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks'
- pid: 4c163d4942117179d3e97182e1b280027d7d60a9
  title: Attention on Attention for Image Captioning
- pid: 5ac18d505ed6d10e8692cbb7d33f6852e6782692
  title: The Open Images Dataset V4
- pid: cf336d272a30d6ad6141db67faa64deb8791cd61
  title: A Corpus for Reasoning about Natural Language Grounded in Photographs
- pid: 136c05cb8dd359fb8e0dc7947172a9ecb74ccbec
  title: 'Learning by Abstraction: The Neural State Machine'
- pid: 6c8353697cdbb98dfba4f493875778c4286d3e3a
  title: Self-Critical Sequence Training for Image Captioning
- pid: 320464aa0231bc728c7d9ab7e71e552c12a7486b
  title: Meta Module Network for Compositional Visual Reasoning
- pid: 8e080b98efbe65c02a116439205ca2344b9f7cd4
  title: 'Im2Text: Describing Images Using 1 Million Captioned Photographs'
- pid: 44040913380206991b1991daf1192942e038fe31
  title: 'From image descriptions to visual denotations: New similarity metrics for
    semantic inference over event descriptions'
- pid: 1c46943103bd7b7a2c7be86859995a4144d1938b
  title: Visualizing Data using t-SNE
- pid: 5ac18d505ed6d10e8692cbb7d33f6852e6782692
  title: The Open Images Dataset V4
slug: Oscar:-Object-Semantics-Aligned-Pre-training-for-Li-Yin
title: 'Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks'
url: https://www.semanticscholar.org/paper/Oscar:-Object-Semantics-Aligned-Pre-training-for-Li-Yin/818e5cbc337e4e1b98e65a2d7c2d6d2a0318cd57?sort=total-citations
venue: ECCV
year: 2020
