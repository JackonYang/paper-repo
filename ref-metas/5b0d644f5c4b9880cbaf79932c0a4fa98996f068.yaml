authors:
- A. Mnih
- Y. Teh
badges:
- id: OPEN_ACCESS
corpusId: 6633369
fieldsOfStudy:
- Computer Science
numCitedBy: 520
numCiting: 24
paperAbstract: "In spite of their superior performance, neural probabilistic language\
  \ models (NPLMs) remain far less widely used than n-gram models due to their notoriously\
  \ long training times, which are measured in weeks even for moderately-sized datasets.\
  \ Training NPLMs is computationally expensive because they are explicitly normalized,\
  \ which leads to having to consider all words in the vocabulary when computing the\
  \ log-likelihood gradients. \n \nWe propose a fast and simple algorithm for training\
  \ NPLMs based on noise-contrastive estimation, a newly introduced procedure for\
  \ estimating unnormalized continuous distributions. We investigate the behaviour\
  \ of the algorithm on the Penn Treebank corpus and show that it reduces the training\
  \ times by more than an order of magnitude without affecting the quality of the\
  \ resulting models. The algorithm is also more efficient and much more stable than\
  \ importance sampling because it requires far fewer noise samples to perform well.\
  \ \n \nWe demonstrate the scalability of the proposed approach by training several\
  \ neural language models on a 47M-word corpus with a 80K-word vocabulary, obtaining\
  \ state-of-the-art results on the Microsoft Research Sentence Completion Challenge\
  \ dataset."
ref_count: 24
references:
- pid: a9fc84f8abe740cdc7ee82e69444d1d00dbe0ceb
  title: A Scalable Hierarchical Distributed Language Model
- pid: 6c2b28f9354f667cd5bd07afc0471d8334430da7
  title: A Neural Probabilistic Language Model
- pid: 8b395470a57c48d174c4216ea21a7a58bc046917
  title: Training Neural Network Language Models on Very Large Corpora
- pid: c19fbefdeead6a4154a22a9c8551a18b1530033a
  title: Hierarchical Probabilistic Neural Network Language Model
- pid: 699d5ab38deee78b1fd17cc8ad233c74196d16e9
  title: Adaptive Importance Sampling to Accelerate Training of a Neural Probabilistic
    Language Model
- pid: fac2ca048fdd7e848f0b9ba2f7be25bb49186770
  title: The Microsoft Research Sentence Completion Challenge
- pid: 9819b600a828a57e1cde047bbe710d3446b30da5
  title: Recurrent neural network based language model
- pid: bd7d93193aad6c4b71cc8942e808753019e87706
  title: Three new graphical models for statistical language modelling
- pid: 77dfe038a9bdab27c4505444931eaa976e9ec667
  title: Empirical Evaluation and Combination of Advanced Language Modeling Techniques
- pid: dac72f2c509aee67524d3321f77e97e8eff51de6
  title: 'Word Representations: A Simple and General Method for Semi-Supervised Learning'
- pid: 57458bc1cffe5caa45a885af986d70f723f406b4
  title: 'A unified architecture for natural language processing: deep neural networks
    with multitask learning'
- pid: 7a65f23d990231d461418067c808b09d84c19b2c
  title: Natural Language Processing with Python
- pid: 9c0ddf74f87d154db88d79c640578c1610451eec
  title: Parsing Natural Scenes and Natural Language with Recursive Neural Networks
- pid: e3ce36b9deb47aa6bb2aa19c4bfa71283b505025
  title: 'Noise-contrastive estimation: A new estimation principle for unnormalized
    statistical models'
- pid: 399da68d3b97218b6c80262df7963baa89dcc71b
  title: SRILM - an extensible language modeling toolkit
- pid: fb486e03369a64de2d5b0df86ec0a7b55d3907db
  title: A Maximum Entropy Approach to Natural Language Processing
slug: A-fast-and-simple-algorithm-for-training-neural-Mnih-Teh
title: A fast and simple algorithm for training neural probabilistic language models
url: https://www.semanticscholar.org/paper/A-fast-and-simple-algorithm-for-training-neural-Mnih-Teh/5b0d644f5c4b9880cbaf79932c0a4fa98996f068?sort=total-citations
venue: ICML
year: 2012
