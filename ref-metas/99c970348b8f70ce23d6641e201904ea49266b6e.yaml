authors:
- Andrew M. Saxe
- James L. McClelland
- S. Ganguli
badges:
- id: OPEN_ACCESS
corpusId: 17272965
fieldsOfStudy:
- Computer Science
numCitedBy: 1263
numCiting: 27
paperAbstract: 'Despite the widespread practical success of deep learning methods,
  our theoretical understanding of the dynamics of learning in deep neural networks
  remains quite sparse. We attempt to bridge the gap between the theory and practice
  of deep learning by systematically analyzing learning dynamics for the restricted
  case of deep linear neural networks. Despite the linearity of their input-output
  map, such networks have nonlinear gradient descent dynamics on weights that change
  with the addition of each new hidden layer. We show that deep linear networks exhibit
  nonlinear learning phenomena similar to those seen in simulations of nonlinear networks,
  including long plateaus followed by rapid transitions to lower error solutions,
  and faster convergence from greedy unsupervised pretraining initial conditions than
  from random initial conditions. We provide an analytical description of these phenomena
  by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical
  analysis also reveals the surprising finding that as the depth of a network approaches
  infinity, learning speed can nevertheless remain finite: for a special class of
  initial conditions on the weights, very deep networks incur only a finite, depth
  independent, delay in learning speed relative to shallow networks. We show that,
  under certain conditions on the training data, unsupervised pretraining can find
  this special class of initial conditions, while scaled random Gaussian initializations
  cannot. We further exhibit a new class of random orthogonal initial conditions on
  weights that, like unsupervised pre-training, enjoys depth independent learning
  times. We further show that these initial conditions also lead to faithful propagation
  of gradients even in deep nonlinear networks, as long as they operate in a special
  regime known as the edge of chaos.'
ref_count: 27
references:
- pid: b71ac1e9fb49420d13e084ac67254a0bbd40f83f
  title: Understanding the difficulty of training deep feedforward neural networks
- pid: 355d44f53428b1ac4fb2ab468d593c720640e5bd
  title: Greedy Layer-Wise Training of Deep Networks
- pid: aa7bfd2304201afbb19971ebde87b17e40242e91
  title: On the importance of initialization and momentum in deep learning
- pid: ccf415df5a83b343dae261286d29a40e8b80e6c6
  title: The Difficulty of Training Deep Architectures and the Effect of Unsupervised
    Pre-Training
- pid: 0d2336389dff3031910bd21dd1c44d1b4cd51725
  title: Why Does Unsupervised Pre-training Help Deep Learning?
- pid: b1a5961609c623fc816aaa77565ba38b25531a8e
  title: 'Neural Networks: Tricks of the Trade'
- pid: 9552ac39a57daacf3d75865a268935b5a0df9bbb
  title: 'Neural networks and principal component analysis: Learning from examples
    without local minima'
- pid: e60ff004dde5c13ec53087872cfcdd12e85beb57
  title: Learning Deep Architectures for AI
- pid: 84069287da0a6b488b8c933f3cb5be759cb6237e
  title: On the difficulty of training recurrent neural networks
- pid: 6fdb77260fc83dff91c44fea0f31a2cb8ed13d04
  title: Scaling learning algorithms towards AI
- pid: d0be39ee052d246ae99c082a565aba25b811be2d
  title: Learning long-term dependencies with gradient descent is difficult
- pid: 46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e
  title: Reducing the Dimensionality of Data with Neural Networks
- pid: abd1c342495432171beb7ca8fd9551ef13cbd0ff
  title: ImageNet classification with deep convolutional neural networks
- pid: d2b62f77cb2864e465aa60bca6c26bb1d2f84963
  title: Acoustic Modeling Using Deep Belief Networks
- pid: 4c46347fbc272b21468efe3d9af34b4b2bad6684
  title: Deep learning via Hessian-free optimization
- pid: 72e93aa6767ee683de7f001fa72f1314e40a8f35
  title: Building high-level features using large scale unsupervised learning
- pid: 398c296d0cc7f9d180f84969f8937e6d3a413796
  title: Multi-column deep neural networks for image classification
- pid: 57458bc1cffe5caa45a885af986d70f723f406b4
  title: 'A unified architecture for natural language processing: deep neural networks
    with multitask learning'
- pid: acc4e56c44771ebf69302a06af51498aeb0a6ac8
  title: Parsing with Compositional Vector Grammars
- pid: 3f3d13e95c25a8f6a753e38dfce88885097cbd43
  title: Untersuchungen zu dynamischen neuronalen Netzen
- pid: b87274e6d9aa4e6ba5148898aa92941617d2b6ed
  title: Efficient BackProp
slug: Exact-solutions-to-the-nonlinear-dynamics-of-in-Saxe-McClelland
title: Exact solutions to the nonlinear dynamics of learning in deep linear neural
  networks
url: https://www.semanticscholar.org/paper/Exact-solutions-to-the-nonlinear-dynamics-of-in-Saxe-McClelland/99c970348b8f70ce23d6641e201904ea49266b6e?sort=total-citations
venue: ICLR
year: 2014
