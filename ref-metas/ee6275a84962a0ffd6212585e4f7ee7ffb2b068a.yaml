authors:
- A. Ng
badges:
- id: OPEN_ACCESS
corpusId: 11258400
fieldsOfStudy:
- Computer Science
numCitedBy: 1635
numCiting: 42
paperAbstract: We consider supervised learning in the presence of very many irrelevant
  features, and study two different regularization methods for preventing overfitting.
  Focusing on logistic regression, we show that using L1 regularization of the parameters,
  the sample complexity (i.e., the number of training examples required to learn "well,")
  grows only logarithmically in the number of irrelevant features. This logarithmic
  rate matches the best known bounds for feature selection, and indicates that L1
  regularized logistic regression can be effective even if there are exponentially
  many irrelevant features as there are training examples. We also give a lower-bound
  showing that any rotationally invariant algorithm---including logistic regression
  with L2 regularization, SVMs, and neural networks trained by backpropagation---has
  a worst case sample complexity that grows at least linearly in the number of irrelevant
  features.
ref_count: 42
references:
- pid: 015999a72c70a960e59c51078b09c8f672af0d2c
  title: 'The Sample Complexity of Pattern Classification with Neural Networks: The
    Size of the Weights is More Important than the Size of the Network'
- pid: a1dace286582d91916fe470d08f30381cf453f20
  title: 'Learning Quickly When Irrelevant Attributes Abound: A New Linear-Threshold
    Algorithm'
- pid: 98eed3f082351c4821d1edb315846207a8fefbe9
  title: Exponentiated Gradient Versus Gradient Descent for Linear Predictors
- pid: 9360e5ce9c98166bb179ad479a9d2919ff13d022
  title: Training Products of Experts by Minimizing Contrastive Divergence
- pid: b365b8e45b7d81f081de44ac8f9eadf9144f3ca5
  title: Regression Shrinkage and Selection via the Lasso
- pid: b83396caf4762c906530c9219a9e4dd0658232b0
  title: A general lower bound on the number of examples needed for learning
- pid: 656859af2ed88cfa23f2bd063c1816a8fc04c47e
  title: Using Maximum Entropy for Text Classification
- pid: 385197d4c02593e2823c71e4f90a0993b703620e
  title: Statistical learning theory
- pid: 65a69968bb8c41aad0113cec4c2d981bddf50bc8
  title: Pattern Classification
- pid: 4a18360a14facea50dc819145b1daf4c53d5d59e
  title: Estimation of Dependences Based on Empirical Data
- pid: 44d71e0ec9f68d8eb802b9ab1dde8368efeac42e
  title: The Elements of Statistical Learning
- pid: b272701e77ddb860741a193ac1701ca382853680
  title: "Convex Analysis\u306E\u4E8C,\u4E09\u306E\u9032\u5C55\u306B\u3064\u3044\u3066"
slug: Feature-selection,-L1-vs.-L2-regularization,-and-Ng
title: Feature selection, L1 vs. L2 regularization, and rotational invariance
url: https://www.semanticscholar.org/paper/Feature-selection,-L1-vs.-L2-regularization,-and-Ng/ee6275a84962a0ffd6212585e4f7ee7ffb2b068a?sort=total-citations
venue: Twenty-first international conference on Machine learning  - ICML '04
year: 2004
