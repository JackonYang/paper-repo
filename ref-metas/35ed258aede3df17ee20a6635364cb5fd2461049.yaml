authors:
- Luowei Zhou
- Yingbo Zhou
- Jason J. Corso
- R. Socher
- Caiming Xiong
badges:
- id: UNPAYWALL
- id: OPEN_ACCESS
corpusId: 4564155
fieldsOfStudy:
- Computer Science
numCitedBy: 269
numCiting: 46
paperAbstract: Dense video captioning aims to generate text descriptions for all events
  in an untrimmed video. This involves both detecting and describing events. Therefore,
  all previous methods on dense video captioning tackle this problem by building two
  models, i.e. an event proposal and a captioning model, for these two sub-problems.
  The models are either trained separately or in alternation. This prevents direct
  influence of the language description to the event proposal, which is important
  for generating accurate descriptions. To address this problem, we propose an end-to-end
  transformer model for dense video captioning. The encoder encodes the video into
  appropriate representations. The proposal decoder decodes from the encoding with
  different anchors to form video event proposals. The captioning decoder employs
  a masking network to restrict its attention to the proposal event over the encoding
  feature. This masking network converts the event proposal to a differentiable mask,
  which ensures the consistency between the proposal and captioning during training.
  In addition, our model employs a self-attention mechanism, which enables the use
  of efficient non-recurrent structure during encoding and leads to performance improvements.
  We demonstrate the effectiveness of this end-to-end model on ActivityNet Captions
  and YouCookII datasets, where we achieved 10.12 and 6.58 METEOR score, respectively.
ref_count: 46
references:
- pid: e58a110fa1e4ddf247d5c614d117d64bfbe135c4
  title: Sequence to Sequence -- Video to Text
- pid: 5f425b7abf2ed3172ed060df85bb1885860a297e
  title: Describing Videos by Exploiting Temporal Structure
- pid: 032274e57f7d8b456bd255fe76b909b2c1d7458e
  title: A Deep Reinforced Model for Abstractive Summarization
- pid: 778ce81457383bd5e3fdb11b145ded202ebb4970
  title: Semantic Compositional Networks for Visual Captioning
- pid: d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0
  title: 'Show and tell: A neural image caption generator'
- pid: 5785466bc14529e94e54baa4ed051f7037f3b1d3
  title: Boosting Image Captioning with Attributes
- pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  title: Attention is All you Need
- pid: a23ab0fb7d9e9961e92d704ed71e3dbc15c0d908
  title: 'A Thousand Frames in Just a Few Words: Lingual Description of Videos through
    Latent Topics and Sparse Object Stitching'
- pid: 43795b7bac3d921c4e579964b54187bdbf6c6330
  title: Translating Videos to Natural Language Using Deep Recurrent Neural Networks
- pid: bf55591e09b58ea9ce8d66110d6d3000ee804bdd
  title: Image Captioning with Semantic Attention
- pid: f01fc808592ea7c473a69a6e7484040a435f36d9
  title: Long-term recurrent convolutional networks for visual recognition and description
- pid: 6c8353697cdbb98dfba4f493875778c4286d3e3a
  title: Self-Critical Sequence Training for Image Captioning
- pid: 424561d8585ff8ebce7d5d07de8dbf7aae5e7270
  title: 'Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks'
- pid: df137487e20ba7c6e1e2b9a1e749f2a578b5ad99
  title: Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks
- pid: 4d8f2d14af5991d4f0d050d22216825cac3157bd
  title: 'Show, Attend and Tell: Neural Image Caption Generation with Visual Attention'
- pid: fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5
  title: Neural Machine Translation by Jointly Learning to Align and Translate
- pid: eb42cf88027de515750f230b23b1a057dc782108
  title: Very Deep Convolutional Networks for Large-Scale Image Recognition
- pid: 204a4a70428f3938d2c538a4d74c7ae0416306d8
  title: A Structured Self-attentive Sentence Embedding
- pid: 2c03df8b48bf3fa39054345bafabfeff15bfd11d
  title: Deep Residual Learning for Image Recognition
- pid: 96a0320ef14877038906947b684011cf7378c440
  title: Grounded Language Learning from Video Described with Sentences
- pid: 4d376d6978dad0374edfa6709c9556b42d3594d3
  title: 'Batch Normalization: Accelerating Deep Network Training by Reducing Internal
    Covariate Shift'
- pid: 2f83f6e1afadf0963153974968af6b8342775d82
  title: Framewise phoneme classification with bidirectional LSTM and other neural
    network architectures
- pid: 0c1f9ca23f4f09ecfc44bcc8ca1c2736624f4652
  title: A Theoretically Grounded Application of Dropout in Recurrent Neural Networks
- pid: 7ffdbc358b63378f07311e883dddacc9faeeaf4b
  title: Fast R-CNN
- pid: 97fb4e3d45bb098e27e0071448b6152217bd35a5
  title: Layer Normalization
- pid: 44d2abe2175df8153f465f6c39b68b76a0d40ab9
  title: Long Short-Term Memory
- pid: 84069287da0a6b488b8c933f3cb5be759cb6237e
  title: On the difficulty of training recurrent neural networks
- pid: 0da353e79f666a3ae7dd0a5d28c75b852a7f60bf
  title: SHOW
- pid: 927881a5602f430ffd145d44b8c35cf7a07b464d
  title: '[C]'
slug: End-to-End-Dense-Video-Captioning-with-Masked-Zhou-Zhou
title: End-to-End Dense Video Captioning with Masked Transformer
url: https://www.semanticscholar.org/paper/End-to-End-Dense-Video-Captioning-with-Masked-Zhou-Zhou/35ed258aede3df17ee20a6635364cb5fd2461049?sort=total-citations
venue: 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition
year: 2018
