authors:
- Zhenzhong Lan
- Mingda Chen
- Sebastian Goodman
- Kevin Gimpel
- Piyush Sharma
- Radu Soricut
badges:
- id: OPEN_ACCESS
corpusId: 202888986
fieldsOfStudy:
- Computer Science
meta_key: albert-a-lite-bert-for-self-supervised-learning-of-language-representations
numCitedBy: 2708
numCiting: 83
paperAbstract: Increasing model size when pretraining natural language representations
  often results in improved performance on downstream tasks. However, at some point
  further model increases become harder due to GPU/TPU memory limitations and longer
  training times. To address these problems, we present two parameter-reduction techniques
  to lower memory consumption and increase the training speed of BERT. Comprehensive
  empirical evidence shows that our proposed methods lead to models that scale much
  better compared to the original BERT. We also use a self-supervised loss that focuses
  on modeling inter-sentence coherence, and show it consistently helps downstream
  tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art
  results on the GLUE, RACE, and \squad benchmarks while having fewer parameters compared
  to BERT-large. The code and the pretrained models are available at this https URL.
ref_count: 83
references:
- fieldsOfStudy:
  - Computer Science
  meta_key: bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding
  numCitedBy: 33754
  pid: df2b0e26d0599ce3e70df8a9da02e51594e0e992
  show_ref_link: true
  title: BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: roberta-a-robustly-optimized-bert-pretraining-approach
  numCitedBy: 7267
  pid: 077f8329a7b6fa3b7c877a57b81eb6c18b5f87de
  show_ref_link: true
  title: RoBERTa - A Robustly Optimized BERT Pretraining Approach
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: well-read-students-learn-better-on-the-importance-of-pre-training-compact-models
  numCitedBy: 248
  pid: 7402b604f14b8b91c53ed6eed04af92c59636c97
  show_ref_link: false
  title: Well-Read Students Learn Better - On the Importance of Pre-training Compact
    Models
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: efficient-training-of-bert-by-progressively-stacking
  numCitedBy: 57
  pid: 5a3749929bf5fb8b1f98a7b2a43c3b957bcf6c88
  show_ref_link: false
  title: Efficient Training of BERT by Progressively Stacking
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: structbert-incorporating-language-structures-into-pre-training-for-deep-language-understanding
  numCitedBy: 140
  pid: d56c1fc337fb07ec004dc846f80582c327af717c
  show_ref_link: true
  title: StructBERT - Incorporating Language Structures into Pre-training for Deep
    Language Understanding
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: well-read-students-learn-better-the-impact-of-student-initialization-on-knowledge-distillation
  numCitedBy: 87
  pid: 93ad19fbc85360043988fa9ea7932b7fdf1fa948
  show_ref_link: false
  title: Well-Read Students Learn Better - The Impact of Student Initialization on
    Knowledge Distillation
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: xlnet-generalized-autoregressive-pretraining-for-language-understanding
  numCitedBy: 4227
  pid: e0c6abdbdecf04ffac65c440da77fb9d66bb474c
  show_ref_link: true
  title: XLNet - Generalized Autoregressive Pretraining for Language Understanding
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: language-models-are-unsupervised-multitask-learners
  numCitedBy: 6284
  pid: 9405cc0d6169988371b2755e573cc28650d14dfe
  show_ref_link: true
  title: Language Models are Unsupervised Multitask Learners
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: bam-born-again-multi-task-networks-for-natural-language-understanding
  numCitedBy: 139
  pid: ef6948edae12eba6f1d486b8600108b9762f36ab
  show_ref_link: true
  title: BAM! Born-Again Multi-Task Networks for Natural Language Understanding
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: adaptive-input-representations-for-neural-language-modeling
  numCitedBy: 207
  pid: d170bd486e4c0fe82601e322b0e9e0dde63ab299
  show_ref_link: false
  title: Adaptive Input Representations for Neural Language Modeling
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: patient-knowledge-distillation-for-bert-model-compression
  numCitedBy: 363
  pid: 80cf2a6af4200ecfca1c18fc89de16148f1cd4bf
  show_ref_link: false
  title: Patient Knowledge Distillation for BERT Model Compression
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: exploring-the-limits-of-transfer-learning-with-a-unified-text-to-text-transformer
  numCitedBy: 3764
  pid: 3cfb319689f06bf04c2e28399361f414ca32c4b3
  show_ref_link: true
  title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: improving-language-understanding-by-generative-pre-training
  numCitedBy: 3533
  pid: cd18800a0fe0b668a1cc19f2ec95b5003d0a5035
  show_ref_link: true
  title: Improving Language Understanding by Generative Pre-Training
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: attention-is-all-you-need
  numCitedBy: 35157
  pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  show_ref_link: true
  title: Attention is All you Need
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: transformer-xl-attentive-language-models-beyond-a-fixed-length-context
  numCitedBy: 1771
  pid: c4744a7c2bb298e4a52289a1e085c71cc3d37bc6
  show_ref_link: true
  title: Transformer-XL - Attentive Language Models beyond a Fixed-Length Context
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: megatron-lm-training-multi-billion-parameter-language-models-using-model-parallelism
  numCitedBy: 498
  pid: 8323c591e119eb09b28b29fd6c7bc76bd889df7a
  show_ref_link: false
  title: Megatron-LM - Training Multi-Billion Parameter Language Models Using Model
    Parallelism
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: spanbert-improving-pre-training-by-representing-and-predicting-spans
  numCitedBy: 879
  pid: 81f5810fbbab9b7203b9556f4ce3c741875407bc
  show_ref_link: true
  title: SpanBERT - Improving Pre-training by Representing and Predicting Spans
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: glue-a-multi-task-benchmark-and-analysis-platform-for-natural-language-understanding
  numCitedBy: 2635
  pid: 93b8da28d006415866bf48f9a6e06b5242129195
  show_ref_link: true
  title: GLUE - A Multi-Task Benchmark and Analysis Platform for Natural Language
    Understanding
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: modeling-recurrence-for-transformer
  numCitedBy: 60
  pid: 528b5f5356bc7ad91edc4dc074b0273e1e55fb03
  show_ref_link: false
  title: Modeling Recurrence for Transformer
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: generating-long-sequences-with-sparse-transformers
  numCitedBy: 663
  pid: 21da617a0f79aabf94272107184606cefe90ab75
  show_ref_link: false
  title: Generating Long Sequences with Sparse Transformers
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: reducing-bert-pre-training-time-from-3-days-to-76-minutes
  numCitedBy: 100
  pid: 3c6dca9041f54583aeab60587c9e6e9272104dc1
  show_ref_link: false
  title: Reducing BERT Pre-Training Time from 3 Days to 76 Minutes
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: a-simple-method-for-commonsense-reasoning
  numCitedBy: 238
  pid: d7b6753a2d4a2b286c396854063bde3a91b75535
  show_ref_link: true
  title: A Simple Method for Commonsense Reasoning
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: skip-thought-vectors
  numCitedBy: 1928
  pid: 6e795c6e9916174ae12349f5dc3f516570c17ce8
  show_ref_link: true
  title: Skip-Thought Vectors
  year: 2015
- fieldsOfStudy:
  - Computer Science
  meta_key: dissent-learning-sentence-representations-from-explicit-discourse-relations
  numCitedBy: 55
  pid: eaea866271ce0bdf1b9bf8e11581a66ec58806c6
  show_ref_link: false
  title: DisSent - Learning Sentence Representations from Explicit Discourse Relations
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: semi-supervised-sequence-learning
  numCitedBy: 881
  pid: 4aa9f5150b46320f534de4747a2dd0cd7f3fe292
  show_ref_link: false
  title: Semi-supervised Sequence Learning
  year: 2015
- fieldsOfStudy:
  - Computer Science
  meta_key: universal-language-model-fine-tuning-for-text-classification
  numCitedBy: 2251
  pid: 1e077413b25c4d34945cc2707e17e46ed4fe784a
  show_ref_link: true
  title: Universal Language Model Fine-tuning for Text Classification
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: deep-equilibrium-models
  numCitedBy: 240
  pid: 9a618cca0d2fc78db1be1aed70517401cb3f3859
  show_ref_link: false
  title: Deep Equilibrium Models
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: training-deep-nets-with-sublinear-memory-cost
  numCitedBy: 455
  pid: 942deb7d865b7782c03176d95e3a0d56cb71009e
  show_ref_link: false
  title: Training Deep Nets with Sublinear Memory Cost
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: efficient-softmax-approximation-for-gpus
  numCitedBy: 196
  pid: 9ec499af9b85f30bdbdd6cdfbb07d484808c526a
  show_ref_link: false
  title: Efficient softmax approximation for GPUs
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: recursive-deep-models-for-semantic-compositionality-over-a-sentiment-treebank
  numCitedBy: 5366
  pid: 687bac2d3320083eb4530bf18bb8f8f721477600
  show_ref_link: true
  title: Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank
  year: 2013
- fieldsOfStudy:
  - Computer Science
  meta_key: understanding-the-disharmony-between-dropout-and-batch-normalization-by-variance-shift
  numCitedBy: 191
  pid: e8abbf36087d293a3426e2859bdef5f17397f2d7
  show_ref_link: false
  title: Understanding the Disharmony Between Dropout and Batch Normalization by Variance
    Shift
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: learning-distributed-representations-of-sentences-from-unlabelled-data
  numCitedBy: 458
  pid: 26e743d5bd465f49b9538deaf116c15e61b7951f
  show_ref_link: true
  title: Learning Distributed Representations of Sentences from Unlabelled Data
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: bi-directional-block-self-attention-for-fast-and-memory-efficient-sequence-modeling
  numCitedBy: 124
  pid: 0ef460c47377c3b9482d8177cbcafad1730a91a5
  show_ref_link: false
  title: Bi-Directional Block Self-Attention for Fast and Memory-Efficient Sequence
    Modeling
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: deep-contextualized-word-representations
  numCitedBy: 7987
  pid: 3febb2bed8865945e7fddc99efd791887bb7e14f
  show_ref_link: true
  title: Deep Contextualized Word Representations
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: squad-100-000-questions-for-machine-comprehension-of-text
  numCitedBy: 4263
  pid: 05dd7254b632376973f3a1b4d39485da17814df5
  show_ref_link: true
  title: SQuAD - 100,000+ Questions for Machine Comprehension of Text
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: inception-v4-inception-resnet-and-the-impact-of-residual-connections-on-learning
  numCitedBy: 8045
  pid: b5c26ab8767d046cb6e32d959fdf726aee89bb62
  show_ref_link: true
  title: Inception-v4, Inception-ResNet and the Impact of Residual Connections on
    Learning
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: glove-global-vectors-for-word-representation
  numCitedBy: 22536
  pid: f37e1b62a767a307c046404ca96bc140b3e68cb5
  show_ref_link: true
  title: GloVe - Global Vectors for Word Representation
  year: 2014
- fieldsOfStudy:
  - Computer Science
  meta_key: learned-in-translation-contextualized-word-vectors
  numCitedBy: 710
  pid: bc8fa64625d9189f5801837e7b133e7fe3c581f7
  show_ref_link: true
  title: Learned in Translation - Contextualized Word Vectors
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: learning-generic-sentence-representations-using-convolutional-neural-networks
  numCitedBy: 89
  pid: 513c670a9b3e0cdde301f2ea201db576b31750e3
  show_ref_link: false
  title: Learning Generic Sentence Representations Using Convolutional Neural Networks
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: a-broad-coverage-challenge-corpus-for-sentence-understanding-through-inference
  numCitedBy: 2037
  pid: 5ded2b8c64491b4a67f6d39ce473d4b9347a672e
  show_ref_link: true
  title: A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: distributed-representations-of-sentences-and-documents
  numCitedBy: 7044
  pid: f527bcfb09f32e6a4a8afc0b37504941c1ba2cee
  show_ref_link: true
  title: Distributed Representations of Sentences and Documents
  year: 2014
- fieldsOfStudy:
  - Computer Science
  meta_key: distributed-representations-of-words-and-phrases-and-their-compositionality
  numCitedBy: 26053
  pid: 87f40e6f3022adbc1f1905e3e506abad05a9964f
  show_ref_link: true
  title: Distributed Representations of Words and Phrases and their Compositionality
  year: 2013
- fieldsOfStudy:
  - Computer Science
  meta_key: mesh-tensorflow-deep-learning-for-supercomputers
  numCitedBy: 209
  pid: 2270b8628fd8ca67ae39d277f45bc3c38ac63d5f
  show_ref_link: true
  title: Mesh-TensorFlow - Deep Learning for Supercomputers
  year: 2018
- fieldsOfStudy:
  - Computer Science
  - Psychology
  meta_key: semeval-2017-task-1-semantic-textual-similarity-multilingual-and-crosslingual-focused-evaluation
  numCitedBy: 934
  pid: a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096
  show_ref_link: true
  title: SemEval-2017 Task 1 - Semantic Textual Similarity Multilingual and Crosslingual
    Focused Evaluation
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: the-reversible-residual-network-backpropagation-without-storing-activations
  numCitedBy: 297
  pid: 3a6d4cd0768ae8768e733280d362bdb4d25924e7
  show_ref_link: false
  title: The Reversible Residual Network - Backpropagation Without Storing Activations
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: race-large-scale-reading-comprehension-dataset-from-examinations
  numCitedBy: 698
  pid: 636a79420d838eabe4af7fb25d6437de45ab64e8
  show_ref_link: true
  title: RACE - Large-scale ReAding Comprehension Dataset From Examinations
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: sentencepiece-a-simple-and-language-independent-subword-tokenizer-and-detokenizer-for-neural-text-processing
  numCitedBy: 1513
  pid: b5246fa284f86b544a7c31f050b3bd0defd053fd
  show_ref_link: false
  title: SentencePiece - A simple and language independent subword tokenizer and detokenizer
    for Neural Text Processing
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: know-what-you-don-t-know-unanswerable-questions-for-squad
  numCitedBy: 1398
  pid: 4d1c856275744c0284312a3a50efb6ca9dc4cd4c
  show_ref_link: true
  title: Know What You Don't Know - Unanswerable Questions for SQuAD
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: aligning-books-and-movies-towards-story-like-visual-explanations-by-watching-movies-and-reading-books
  numCitedBy: 1418
  pid: 0e6824e137847be0599bb0032e37042ed2ef5045
  show_ref_link: true
  title: Aligning Books and Movies - Towards Story-Like Visual Explanations by Watching
    Movies and Reading Books
  year: 2015
- fieldsOfStudy:
  - Computer Science
  - Linguistics
  meta_key: neural-network-acceptability-judgments
  numCitedBy: 545
  pid: cb0f3ee1e98faf92429d601cdcd76c69c1e484eb
  show_ref_link: true
  title: Neural Network Acceptability Judgments
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: the-sixth-pascal-recognizing-textual-entailment-challenge
  numCitedBy: 439
  pid: db8885a0037fe47d973ade79d696586453710233
  show_ref_link: false
  title: The Sixth PASCAL Recognizing Textual Entailment Challenge
  year: 2009
- fieldsOfStudy:
  - Computer Science
  meta_key: discourse-based-objectives-for-fast-unsupervised-sentence-representation-learning
  numCitedBy: 94
  pid: a97dc52807d80454e78d255f9fbd7b0fab56bd03
  show_ref_link: false
  title: Discourse-Based Objectives for Fast Unsupervised Sentence Representation
    Learning
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: dual-co-matching-network-for-multi-choice-reading-comprehension
  numCitedBy: 77
  pid: 8c473a8adca5635c3cde5af793ed7b68afec9d77
  show_ref_link: false
  title: Dual Co-Matching Network for Multi-choice Reading Comprehension
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: the-seventh-pascal-recognizing-textual-entailment-challenge
  numCitedBy: 390
  pid: 0f8468de03ee9f12d693237bec87916311bf1c24
  show_ref_link: false
  title: The Seventh PASCAL Recognizing Textual Entailment Challenge
  year: 2011
- fieldsOfStudy:
  - Computer Science
  meta_key: the-second-pascal-recognising-textual-entailment-challenge
  numCitedBy: 408
  pid: 136326377c122560768db674e35f5bcd6de3bc40
  show_ref_link: false
  title: The Second PASCAL Recognising Textual Entailment Challenge
  year: 2006
- fieldsOfStudy:
  - Computer Science
  meta_key: automatically-constructing-a-corpus-of-sentential-paraphrases
  numCitedBy: 834
  pid: 475354f10798f110d34792b6d88f31d6d5cb099e
  show_ref_link: false
  title: Automatically Constructing a Corpus of Sentential Paraphrases
  year: 2005
- fieldsOfStudy:
  - Computer Science
  meta_key: the-pascal-recognising-textual-entailment-challenge
  numCitedBy: 1762
  pid: de794d50713ea5f91a7c9da3d72041e2f5ef8452
  show_ref_link: false
  title: The PASCAL Recognising Textual Entailment Challenge
  year: 2005
- fieldsOfStudy:
  - Linguistics
  meta_key: the-winograd-schema-challenge
  numCitedBy: 691
  pid: 128cb6b891aee1b5df099acb48e2efecfcff689f
  show_ref_link: false
  title: The Winograd Schema Challenge
  year: 2011
- fieldsOfStudy:
  - Computer Science
  meta_key: gaussian-error-linear-units-gelus
  numCitedBy: 971
  pid: 15f4c35889ccc1ae258b680c2ca2fcbfe1e260f7
  show_ref_link: true
  title: Gaussian Error Linear Units (GELUs)
  year: 2016
- fieldsOfStudy:
  - Sociology
  meta_key: centering-a-framework-for-modeling-the-local-coherence-of-discourse
  numCitedBy: 2138
  pid: 2c538ee7c17c71b2aaa53dcf3c9972c00e6e097b
  show_ref_link: false
  title: Centering - A Framework for Modeling the Local Coherence of Discourse
  year: 1995
- fieldsOfStudy:
  - Philosophy
  meta_key: coherence-and-coreference
  numCitedBy: 780
  pid: f2e1d62340d111dacb3b1038eb0a8676df045566
  show_ref_link: false
  title: Coherence and Coreference
  year: 1979
- fieldsOfStudy:
  - Geology
  meta_key: association-for-computational-linguistics
  numCitedBy: 2172
  pid: 566eb7be43b8a2b2daff82b03711098a84859b2a
  show_ref_link: false
  title: Association for Computational Linguistics
  year: 2001
- fieldsOfStudy:
  - Philosophy
  meta_key: the-third-pascal-recognizing-textual-entailment-challenge
  numCitedBy: 474
  pid: b2815bc4c9e4260227cd7ca0c9d68d41c4c2f58b
  show_ref_link: false
  title: The Third PASCAL Recognizing Textual Entailment Challenge
  year: 2007
slug: ALBERT:-A-Lite-BERT-for-Self-supervised-Learning-of-Lan-Chen
title: ALBERT - A Lite BERT for Self-supervised Learning of Language Representations
url: https://www.semanticscholar.org/paper/ALBERT:-A-Lite-BERT-for-Self-supervised-Learning-of-Lan-Chen/7a064df1aeada7e69e5173f7d4c8606f4470365b?sort=total-citations
venue: ICLR
year: 2020
