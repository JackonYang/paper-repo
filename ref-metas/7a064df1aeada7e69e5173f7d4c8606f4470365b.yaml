authors:
- Zhenzhong Lan
- Mingda Chen
- Sebastian Goodman
- Kevin Gimpel
- Piyush Sharma
- Radu Soricut
badges:
- id: OPEN_ACCESS
corpusId: 202888986
fieldsOfStudy:
- Computer Science
numCitedBy: 2706
numCiting: 83
paperAbstract: Increasing model size when pretraining natural language representations
  often results in improved performance on downstream tasks. However, at some point
  further model increases become harder due to GPU/TPU memory limitations and longer
  training times. To address these problems, we present two parameter-reduction techniques
  to lower memory consumption and increase the training speed of BERT. Comprehensive
  empirical evidence shows that our proposed methods lead to models that scale much
  better compared to the original BERT. We also use a self-supervised loss that focuses
  on modeling inter-sentence coherence, and show it consistently helps downstream
  tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art
  results on the GLUE, RACE, and \squad benchmarks while having fewer parameters compared
  to BERT-large. The code and the pretrained models are available at this https URL.
ref_count: 83
references:
- pid: df2b0e26d0599ce3e70df8a9da02e51594e0e992
  title: 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'
- pid: 077f8329a7b6fa3b7c877a57b81eb6c18b5f87de
  title: 'RoBERTa: A Robustly Optimized BERT Pretraining Approach'
- pid: d56c1fc337fb07ec004dc846f80582c327af717c
  title: 'StructBERT: Incorporating Language Structures into Pre-training for Deep
    Language Understanding'
- pid: e0c6abdbdecf04ffac65c440da77fb9d66bb474c
  title: 'XLNet: Generalized Autoregressive Pretraining for Language Understanding'
- pid: 9405cc0d6169988371b2755e573cc28650d14dfe
  title: Language Models are Unsupervised Multitask Learners
- pid: ef6948edae12eba6f1d486b8600108b9762f36ab
  title: BAM! Born-Again Multi-Task Networks for Natural Language Understanding
- pid: 3cfb319689f06bf04c2e28399361f414ca32c4b3
  title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer
- pid: cd18800a0fe0b668a1cc19f2ec95b5003d0a5035
  title: Improving Language Understanding by Generative Pre-Training
- pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  title: Attention is All you Need
- pid: c4744a7c2bb298e4a52289a1e085c71cc3d37bc6
  title: 'Transformer-XL: Attentive Language Models beyond a Fixed-Length Context'
- pid: 81f5810fbbab9b7203b9556f4ce3c741875407bc
  title: 'SpanBERT: Improving Pre-training by Representing and Predicting Spans'
- pid: 93b8da28d006415866bf48f9a6e06b5242129195
  title: 'GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language
    Understanding'
- pid: d7b6753a2d4a2b286c396854063bde3a91b75535
  title: A Simple Method for Commonsense Reasoning
- pid: 6e795c6e9916174ae12349f5dc3f516570c17ce8
  title: Skip-Thought Vectors
- pid: 4aa9f5150b46320f534de4747a2dd0cd7f3fe292
  title: Semi-supervised Sequence Learning
- pid: 1e077413b25c4d34945cc2707e17e46ed4fe784a
  title: Universal Language Model Fine-tuning for Text Classification
- pid: 687bac2d3320083eb4530bf18bb8f8f721477600
  title: Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank
- pid: 26e743d5bd465f49b9538deaf116c15e61b7951f
  title: Learning Distributed Representations of Sentences from Unlabelled Data
- pid: 3febb2bed8865945e7fddc99efd791887bb7e14f
  title: Deep Contextualized Word Representations
- pid: 05dd7254b632376973f3a1b4d39485da17814df5
  title: 'SQuAD: 100,000+ Questions for Machine Comprehension of Text'
- pid: b5c26ab8767d046cb6e32d959fdf726aee89bb62
  title: Inception-v4, Inception-ResNet and the Impact of Residual Connections on
    Learning
- pid: f37e1b62a767a307c046404ca96bc140b3e68cb5
  title: 'GloVe: Global Vectors for Word Representation'
- pid: bc8fa64625d9189f5801837e7b133e7fe3c581f7
  title: 'Learned in Translation: Contextualized Word Vectors'
- pid: 5ded2b8c64491b4a67f6d39ce473d4b9347a672e
  title: A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference
- pid: f527bcfb09f32e6a4a8afc0b37504941c1ba2cee
  title: Distributed Representations of Sentences and Documents
- pid: 87f40e6f3022adbc1f1905e3e506abad05a9964f
  title: Distributed Representations of Words and Phrases and their Compositionality
- pid: 2270b8628fd8ca67ae39d277f45bc3c38ac63d5f
  title: 'Mesh-TensorFlow: Deep Learning for Supercomputers'
- pid: a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096
  title: 'SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual
    Focused Evaluation'
- pid: 636a79420d838eabe4af7fb25d6437de45ab64e8
  title: 'RACE: Large-scale ReAding Comprehension Dataset From Examinations'
- pid: 4d1c856275744c0284312a3a50efb6ca9dc4cd4c
  title: "Know What You Don\u2019t Know: Unanswerable Questions for SQuAD"
- pid: 0e6824e137847be0599bb0032e37042ed2ef5045
  title: 'Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching
    Movies and Reading Books'
- pid: cb0f3ee1e98faf92429d601cdcd76c69c1e484eb
  title: Neural Network Acceptability Judgments
- pid: db8885a0037fe47d973ade79d696586453710233
  title: The Sixth PASCAL Recognizing Textual Entailment Challenge
- pid: 0f8468de03ee9f12d693237bec87916311bf1c24
  title: The Seventh PASCAL Recognizing Textual Entailment Challenge
- pid: 136326377c122560768db674e35f5bcd6de3bc40
  title: The Second PASCAL Recognising Textual Entailment Challenge
- pid: 475354f10798f110d34792b6d88f31d6d5cb099e
  title: Automatically Constructing a Corpus of Sentential Paraphrases
- pid: de794d50713ea5f91a7c9da3d72041e2f5ef8452
  title: The PASCAL Recognising Textual Entailment Challenge
- pid: 128cb6b891aee1b5df099acb48e2efecfcff689f
  title: The Winograd Schema Challenge
- pid: 15f4c35889ccc1ae258b680c2ca2fcbfe1e260f7
  title: Gaussian Error Linear Units (GELUs)
- pid: 566eb7be43b8a2b2daff82b03711098a84859b2a
  title: Association for Computational Linguistics
- pid: b2815bc4c9e4260227cd7ca0c9d68d41c4c2f58b
  title: The Third PASCAL Recognizing Textual Entailment Challenge
slug: ALBERT:-A-Lite-BERT-for-Self-supervised-Learning-of-Lan-Chen
title: 'ALBERT: A Lite BERT for Self-supervised Learning of Language Representations'
url: https://www.semanticscholar.org/paper/ALBERT:-A-Lite-BERT-for-Self-supervised-Learning-of-Lan-Chen/7a064df1aeada7e69e5173f7d4c8606f4470365b?sort=total-citations
venue: ICLR
year: 2020
