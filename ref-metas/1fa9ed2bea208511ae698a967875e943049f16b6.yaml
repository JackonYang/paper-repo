authors:
- Thomas Wolf
- Lysandre Debut
- Victor Sanh
- Julien Chaumond
- Clement Delangue
- Anthony Moi
- Pierric Cistac
- T. Rault
- "R\xE9mi Louf"
- Morgan Funtowicz
- Jamie Brew
badges:
- id: OPEN_ACCESS
corpusId: 204509627
fieldsOfStudy:
- Computer Science
numCitedBy: 2727
numCiting: 80
paperAbstract: Recent progress in natural language processing has been driven by advances
  in both model architecture and model pretraining. Transformer architectures have
  facilitated building higher-capacity models and pretraining has made it possible
  to effectively utilize this capacity for a wide variety of tasks. \textit{Transformers}
  is an open-source library with the goal of opening up these advances to the wider
  machine learning community. The library consists of carefully engineered state-of-the
  art Transformer architectures under a unified API. Backing this library is a curated
  collection of pretrained models made by and available for the community. \textit{Transformers}
  is designed to be extensible by researchers, simple for practitioners, and fast
  and robust in industrial deployments. The library is available at \url{this https
  URL}.
ref_count: 80
references:
- pid: df2b0e26d0599ce3e70df8a9da02e51594e0e992
  title: 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'
- pid: 93b4cc549a1bc4bc112189da36c318193d05d806
  title: 'AllenNLP: A Deep Semantic Natural Language Processing Platform'
- pid: 3cfb319689f06bf04c2e28399361f414ca32c4b3
  title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer
- pid: a54b56af24bb4873ed0163b77df63b92bd018ddc
  title: 'DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter'
- pid: 7a064df1aeada7e69e5173f7d4c8606f4470365b
  title: 'ALBERT: A Lite BERT for Self-supervised Learning of Language Representations'
- pid: d9f6ada77448664b71128bb19df15765336974a6
  title: 'SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding
    Systems'
- pid: c4744a7c2bb298e4a52289a1e085c71cc3d37bc6
  title: 'Transformer-XL: Attentive Language Models beyond a Fixed-Length Context'
- pid: 327d7e55d64cb34d55bd3a3fe58233c238a312cd
  title: 'exBERT: A Visual Analysis Tool to Explore Learned Representations in Transformer
    Models'
- pid: 93b8da28d006415866bf48f9a6e06b5242129195
  title: 'GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language
    Understanding'
- pid: e0c6abdbdecf04ffac65c440da77fb9d66bb474c
  title: 'XLNet: Generalized Autoregressive Pretraining for Language Understanding'
- pid: 9405cc0d6169988371b2755e573cc28650d14dfe
  title: Language Models are Unsupervised Multitask Learners
- pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  title: Attention is All you Need
- pid: 395de0bd3837fdf4b4b5e5f04835bcc69c279481
  title: 'BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation,
    Translation, and Comprehension'
- pid: cd18800a0fe0b668a1cc19f2ec95b5003d0a5035
  title: Improving Language Understanding by Generative Pre-Training
- pid: 5e98fe2163640da8ab9695b9ee9c433bb30f5353
  title: 'SciBERT: A Pretrained Language Model for Scientific Text'
- pid: 1e077413b25c4d34945cc2707e17e46ed4fe784a
  title: Universal Language Model Fine-tuning for Text Classification
- pid: ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc
  title: Cross-lingual Language Model Pretraining
- pid: 077f8329a7b6fa3b7c877a57b81eb6c18b5f87de
  title: 'RoBERTa: A Robustly Optimized BERT Pretraining Approach'
- pid: 687bac2d3320083eb4530bf18bb8f8f721477600
  title: Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank
- pid: 3febb2bed8865945e7fddc99efd791887bb7e14f
  title: Deep Contextualized Word Representations
- pid: 81f5810fbbab9b7203b9556f4ce3c741875407bc
  title: 'SpanBERT: Improving Pre-training by Representing and Predicting Spans'
- pid: 421fc2556836a6b441de806d7b393a35b6eaea58
  title: Contextual String Embeddings for Sequence Labeling
- pid: bc8fa64625d9189f5801837e7b133e7fe3c581f7
  title: 'Learned in Translation: Contextualized Word Vectors'
- pid: 5ded2b8c64491b4a67f6d39ce473d4b9347a672e
  title: A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference
- pid: b36a5bb1707bb9c70025294b3a310138aae8327a
  title: Automatic differentiation in PyTorch
- pid: 05dd7254b632376973f3a1b4d39485da17814df5
  title: 'SQuAD: 100,000+ Questions for Machine Comprehension of Text'
- pid: 2f5102ec3f70d0dea98c957cc2cab4d15d83a2da
  title: The Stanford CoreNLP Natural Language Processing Toolkit
- pid: aab5002a22b9b4244a8329b140bd0a86021aa2d1
  title: 'OpenNMT: Open-Source Toolkit for Neural Machine Translation'
- pid: db8885a0037fe47d973ade79d696586453710233
  title: The Sixth PASCAL Recognizing Textual Entailment Challenge
- pid: 0f8468de03ee9f12d693237bec87916311bf1c24
  title: The Seventh PASCAL Recognizing Textual Entailment Challenge
- pid: 136326377c122560768db674e35f5bcd6de3bc40
  title: The Second PASCAL Recognising Textual Entailment Challenge
- pid: 475354f10798f110d34792b6d88f31d6d5cb099e
  title: Automatically Constructing a Corpus of Sentential Paraphrases
- pid: cb0f3ee1e98faf92429d601cdcd76c69c1e484eb
  title: Neural Network Acceptability Judgments
- pid: 636a79420d838eabe4af7fb25d6437de45ab64e8
  title: 'RACE: Large-scale ReAding Comprehension Dataset From Examinations'
- pid: a6cb366736791bcccc5c8639de5a8f9636bf87e8
  title: 'Adam: A Method for Stochastic Optimization'
- pid: 4d1c856275744c0284312a3a50efb6ca9dc4cd4c
  title: "Know What You Don\u2019t Know: Unanswerable Questions for SQuAD"
- pid: 01a660ec8aa995a88a00bfb41cb86c022047a9db
  title: 'NLTK: The Natural Language Toolkit'
- pid: d07284a6811f1b2745d91bdb06b040b57f226882
  title: Decoupled Weight Decay Regularization
- pid: 128cb6b891aee1b5df099acb48e2efecfcff689f
  title: The Winograd Schema Challenge
- pid: b2815bc4c9e4260227cd7ca0c9d68d41c4c2f58b
  title: The Third PASCAL Recognizing Textual Entailment Challenge
- pid: e03d300581e16f6664157d2c1c6ceec33ec528ce
  title: Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object
    Classification, and Recognising Tectual Entailment
slug: HuggingFace's-Transformers:-State-of-the-art-Wolf-Debut
title: 'HuggingFace''s Transformers: State-of-the-art Natural Language Processing'
url: https://www.semanticscholar.org/paper/HuggingFace's-Transformers:-State-of-the-art-Wolf-Debut/1fa9ed2bea208511ae698a967875e943049f16b6?sort=total-citations
venue: ArXiv
year: 2019
