authors:
- Jonathan Baxter
badges:
- id: UNPAYWALL
- id: OPEN_ACCESS
corpusId: 6211302
fieldsOfStudy:
- Computer Science
numCitedBy: 343
numCiting: 20
paperAbstract: "Probably the most important problem in machine learning is the preliminary\
  \ biasing of a learner's hypothesis space so that it is small enough to ensure good\
  \ generalisation from reasonable training sets, yet large enough that it contains\
  \ a good solution to the problem being learnt. In this paper a mechanism for {\\\
  em automatically} learning or biasing the learner's hypothesis space is introduced.\
  \ It works by first learning an appropriate {\\em internal representation} for a\
  \ learning environment and then using that representation to bias the learner's\
  \ hypothesis space for the learning of future tasks drawn from the same environment.\
  \ \nAn internal representation must be learnt by sampling from {\\em many similar\
  \ tasks}, not just a single task as occurs in ordinary machine learning. It is proved\
  \ that the number of examples $m$ {\\em per task} required to ensure good generalisation\
  \ from a representation learner obeys $m = O(a+b/n)$ where $n$ is the number of\
  \ tasks being learnt and $a$ and $b$ are constants. If the tasks are learnt independently\
  \ ({\\em i.e.} without a common representation) then $m=O(a+b)$. It is argued that\
  \ for learning environments such as speech and character recognition $b\\gg a$ and\
  \ hence representation learning in these environments can potentially yield a drastic\
  \ reduction in the number of examples required per task. It is also proved that\
  \ if $n = O(b)$ (with $m=O(a+b/n)$) then the representation learnt will be good\
  \ for learning novel tasks from the same environment, and that the number of examples\
  \ required to generalise well on a novel task will be reduced to $O(a)$ (as opposed\
  \ to $O(a+b)$ if no representation is used). \nIt is shown that gradient descent\
  \ can be used to train neural network representations and experiment results are\
  \ reported providing strong qualitative support for the theoretical results."
ref_count: 20
references:
- pid: fedfc9fbcfe46d50b81078560bce724678f90176
  title: Decision Theoretic Generalizations of the PAC Model for Neural Net and Other
    Learning Applications
- pid: 25406e6733a698bfc4ac836f8e74f458e75dad4f
  title: What Size Net Gives Valid Generalization?
- pid: 10ddb646feddc12337b5a755c72e153e37088c02
  title: A theory of the learnable
- pid: 052b1d8ce63b07fec3de9dbb583772d860b7c769
  title: Learning representations by back-propagating errors
- pid: 5e6dfb46ed298ff037e166291c128a465f90bfc0
  title: Some special vapnik-chervonenkis classes
- pid: a34e35dbbc6911fa7b94894dffdc0076a261b6f0
  title: Neural Networks and the Bias/Variance Dilemma
- pid: 10fd7180b2c0f14e5575b4892e74932b983af822
  title: Central Limit Theorems for Empirical Measures
- pid: 01a1d065a5292be740e75029622a3ab5e71e3150
  title: Convergence of stochastic processes
- pid: 7dbdb4209626fd92d2436a058663206216036e68
  title: Elements of Information Theory
- pid: a36b028d024bf358c4af1a5e1dc3ca0aed23b553
  title: 'Chervonenkis: On the uniform convergence of relative frequencies of events
    to their probabilities'
slug: Learning-internal-representations-Baxter
title: Learning internal representations
url: https://www.semanticscholar.org/paper/Learning-internal-representations-Baxter/a24508e65e599b5b20c33af96dbe7017d5caca37?sort=total-citations
venue: COLT '95
year: 1995
