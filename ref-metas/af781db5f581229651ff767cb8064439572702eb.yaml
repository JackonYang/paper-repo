authors:
- E. Grossmann
badges: []
corpusId: 31678135
fieldsOfStudy:
- Computer Science
numCitedBy: 37
numCiting: 0
paperAbstract: We present a boosting method that results in a decision tree rather
  than a fixed linear sequence of classifiers. An equally correct statement is that
  we present a tree-growing method whose performance can be analysed in the framework
  of Adaboost. We argue that Adaboost can be improved by presenting the input to a
  sequence of weak classifiers, each one tuned to the conditional probability determined
  by the output of previous weak classifiers. As a result, the final classifier has
  a tree structure, rather than being linear, thus the name "Adatree". One of the
  consequences of the tree structure is that different input data may have different
  processing time. Early experimentation shows a reduced computation cost with respect
  to Adaboost. One of our intended applications is real-time detection, where cascades
  of boosted detectors have recently become successful. The reduced computation cost
  of the proposed method shows some potential for being used directly in detection
  problems, without need of a cascade.
ref_count: 0
references: []
slug: AdaTree:-Boosting-a-Weak-Classifier-into-a-Decision-Grossmann
title: 'AdaTree: Boosting a Weak Classifier into a Decision Tree'
url: https://www.semanticscholar.org/paper/AdaTree:-Boosting-a-Weak-Classifier-into-a-Decision-Grossmann/af781db5f581229651ff767cb8064439572702eb?sort=total-citations
venue: 2004 Conference on Computer Vision and Pattern Recognition Workshop
year: 2004
