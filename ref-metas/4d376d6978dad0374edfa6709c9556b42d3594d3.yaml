authors:
- S. Ioffe
- Christian Szegedy
badges:
- id: OPEN_ACCESS
corpusId: 5808102
fieldsOfStudy:
- Computer Science
numCitedBy: 29233
numCiting: 33
paperAbstract: 'Training Deep Neural Networks is complicated by the fact that the
  distribution of each layer''s inputs changes during training, as the parameters
  of the previous layers change. This slows down the training by requiring lower learning
  rates and careful parameter initialization, and makes it notoriously hard to train
  models with saturating nonlinearities. We refer to this phenomenon as internal covariate
  shift, and address the problem by normalizing layer inputs. Our method draws its
  strength from making normalization a part of the model architecture and performing
  the normalization for each training mini-batch. Batch Normalization allows us to
  use much higher learning rates and be less careful about initialization, and in
  some cases eliminates the need for Dropout. Applied to a state-of-the-art image
  classification model, Batch Normalization achieves the same accuracy with 14 times
  fewer training steps, and beats the original model by a significant margin. Using
  an ensemble of batch-normalized networks, we improve upon the best published result
  on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy
  of human raters.'
ref_count: 33
references:
- pid: b8ef1230a5cc9ea7cd8358f1ae7d1af97813ba14
  title: Deep Learning Made Easier by Linear Transformations in Perceptrons
- pid: aa7bfd2304201afbb19971ebde87b17e40242e91
  title: On the importance of initialization and momentum in deep learning
- pid: 34f25a8704614163c4095b3ee2fc969b60de4698
  title: 'Dropout: a simple way to prevent neural networks from overfitting'
- pid: b71ac1e9fb49420d13e084ac67254a0bbd40f83f
  title: Understanding the difficulty of training deep feedforward neural networks
- pid: d6f2f611da110b5b5061731be3fc4c7f45d8ee23
  title: 'Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet
    Classification'
- pid: 3127190433230b3dc1abd0680bb58dced4bcd90e
  title: Large Scale Distributed Deep Networks
- pid: 99c970348b8f70ce23d6641e201904ea49266b6e
  title: Exact solutions to the nonlinear dynamics of learning in deep linear neural
    networks
- pid: e15cf50aa89fee8535703b9f9512fca5bfc43327
  title: Going deeper with convolutions
- pid: f42b865e20e61a954239f421b42007236e671f19
  title: GradientBased Learning Applied to Document Recognition
- pid: 413c1142de9d91804d6d11c67ff3fed59c9fc279
  title: Adaptive Subgradient Methods for Online Learning and Stochastic Optimization
- pid: 162d958ff885f1462aeda91cd72582323fd6a1f4
  title: Gradient-based learning applied to document recognition
- pid: 84069287da0a6b488b8c933f3cb5be759cb6237e
  title: On the difficulty of training recurrent neural networks
- pid: a538b05ebb01a40323997629e171c91aa28b8e2f
  title: Rectified Linear Units Improve Restricted Boltzmann Machines
- pid: f5ce3abf942cdd685fb0f290f3e741f7b4749f0a
  title: 'Deep Image: Scaling up Image Recognition'
- pid: 577d19a115f9ef6f002483fcf88adbb3b5479556
  title: 'Independent component analysis: algorithms and applications'
- pid: e74f9b7f8eec6ba4704c206b93bc8079af3da4bd
  title: ImageNet Large Scale Visual Recognition Challenge
- pid: b87274e6d9aa4e6ba5148898aa92941617d2b6ed
  title: Efficient BackProp
slug: Batch-Normalization:-Accelerating-Deep-Network-by-Ioffe-Szegedy
title: 'Batch Normalization: Accelerating Deep Network Training by Reducing Internal
  Covariate Shift'
url: https://www.semanticscholar.org/paper/Batch-Normalization:-Accelerating-Deep-Network-by-Ioffe-Szegedy/4d376d6978dad0374edfa6709c9556b42d3594d3?sort=total-citations
venue: ICML
year: 2015
