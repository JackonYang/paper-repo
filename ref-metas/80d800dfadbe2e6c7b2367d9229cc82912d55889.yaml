authors:
- A. Krizhevsky
badges:
- id: OPEN_ACCESS
corpusId: 5556470
fieldsOfStudy:
- Computer Science
numCitedBy: 883
numCiting: 7
paperAbstract: I present a new way to parallelize the training of convolutional neural
  networks across multiple GPUs. The method scales significantly better than all alternatives
  when applied to modern convolutional neural networks.
ref_count: 8
references:
- pid: d1208ac421cf8ff67b27d93cd19ae42b8d596f95
  title: Deep learning with COTS HPC systems
- pid: abd1c342495432171beb7ca8fd9551ef13cbd0ff
  title: ImageNet classification with deep convolutional neural networks
- pid: 3127190433230b3dc1abd0680bb58dced4bcd90e
  title: Large Scale Distributed Deep Networks
- pid: 36f49b05d764bf5c10428b082c2d96c13c4203b9
  title: 'Hogwild: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent'
- pid: d2c733e34d48784a37d717fe43d9e93277a8c53e
  title: 'ImageNet: A large-scale hierarchical image database'
slug: One-weird-trick-for-parallelizing-convolutional-Krizhevsky
title: One weird trick for parallelizing convolutional neural networks
url: https://www.semanticscholar.org/paper/One-weird-trick-for-parallelizing-convolutional-Krizhevsky/80d800dfadbe2e6c7b2367d9229cc82912d55889?sort=total-citations
venue: ArXiv
year: 2014
