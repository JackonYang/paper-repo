authors:
- D. Ruppert
badges: []
corpusId: 118901444
fieldsOfStudy:
- Computer Science
meta_key: the-elements-of-statistical-learning-data-mining-inference-and-prediction
numCitedBy: 11811
numCiting: 2
paperAbstract: "In the words of the authors, the goal of this book was to \u201Cbring\
  \ together many of the important new ideas in learning, and explain them in a statistical\
  \ framework.\u201D The authors have been quite successful in achieving this objective,\
  \ and their work is a welcome addition to the statistics and learning literatures.\
  \ Statistics has always been interdisciplinary, borrowing ideas from diverse \x8E\
  \ elds and repaying the debt with contributions, both theoretical and practical,\
  \ to the other intellectual disciplines. For statistical learning, this cross-fertilization\
  \ is especially noticeable. This book is a valuable resource, both for the statistician\
  \ needing an introduction to machine learning and related \x8E elds and for the\
  \ computer scientist wishing to learn more about statistics. Statisticians will\
  \ especially appreciate that it is written in their own language. The level of the\
  \ book is roughly that of a second-year doctoral student in statistics, and it will\
  \ be useful as a textbook for such students. In a stimulating article, Breiman (2001)\
  \ argued that statistics has been focused too much on a \u201Cdata modeling culture,\u201D\
  \ where the model is paramount. Breiman argued instead for an \u201Calgorithmic\
  \ modeling culture,\u201D with emphasis on black-box types of prediction. Breiman\u2019\
  s article is controversial, and in his discussion, Efron objects that \u201Cprediction\
  \ is certainly an interesting subject, but Leo\u2019s paper overstates both its\
  \ role and our profession\u2019s lack of interest in it.\u201D Although I mostly\
  \ agree with Efron, I worry that the courses offered by most statistics departments\
  \ include little, if any, treatment of statistical learning and prediction. (Stanford,\
  \ where Efron and the authors of this book teach, is an exception.) Graduate students\
  \ in statistics certainly need to know more than they do now about prediction, machine\
  \ learning, statistical learning, and data mining (not disjoint subjects). I hope\
  \ that graduate courses covering the topics of this book will become more common\
  \ in statistics curricula. Most of the book is focused on supervised learning, where\
  \ one has inputs and outputs from some system and wishes to predict unknown outputs\
  \ corresponding to known inputs. The methods discussed for supervised learning include\
  \ linear and logistic regression; basis expansion, such as splines and wavelets;\
  \ kernel techniques, such as local regression, local likelihood, and radial basis\
  \ functions; neural networks; additive models; decision trees based on recursive\
  \ partitioning, such as CART; and support vector machines. There is a \x8E nal chapter\
  \ on unsupervised learning, including association rules, cluster analysis, self-organizing\
  \ maps, principal components and curves, and independent component analysis. Many\
  \ statisticians will be unfamiliar with at least some of these algorithms. Association\
  \ rules are popular for mining commercial data in what is called \u201Cmarket basket\
  \ analysis.\u201D The aim is to discover types of products often purchased together.\
  \ Such knowledge can be used to develop marketing strategies, such as store or catalog\
  \ layouts. Self-organizing maps (SOMs) involve essentially constrained k-means clustering,\
  \ where prototypes are mapped to a two-dimensional curved coordinate system. Independent\
  \ components analysis is similar to principal components analysis and factor analysis,\
  \ but it uses higher-order moments to achieve independence, not merely zero correlation\
  \ between components. A strength of the book is the attempt to organize a plethora\
  \ of methods into a coherent whole. The relationships among the methods are emphasized.\
  \ I know of no other book that covers so much ground. Of course, with such broad\
  \ coverage, it is not possible to cover any single topic in great depth, so this\
  \ book will encourage further reading. Fortunately, each chapter includes bibliographic\
  \ notes surveying the recent literature. These notes and the extensive references\
  \ provide a good introduction to the learning literature, including much outside\
  \ of statistics. The book might be more suitable as a textbook if less material\
  \ were covered in greater depth; however, such a change would compromise the book\u2019\
  s usefulness as a reference, and so I am happier with the book as it was written."
ref_count: 2
references:
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1623
  pid: e5df6bc6da5653ad98e754b08f63326c2e52b372
  show_ref_link: false
  title: 'Statistical modeling: The two cultures'
  year: 2001
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 2466
  pid: cde1153a69ddd7c88f3a736c96bd64a25afb1ba3
  show_ref_link: false
  title: 'Statistical Modeling: The Two Cultures (with comments and a rejoinder by
    the author)'
  year: 2001
slug: The-Elements-of-Statistical-Learning:-Data-Mining,-Ruppert
title: 'The Elements of Statistical Learning: Data Mining, Inference, and Prediction'
url: https://www.semanticscholar.org/paper/The-Elements-of-Statistical-Learning:-Data-Mining,-Ruppert/5176a2f31dace77db9135dde7020d2c37f78cca0?sort=total-citations
venue: ''
year: 2004
