authors:
- Quoc V. Le
- Jiquan Ngiam
- Adam Coates
- A. Lahiri
- B. Prochnow
- A. Ng
badges:
- id: OPEN_ACCESS
corpusId: 6076653
fieldsOfStudy:
- Computer Science
numCitedBy: 884
numCiting: 49
paperAbstract: The predominant methodology in training deep learning advocates the
  use of stochastic gradient descent methods (SGDs). Despite its ease of implementation,
  SGDs are difficult to tune and parallelize. These problems make it challenging to
  develop, debug and scale up deep learning algorithms with SGDs. In this paper, we
  show that more sophisticated off-the-shelf optimization methods such as Limited
  memory BFGS (L-BFGS) and Conjugate gradient (CG) with line search can significantly
  simplify and speed up the process of pretraining deep algorithms. In our experiments,
  the difference between L-BFGS/CG and SGDs are more pronounced if we consider algorithmic
  extensions (e.g., sparsity regularization) and hardware extensions (e.g., GPUs or
  computer clusters). Our experiments with distributed optimization support the use
  of L-BFGS with locally connected networks and convolutional neural networks. Using
  L-BFGS, our convolutional network model achieves 0.69% on the standard MNIST dataset.
  This is a state-of-the-art result on MNIST among algorithms that do not use distortions
  or pretraining.
ref_count: 49
references:
- pid: 355d44f53428b1ac4fb2ab468d593c720640e5bd
  title: Greedy Layer-Wise Training of Deep Networks
- pid: be9a17321537d9289875fe475b71f4821457b435
  title: An Analysis of Single-Layer Networks in Unsupervised Feature Learning
- pid: 4c46347fbc272b21468efe3d9af34b4b2bad6684
  title: Deep learning via Hessian-free optimization
- pid: e337c5e4c23999c36f64bcb33ebe6b284e1bcbf1
  title: Large-scale deep unsupervised learning using graphics processors
- pid: e60ff004dde5c13ec53087872cfcdd12e85beb57
  title: Learning Deep Architectures for AI
- pid: 8978cf7574ceb35f4c3096be768c7547b28a35d0
  title: A Fast Learning Algorithm for Deep Belief Nets
- pid: 3137bc367c61c0e507a5e3c1f8caeb26f292d79f
  title: Measuring Invariances in Deep Networks
- pid: 5a2668bf420d8509a4dfa28e1cdcdac14c649975
  title: 3D Object Recognition with Deep Belief Nets
- pid: 843959ffdccf31c6694d135fad07425924f785b1
  title: Extracting and composing robust features with denoising autoencoders
- pid: 05cc38e249a6f642363b5a5cbd71cda67cea5893
  title: Tiled convolutional neural networks
- pid: 1e80f755bcbf10479afd2338cec05211fdbd325c
  title: Convolutional deep belief networks for scalable unsupervised learning of
    hierarchical representations
- pid: a53da9916b87fa295837617c16ef2ca6462cafb8
  title: Classification using discriminative restricted Boltzmann machines
- pid: 9691f67f5075bde2fd70da0135a4a70f25ef042b
  title: 'Pegasos: primal estimated sub-gradient solver for SVM'
- pid: bf38dfb13352449b965c08282b66d3ffc5a0539f
  title: Unsupervised feature learning for audio classification using convolutional
    deep belief networks
- pid: 162d958ff885f1462aeda91cd72582323fd6a1f4
  title: Gradient-based learning applied to document recognition
- pid: b44ff78214ccd975ce16fbbc333423ca78d99141
  title: 'SGD-QN: Careful Quasi-Newton Stochastic Gradient Descent'
- pid: 202cbbf671743aefd380d2f23987bd46b9caaf97
  title: Sparse deep belief net model for visual area V2
- pid: 950e8d556e30d2a873172bfa90f4f36da5286c07
  title: Accelerated training of conditional random fields with stochastic gradient
    methods
- pid: 5562a56da3a96dae82add7de705e2bd841eb00fc
  title: Best practices for convolutional neural networks applied to visual document
    analysis
- pid: 42269d0438c0ae4ca892334946ed779999691074
  title: Learning hierarchical invariant spatio-temporal features for action recognition
    with independent subspace analysis
- pid: 0c9633aedafe4ee8cf238fa06c40b84f47e17362
  title: Linear spatial pyramid matching using sparse coding for image classification
- pid: 46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e
  title: Reducing the Dimensionality of Data with Neural Networks
- pid: e95d3934e51107da7610acd0b1bcb6551671f9f1
  title: A Practical Guide to Training Restricted Boltzmann Machines
- pid: 4d476b96be73fccc61f2076befbf5a468caa4180
  title: Convolutional Learning of Spatio-temporal Features
- pid: b3852f0113fcf8a3913c55ae92393ae6ccde347e
  title: 'Self-taught learning: transfer learning from unlabeled data'
- pid: ccd52aff02b0f902f4ce7247c4fee7273014c41c
  title: Unsupervised Learning of Invariant Feature Hierarchies with Applications
    to Object Recognition
- pid: 8012c4a1e2ca663f1a04e80cbb19631a00cbab27
  title: Emergence of simple-cell receptive field properties by learning a sparse
    code for natural images
- pid: 1f88427d7aa8225e47f946ac41a0667d7b69ac52
  title: What is the best multi-stage architecture for object recognition?
- pid: e64a9960734215e2b1866ea3cb723ffa5585ac14
  title: Efficient sparse coding algorithms
- pid: 5936754b5762260bf102ac95d7b26cfc9d31956a
  title: The Tradeoffs of Large Scale Learning
- pid: b98cd08b75ebf2bd1d1ec47c51ef75777a7e64bd
  title: Deep, Big, Simple Neural Nets for Handwritten Digit Recognition
- pid: 4f7476037408ac3d993f5088544aab427bc319c1
  title: 'Information processing in dynamical systems: foundations of harmony theory'
- pid: 9e2964d36f154cc13b0af39c5b36cb3d76b27da1
  title: Deep Big Simple Neural Nets Excel on Handwritten Digit Recognition
- pid: b87274e6d9aa4e6ba5148898aa92941617d2b6ed
  title: Efficient BackProp
slug: On-optimization-methods-for-deep-learning-Le-Ngiam
title: On optimization methods for deep learning
url: https://www.semanticscholar.org/paper/On-optimization-methods-for-deep-learning-Le-Ngiam/053912e76e50c9f923a1fc1c173f1365776060cc?sort=total-citations
venue: ICML
year: 2011
