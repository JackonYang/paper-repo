authors:
- Yinhan Liu
- Myle Ott
- Naman Goyal
- Jingfei Du
- Mandar Joshi
- Danqi Chen
- Omer Levy
- M. Lewis
- Luke Zettlemoyer
- Veselin Stoyanov
badges:
- id: OPEN_ACCESS
corpusId: 198953378
fieldsOfStudy:
- Computer Science
meta_key: roberta-a-robustly-optimized-bert-pretraining-approach
numCitedBy: 7267
numCiting: 58
paperAbstract: Language model pretraining has led to significant performance gains
  but careful comparison between different approaches is challenging. Training is
  computationally expensive, often done on private datasets of different sizes, and,
  as we will show, hyperparameter choices have significant impact on the final results.
  We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully
  measures the impact of many key hyperparameters and training data size. We find
  that BERT was significantly undertrained, and can match or exceed the performance
  of every model published after it. Our best model achieves state-of-the-art results
  on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked
  design choices, and raise questions about the source of recently reported improvements.
  We release our models and code.
ref_count: 58
references:
- fieldsOfStudy:
  - Computer Science
  meta_key: superglue-a-stickier-benchmark-for-general-purpose-language-understanding-systems
  numCitedBy: 823
  pid: d9f6ada77448664b71128bb19df15765336974a6
  show_ref_link: true
  title: 'SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding
    Systems'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: xlnet-generalized-autoregressive-pretraining-for-language-understanding
  numCitedBy: 4226
  pid: e0c6abdbdecf04ffac65c440da77fb9d66bb474c
  show_ref_link: true
  title: 'XLNet: Generalized Autoregressive Pretraining for Language Understanding'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: cloze-driven-pretraining-of-self-attention-networks
  numCitedBy: 149
  pid: 9f1c5777a193b2c3bb2b25e248a156348e5ba56d
  show_ref_link: false
  title: Cloze-driven Pretraining of Self-attention Networks
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding
  numCitedBy: 33744
  pid: df2b0e26d0599ce3e70df8a9da02e51594e0e992
  show_ref_link: true
  title: 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: reducing-bert-pre-training-time-from-3-days-to-76-minutes
  numCitedBy: 100
  pid: 3c6dca9041f54583aeab60587c9e6e9272104dc1
  show_ref_link: false
  title: Reducing BERT Pre-Training Time from 3 Days to 76 Minutes
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: spanbert-improving-pre-training-by-representing-and-predicting-spans
  numCitedBy: 879
  pid: 81f5810fbbab9b7203b9556f4ce3c741875407bc
  show_ref_link: true
  title: 'SpanBERT: Improving Pre-training by Representing and Predicting Spans'
  year: 2020
- fieldsOfStudy:
  - Computer Science
  meta_key: automatic-differentiation-in-pytorch
  numCitedBy: 10324
  pid: b36a5bb1707bb9c70025294b3a310138aae8327a
  show_ref_link: true
  title: Automatic differentiation in PyTorch
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: adam-a-method-for-stochastic-optimization
  numCitedBy: 90054
  pid: a6cb366736791bcccc5c8639de5a8f9636bf87e8
  show_ref_link: true
  title: 'Adam: A Method for Stochastic Optimization'
  year: 2015
- fieldsOfStudy:
  - Computer Science
  meta_key: scaling-neural-machine-translation
  numCitedBy: 474
  pid: bf8fe437f779f2098f9af82b534aa51dc9edb06f
  show_ref_link: true
  title: Scaling Neural Machine Translation
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: a-surprisingly-robust-trick-for-the-winograd-schema-challenge
  numCitedBy: 80
  pid: c57298fe3faf87f9f24414821b0df7ebb7634320
  show_ref_link: false
  title: A Surprisingly Robust Trick for the Winograd Schema Challenge
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: universal-language-model-fine-tuning-for-text-classification
  numCitedBy: 2251
  pid: 1e077413b25c4d34945cc2707e17e46ed4fe784a
  show_ref_link: true
  title: Universal Language Model Fine-tuning for Text Classification
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: attention-is-all-you-need
  numCitedBy: 35150
  pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  show_ref_link: true
  title: Attention is All you Need
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: fine-tuned-language-models-for-text-classification
  numCitedBy: 221
  pid: ad76c236fe641aa52d1d6c28bf362ae9ffac91e7
  show_ref_link: false
  title: Fine-tuned Language Models for Text Classification
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: language-models-are-unsupervised-multitask-learners
  numCitedBy: 6284
  pid: 9405cc0d6169988371b2755e573cc28650d14dfe
  show_ref_link: true
  title: Language Models are Unsupervised Multitask Learners
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: a-simple-method-for-commonsense-reasoning
  numCitedBy: 238
  pid: d7b6753a2d4a2b286c396854063bde3a91b75535
  show_ref_link: true
  title: A Simple Method for Commonsense Reasoning
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: multi-task-deep-neural-networks-for-natural-language-understanding
  numCitedBy: 732
  pid: 658721bc13b0fa97366d38c05a96bf0a9f4bb0ac
  show_ref_link: true
  title: Multi-Task Deep Neural Networks for Natural Language Understanding
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: semi-supervised-sequence-learning
  numCitedBy: 881
  pid: 4aa9f5150b46320f534de4747a2dd0cd7f3fe292
  show_ref_link: false
  title: Semi-supervised Sequence Learning
  year: 2015
- fieldsOfStudy:
  - Computer Science
  meta_key: cross-lingual-language-model-pretraining
  numCitedBy: 1509
  pid: ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc
  show_ref_link: true
  title: Cross-lingual Language Model Pretraining
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: improving-multi-task-deep-neural-networks-via-knowledge-distillation-for-natural-language-understanding
  numCitedBy: 103
  pid: 7ebed46b7f3ec913e508e6468304fcaea832eda1
  show_ref_link: false
  title: Improving Multi-Task Deep Neural Networks via Knowledge Distillation for
    Natural Language Understanding
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: mixed-precision-training
  numCitedBy: 746
  pid: 2e10560579f2bdeae0143141f26bd9f0a195b4b7
  show_ref_link: false
  title: Mixed Precision Training
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: unified-language-model-pre-training-for-natural-language-understanding-and-generation
  numCitedBy: 732
  pid: 1c71771c701aadfd72c5866170a9f5d71464bb88
  show_ref_link: true
  title: Unified Language Model Pre-training for Natural Language Understanding and
    Generation
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: glue-a-multi-task-benchmark-and-analysis-platform-for-natural-language-understanding
  numCitedBy: 2634
  pid: 93b8da28d006415866bf48f9a6e06b5242129195
  show_ref_link: true
  title: 'GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language
    Understanding'
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: mass-masked-sequence-to-sequence-pre-training-for-language-generation
  numCitedBy: 599
  pid: 145b8b5d99a2beba6029418ca043585b90138d12
  show_ref_link: true
  title: 'MASS: Masked Sequence to Sequence Pre-training for Language Generation'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: squad-100-000-questions-for-machine-comprehension-of-text
  numCitedBy: 4263
  pid: 05dd7254b632376973f3a1b4d39485da17814df5
  show_ref_link: true
  title: 'SQuAD: 100,000+ Questions for Machine Comprehension of Text'
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: kermit-generative-insertion-based-modeling-for-sequences
  numCitedBy: 54
  pid: 130277ff64c7171c90d98d7e73f4bda8a0b0c1f9
  show_ref_link: false
  title: 'KERMIT: Generative Insertion-Based Modeling for Sequences'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: learned-in-translation-contextualized-word-vectors
  numCitedBy: 710
  pid: bc8fa64625d9189f5801837e7b133e7fe3c581f7
  show_ref_link: true
  title: 'Learned in Translation: Contextualized Word Vectors'
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: deep-contextualized-word-representations
  numCitedBy: 7987
  pid: 3febb2bed8865945e7fddc99efd791887bb7e14f
  show_ref_link: true
  title: Deep Contextualized Word Representations
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: fairseq-a-fast-extensible-toolkit-for-sequence-modeling
  numCitedBy: 1565
  pid: faadd7d081c8d67e8c2567e8a5579e46cd6b2280
  show_ref_link: false
  title: 'fairseq: A Fast, Extensible Toolkit for Sequence Modeling'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: know-what-you-don-t-know-unanswerable-questions-for-squad
  numCitedBy: 1398
  pid: 4d1c856275744c0284312a3a50efb6ca9dc4cd4c
  show_ref_link: true
  title: 'Know What You Don''t Know: Unanswerable Questions for SQuAD'
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: recursive-deep-models-for-semantic-compositionality-over-a-sentiment-treebank
  numCitedBy: 5366
  pid: 687bac2d3320083eb4530bf18bb8f8f721477600
  show_ref_link: true
  title: Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank
  year: 2013
- fieldsOfStudy:
  - Computer Science
  meta_key: a-large-annotated-corpus-for-learning-natural-language-inference
  numCitedBy: 2518
  pid: f04df4e20a18358ea2f689b4c129781628ef7fc1
  show_ref_link: true
  title: A large annotated corpus for learning natural language inference
  year: 2015
- fieldsOfStudy:
  - Computer Science
  meta_key: race-large-scale-reading-comprehension-dataset-from-examinations
  numCitedBy: 698
  pid: 636a79420d838eabe4af7fb25d6437de45ab64e8
  show_ref_link: true
  title: 'RACE: Large-scale ReAding Comprehension Dataset From Examinations'
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: a-broad-coverage-challenge-corpus-for-sentence-understanding-through-inference
  numCitedBy: 2036
  pid: 5ded2b8c64491b4a67f6d39ce473d4b9347a672e
  show_ref_link: true
  title: A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: the-sixth-pascal-recognizing-textual-entailment-challenge
  numCitedBy: 439
  pid: db8885a0037fe47d973ade79d696586453710233
  show_ref_link: false
  title: The Sixth PASCAL Recognizing Textual Entailment Challenge
  year: 2009
- fieldsOfStudy:
  - Computer Science
  - Linguistics
  meta_key: neural-network-acceptability-judgments
  numCitedBy: 545
  pid: cb0f3ee1e98faf92429d601cdcd76c69c1e484eb
  show_ref_link: true
  title: Neural Network Acceptability Judgments
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: ernie-enhanced-representation-through-knowledge-integration
  numCitedBy: 389
  pid: 031e4e43aaffd7a479738dcea69a2d5be7957aa3
  show_ref_link: false
  title: 'ERNIE: Enhanced Representation through Knowledge Integration'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: neural-machine-translation-of-rare-words-with-subword-units
  numCitedBy: 4793
  pid: 1af68821518f03568f913ab03fc02080247a27ff
  show_ref_link: true
  title: Neural Machine Translation of Rare Words with Subword Units
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: the-second-pascal-recognising-textual-entailment-challenge
  numCitedBy: 408
  pid: 136326377c122560768db674e35f5bcd6de3bc40
  show_ref_link: false
  title: The Second PASCAL Recognising Textual Entailment Challenge
  year: 2006
- fieldsOfStudy:
  - Computer Science
  meta_key: defending-against-neural-fake-news
  numCitedBy: 405
  pid: ad7129af0644dbcafa9aa2f111cb76526ea444a1
  show_ref_link: false
  title: Defending Against Neural Fake News
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: the-seventh-pascal-recognizing-textual-entailment-challenge
  numCitedBy: 390
  pid: 0f8468de03ee9f12d693237bec87916311bf1c24
  show_ref_link: false
  title: The Seventh PASCAL Recognizing Textual Entailment Challenge
  year: 2011
- fieldsOfStudy:
  - Computer Science
  meta_key: aligning-books-and-movies-towards-story-like-visual-explanations-by-watching-movies-and-reading-books
  numCitedBy: 1418
  pid: 0e6824e137847be0599bb0032e37042ed2ef5045
  show_ref_link: true
  title: 'Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching
    Movies and Reading Books'
  year: 2015
- fieldsOfStudy:
  - Computer Science
  meta_key: the-pascal-recognising-textual-entailment-challenge
  numCitedBy: 1762
  pid: de794d50713ea5f91a7c9da3d72041e2f5ef8452
  show_ref_link: false
  title: The PASCAL Recognising Textual Entailment Challenge
  year: 2005
- fieldsOfStudy:
  - Computer Science
  meta_key: bridging-nonlinearities-and-stochastic-regularizers-with-gaussian-error-linear-units
  numCitedBy: 289
  pid: 4361e64f2d12d63476fdc88faf72a0f70d9a2ffb
  show_ref_link: true
  title: Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear
    Units
  year: 2016
- fieldsOfStudy:
  - Linguistics
  meta_key: the-winograd-schema-challenge
  numCitedBy: 691
  pid: 128cb6b891aee1b5df099acb48e2efecfcff689f
  show_ref_link: false
  title: The Winograd Schema Challenge
  year: 2011
- fieldsOfStudy:
  - Computer Science
  meta_key: gaussian-error-linear-units-gelus
  numCitedBy: 970
  pid: 15f4c35889ccc1ae258b680c2ca2fcbfe1e260f7
  show_ref_link: true
  title: Gaussian Error Linear Units (GELUs)
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: automatically-constructing-a-corpus-of-sentential-paraphrases
  numCitedBy: 834
  pid: 475354f10798f110d34792b6d88f31d6d5cb099e
  show_ref_link: false
  title: Automatically Constructing a Corpus of Sentential Paraphrases
  year: 2005
- fieldsOfStudy:
  - Computer Science
  meta_key: news-please-a-generic-news-crawler-and-extractor
  numCitedBy: 53
  pid: 86f86f7017ca11d4d849006b2938e6f02bfe16d9
  show_ref_link: false
  title: news-please - A Generic News Crawler and Extractor
  year: 2017
- fieldsOfStudy:
  - Philosophy
  meta_key: the-third-pascal-recognizing-textual-entailment-challenge
  numCitedBy: 474
  pid: b2815bc4c9e4260227cd7ca0c9d68d41c4c2f58b
  show_ref_link: false
  title: The Third PASCAL Recognizing Textual Entailment Challenge
  year: 2007
slug: RoBERTa:-A-Robustly-Optimized-BERT-Pretraining-Liu-Ott
title: 'RoBERTa: A Robustly Optimized BERT Pretraining Approach'
url: https://www.semanticscholar.org/paper/RoBERTa:-A-Robustly-Optimized-BERT-Pretraining-Liu-Ott/077f8329a7b6fa3b7c877a57b81eb6c18b5f87de?sort=total-citations
venue: ArXiv
year: 2019
