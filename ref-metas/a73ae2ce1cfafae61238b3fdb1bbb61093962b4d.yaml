authors:
- Andrei Alexandrescu
- Katrin Kirchhoff
badges:
- id: UNPAYWALL
- id: OPEN_ACCESS
corpusId: 937826
fieldsOfStudy:
- Computer Science
numCitedBy: 98
numCiting: 11
paperAbstract: We present a new type of neural probabilistic language model that learns
  a mapping from both words and explicit word features into a continuous space that
  is then used for word prediction. Additionally, we investigate several ways of deriving
  continuous word representations for unknown words from those of known words. The
  resulting model significantly reduces perplexity on sparse-data tasks when compared
  to standard backoff models, standard neural language models, and factored language
  models.
ref_count: 11
references:
- pid: c19fbefdeead6a4154a22a9c8551a18b1530033a
  title: Hierarchical Probabilistic Neural Network Language Model
- pid: 6c2b28f9354f667cd5bd07afc0471d8334430da7
  title: A Neural Probabilistic Language Model
- pid: 3de5d40b60742e3dfa86b19e7f660962298492af
  title: Class-Based n-gram Models of Natural Language
- pid: 9319ca5a532462f9f3515ac3d317668aa9650d5b
  title: Exact training of a neural syntactic language model
- pid: 8b395470a57c48d174c4216ea21a7a58bc046917
  title: Training Neural Network Language Models on Very Large Corpora
slug: Factored-Neural-Language-Models-Alexandrescu-Kirchhoff
title: Factored Neural Language Models
url: https://www.semanticscholar.org/paper/Factored-Neural-Language-Models-Alexandrescu-Kirchhoff/a73ae2ce1cfafae61238b3fdb1bbb61093962b4d?sort=total-citations
venue: NAACL
year: 2006
