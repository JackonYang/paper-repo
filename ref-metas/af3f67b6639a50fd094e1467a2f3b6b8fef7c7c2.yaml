authors:
- Thomas Wolf
- Lysandre Debut
- Victor Sanh
- Julien Chaumond
- Clement Delangue
- Anthony Moi
- Pierric Cistac
- T. Rault
- "R\xE9mi Louf"
- Morgan Funtowicz
- Jamie Brew
badges:
- id: OPEN_ACCESS
corpusId: 208117506
fieldsOfStudy:
- Computer Science
numCitedBy: 2300
numCiting: 48
paperAbstract: Recent progress in natural language processing has been driven by advances
  in both model architecture and model pretraining. Transformer architectures have
  facilitated building higher-capacity models and pretraining has made it possible
  to effectively utilize this capacity for a wide variety of tasks. Transformers is
  an open-source library with the goal of opening up these advances to the wider machine
  learning community. The library consists of carefully engineered state-of-the art
  Transformer architectures under a unified API. Backing this library is a curated
  collection of pretrained models made by and available for the community. Transformers
  is designed to be extensible by researchers, simple for practitioners, and fast
  and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.
ref_count: 48
references:
- pid: df2b0e26d0599ce3e70df8a9da02e51594e0e992
  title: 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'
- pid: 93b4cc549a1bc4bc112189da36c318193d05d806
  title: 'AllenNLP: A Deep Semantic Natural Language Processing Platform'
- pid: 3cfb319689f06bf04c2e28399361f414ca32c4b3
  title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer
- pid: a54b56af24bb4873ed0163b77df63b92bd018ddc
  title: 'DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter'
- pid: c4744a7c2bb298e4a52289a1e085c71cc3d37bc6
  title: 'Transformer-XL: Attentive Language Models beyond a Fixed-Length Context'
- pid: d9f6ada77448664b71128bb19df15765336974a6
  title: 'SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding
    Systems'
- pid: 7a064df1aeada7e69e5173f7d4c8606f4470365b
  title: 'ALBERT: A Lite BERT for Self-supervised Learning of Language Representations'
- pid: 9405cc0d6169988371b2755e573cc28650d14dfe
  title: Language Models are Unsupervised Multitask Learners
- pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  title: Attention is All you Need
- pid: 1e077413b25c4d34945cc2707e17e46ed4fe784a
  title: Universal Language Model Fine-tuning for Text Classification
- pid: 5e98fe2163640da8ab9695b9ee9c433bb30f5353
  title: 'SciBERT: A Pretrained Language Model for Scientific Text'
- pid: 327d7e55d64cb34d55bd3a3fe58233c238a312cd
  title: 'exBERT: A Visual Analysis Tool to Explore Learned Representations in Transformer
    Models'
- pid: 93b8da28d006415866bf48f9a6e06b5242129195
  title: 'GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language
    Understanding'
- pid: e0c6abdbdecf04ffac65c440da77fb9d66bb474c
  title: 'XLNet: Generalized Autoregressive Pretraining for Language Understanding'
- pid: ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc
  title: Cross-lingual Language Model Pretraining
- pid: 395de0bd3837fdf4b4b5e5f04835bcc69c279481
  title: 'BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation,
    Translation, and Comprehension'
- pid: bc8fa64625d9189f5801837e7b133e7fe3c581f7
  title: 'Learned in Translation: Contextualized Word Vectors'
- pid: 3febb2bed8865945e7fddc99efd791887bb7e14f
  title: Deep Contextualized Word Representations
- pid: 756810258e3419af76aff38c895c20343b0602d0
  title: 'ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators'
- pid: 077f8329a7b6fa3b7c877a57b81eb6c18b5f87de
  title: 'RoBERTa: A Robustly Optimized BERT Pretraining Approach'
- pid: aab5002a22b9b4244a8329b140bd0a86021aa2d1
  title: 'OpenNMT: Open-Source Toolkit for Neural Machine Translation'
- pid: 2f5102ec3f70d0dea98c957cc2cab4d15d83a2da
  title: The Stanford CoreNLP Natural Language Processing Toolkit
- pid: 81f5810fbbab9b7203b9556f4ce3c741875407bc
  title: 'SpanBERT: Improving Pre-training by Representing and Predicting Spans'
- pid: 01a660ec8aa995a88a00bfb41cb86c022047a9db
  title: 'NLTK: The Natural Language Toolkit'
slug: Transformers:-State-of-the-Art-Natural-Language-Wolf-Debut
title: 'Transformers: State-of-the-Art Natural Language Processing'
url: https://www.semanticscholar.org/paper/Transformers:-State-of-the-Art-Natural-Language-Wolf-Debut/af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2?sort=total-citations
venue: EMNLP
year: 2020
