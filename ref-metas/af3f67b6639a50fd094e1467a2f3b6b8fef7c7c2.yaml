authors:
- Thomas Wolf
- Lysandre Debut
- Victor Sanh
- Julien Chaumond
- Clement Delangue
- Anthony Moi
- Pierric Cistac
- T. Rault
- "R\xE9mi Louf"
- Morgan Funtowicz
- Jamie Brew
badges:
- id: OPEN_ACCESS
corpusId: 208117506
fieldsOfStudy:
- Computer Science
meta_key: transformers-state-of-the-art-natural-language-processing
numCitedBy: 2300
numCiting: 48
paperAbstract: Recent progress in natural language processing has been driven by advances
  in both model architecture and model pretraining. Transformer architectures have
  facilitated building higher-capacity models and pretraining has made it possible
  to effectively utilize this capacity for a wide variety of tasks. Transformers is
  an open-source library with the goal of opening up these advances to the wider machine
  learning community. The library consists of carefully engineered state-of-the art
  Transformer architectures under a unified API. Backing this library is a curated
  collection of pretrained models made by and available for the community. Transformers
  is designed to be extensible by researchers, simple for practitioners, and fast
  and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.
ref_count: 48
references:
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 33744
  pid: df2b0e26d0599ce3e70df8a9da02e51594e0e992
  show_ref_link: true
  title: 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 893
  pid: 93b4cc549a1bc4bc112189da36c318193d05d806
  show_ref_link: true
  title: 'AllenNLP: A Deep Semantic Natural Language Processing Platform'
  year: 2018
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 3764
  pid: 3cfb319689f06bf04c2e28399361f414ca32c4b3
  show_ref_link: true
  title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer
  year: 2020
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 726
  pid: 055fd6a9f7293269f1b22c1470e63bd02d8d9500
  show_ref_link: false
  title: 'Reformer: The Efficient Transformer'
  year: 2020
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 2086
  pid: a54b56af24bb4873ed0163b77df63b92bd018ddc
  show_ref_link: true
  title: 'DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1771
  pid: c4744a7c2bb298e4a52289a1e085c71cc3d37bc6
  show_ref_link: true
  title: 'Transformer-XL: Attentive Language Models beyond a Fixed-Length Context'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 823
  pid: d9f6ada77448664b71128bb19df15765336974a6
  show_ref_link: true
  title: 'SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding
    Systems'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 2706
  pid: 7a064df1aeada7e69e5173f7d4c8606f4470365b
  show_ref_link: true
  title: 'ALBERT: A Lite BERT for Self-supervised Learning of Language Representations'
  year: 2020
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 350
  pid: 3a7bbc46795929f0eace82b64c44c92a48682fb5
  show_ref_link: false
  title: 'FLAIR: An Easy-to-Use Framework for State-of-the-Art NLP'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 6284
  pid: 9405cc0d6169988371b2755e573cc28650d14dfe
  show_ref_link: true
  title: Language Models are Unsupervised Multitask Learners
  year: 2019
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 160
  pid: a3b8528104baeb0b979f94810064eea701099703
  show_ref_link: false
  title: 'FlauBERT: Unsupervised Language Model Pre-training for French'
  year: 2020
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 35148
  pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  show_ref_link: true
  title: Attention is All you Need
  year: 2017
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 2251
  pid: 1e077413b25c4d34945cc2707e17e46ed4fe784a
  show_ref_link: true
  title: Universal Language Model Fine-tuning for Text Classification
  year: 2018
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1018
  pid: 5e98fe2163640da8ab9695b9ee9c433bb30f5353
  show_ref_link: true
  title: 'SciBERT: A Pretrained Language Model for Scientific Text'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 79
  pid: 327d7e55d64cb34d55bd3a3fe58233c238a312cd
  show_ref_link: false
  title: 'exBERT: A Visual Analysis Tool to Explore Learned Representations in Transformer
    Models'
  year: 2020
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 2634
  pid: 93b8da28d006415866bf48f9a6e06b5242129195
  show_ref_link: true
  title: 'GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language
    Understanding'
  year: 2018
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 4226
  pid: e0c6abdbdecf04ffac65c440da77fb9d66bb474c
  show_ref_link: true
  title: 'XLNet: Generalized Autoregressive Pretraining for Language Understanding'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 498
  pid: 8323c591e119eb09b28b29fd6c7bc76bd889df7a
  show_ref_link: false
  title: 'Megatron-LM: Training Multi-Billion Parameter Language Models Using Model
    Parallelism'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1509
  pid: ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc
  show_ref_link: true
  title: Cross-lingual Language Model Pretraining
  year: 2019
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 852
  pid: 71b6394ad5654f5cd0fba763768ba4e523f7bbca
  show_ref_link: false
  title: 'Longformer: The Long-Document Transformer'
  year: 2020
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 2422
  pid: 395de0bd3837fdf4b4b5e5f04835bcc69c279481
  show_ref_link: true
  title: 'BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation,
    Translation, and Comprehension'
  year: 2020
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 552
  pid: 42e863f93203d37a2518da381beaf06e4c70fb3d
  show_ref_link: false
  title: 'Stanza: A Python Natural Language Processing Toolkit for Many Human Languages'
  year: 2020
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 52
  pid: 00b30ed463625da04166eb78ca617539b41a9846
  show_ref_link: false
  title: 'jiant: A Software Toolkit for Research on General-Purpose Text Understanding
    Models'
  year: 2020
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 710
  pid: bc8fa64625d9189f5801837e7b133e7fe3c581f7
  show_ref_link: true
  title: 'Learned in Translation: Contextualized Word Vectors'
  year: 2017
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 678
  pid: 97906df07855b029b7aae7c2a1c6c5e8df1d531c
  show_ref_link: false
  title: BERT Rediscovers the Classical NLP Pipeline
  year: 2019
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 7987
  pid: 3febb2bed8865945e7fddc99efd791887bb7e14f
  show_ref_link: true
  title: Deep Contextualized Word Representations
  year: 2018
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1191
  pid: 756810258e3419af76aff38c895c20343b0602d0
  show_ref_link: true
  title: 'ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators'
  year: 2020
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 384
  pid: f48ae425e2567be2d993efcaaf74c2274fc9d7c5
  show_ref_link: false
  title: 'COMET: Commonsense Transformers for Automatic Knowledge Graph Construction'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 7266
  pid: 077f8329a7b6fa3b7c877a57b81eb6c18b5f87de
  show_ref_link: true
  title: 'RoBERTa: A Robustly Optimized BERT Pretraining Approach'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 21
  pid: 75f90cbbf3c27a8b27567d6a9c8c4538743c8fff
  show_ref_link: false
  title: 'Texar: A Modularized, Versatile, and Extensible Toolkit for Text Generation'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1397
  pid: aab5002a22b9b4244a8329b140bd0a86021aa2d1
  show_ref_link: true
  title: 'OpenNMT: Open-Source Toolkit for Neural Machine Translation'
  year: 2017
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 6057
  pid: 2f5102ec3f70d0dea98c957cc2cab4d15d83a2da
  show_ref_link: true
  title: The Stanford CoreNLP Natural Language Processing Toolkit
  year: 2014
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1565
  pid: faadd7d081c8d67e8c2567e8a5579e46cd6b2280
  show_ref_link: false
  title: 'fairseq: A Fast, Extensible Toolkit for Sequence Modeling'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 879
  pid: 81f5810fbbab9b7203b9556f4ce3c741875407bc
  show_ref_link: true
  title: 'SpanBERT: Improving Pre-training by Representing and Predicting Spans'
  year: 2020
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 272
  pid: a798c13da2d500dd76c4ff76f18ded43df0d59bc
  show_ref_link: false
  title: 'LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in Recurrent
    Neural Networks'
  year: 2018
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 3447
  pid: 01a660ec8aa995a88a00bfb41cb86c022047a9db
  show_ref_link: true
  title: 'NLTK: The Natural Language Toolkit'
  year: 2004
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 699
  pid: df013a17ab84d5403361da4538a04d574f58be83
  show_ref_link: false
  title: 'TVM: An Automated End-to-End Optimizing Compiler for Deep Learning'
  year: 2018
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 85
  pid: 0a7620afb12870a5df0e178dd175d37a5cbc8c0c
  show_ref_link: false
  title: Supervised Multimodal Bitransformers for Classifying Images and Text
  year: 2019
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 667
  pid: 7365f887c938ca21a6adbef08b5a520ebbd4638f
  show_ref_link: false
  title: Model Cards for Model Reporting
  year: 2019
slug: Transformers:-State-of-the-Art-Natural-Language-Wolf-Debut
title: 'Transformers: State-of-the-Art Natural Language Processing'
url: https://www.semanticscholar.org/paper/Transformers:-State-of-the-Art-Natural-Language-Wolf-Debut/af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2?sort=total-citations
venue: EMNLP
year: 2020
