authors:
- M. Lehr
- I. Shafran
badges:
- id: OPEN_ACCESS
corpusId: 13953745
fieldsOfStudy:
- Computer Science
numCitedBy: 26
numCiting: 6
paperAbstract: We introduce a discriminative model for speech recognition that integrates
  acoustic, duration and language components. In the framework of finite state machines,
  a general model for speech recognition G is a finite state transduction from acoustic
  state sequences to word sequences (e.g., search graph in many speech recognizers).
  The lattices from a baseline recognizer can be viewed as an a posteriori version
  of G after having observed an utterance. So far, discriminative language models
  have been proposed to correct the output side of G and is applied on the lattices.
  The acoustic state sequences on the input side of these lattice can also be exploited
  to improve the choice of the best hypotheses through the lattice. Taking this view,
  the model proposed in this paper jointly estimates the parameters for acoustic and
  language components in a discriminative setting. The resulting model can be factored
  as corrections for the input and the output sides of the general model G. This formulation
  allows us to incorporate duration cues seamlessly. Empirical results on a large
  vocabulary Arabic GALE task demonstrate that the proposed model improves word error
  rate substantially, with a gain of 1.6% absolute. Through a series of experiments
  we analyze the contributions from and interactions between acoustic, duration and
  language components to find that duration cues play an important role in Arabic
  task.
ref_count: 6
references:
- pid: a80a452e587bd7f06ece1be101d6775fcee0f7af
  title: Weighted finite-state transducers in speech recognition
slug: Discriminatively-estimated-joint-acoustic,-and-for-Lehr-Shafran
title: Discriminatively estimated joint acoustic, duration, and language model for
  speech recognition
url: https://www.semanticscholar.org/paper/Discriminatively-estimated-joint-acoustic,-and-for-Lehr-Shafran/a2cd4d03695b9d5f4440f38e89e7a39b43ffa7f0?sort=total-citations
venue: 2010 IEEE International Conference on Acoustics, Speech and Signal Processing
year: 2010
