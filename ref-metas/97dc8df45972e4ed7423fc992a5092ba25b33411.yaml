authors:
- Dmytro Mishkin
- Jiri Matas
badges:
- id: OPEN_ACCESS
corpusId: 2780493
fieldsOfStudy:
- Computer Science
numCitedBy: 489
numCiting: 44
paperAbstract: "Layer-sequential unit-variance (LSUV) initialization - a simple method\
  \ for weight initialization for deep net learning - is proposed. The method consists\
  \ of the two steps. First, pre-initialize weights of each convolution or inner-product\
  \ layer with orthonormal matrices. Second, proceed from the first to the final layer,\
  \ normalizing the variance of the output of each layer to be equal to one. \nExperiment\
  \ with different activation functions (maxout, ReLU-family, tanh) show that the\
  \ proposed initialization leads to learning of very deep nets that (i) produces\
  \ networks with test accuracy better or equal to standard methods and (ii) is at\
  \ least as fast as the complex schemes proposed specifically for very deep nets\
  \ such as FitNets (Romero et al. (2015)) and Highway (Srivastava et al. (2015)).\
  \ \nPerformance is evaluated on GoogLeNet, CaffeNet, FitNets and Residual nets and\
  \ the state-of-the-art, or very close to it, is achieved on the MNIST, CIFAR-10/100\
  \ and ImageNet datasets."
ref_count: 43
references:
- pid: fb91db6aa4f710814f8aec28a7f3ecbc4e5ad4fd
  title: Deeply-Supervised Nets
- pid: cd85a549add0c7c7def36aca29837efd24b24080
  title: 'FitNets: Hints for Thin Deep Nets'
- pid: 0f84a81f431b18a78bd97f59ed4b9d8eda390970
  title: 'Striving for Simplicity: The All Convolutional Net'
- pid: d6f2f611da110b5b5061731be3fc4c7f45d8ee23
  title: 'Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet
    Classification'
- pid: 4d376d6978dad0374edfa6709c9556b42d3594d3
  title: 'Batch Normalization: Accelerating Deep Network Training by Reducing Internal
    Covariate Shift'
- pid: 2c03df8b48bf3fa39054345bafabfeff15bfd11d
  title: Deep Residual Learning for Image Recognition
- pid: abd1c342495432171beb7ca8fd9551ef13cbd0ff
  title: ImageNet classification with deep convolutional neural networks
- pid: e15cf50aa89fee8535703b9f9512fca5bfc43327
  title: Going deeper with convolutions
- pid: 355d44f53428b1ac4fb2ab468d593c720640e5bd
  title: Greedy Layer-Wise Training of Deep Networks
- pid: eb42cf88027de515750f230b23b1a057dc782108
  title: Very Deep Convolutional Networks for Large-Scale Image Recognition
- pid: 55dda8f230566867acbfaa7bdd08fd8c7b8721ed
  title: Fractional Max-Pooling
- pid: 5d90f06bb70a0a3dced62413346235c02b1aa086
  title: Learning Multiple Layers of Features from Tiny Images
- pid: b71ac1e9fb49420d13e084ac67254a0bbd40f83f
  title: Understanding the difficulty of training deep feedforward neural networks
- pid: 6bdb186ec4726e00a8051119636d4df3b94043b5
  title: 'Caffe: Convolutional Architecture for Fast Feature Embedding'
- pid: b7b915d508987b73b61eccd2b237e7ed099a2d29
  title: Maxout Networks
- pid: 0c908739fbff75f03469d13d4a1a07de3414ee19
  title: Distilling the Knowledge in a Neural Network
- pid: b92aa7024b87f50737b372e5df31ef091ab54e62
  title: Training Very Deep Networks
- pid: 162d958ff885f1462aeda91cd72582323fd6a1f4
  title: Gradient-based learning applied to document recognition
- pid: 94e3e7bc3d23276f0ee2d1cb8f9d14aa19668d5f
  title: Under Review as a Conference Paper at Iclr 2017 Delving into Transferable
    Adversarial Ex- Amples and Black-box Attacks
- pid: 99c970348b8f70ce23d6641e201904ea49266b6e
  title: Exact solutions to the nonlinear dynamics of learning in deep linear neural
    networks
- pid: 367f2c63a6f6a10b3b64b8729d601e69337ee3cc
  title: Rectifier Nonlinearities Improve Neural Network Acoustic Models
- pid: e74f9b7f8eec6ba4704c206b93bc8079af3da4bd
  title: ImageNet Large Scale Visual Recognition Challenge
- pid: bcd857d75841aa3e92cd4284a8818aba9f6c0c3f
  title: Published as a conference paper at ICLR 2018 S IMULATING A CTION D YNAMICS
    WITH N EURAL P ROCESS N ETWORKS
- pid: 67107f78a84bdb2411053cb54e94fa226eea6d8e
  title: Deep Sparse Rectifier Neural Networks
slug: All-you-need-is-a-good-init-Mishkin-Matas
title: All you need is a good init
url: https://www.semanticscholar.org/paper/All-you-need-is-a-good-init-Mishkin-Matas/97dc8df45972e4ed7423fc992a5092ba25b33411?sort=total-citations
venue: ICLR
year: 2016
