authors:
- Benjamin Hoover
- Hendrik Strobelt
- Sebastian Gehrmann
badges:
- id: OPEN_ACCESS
corpusId: 204402756
fieldsOfStudy:
- Computer Science
numCitedBy: 79
numCiting: 35
paperAbstract: Large Transformer-based language models can route and reshape complex
  information via their multi-headed attention mechanism. Although the attention never
  receives explicit supervision, it can exhibit recognizable patterns following linguistic
  or positional information. Analyzing the learned representations and attentions
  is paramount to furthering our understanding of the inner workings of these models.
  However, analyses have to catch up with the rapid release of new models and the
  growing diversity of investigation techniques. To support analysis for a wide variety
  of models, we introduce exBERT, a tool to help humans conduct flexible, interactive
  investigations and formulate hypotheses for the model-internal reasoning process.
  exBERT provides insights into the meaning of the contextual representations and
  attention by matching a human-specified input to similar contexts in large annotated
  datasets. By aggregating the annotations of the matched contexts, exBERT can quickly
  replicate findings from literature and extend them to previously not analyzed models.
ref_count: 35
references:
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 2635
  pid: 93b8da28d006415866bf48f9a6e06b5242129195
  title: 'GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language
    Understanding'
  year: 2018
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 33754
  pid: df2b0e26d0599ce3e70df8a9da02e51594e0e992
  title: 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 6284
  pid: 9405cc0d6169988371b2755e573cc28650d14dfe
  title: Language Models are Unsupervised Multitask Learners
  year: 2019
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 2086
  pid: a54b56af24bb4873ed0163b77df63b92bd018ddc
  title: 'DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 823
  pid: d9f6ada77448664b71128bb19df15765336974a6
  title: 'SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding
    Systems'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 35157
  pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  title: Attention is All you Need
  year: 2017
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1042
  pid: efbd381493bb9636f489b965a2034d529cd56bcd
  title: Pointer Sentinel Mixture Models
  year: 2017
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 7267
  pid: 077f8329a7b6fa3b7c877a57b81eb6c18b5f87de
  title: 'RoBERTa: A Robustly Optimized BERT Pretraining Approach'
  year: 2019
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 4794
  pid: 1af68821518f03568f913ab03fc02080247a27ff
  title: Neural Machine Translation of Rare Words with Subword Units
  year: 2016
slug: exBERT:-A-Visual-Analysis-Tool-to-Explore-Learned-Hoover-Strobelt
title: 'exBERT: A Visual Analysis Tool to Explore Learned Representations in Transformer
  Models'
url: https://www.semanticscholar.org/paper/exBERT:-A-Visual-Analysis-Tool-to-Explore-Learned-Hoover-Strobelt/327d7e55d64cb34d55bd3a3fe58233c238a312cd?sort=total-citations
venue: ACL
year: 2020
