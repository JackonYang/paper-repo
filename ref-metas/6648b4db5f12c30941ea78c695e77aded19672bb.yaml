authors:
- Luowei Zhou
- H. Palangi
- Lei Zhang
- Houdong Hu
- Jason J. Corso
- Jianfeng Gao
badges:
- id: OPEN_ACCESS
corpusId: 202734445
fieldsOfStudy:
- Computer Science
numCitedBy: 355
numCiting: 48
paperAbstract: 'This paper presents a unified Vision-Language Pre-training (VLP) model.
  The model is unified in that (1) it can be fine-tuned for either vision-language
  generation (e.g., image captioning) or understanding (e.g., visual question answering)
  tasks, and (2) it uses a shared multi-layer transformer network for both encoding
  and decoding, which differs from many existing methods where the encoder and decoder
  are implemented using separate models. The unified VLP model is pre-trained on a
  large amount of image-text pairs using the unsupervised learning objectives of two
  tasks: bidirectional and sequence-to-sequence (seq2seq) masked vision-language prediction.
  The two tasks differ solely in what context the prediction conditions on. This is
  controlled by utilizing specific self-attention masks for the shared transformer
  network. To the best of our knowledge, VLP is the first reported model that achieves
  state-of-the-art results on both vision-language generation and understanding tasks,
  as disparate as image captioning and visual question answering, across three challenging
  benchmark datasets: COCO Captions, Flickr30k Captions, and VQA 2.0. The code and
  the pre-trained models are available at https://github.com/LuoweiZhou/VLP.'
ref_count: 48
references:
- pid: 2bc1c8bd00bbf7401afcb5460277840fd8bab029
  title: 'Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal
    Pre-training'
- pid: d8a305b9366608d54452ac30459ee57b4f5cf1c9
  title: 'UNITER: UNiversal Image-TExt Representation Learning'
- pid: 1c71771c701aadfd72c5866170a9f5d71464bb88
  title: Unified Language Model Pre-training for Natural Language Understanding and
    Generation
- pid: 65a9c7b0800c86a196bc14e7621ff895cc6ab287
  title: 'ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language
    Tasks'
- pid: 54416048772b921720f19869ed11c2a360589d03
  title: 'UNITER: Learning UNiversal Image-TExt Representations'
- pid: 79c93274429d6355959f1e4374c2147bb81ea649
  title: 'LXMERT: Learning Cross-Modality Encoder Representations from Transformers'
- pid: c41a11c0e9b8b92b4faaf97749841170b760760a
  title: 'VideoBERT: A Joint Model for Video and Language Representation Learning'
- pid: 2527626c11a84f15709e943fbfa2356e19930e3b
  title: 'VL-BERT: Pre-training of Generic Visual-Linguistic Representations'
- pid: 5aec474c31a2f4b74703c6f786c0a8ff85c450da
  title: 'VisualBERT: A Simple and Performant Baseline for Vision and Language'
- pid: 3bf09b2e2639add154a9fe6ff98cc373d3e90e4e
  title: Neural Baby Talk
- pid: 9405cc0d6169988371b2755e573cc28650d14dfe
  title: Language Models are Unsupervised Multitask Learners
- pid: 658721bc13b0fa97366d38c05a96bf0a9f4bb0ac
  title: Multi-Task Deep Neural Networks for Natural Language Understanding
- pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  title: Attention is All you Need
- pid: df2b0e26d0599ce3e70df8a9da02e51594e0e992
  title: 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'
- pid: 0000fcfd467a19cf0e59169c2f07d730a0f3a8b9
  title: Exploring Visual Relationship for Image Captioning
- pid: 35ed258aede3df17ee20a6635364cb5fd2461049
  title: End-to-End Dense Video Captioning with Masked Transformer
- pid: 4c163d4942117179d3e97182e1b280027d7d60a9
  title: Attention on Attention for Image Captioning
- pid: 5fa973b8d284145bf0ced9acf2913a74674260f6
  title: 'Yin and Yang: Balancing and Answering Binary Visual Questions'
- pid: cd18800a0fe0b668a1cc19f2ec95b5003d0a5035
  title: Improving Language Understanding by Generative Pre-Training
- pid: a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8
  title: Bottom-Up and Top-Down Attention for Image Captioning and Visual Question
    Answering
- pid: f259bc7ef31c4ec7dd041c94bfd6b2f93b99b47c
  title: Contrastive Bidirectional Transformer for Temporal Representation Learning
- pid: a27ed1310b9c0832bde8f906e0fcd23d1f3aa79c
  title: 'Making the V in VQA Matter: Elevating the Role of Image Understanding in
    Visual Question Answering'
- pid: b82153bf85d5d1edd3f170aace830e5328ca9ed0
  title: Fusion of Detected Objects in Text for Visual Question Answering
- pid: f6feb1af1809dfd872d868dfcc13021cc42f496c
  title: Auto-Encoding Scene Graphs for Image Captioning
- pid: 97ad70a9fa3f99adf18030e5e38ebe3d90daa2db
  title: 'VQA: Visual Question Answering'
- pid: afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d
  title: 'Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image
    Annotations'
- pid: 025a0dc4a2a98742f1b410b6318a46de2c854b22
  title: Learning Video Representations using Contrastive Bidirectional Transformer
- pid: 90873a97aa9a43775e5aeea01b03aea54b28bfbd
  title: 'Don''t Just Assume; Look and Answer: Overcoming Priors for Visual Question
    Answering'
- pid: e9b13731027418ed38103d1dfc8a70f6881bc684
  title: Dynamic Fusion With Intra- and Inter-Modality Attention Flow for Visual Question
    Answering
- pid: 6c8353697cdbb98dfba4f493875778c4286d3e3a
  title: Self-Critical Sequence Training for Image Captioning
- pid: f6e0856b4a9199fa968ac00da612a9407b5cb85c
  title: Aggregated Residual Transformations for Deep Neural Networks
- pid: b4df354db88a70183a64dbc9e56cf14e7669a6c0
  title: 'Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic
    Image Captioning'
- pid: 36c3972569a6949ecca90bfa6f8e99883e092845
  title: 'Pythia v0.1: the Winning Entry to the VQA Challenge 2018'
- pid: 424561d8585ff8ebce7d5d07de8dbf7aae5e7270
  title: 'Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks'
- pid: a5d10341717c0519cf63151b496a6d2ed67aa05f
  title: Bilinear Attention Networks
- pid: 077f8329a7b6fa3b7c877a57b81eb6c18b5f87de
  title: 'RoBERTa: A Robustly Optimized BERT Pretraining Approach'
- pid: 696ca58d93f6404fea0fc75c62d1d7b378f47628
  title: 'Microsoft COCO Captions: Data Collection and Evaluation Server'
- pid: 44040913380206991b1991daf1192942e038fe31
  title: 'From image descriptions to visual denotations: New similarity metrics for
    semantic inference over event descriptions'
slug: Unified-Vision-Language-Pre-Training-for-Image-and-Zhou-Palangi
title: Unified Vision-Language Pre-Training for Image Captioning and VQA
url: https://www.semanticscholar.org/paper/Unified-Vision-Language-Pre-Training-for-Image-and-Zhou-Palangi/6648b4db5f12c30941ea78c695e77aded19672bb?sort=total-citations
venue: AAAI
year: 2020
