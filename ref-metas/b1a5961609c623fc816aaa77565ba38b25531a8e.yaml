authors:
- J. Hartmanis
- J. V. Leeuwen
badges:
- id: OPEN_ACCESS
corpusId: 26661612
fieldsOfStudy:
- Computer Science
numCitedBy: 1303
numCiting: 0
paperAbstract: "Many algorithms are available to learn deep hierarchies of features\
  \ from unlabeled data, especially images. In many cases, these algorithms involve\
  \ multi-layered networks of features (eg, neural networks) that are sometimes tricky\
  \ to train and tune and are difficult. Abstract A commonly encountered problem in\
  \ MLP (multi-layer perceptron) classification problems is related to the prior probabilities\
  \ of the individual classes-if the number of training examples that correspond to\
  \ each class varies significantly between the classes, then. Abstract Validation\
  \ can be used to detect when overfitting starts during supervised training of a\
  \ neural network; training is then stopped before convergence to avoid the overfitting\
  \ (\xE2 \u0153early stopping\xE2 ). The exact criterion used for validation-based\
  \ early stopping, however, is usually. Abstract Reservoir computing has emerged\
  \ in the last decade as an alternative to gradient descent methods for training\
  \ recurrent neural networks. Echo State Network (ESN) is one of the key reservoir\
  \ computing \xE2 \u0153flavors\xE2 . While being practical, conceptually simple,\
  \ and easy. Preface In many cases, the amount of labeled data is limited and does\
  \ not allow for fully identifying the function that needs to be learned. When labeled\
  \ data is scarce, the learning algorithm is exposed to simultaneous underfitting\
  \ and overfitting. The learning algorithm. Abstract Restricted Boltzmann machines\
  \ (RBMs) have been used as generative models of many different types of data. RBMs\
  \ are usually trained using the contrastive divergence learning procedure. This\
  \ requires a certain amount of practical experience to decide.A commonly encountered\
  \ problem in MLP (multi-layer perceptron) classification problems is related to\
  \ the prior probabilities of the individual classes-if the number of training examples\
  \ that correspond to each class varies significantly between the classes, then.\
  \ Abstract Validation can be used to detect when overfitting starts during supervised\
  \ training of a neural network; training is then stopped before convergence to avoid\
  \ the overfitting (\xE2 \u0153early stopping\xE2 ). The exact criterion used for\
  \ validation-based early stopping, however, is usually. Abstract Reservoir computing\
  \ has emerged in the last decade as an alternative to gradient descent methods for\
  \ training recurrent neural networks. Echo State Network (ESN) is one of the key\
  \ reservoir computing \xE2 \u0153flavors\xE2 . While being practical, conceptually\
  \ simple, and easy. Preface In many cases, the amount of labeled data is limited\
  \ and does not allow for fully identifying the function that needs to be learned.\
  \ When labeled data is scarce, the learning algorithm is exposed to simultaneous\
  \ underfitting and overfitting. The learning algorithm. Abstract Restricted Boltzmann\
  \ machines (RBMs) have been used as generative models of many different types of\
  \ data. RBMs are usually trained using the contrastive divergence learning procedure.\
  \ This requires a certain amount of practical experience to decide. It is our belief\
  \ that researchers and practitioners acquire, through experience and word-ofmouth,\
  \ techniques and heuristics that help them successfully apply neural networks to\
  \ di cult real world problems. Often these\\ tricks\" are theo-tically well motivated.\
  \ Sometimes they. Abstract The convergence of back-propagation learning is analyzed\
  \ so as to explain common phenomenon observed by practitioners. Many undesirable\
  \ behaviors of backprop can be avoided with tricks that are rarely exposed in serious\
  \ technical publications. This. Abstract Chapter 1 strongly advocates the stochastic\
  \ back-propagation method to train neural networks. This is in fact an instance\
  \ of a more general technique called stochastic gradient descent (SGD). This chapter\
  \ provides background material, explains why SGD is a. Abstract WeChapter 1 strongly\
  \ advocates the stochastic back-propagation method to train neural networks. This\
  \ is in fact an instance of a more general technique called stochastic gradient\
  \ descent (SGD). This chapter provides background material, explains why SGD is\
  \ a. Abstract We show how nonlinear semi-supervised embedding algorithms popular\
  \ for use with \xE2 \u0153shallow\xE2 learning techniques such as kernel methods\
  \ can be easily applied to deep multi-layer architectures, either as a regularizer\
  \ at the output layer, or on each layer."
ref_count: 0
references: []
slug: Neural-Networks:-Tricks-of-the-Trade-Hartmanis-Leeuwen
title: 'Neural Networks: Tricks of the Trade'
url: https://www.semanticscholar.org/paper/Neural-Networks:-Tricks-of-the-Trade-Hartmanis-Leeuwen/b1a5961609c623fc816aaa77565ba38b25531a8e?sort=total-citations
venue: Lecture Notes in Computer Science
year: 2002
