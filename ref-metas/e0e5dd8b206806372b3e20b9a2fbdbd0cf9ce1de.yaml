authors:
- Ilya Sutskever
- James Martens
- Geoffrey E. Hinton
badges:
- id: OPEN_ACCESS
corpusId: 8843166
fieldsOfStudy:
- Computer Science
numCitedBy: 1254
numCiting: 30
paperAbstract: "Recurrent Neural Networks (RNNs) are very powerful sequence models\
  \ that do not enjoy widespread use because it is extremely difficult to train them\
  \ properly. Fortunately, recent advances in Hessian-free optimization have been\
  \ able to overcome the difficulties associated with training RNNs, making it possible\
  \ to apply them successfully to challenging sequence problems. In this paper we\
  \ demonstrate the power of RNNs trained with the new Hessian-Free optimizer (HF)\
  \ by applying them to character-level language modeling tasks. The standard RNN\
  \ architecture, while effective, is not ideally suited for such tasks, so we introduce\
  \ a new RNN variant that uses multiplicative (or \"gated\") connections which allow\
  \ the current input character to determine the transition matrix from one hidden\
  \ state vector to the next. After training the multiplicative RNN with the HF optimizer\
  \ for five days on 8 high-end Graphics Processing Units, we were able to surpass\
  \ the performance of the best previous single method for character-level language\
  \ modeling \u2013 a hierarchical non-parametric sequence model. To our knowledge\
  \ this represents the largest recurrent neural network application to date."
ref_count: 30
references:
- pid: 0d6203718c15f137fda2f295c96269bc2b254644
  title: Learning Recurrent Neural Networks with Hessian-Free Optimization
- pid: 9819b600a828a57e1cde047bbe710d3446b30da5
  title: Recurrent neural network based language model
- pid: d0be39ee052d246ae99c082a565aba25b811be2d
  title: Learning long-term dependencies with gradient descent is difficult
- pid: a9fc84f8abe740cdc7ee82e69444d1d00dbe0ceb
  title: A Scalable Hierarchical Distributed Language Model
- pid: 44d2abe2175df8153f465f6c39b68b76a0d40ab9
  title: Long Short-Term Memory
- pid: 052b1d8ce63b07fec3de9dbb583772d860b7c769
  title: Learning representations by back-propagating errors
- pid: c08d0525bd42fa1c24f9f5df72f4c8fcf7063b22
  title: Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks
- pid: 346fbcffe4237aa60e8bcb3d4294a8b99436f1d0
  title: Factored conditional restricted Boltzmann Machines for modeling motion style
- pid: 1a3d22599028a05669e884f3eaf19a342e190a87
  title: 'Backpropagation Through Time: What It Does and How to Do It'
- pid: c6629770cb6a00ad585918e71fe6dbad829ad0d1
  title: An application of recurrent nets to phone probability estimation
- pid: 4c46347fbc272b21468efe3d9af34b4b2bad6684
  title: Deep learning via Hessian-free optimization
- pid: ae3fe34be9230c98b04d68b4621c89b7dbc2d717
  title: Learning representations by backpropagating errors
- pid: 0ea90fac0958d84bcf4a2875c2b169478358b480
  title: 'CUDAMat: a CUDA-based matrix class for Python'
- pid: 0d073966e48ffb6dccde1e4eb3f0380c10c6a766
  title: 'Harnessing Nonlinearity: Predicting Chaotic Systems and Saving Energy in
    Wireless Communication'
- pid: 20d673dc3200ac1742ee0827535a291eb6e051f8
  title: Arithmetic Coding
- pid: 3f3d13e95c25a8f6a753e38dfce88885097cbd43
  title: Untersuchungen zu dynamischen neuronalen Netzen
slug: Generating-Text-with-Recurrent-Neural-Networks-Sutskever-Martens
title: Generating Text with Recurrent Neural Networks
url: https://www.semanticscholar.org/paper/Generating-Text-with-Recurrent-Neural-Networks-Sutskever-Martens/e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de?sort=total-citations
venue: ICML
year: 2011
