authors:
- Oriol Vinyals
- Daniel Povey
badges:
- id: OPEN_ACCESS
corpusId: 6318468
fieldsOfStudy:
- Computer Science
numCitedBy: 119
numCiting: 20
paperAbstract: In this paper, we propose a second order optimization method to learn
  models where both the dimensionality of the parameter space and the number of training
  samples is high. In our method, we construct on each iteration a Krylov subspace
  formed by the gradient and an approximation to the Hessian matrix, and then use
  a subset of the training data samples to optimize over this subspace. As with the
  Hessian Free (HF) method of Martens (2010), the Hessian matrix is never explicitly
  constructed, and is computed using a subset of data. In practice, as in HF, we typically
  use a positive definite substitute for the Hessian matrix such as the Gauss-Newton
  matrix. We investigate the effectiveness of our proposed method on deep neural networks,
  and compare its performance to widely used methods such as stochastic gradient descent,
  conjugate gradient descent and L-BFGS, and also to HF. Our method leads to faster
  convergence than either L-BFGS or HF, and generally performs better than either
  of them in cross-validation accuracy. It is also simpler and more general than HF,
  as it does not require a positive semidefinite approximation of the Hessian matrix
  to work well nor the setting of a damping parameter. The chief drawback versus HF
  is the need for memory to store a basis for the Krylov subspace.
ref_count: 20
references:
- pid: 4c46347fbc272b21468efe3d9af34b4b2bad6684
  title: Deep learning via Hessian-free optimization
- pid: 053912e76e50c9f923a1fc1c173f1365776060cc
  title: On optimization methods for deep learning
- pid: c6867b6b564462d6b902f68e0bfa58f4717ca1cc
  title: Fast Exact Multiplication by the Hessian
- pid: 0d6203718c15f137fda2f295c96269bc2b254644
  title: Learning Recurrent Neural Networks with Hessian-Free Optimization
- pid: b71ac1e9fb49420d13e084ac67254a0bbd40f83f
  title: Understanding the difficulty of training deep feedforward neural networks
- pid: 5a767a341364de1f75bea85e0b12ba7d3586a461
  title: Natural Gradient Works Efficiently in Learning
- pid: ccf415df5a83b343dae261286d29a40e8b80e6c6
  title: The Difficulty of Training Deep Architectures and the Effect of Unsupervised
    Pre-Training
- pid: 0d2336389dff3031910bd21dd1c44d1b4cd51725
  title: Why Does Unsupervised Pre-training Help Deep Learning?
- pid: ffa94bba647817fa5e8f8d3250fc977435b5ca76
  title: Fast Curvature Matrix-Vector Products for Second-Order Gradient Descent
- pid: 6ed460701019072ee2e364a1a491f73dd931f27f
  title: Topmoumoute Online Natural Gradient Algorithm
- pid: 46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e
  title: Reducing the Dimensionality of Data with Neural Networks
- pid: 43c3bfffdcd313c549b2045980855ea001d6f13b
  title: Numerical Optimization
slug: Krylov-Subspace-Descent-for-Deep-Learning-Vinyals-Povey
title: Krylov Subspace Descent for Deep Learning
url: https://www.semanticscholar.org/paper/Krylov-Subspace-Descent-for-Deep-Learning-Vinyals-Povey/a98483785378bde7e2384a3035b2b501ee03654b?sort=total-citations
venue: AISTATS
year: 2012
