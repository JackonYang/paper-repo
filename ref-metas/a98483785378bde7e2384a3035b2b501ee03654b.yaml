authors:
- Oriol Vinyals
- Daniel Povey
badges:
- id: OPEN_ACCESS
corpusId: 6318468
fieldsOfStudy:
- Computer Science
numCitedBy: 119
numCiting: 20
paperAbstract: In this paper, we propose a second order optimization method to learn
  models where both the dimensionality of the parameter space and the number of training
  samples is high. In our method, we construct on each iteration a Krylov subspace
  formed by the gradient and an approximation to the Hessian matrix, and then use
  a subset of the training data samples to optimize over this subspace. As with the
  Hessian Free (HF) method of Martens (2010), the Hessian matrix is never explicitly
  constructed, and is computed using a subset of data. In practice, as in HF, we typically
  use a positive definite substitute for the Hessian matrix such as the Gauss-Newton
  matrix. We investigate the effectiveness of our proposed method on deep neural networks,
  and compare its performance to widely used methods such as stochastic gradient descent,
  conjugate gradient descent and L-BFGS, and also to HF. Our method leads to faster
  convergence than either L-BFGS or HF, and generally performs better than either
  of them in cross-validation accuracy. It is also simpler and more general than HF,
  as it does not require a positive semidefinite approximation of the Hessian matrix
  to work well nor the setting of a damping parameter. The chief drawback versus HF
  is the need for memory to store a basis for the Krylov subspace.
ref_count: 20
references:
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 845
  pid: 4c46347fbc272b21468efe3d9af34b4b2bad6684
  title: Deep learning via Hessian-free optimization
  year: 2010
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 884
  pid: 053912e76e50c9f923a1fc1c173f1365776060cc
  title: On optimization methods for deep learning
  year: 2011
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 586
  pid: c6867b6b564462d6b902f68e0bfa58f4717ca1cc
  title: Fast Exact Multiplication by the Hessian
  year: 1994
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 585
  pid: 0d6203718c15f137fda2f295c96269bc2b254644
  title: Learning Recurrent Neural Networks with Hessian-Free Optimization
  year: 2011
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 12435
  pid: b71ac1e9fb49420d13e084ac67254a0bbd40f83f
  title: Understanding the difficulty of training deep feedforward neural networks
  year: 2010
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 2730
  pid: 5a767a341364de1f75bea85e0b12ba7d3586a461
  title: Natural Gradient Works Efficiently in Learning
  year: 1998
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 395
  pid: ccf415df5a83b343dae261286d29a40e8b80e6c6
  title: The Difficulty of Training Deep Architectures and the Effect of Unsupervised
    Pre-Training
  year: 2009
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1726
  pid: 0d2336389dff3031910bd21dd1c44d1b4cd51725
  title: Why Does Unsupervised Pre-training Help Deep Learning?
  year: 2010
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 275
  pid: ffa94bba647817fa5e8f8d3250fc977435b5ca76
  title: Fast Curvature Matrix-Vector Products for Second-Order Gradient Descent
  year: 2002
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 179
  pid: 6ed460701019072ee2e364a1a491f73dd931f27f
  title: Topmoumoute Online Natural Gradient Algorithm
  year: 2007
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 14646
  pid: 46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e
  title: Reducing the Dimensionality of Data with Neural Networks
  year: 2006
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 10652
  pid: 43c3bfffdcd313c549b2045980855ea001d6f13b
  title: Numerical Optimization
  year: 1999
slug: Krylov-Subspace-Descent-for-Deep-Learning-Vinyals-Povey
title: Krylov Subspace Descent for Deep Learning
url: https://www.semanticscholar.org/paper/Krylov-Subspace-Descent-for-Deep-Learning-Vinyals-Povey/a98483785378bde7e2384a3035b2b501ee03654b?sort=total-citations
venue: AISTATS
year: 2012
