authors:
- D. Servan-Schreiber
- A. Cleeremans
- James L. McClelland
badges:
- id: OPEN_ACCESS
corpusId: 18569620
fieldsOfStudy:
- Computer Science
numCitedBy: 177
numCiting: 9
paperAbstract: We explore a network architecture introduced by Elman (1988) for predicting
  successive elements of a sequence. The network uses the pattern of activation over
  a set of hidden units from time-step t-1, together with element t, to predict element
  t+1. When the network is trained with strings from a particular finite-state grammar,
  it can learn to be a perfect finite-state recognizer for the grammar. Cluster analyses
  of the hidden-layer patterns of activation showed that they encode prediction-relevant
  information about the entire path traversed through the network. We illustrate the
  phases of learning with cluster analyses performed at different points during training.
ref_count: 9
references:
- pid: ce9a21b93ba29d4145a8ef6bf401e77f261848de
  title: A Learning Algorithm for Continually Running Fully Recurrent Neural Networks
- pid: 668087f0ae7ce1de6e0bd0965dbb480c08103260
  title: Finding Structure in Time
- pid: 406033f22b6a671b94bcbdfaf63070b7ce6f3e48
  title: 'NETtalk: a parallel network that learns to read aloud'
- pid: 749ce8ccd9453d1b34901143cddf5f9bee2977cf
  title: Learning representations by back-propagation errors, nature
- pid: 1d453386011ef21285fa81fb4f87fdf811c6ad7a
  title: Learning internal representations by back-propagating errors
slug: Learning-Subsequential-Structure-in-Simple-Networks-Servan-Schreiber-Cleeremans
title: Learning Subsequential Structure in Simple Recurrent Networks
url: https://www.semanticscholar.org/paper/Learning-Subsequential-Structure-in-Simple-Networks-Servan-Schreiber-Cleeremans/d0d7f8b5d54e3d68fd45a70d4a0d13f42e8d71ff?sort=total-citations
venue: NIPS
year: 1988
