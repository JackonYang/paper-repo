authors:
- Stephen Merity
- Caiming Xiong
- James Bradbury
- R. Socher
badges:
- id: OPEN_ACCESS
corpusId: 16299141
fieldsOfStudy:
- Computer Science
numCitedBy: 1042
numCiting: 38
paperAbstract: Recent neural network sequence models with softmax classifiers have
  achieved their best language modeling performance only with very large hidden states
  and large vocabularies. Even then they struggle to predict rare or unseen words
  even if the context makes the prediction unambiguous. We introduce the pointer sentinel
  mixture architecture for neural sequence models which has the ability to either
  reproduce a word from the recent context or produce a word from a standard softmax
  classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling
  performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters
  than a standard softmax LSTM. In order to evaluate how well language models can
  exploit longer contexts and deal with more realistic vocabularies and larger corpora
  we also introduce the freely available WikiText corpus.
ref_count: 38
references:
- pid: d1275b2a2ab53013310e759e5c6878b96df643d4
  title: Context dependent recurrent neural network language model
- pid: 5d833331b0e22ff359db05c62a8bca18c4f04b68
  title: One billion word benchmark for measuring progress in statistical language
    modeling
- pid: aa5b35dcf8b024f5352db73cc3944e8fad4f3793
  title: Pointing the Unknown Words
- pid: 891ce1687e2befddd19f54e4eef1d3f39c8dbaf7
  title: Character-Aware Neural Language Models
- pid: 452059171226626718eb677358836328f884298e
  title: 'Ask Me Anything: Dynamic Memory Networks for Natural Language Processing'
- pid: 4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e
  title: End-To-End Memory Networks
- pid: 9819b600a828a57e1cde047bbe710d3446b30da5
  title: Recurrent neural network based language model
- pid: f2e50e2ee4021f199877c8920f1f984481c723aa
  title: Text Understanding with the Attention Sum Reader Network
- pid: ba30df190664193514d1d309cb673728ed48f449
  title: Incorporating Copying Mechanism in Sequence-to-Sequence Learning
- pid: 9f0687bcd0a7d7fc91b8c5d36c003a38b8853105
  title: 'Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations'
- pid: 0c1f9ca23f4f09ecfc44bcc8ca1c2736624f4652
  title: A Theoretically Grounded Application of Dropout in Recurrent Neural Networks
- pid: f96898d15a1bf1fa8925b1280d0e07a7a8e72194
  title: Dynamic Memory Networks for Visual and Textual Question Answering
- pid: 9653d5c2c7844347343d073bbedd96e05d52f69b
  title: Pointer Networks
- pid: f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97
  title: Recurrent Neural Network Regularization
- pid: fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5
  title: Neural Machine Translation by Jointly Learning to Align and Translate
- pid: 076fa8d095c37c657f2aff39cf90bc2ea883b7cb
  title: A maximum entropy approach to adaptive statistical language modelling
- pid: 44d2abe2175df8153f465f6c39b68b76a0d40ab9
  title: Long Short-Term Memory
- pid: 0b44fcbeea9415d400c5f5789d6b892b6f98daff
  title: 'Building a Large Annotated Corpus of English: The Penn Treebank'
- pid: 533ee188324b833e059cb59b654e6160776d5812
  title: How to Construct Deep Recurrent Neural Networks
- pid: 84069287da0a6b488b8c933f3cb5be759cb6237e
  title: On the difficulty of training recurrent neural networks
- pid: 4ee2eab4c298c1824a9fb8799ad8eed21be38d21
  title: 'Moses: Open Source Toolkit for Statistical Machine Translation'
- pid: 99e8d34817ae10d7304521e89c5fbf908b9d856b
  title: 'Open Source Toolkit for Statistical Machine Translation: Factored Translation
    Models and Lattice Decoding'
slug: Pointer-Sentinel-Mixture-Models-Merity-Xiong
title: Pointer Sentinel Mixture Models
url: https://www.semanticscholar.org/paper/Pointer-Sentinel-Mixture-Models-Merity-Xiong/efbd381493bb9636f489b965a2034d529cd56bcd?sort=total-citations
venue: ICLR
year: 2017
