authors:
- T. Minka
badges:
- id: OPEN_ACCESS
corpusId: 8632802
fieldsOfStudy:
- Computer Science
numCitedBy: 986
numCiting: 126
paperAbstract: "One of the major obstacles to using Bayesian methods for pattern recognition\
  \ has been its computational expense. This thesis presents an approximation technique\
  \ that can perform Bayesian inference faster and more accurately than previously\
  \ possible. This method, \u201CExpectation Propagation,\u201D unifies and generalizes\
  \ two previous techniques: assumed-density filtering, an extension of the Kalman\
  \ filter, and loopy belief propagation, an extension of belief propagation in Bayesian\
  \ networks. The unification shows how both of these algorithms can be viewed as\
  \ approximating the true posterior distribution with simpler distribution, which\
  \ is close in the sense of KL-divergence. Expectation Propagation exploits the best\
  \ of both algorithms: the generality of assumed-density filtering and the accuracy\
  \ of loopy belief propagation. \nLoopy belief propagation, because it propagates\
  \ exact belief states, is useful for limited types of belief networks, such as purely\
  \ discrete networks. Expectation Propagation approximates the belief states with\
  \ expectations, such as means and variances, giving it much wider scope. Expectation\
  \ Propagation also extends belief propagation in the opposite direction\u2014propagating\
  \ richer belief states which incorporate correlations between variables. \nThis\
  \ framework is demonstrated in a variety of statistical models using synthetic and\
  \ real-world data. On Gaussian mixture problems, Expectation Propagation is found,\
  \ for the same amount of computation, to be convincingly better than rival approximation\
  \ techniques: Monte Carlo, Laplace's method, and variational Bayes. For pattern\
  \ recognition, Expectation Propagation provides an algorithm for training Bayes\
  \ Point Machine classifiers that is faster and more accurate than any previously\
  \ known. The resulting classifiers outperform Support Vector Machines on several\
  \ standard datasets, in addition to having a comparable training time. Expectation\
  \ Propagation can also be used to choose an appropriate feature set for classification,\
  \ via Bayesian model selection. (Copies available exclusively from MIT Libraries,\
  \ Rm. 14-0551, Cambridge, MA 02139-4307. Ph. 617-253-5668; Fax 617-253-1690.)"
ref_count: 127
references:
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1774
  pid: 19908640236767427ebf0524dc3a4bb09d65145e
  title: 'Loopy Belief Propagation for Approximate Inference: An Empirical Study'
  year: 1999
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1127
  pid: b2799fd1254689eec52f86daf3668a5aac3ea943
  title: Generalized Belief Propagation
  year: 2000
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 633
  pid: 9b4f8e1b4d781e3b47b72724d2e0c50fad87e464
  title: Inferring Parameters and Structure of Latent Variable Models by Variational
    Bayes
  year: 1999
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 493
  pid: a87f270ac2c8420db2669e5e12abb6aff0755115
  title: Propagation of Probabilities, Means, and Variances in Mixed Graphical Association
    Models
  year: 1992
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 896
  pid: 3ce9da2d2182a2fbc4b460bdb56d3c34110b3e39
  title: Probable networks and plausible predictions - a review of practical Bayesian
    methods for supervised neural networks
  year: 1995
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 4132
  pid: 08c370eb9ba13bfb836349e7f3ea428be4697818
  title: Factor graphs and the sum-product algorithm
  year: 2001
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 7053
  pid: 8d990deca66c9afefbe042f95e41ada0c7227877
  title: Sampling-Based Approaches to Calculating Marginal Densities
  year: 1990
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 370
  pid: b8871254256b95f52fe6a2c0edeee0fa706c1117
  title: 'A Revolution: Belief Propagation in Graphs with Cycles'
  year: 1997
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 934
  pid: 25c9f33aceac6dcff357727cbe2faf145b01d13c
  title: Keeping the neural networks simple by minimizing the description length of
    the weights
  year: 1993
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 873
  pid: 8198e70878c907e1bd05e7a3fa4280d8c338df60
  title: Semi-Supervised Support Vector Machines
  year: 1998
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 13446
  pid: e068be31ded63600aea068eacd12931efd2a1029
  title: UCI Repository of machine learning databases
  year: 1998
slug: A-family-of-algorithms-for-approximate-Bayesian-Minka
title: A family of algorithms for approximate Bayesian inference
url: https://www.semanticscholar.org/paper/A-family-of-algorithms-for-approximate-Bayesian-Minka/bf527ca11d7d81a15ff5b5603374a4e9d53b55b6?sort=total-citations
venue: ''
year: 2001
