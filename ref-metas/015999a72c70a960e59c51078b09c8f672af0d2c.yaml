authors:
- P. Bartlett
badges:
- id: OPEN_ACCESS
corpusId: 685382
fieldsOfStudy:
- Computer Science
numCitedBy: 1198
numCiting: 55
paperAbstract: 'Sample complexity results from computational learning theory, when
  applied to neural network learning for pattern classification problems, suggest
  that for good generalization performance the number of training examples should
  grow at least linearly with the number of adjustable parameters in the network.
  Results in this paper show that if a large neural network is used for a pattern
  classification problem and the learning algorithm finds a network with small weights
  that has small squared error on the training patterns, then the generalization performance
  depends on the size of the weights rather than the number of weights. For example,
  consider a two-layer feedforward network of sigmoid units, in which the sum of the
  magnitudes of the weights associated with each unit is bounded by A and the input
  dimension is n. We show that the misclassification probability is no more than a
  certain error estimate (that is related to squared error on the training set) plus
  A/sup 3/ /spl radic/((log n)/m) (ignoring log A and log m factors), where m is the
  number of training patterns. This may explain the generalization performance of
  neural networks, particularly when the number of training examples is considerably
  smaller than the number of weights. It also supports heuristics (such as weight
  decay and early stopping) that attempt to keep the weights small during training.
  The proof techniques appear to be useful for the analysis of other pattern classifiers:
  when the input domain is a totally bounded metric space, we use the same approach
  to give upper bounds on misclassification probability for classifiers with decision
  boundaries that are far from the training examples.'
ref_count: 55
references:
- pid: 25406e6733a698bfc4ac836f8e74f458e75dad4f
  title: What Size Net Gives Valid Generalization?
- pid: 04113e8974341f97258800126d05fd8df2751b7e
  title: Universal approximation bounds for superpositions of a sigmoidal function
- pid: 2599131a4bc2fa957338732a37c744cfe3e17b24
  title: A training algorithm for optimal margin classifiers
- pid: 4d19272112b50547614479a0c409fca66e3b05f7
  title: 'Boosting the margin: A new explanation for the effectiveness of voting methods'
- pid: fedfc9fbcfe46d50b81078560bce724678f90176
  title: Decision Theoretic Generalizations of the PAC Model for Neural Net and Other
    Learning Applications
- pid: 1f5a3dc5867218b86ab29cbf0046f2a02ee6ded5
  title: Structural Risk Minimization Over Data-Dependent Hierarchies
- pid: ccf5208521cb8c35f50ee8873df89294b8ed7292
  title: A decision-theoretic generalization of on-line learning and an application
    to boosting
- pid: 68c1bfe375dde46777fe1ac8f3636fb651e3f0f8
  title: Experiments with a New Boosting Algorithm
- pid: e07c3df9d8d53c3be8cb9e982da98a4471322d90
  title: Scale-sensitive dimensions, uniform convergence, and learnability
- pid: b83396caf4762c906530c9219a9e4dd0658232b0
  title: A general lower bound on the number of examples needed for learning
- pid: 6c0cbbd275bb43e09f0527a31ddd61824eca295b
  title: Introduction to the theory of neural computation
- pid: d474299d7a51b89a1d7394d426cf881a89b8013d
  title: Efficient distribution-free learning of probabilistic concepts
- pid: 43fcdee6c6d885ac2bd32e122dbf282f93720c22
  title: A Probabilistic Theory of Pattern Recognition
- pid: e0b8fa3496283d4d808fba9ff62d5f024bcf23be
  title: Learnability and the Vapnik-Chervonenkis dimension
- pid: c66db9b93d75c0ff52e7f84605b8389345307006
  title: Probability Inequalities for sums of Bounded Random Variables
- pid: 04a5241e96d0e7ac63eb44ed8cfbcdcda9df1583
  title: IEEE TRANSACTIONS ON INFORMATION THEORY
- pid: 01a1d065a5292be740e75029622a3ab5e71e3150
  title: Convergence of stochastic processes
- pid: 67f9e3de2fb39f051ef23b8fbed6d72de7b02900
  title: A framework for structural risk minimisation
- pid: a36b028d024bf358c4af1a5e1dc3ca0aed23b553
  title: 'Chervonenkis: On the uniform convergence of relative frequencies of events
    to their probabilities'
slug: The-Sample-Complexity-of-Pattern-Classification-The-Bartlett
title: 'The Sample Complexity of Pattern Classification with Neural Networks: The
  Size of the Weights is More Important than the Size of the Network'
url: https://www.semanticscholar.org/paper/The-Sample-Complexity-of-Pattern-Classification-The-Bartlett/015999a72c70a960e59c51078b09c8f672af0d2c?sort=total-citations
venue: IEEE Trans. Inf. Theory
year: 1998
