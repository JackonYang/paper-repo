authors:
- P. Bartlett
badges:
- id: OPEN_ACCESS
corpusId: 685382
fieldsOfStudy:
- Computer Science
numCitedBy: 1198
numCiting: 55
paperAbstract: 'Sample complexity results from computational learning theory, when
  applied to neural network learning for pattern classification problems, suggest
  that for good generalization performance the number of training examples should
  grow at least linearly with the number of adjustable parameters in the network.
  Results in this paper show that if a large neural network is used for a pattern
  classification problem and the learning algorithm finds a network with small weights
  that has small squared error on the training patterns, then the generalization performance
  depends on the size of the weights rather than the number of weights. For example,
  consider a two-layer feedforward network of sigmoid units, in which the sum of the
  magnitudes of the weights associated with each unit is bounded by A and the input
  dimension is n. We show that the misclassification probability is no more than a
  certain error estimate (that is related to squared error on the training set) plus
  A/sup 3/ /spl radic/((log n)/m) (ignoring log A and log m factors), where m is the
  number of training patterns. This may explain the generalization performance of
  neural networks, particularly when the number of training examples is considerably
  smaller than the number of weights. It also supports heuristics (such as weight
  decay and early stopping) that attempt to keep the weights small during training.
  The proof techniques appear to be useful for the analysis of other pattern classifiers:
  when the input domain is a totally bounded metric space, we use the same approach
  to give upper bounds on misclassification probability for classifiers with decision
  boundaries that are far from the training examples.'
ref_count: 55
references:
- fieldsOfStudy:
  - Mathematics
  - Computer Science
  numCitedBy: 1696
  pid: 25406e6733a698bfc4ac836f8e74f458e75dad4f
  title: What Size Net Gives Valid Generalization?
  year: 1989
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 2593
  pid: 04113e8974341f97258800126d05fd8df2751b7e
  title: Universal approximation bounds for superpositions of a sigmoidal function
  year: 1993
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 10840
  pid: 2599131a4bc2fa957338732a37c744cfe3e17b24
  title: A training algorithm for optimal margin classifiers
  year: 1992
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 2844
  pid: 4d19272112b50547614479a0c409fca66e3b05f7
  title: 'Boosting the margin: A new explanation for the effectiveness of voting methods'
  year: 1997
- fieldsOfStudy:
  - Mathematics
  - Computer Science
  numCitedBy: 979
  pid: fedfc9fbcfe46d50b81078560bce724678f90176
  title: Decision Theoretic Generalizations of the PAC Model for Neural Net and Other
    Learning Applications
  year: 1992
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 619
  pid: 1f5a3dc5867218b86ab29cbf0046f2a02ee6ded5
  title: Structural Risk Minimization Over Data-Dependent Hierarchies
  year: 1998
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 13124
  pid: ccf5208521cb8c35f50ee8873df89294b8ed7292
  title: A decision-theoretic generalization of on-line learning and an application
    to boosting
  year: 1995
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 8626
  pid: 68c1bfe375dde46777fe1ac8f3636fb651e3f0f8
  title: Experiments with a New Boosting Algorithm
  year: 1996
- fieldsOfStudy:
  - Mathematics
  numCitedBy: 412
  pid: e07c3df9d8d53c3be8cb9e982da98a4471322d90
  title: Scale-sensitive dimensions, uniform convergence, and learnability
  year: 1997
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 499
  pid: b83396caf4762c906530c9219a9e4dd0658232b0
  title: A general lower bound on the number of examples needed for learning
  year: 1988
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 6517
  pid: 6c0cbbd275bb43e09f0527a31ddd61824eca295b
  title: Introduction to the theory of neural computation
  year: 1991
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 217
  pid: d474299d7a51b89a1d7394d426cf881a89b8013d
  title: Efficient distribution-free learning of probabilistic concepts
  year: 1990
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 3565
  pid: 43fcdee6c6d885ac2bd32e122dbf282f93720c22
  title: A Probabilistic Theory of Pattern Recognition
  year: 1996
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1909
  pid: e0b8fa3496283d4d808fba9ff62d5f024bcf23be
  title: Learnability and the Vapnik-Chervonenkis dimension
  year: 1989
- fieldsOfStudy:
  - Mathematics
  numCitedBy: 8037
  pid: c66db9b93d75c0ff52e7f84605b8389345307006
  title: Probability Inequalities for sums of Bounded Random Variables
  year: 1963
- fieldsOfStudy:
  - Mathematics
  numCitedBy: 1598
  pid: 04a5241e96d0e7ac63eb44ed8cfbcdcda9df1583
  title: IEEE TRANSACTIONS ON INFORMATION THEORY
  year: 1998
- fieldsOfStudy:
  - Mathematics
  numCitedBy: 1852
  pid: 01a1d065a5292be740e75029622a3ab5e71e3150
  title: Convergence of stochastic processes
  year: 1984
- fieldsOfStudy:
  - Business
  numCitedBy: 101
  pid: 67f9e3de2fb39f051ef23b8fbed6d72de7b02900
  title: A framework for structural risk minimisation
  year: 1996
- fieldsOfStudy:
  - Mathematics
  numCitedBy: 3709
  pid: a36b028d024bf358c4af1a5e1dc3ca0aed23b553
  title: 'Chervonenkis: On the uniform convergence of relative frequencies of events
    to their probabilities'
  year: 1971
slug: The-Sample-Complexity-of-Pattern-Classification-The-Bartlett
title: 'The Sample Complexity of Pattern Classification with Neural Networks: The
  Size of the Weights is More Important than the Size of the Network'
url: https://www.semanticscholar.org/paper/The-Sample-Complexity-of-Pattern-Classification-The-Bartlett/015999a72c70a960e59c51078b09c8f672af0d2c?sort=total-citations
venue: IEEE Trans. Inf. Theory
year: 1998
