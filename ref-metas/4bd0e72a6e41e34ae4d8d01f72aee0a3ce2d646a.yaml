authors:
- Rafal Powalski
- "\u0141ukasz Borchmann"
- Dawid Jurkiewicz
- Tomasz Dwojak
- Michal Pietruszka
- "Gabriela Pa\u0142ka"
badges:
- id: OPEN_ACCESS
corpusId: 231951453
fieldsOfStudy:
- Computer Science
numCitedBy: 29
numCiting: 65
paperAbstract: We address the challenging problem of Natural Language Comprehension
  beyond plain-text documents by introducing the TILT neural network architecture
  which simultaneously learns layout information, visual features, and textual semantics.
  Contrary to previous approaches, we rely on a decoder capable of unifying a variety
  of problems involving natural language. The layout is represented as an attention
  bias and complemented with contextualized visual information, while the core of
  our model is a pretrained encoder-decoder Transformer. Our novel approach achieves
  state-of-the-art results in extracting information from documents and answering
  questions which demand layout understanding (DocVQA, CORD, SROIE). At the same time,
  we simplify the process by employing an end-to-end model.
ref_count: 64
references:
- pid: b40bfcf339de3f0dba08fabb2b58b9368ff4c51a
  title: 'DocVQA: A Dataset for VQA on Document Images'
- pid: 3cfb319689f06bf04c2e28399361f414ca32c4b3
  title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer
- pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  title: Attention is All you Need
- pid: bd86b4b551b9d3fb498f62008b037e7599365018
  title: Evaluation of deep convolutional nets for document image classification and
    retrieval
- pid: d00cbb0c05c1dc922126fe72c1078b773d01c688
  title: ICDAR2019 Competition on Scanned Receipt OCR and Information Extraction
- pid: 9e10e2cae05b2906330eb7dde2f27042966413b1
  title: Unifying Question Answering and Text Classification via Span Extraction
- pid: 9405cc0d6169988371b2755e573cc28650d14dfe
  title: Language Models are Unsupervised Multitask Learners
- pid: e551546e51c570fe796b67ae23d97297b0717921
  title: 'LAMBERT: Layout-Aware language Modeling using BERT for information extraction'
- pid: aa111c8920e963195968360f59c9de271ae470c2
  title: 'BROS: A PRE-TRAINED LANGUAGE MODEL'
- pid: 0197abda042e6de87b5f716caa708a6a459f078c
  title: 'LayoutLMv2: Multi-modal Pre-training for Visually-rich Document Understanding'
- pid: 20d0564fd3fdbc24f266ca2076826a2271c3ea08
  title: Spatial Dependency Parsing for Semi-Structured Document Information Extraction
- pid: 33eadd4e666a894306a22ba0839c5e0cef77280e
  title: 'TextCaps: a Dataset for Image Captioning with Reading Comprehension'
- pid: e2d2f64b3bb200c2c3db5ddc367b06311c369341
  title: 'Kleister: A novel task for Information Extraction involving Long Documents
    with Complex Layout'
- pid: 3465c06c872d8c48d628c5fc2d484087719351b6
  title: 'LayoutLM: Pre-training of Text and Layout for Document Image Understanding'
- pid: c69942bf1b4f75e53cb62d0c5126c1cb4a5aa7bc
  title: 'CORD: A Consolidated Receipt Dataset for Post-OCR Parsing'
- pid: 395de0bd3837fdf4b4b5e5f04835bcc69c279481
  title: 'BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation,
    Translation, and Comprehension'
- pid: 2fbda89395f993040b7665730c64182ade3be195
  title: 'BERTgrid: Contextualized Embedding for 2D Document Representation and Understanding'
- pid: 2527626c11a84f15709e943fbfa2356e19930e3b
  title: 'VL-BERT: Pre-training of Generic Visual-Linguistic Representations'
- pid: 5aec474c31a2f4b74703c6f786c0a8ff85c450da
  title: 'VisualBERT: A Simple and Performant Baseline for Vision and Language'
- pid: 4b3b0d8d5a24630c940049442a8d824534faf8b9
  title: ICDAR 2019 Competition on Scene Text Visual Question Answering
- pid: 58c793e278cdbf669a615b2c2479cd69ff785d63
  title: 'FUNSD: A Dataset for Form Understanding in Noisy Scanned Documents'
- pid: 17dbd7b72029181327732e4d11b52a08ed4630d0
  title: 'Natural Questions: A Benchmark for Question Answering Research'
- pid: 04df8c70257b5280b9d303502c9d7ddf946f181b
  title: Graph Convolution for Multimodal Information Extraction from Visually Rich
    Documents
- pid: af1f7739283bdbd2b7a94903041f6d6afd991907
  title: Towards VQA Models That Can Read
- pid: 9784fbf77295860b2e412137b86356d70b25e3c0
  title: 'The Natural Language Decathlon: Multitask Learning as Question Answering'
- pid: e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e
  title: 'Subword Regularization: Improving Neural Network Translation Models with
    Multiple Subword Candidates'
- pid: 45dd2a3cd7c27f2e9509b023d702408f5ac11c9d
  title: Stacked Cross Attention for Image-Text Matching
- pid: 7289a240c9425bc7cad87b3b835e5f0cac22f488
  title: 'DVQA: Understanding Data Visualizations via Question Answering'
- pid: 55ca9fe4ae98904bfe026d22dcf1420ff9c0dd86
  title: 'FigureQA: An Annotated Figure Dataset for Visual Reasoning'
- pid: 01a50b9662a59070c4ff53873453d8854c15ade1
  title: CloudScan - A Configuration-Free Invoice Analysis System Using Recurrent
    Neural Networks
- pid: 636a79420d838eabe4af7fb25d6437de45ab64e8
  title: 'RACE: Large-scale ReAding Comprehension Dataset From Examinations'
- pid: 832fc9327695f7425d8759c6aaeec0fa2d7b0a90
  title: 'WikiReading: A Novel Large-scale Language Understanding Task over Wikipedia'
- pid: 05dd7254b632376973f3a1b4d39485da17814df5
  title: 'SQuAD: 100,000+ Questions for Machine Comprehension of Text'
- pid: b724c3f7ff395235b62537203ddeb710f0eb27bb
  title: 'R-FCN: Object Detection via Region-based Fully Convolutional Networks'
- pid: 1af68821518f03568f913ab03fc02080247a27ff
  title: Neural Machine Translation of Rare Words with Subword Units
- pid: 452059171226626718eb677358836328f884298e
  title: 'Ask Me Anything: Dynamic Memory Networks for Natural Language Processing'
- pid: 6364fdaa0a0eccd823a779fcdd489173f938e91a
  title: 'U-Net: Convolutional Networks for Biomedical Image Segmentation'
slug: Going-Full-TILT-Boogie-on-Document-Understanding-Powalski-Borchmann
title: Going Full-TILT Boogie on Document Understanding with Text-Image-Layout Transformer
url: https://www.semanticscholar.org/paper/Going-Full-TILT-Boogie-on-Document-Understanding-Powalski-Borchmann/4bd0e72a6e41e34ae4d8d01f72aee0a3ce2d646a?sort=total-citations
venue: ICDAR
year: 2021
