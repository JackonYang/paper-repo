authors:
- E. Baum
- F. Wilczek
badges:
- id: OPEN_ACCESS
corpusId: 10578219
fieldsOfStudy:
- Computer Science
numCitedBy: 187
numCiting: 12
paperAbstract: We propose that the back propagation algorithm for supervised learning
  can be generalized, put on a satisfactory conceptual footing, and very likely made
  more efficient by defining the values of the output and input neurons as probabilities
  and varying the synaptic weights in the gradient direction of the log likelihood,
  rather than the 'error'.
ref_count: 12
references:
- fieldsOfStudy:
  - Biology
  numCitedBy: 19356
  pid: 111fd833a4ae576cfdbb27d87d2f8fc0640af355
  title: Learning internal representations by error propagation
  year: 1986
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 16926
  pid: b07ce649d6f6eb636872527104b0209d3edc8188
  title: Pattern classification and scene analysis
  year: 1973
- fieldsOfStudy:
  - Biology
  numCitedBy: 1904
  pid: 9b486c647916df9f8be0f8d4fc5c94c493bfaa80
  title: PRINCIPLES OF NEURODYNAMICS. PERCEPTRONS AND THE THEORY OF BRAIN MECHANISMS
  year: 1963
- fieldsOfStudy:
  - Biology
  numCitedBy: 4468
  pid: 56623a496727d5c71491850e04512ddf4152b487
  title: 'Beyond Regression : "New Tools for Prediction and Analysis in the Behavioral
    Sciences'
  year: 1974
slug: Supervised-Learning-of-Probability-Distributions-by-Baum-Wilczek
title: Supervised Learning of Probability Distributions by Neural Networks
url: https://www.semanticscholar.org/paper/Supervised-Learning-of-Probability-Distributions-by-Baum-Wilczek/7d9ed799fcc2ba2f929532a4f403091198bcfd83?sort=total-citations
venue: NIPS
year: 1987
