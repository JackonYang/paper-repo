authors:
- E. Baum
- F. Wilczek
badges:
- id: OPEN_ACCESS
corpusId: 10578219
fieldsOfStudy:
- Computer Science
numCitedBy: 187
numCiting: 12
paperAbstract: We propose that the back propagation algorithm for supervised learning
  can be generalized, put on a satisfactory conceptual footing, and very likely made
  more efficient by defining the values of the output and input neurons as probabilities
  and varying the synaptic weights in the gradient direction of the log likelihood,
  rather than the 'error'.
ref_count: 12
references:
- pid: 111fd833a4ae576cfdbb27d87d2f8fc0640af355
  title: Learning internal representations by error propagation
- pid: b07ce649d6f6eb636872527104b0209d3edc8188
  title: Pattern classification and scene analysis
- pid: 9b486c647916df9f8be0f8d4fc5c94c493bfaa80
  title: PRINCIPLES OF NEURODYNAMICS. PERCEPTRONS AND THE THEORY OF BRAIN MECHANISMS
- pid: 56623a496727d5c71491850e04512ddf4152b487
  title: 'Beyond Regression : "New Tools for Prediction and Analysis in the Behavioral
    Sciences'
slug: Supervised-Learning-of-Probability-Distributions-by-Baum-Wilczek
title: Supervised Learning of Probability Distributions by Neural Networks
url: https://www.semanticscholar.org/paper/Supervised-Learning-of-Probability-Distributions-by-Baum-Wilczek/7d9ed799fcc2ba2f929532a4f403091198bcfd83?sort=total-citations
venue: NIPS
year: 1987
