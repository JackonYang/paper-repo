authors:
- Samy Bengio
- Oriol Vinyals
- Navdeep Jaitly
- Noam M. Shazeer
badges:
- id: OPEN_ACCESS
corpusId: 1820089
fieldsOfStudy:
- Computer Science
numCitedBy: 1414
numCiting: 29
paperAbstract: Recurrent Neural Networks can be trained to produce sequences of tokens
  given some input, as exemplified by recent results in machine translation and image
  captioning. The current approach to training them consists of maximizing the likelihood
  of each token in the sequence given the current (recurrent) state and the previous
  token. At inference, the unknown previous token is then replaced by a token generated
  by the model itself. This discrepancy between training and inference can yield errors
  that can accumulate quickly along the generated sequence. We propose a curriculum
  learning strategy to gently change the training process from a fully guided scheme
  using the true previous token, towards a less guided scheme which mostly uses the
  generated token instead. Experiments on several sequence prediction tasks show that
  this approach yields significant improvements. Moreover, it was used succesfully
  in our winning entry to the MSCOCO image captioning challenge, 2015.
ref_count: 29
references:
- pid: cea967b59209c6be22829699f05b8b1ac4dc092d
  title: Sequence to Sequence Learning with Neural Networks
- pid: d0be39ee052d246ae99c082a565aba25b811be2d
  title: Learning long-term dependencies with gradient descent is difficult
- pid: 54b2b6f35f1b5704dddfaa3a137a2f4ad3dfe745
  title: Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)
- pid: d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0
  title: 'Show and tell: A neural image caption generator'
- pid: f01fc808592ea7c473a69a6e7484040a435f36d9
  title: Long-term recurrent convolutional networks for visual recognition and description
- pid: fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5
  title: Neural Machine Translation by Jointly Learning to Align and Translate
- pid: 47d2dc34e1d02a8109f5c04bb6939725de23716d
  title: 'End-to-end Continuous Speech Recognition using Attention-based Recurrent
    NN: First Results'
- pid: 44d2abe2175df8153f465f6c39b68b76a0d40ab9
  title: Long Short-Term Memory
- pid: 4d376d6978dad0374edfa6709c9556b42d3594d3
  title: 'Batch Normalization: Accelerating Deep Network Training by Reducing Internal
    Covariate Shift'
- pid: 79ab3c49903ec8cb339437ccf5cf998607fc313e
  title: A Reduction of Imitation Learning and Structured Prediction to No-Regret
    Online Learning
- pid: 2e36ea91a3c8fbff92be2989325531b4002e2afc
  title: Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models
- pid: 8de174ab5419b9d3127695405efd079808e956e8
  title: Curriculum learning
- pid: 15f102c3c9f4d4fe6ba105e221df48c6e8902b3b
  title: From captions to visual concepts and back
- pid: 55e022fb7581bb9e1fce678d21fb25ffbb3fbb88
  title: Deep Visual-Semantic Alignments for Generating Image Descriptions
- pid: 41828fc3dab24784f95e6976e8aaa73f68e1840e
  title: Incremental Parsing with the Perceptron Algorithm
- pid: f4ba954b0412773d047dc41231c733de0c1f4926
  title: 'Conditional Random Fields: Probabilistic Models for Segmenting and Labeling
    Sequence Data'
- pid: 47570e7f63e296f224a0e7f9a0d08b0de3cbaf40
  title: Grammar as a Foreign Language
- pid: 3c9d9f3c6f7508f4e29730924529dc993c27cddc
  title: Search-based structured prediction
- pid: 3a1a2cff2b70fb84a7ca7d97f8adcc5855851795
  title: The Kaldi Speech Recognition Toolkit
- pid: 258986132bf17755fe8263e42429fe73218c1534
  title: 'CIDEr: Consensus-based image description evaluation'
- pid: 71b7178df5d2b112d07e45038cb5637208659ff7
  title: 'Microsoft COCO: Common Objects in Context'
- pid: e54d8b07ef659f9ee2671441c4355e414e408836
  title: 'OntoNotes: The 90% Solution'
slug: Scheduled-Sampling-for-Sequence-Prediction-with-Bengio-Vinyals
title: Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks
url: https://www.semanticscholar.org/paper/Scheduled-Sampling-for-Sequence-Prediction-with-Bengio-Vinyals/df137487e20ba7c6e1e2b9a1e749f2a578b5ad99?sort=total-citations
venue: NIPS
year: 2015
