authors:
- T. Schaul
- Sixin Zhang
- Yann LeCun
badges:
- id: OPEN_ACCESS
corpusId: 5886221
fieldsOfStudy:
- Computer Science
numCitedBy: 410
numCiting: 27
paperAbstract: The performance of stochastic gradient descent (SGD) depends critically
  on how learning rates are tuned and decreased over time. We propose a method to
  automatically adjust multiple learning rates so as to minimize the expected error
  at any one time. The method relies on local gradient variations across samples.
  In our approach, learning rates can increase as well as decrease, making it suitable
  for non-stationary problems. Using a number of convex and non-convex learning tasks,
  we show that the resulting algorithm matches the performance of SGD or other adaptive
  approaches with their best settings obtained through systematic search, and effectively
  removes the need for learning rate tuning.
ref_count: 27
references:
- pid: 413c1142de9d91804d6d11c67ff3fed59c9fc279
  title: Adaptive Subgradient Methods for Online Learning and Stochastic Optimization
- pid: 6ed460701019072ee2e364a1a491f73dd931f27f
  title: Topmoumoute Online Natural Gradient Algorithm
- pid: b44ff78214ccd975ce16fbbc333423ca78d99141
  title: 'SGD-QN: Careful Quasi-Newton Stochastic Gradient Descent'
- pid: b71ac1e9fb49420d13e084ac67254a0bbd40f83f
  title: Understanding the difficulty of training deep feedforward neural networks
- pid: 5936754b5762260bf102ac95d7b26cfc9d31956a
  title: The Tradeoffs of Large Scale Learning
- pid: 34ddd8865569c2c32dec9bf7ffc817ff42faaa01
  title: A Stochastic Approximation Method
- pid: ffa94bba647817fa5e8f8d3250fc977435b5ca76
  title: Fast Curvature Matrix-Vector Products for Second-Order Gradient Descent
- pid: 5d90f06bb70a0a3dced62413346235c02b1aa086
  title: Learning Multiple Layers of Features from Tiny Images
- pid: 990b10ce4ef643e148b6c719e99dbf2430671a74
  title: Adaptive Algorithms and Stochastic Approximations
- pid: b87274e6d9aa4e6ba5148898aa92941617d2b6ed
  title: Efficient BackProp
slug: No-more-pesky-learning-rates-Schaul-Zhang
title: No more pesky learning rates
url: https://www.semanticscholar.org/paper/No-more-pesky-learning-rates-Schaul-Zhang/e5a685f40338f9c2f3e68e142efa217aad16dd56?sort=total-citations
venue: ICML
year: 2013
