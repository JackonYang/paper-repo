authors:
- Radford M. Neal
- Geoffrey E. Hinton
badges: []
corpusId: 17947141
fieldsOfStudy:
- Mathematics
- Computer Science
numCitedBy: 2597
numCiting: 13
paperAbstract: The EM algorithm performs maximum likelihood estimation for data in
  which some variables are unobserved. We present a function that resembles negative
  free energy and show that the M step maximizes this function with respect to the
  model parameters and the E step maximizes it with respect to the distribution over
  the unobserved variables. From this perspective, it is easy to justify an incremental
  variant of the EM algorithm in which the distribution for only one of the unobserved
  variables is recalculated in each E step. This variant is shown empirically to give
  faster convergence in a mixture estimation problem. A variant of the algorithm that
  exploits sparse conditional distributions is also described, and a wide range of
  other variant algorithms are also seen to be possible.
ref_count: 13
references:
- pid: 59fa47fc237a0781b4bf1c84fedb728d20db26a1
  title: 'Soft competitive adaptation: neural network learning algorithms based on
    fitting statistical mixtures'
- pid: d36efb9ad91e00faa334b549ce989bfae7e2907a
  title: Maximum likelihood from incomplete data via the EM - algorithm plus discussions
    on the paper
slug: A-View-of-the-Em-Algorithm-that-Justifies-Sparse,-Neal-Hinton
title: A View of the Em Algorithm that Justifies Incremental, Sparse, and other Variants
url: https://www.semanticscholar.org/paper/A-View-of-the-Em-Algorithm-that-Justifies-Sparse,-Neal-Hinton/9f87a11a523e4680e61966e36ea2eac516096f23?sort=total-citations
venue: Learning in Graphical Models
year: 1998
