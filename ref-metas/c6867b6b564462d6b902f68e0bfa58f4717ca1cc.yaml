authors:
- Barak A. Pearlmutter
badges:
- id: OPEN_ACCESS
corpusId: 1251969
fieldsOfStudy:
- Computer Science
numCitedBy: 586
numCiting: 46
paperAbstract: Just storing the Hessian H (the matrix of second derivatives 2E/wiwj
  of the error E with respect to each pair of weights) of a large neural network is
  difficult. Since a common use of a large matrix like H is to compute its product
  with various vectors, we derive a technique that directly calculates Hv, where v
  is an arbitrary vector. To calculate Hv, we first define a differential operator
  Rv{f(w)} = (/r)f(w rv)|r=0, note that Rv{w} = Hv and Rv{w} = v, and then apply Rv{}
  to the equations used to compute w. The result is an exact and numerically stable
  procedure for computing Hv, which takes about as much computation, and is about
  as local, as a gradient evaluation. We then apply the technique to a one pass gradient
  calculation algorithm (backpropagation), a relaxation gradient calculation algorithm
  (recurrent backpropagation), and two stochastic gradient calculation algorithms
  (Boltzmann machines and weight perturbation). Finally, we show that this technique
  can be used at the heart of many iterative techniques for computing various properties
  of H, obviating any need to calculate the full Hessian.
ref_count: 46
references:
- pid: 2a1e1da81b535e1bead3fc2ab6af8b07877823b9
  title: Exact Calculation of the Hessian Matrix for the Multilayer Perceptron
- pid: 2f4a097b2131784d7ac3fc3c47d1e9283e9ac207
  title: A scaled conjugate gradient algorithm for fast supervised learning
- pid: 934e49dac717a924bfda841bf6e54c32e900f0d1
  title: 'Learning Algorithms for Connectionist Networks: Applied Gradient Methods
    of Nonlinear Optimization'
- pid: 3e9229dd827dda0d462dffbdec7fdf50b724d587
  title: The Eigenvalues of Mega-dimensional Matrices
- pid: a42954d4b9d0ccdf1036e0af46d87a01b94c3516
  title: 'Second Order Derivatives for Network Pruning: Optimal Brain Surgeon'
- pid: c3ecd8e19e016d15670c8953b4b9afaa5186b0f3
  title: Training with Noise is Equivalent to Tikhonov Regularization
- pid: a0d16f0e99f7ce5e6fb70b1a68c685e9ad610657
  title: A Learning Algorithm for Boltzmann Machines
- pid: 2cc3b3a2036c35cb69f9990b86bb5b3b26879434
  title: Bayesian Regularization and Pruning Using a Laplace Prior
- pid: 7e0dab4fe4299bc2f8b4b18f82702af717cf3924
  title: 'The Effective Number of Parameters: An Analysis of Generalization and Regularization
    in Nonlinear Learning Systems'
- pid: a57c6d627ffc667ae3547073876c35d6420accff
  title: Connectionist Learning Procedures
- pid: b0f2433c088591d265891231f1c22424047f1bc1
  title: A Practical Bayesian Framework for Backprop Networks
- pid: b959164d1efca4b73986ba5d21e664aadbbc0457
  title: A Practical Bayesian Framework for Backpropagation Networks
- pid: e7297db245c3feb1897720b173a59fe7e36babb7
  title: Optimal Brain Damage
- pid: 6602985bd326d9996c68627b56ed389e2c90fd08
  title: Generalization of back-propagation to recurrent neural networks.
- pid: 69d7086300e7f5322c06f2f242a565b3a182efb5
  title: In Advances in Neural Information Processing Systems
- pid: 56623a496727d5c71491850e04512ddf4152b487
  title: 'Beyond Regression : "New Tools for Prediction and Analysis in the Behavioral
    Sciences'
- pid: 589d377b23e2bdae7ad161b36a5d6613bcfccdde
  title: Improving the convergence of back-propagation learning with second-order
    methods
- pid: 8be3f21ab796bd9811382b560507c1c679fae37f
  title: A learning rule for asynchronous perceptrons with feedback in a combinatorial
    environment
slug: Fast-Exact-Multiplication-by-the-Hessian-Pearlmutter
title: Fast Exact Multiplication by the Hessian
url: https://www.semanticscholar.org/paper/Fast-Exact-Multiplication-by-the-Hessian-Pearlmutter/c6867b6b564462d6b902f68e0bfa58f4717ca1cc?sort=total-citations
venue: Neural Computation
year: 1994
