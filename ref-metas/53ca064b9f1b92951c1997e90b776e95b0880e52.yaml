authors:
- A. Mnih
- K. Kavukcuoglu
badges:
- id: OPEN_ACCESS
corpusId: 14992849
fieldsOfStudy:
- Computer Science
numCitedBy: 538
numCiting: 19
paperAbstract: "Continuous-valued word embeddings learned by neural language models\
  \ have recently been shown to capture semantic and syntactic information about words\
  \ very well, setting performance records on several word similarity tasks. The best\
  \ results are obtained by learning high-dimensional embeddings from very large quantities\
  \ of data, which makes scalability of the training method a critical factor. \n\
  \ \nWe propose a simple and scalable new approach to learning word embeddings based\
  \ on training log-bilinear models with noise-contrastive estimation. Our approach\
  \ is simpler, faster, and produces better results than the current state-of-the-art\
  \ method. We achieve results comparable to the best ones reported, which were obtained\
  \ on a cluster, using four times less data and more than an order of magnitude less\
  \ computing time. We also investigate several model types and find that the embeddings\
  \ learned by the simpler models perform at least as well as those learned by the\
  \ more complex ones."
ref_count: 20
references:
- pid: 2b669398c4cf2ebe04375c8b1beae20f4ac802fa
  title: Improving Word Representations via Global Context and Multiple Word Prototypes
- pid: dac72f2c509aee67524d3321f77e97e8eff51de6
  title: 'Word Representations: A Simple and General Method for Semi-Supervised Learning'
- pid: 330da625c15427c6e42ccfa3b747fb29e5835bf0
  title: Efficient Estimation of Word Representations in Vector Space
- pid: 5b0d644f5c4b9880cbaf79932c0a4fa98996f068
  title: A fast and simple algorithm for training neural probabilistic language models
- pid: c4fd9c86b2b41df51a6fe212406dda81b1997fd4
  title: Linguistic Regularities in Continuous Space Word Representations
- pid: 6c2b28f9354f667cd5bd07afc0471d8334430da7
  title: A Neural Probabilistic Language Model
- pid: 699d5ab38deee78b1fd17cc8ad233c74196d16e9
  title: Adaptive Importance Sampling to Accelerate Training of a Neural Probabilistic
    Language Model
- pid: 57458bc1cffe5caa45a885af986d70f723f406b4
  title: 'A unified architecture for natural language processing: deep neural networks
    with multitask learning'
- pid: a9fc84f8abe740cdc7ee82e69444d1d00dbe0ceb
  title: A Scalable Hierarchical Distributed Language Model
- pid: c19fbefdeead6a4154a22a9c8551a18b1530033a
  title: Hierarchical Probabilistic Neural Network Language Model
- pid: fac2ca048fdd7e848f0b9ba2f7be25bb49186770
  title: The Microsoft Research Sentence Completion Challenge
- pid: bd7d93193aad6c4b71cc8942e808753019e87706
  title: Three new graphical models for statistical language modelling
- pid: 9819b600a828a57e1cde047bbe710d3446b30da5
  title: Recurrent neural network based language model
- pid: 413c1142de9d91804d6d11c67ff3fed59c9fc279
  title: Adaptive Subgradient Methods for Online Learning and Stochastic Optimization
- pid: 9c0ddf74f87d154db88d79c640578c1610451eec
  title: Parsing Natural Scenes and Natural Language with Recursive Neural Networks
- pid: 1521ddb27860cc8834f8a82e62665bf983c8ad2c
  title: 'The Word-Space Model : Using distributional analysis to represent syntagmatic
    and paradigmatic relations between words in high-dimensional vector spaces'
- pid: 6b388f0151ab37adb3d57738b8f52a3f943f86c8
  title: Quick Training of Probabilistic Neural Nets by Importance Sampling
slug: Learning-word-embeddings-efficiently-with-Mnih-Kavukcuoglu
title: Learning word embeddings efficiently with noise-contrastive estimation
url: https://www.semanticscholar.org/paper/Learning-word-embeddings-efficiently-with-Mnih-Kavukcuoglu/53ca064b9f1b92951c1997e90b776e95b0880e52?sort=total-citations
venue: NIPS
year: 2013
