authors:
- Ilya Sutskever
- Oriol Vinyals
- Quoc V. Le
badges:
- id: OPEN_ACCESS
corpusId: 7961699
fieldsOfStudy:
- Computer Science
numCitedBy: 14881
numCiting: 55
paperAbstract: Deep Neural Networks (DNNs) are powerful models that have achieved
  excellent performance on difficult learning tasks. Although DNNs work well whenever
  large labeled training sets are available, they cannot be used to map sequences
  to sequences. In this paper, we present a general end-to-end approach to sequence
  learning that makes minimal assumptions on the sequence structure. Our method uses
  a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector
  of a fixed dimensionality, and then another deep LSTM to decode the target sequence
  from the vector. Our main result is that on an English to French translation task
  from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score
  of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary
  words. Additionally, the LSTM did not have difficulty on long sentences. For comparison,
  a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When
  we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT
  system, its BLEU score increases to 36.5, which is close to the previous state of
  the art. The LSTM also learned sensible phrase and sentence representations that
  are sensitive to word order and are relatively invariant to the active and the passive
  voice. Finally, we found that reversing the order of the words in all source sentences
  (but not target sentences) improved the LSTM's performance markedly, because doing
  so introduced many short term dependencies between the source and the target sentence
  which made the optimization problem easier.
ref_count: 55
references:
- pid: 7b4f3d0e4e2486a8d5d3f8e00549cf9a117bf88f
  title: Sequence Transduction with Recurrent Neural Networks
- pid: f9a1b3850dfd837793743565a8af95973d395a4e
  title: LSTM Neural Networks for Language Modeling
- pid: fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5
  title: Neural Machine Translation by Jointly Learning to Align and Translate
- pid: 6c2b28f9354f667cd5bd07afc0471d8334430da7
  title: A Neural Probabilistic Language Model
- pid: 96494e722f58705fa20302fe6179d483f52705b4
  title: 'Connectionist temporal classification: labelling unsegmented sequence data
    with recurrent neural networks'
- pid: 6658bbf68995731b2083195054ff45b4eca38b3a
  title: Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech
    Recognition
- pid: 0b544dfe355a5070b60986319a3f51fb45d1348e
  title: "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical\
    \ Machine Translation"
- pid: d0be39ee052d246ae99c082a565aba25b811be2d
  title: Learning long-term dependencies with gradient descent is difficult
- pid: 96364af2d208ea75ca3aeb71892d2f7ce7326b55
  title: Statistical Language Models Based on Neural Networks
- pid: 9819b600a828a57e1cde047bbe710d3446b30da5
  title: Recurrent neural network based language model
- pid: 44d2abe2175df8153f465f6c39b68b76a0d40ab9
  title: Long Short-Term Memory
- pid: 944a1cfd79dbfb6fef460360a0765ba790f4027a
  title: Recurrent Continuous Translation Models
- pid: abd1c342495432171beb7ca8fd9551ef13cbd0ff
  title: ImageNet classification with deep convolutional neural networks
- pid: 330da625c15427c6e42ccfa3b747fb29e5835bf0
  title: Efficient Estimation of Word Representations in Vector Space
- pid: 72e93aa6767ee683de7f001fa72f1314e40a8f35
  title: Building high-level features using large scale unsupervised learning
- pid: 89b1f4740ae37fd04f6ac007577bdd34621f0861
  title: Generating Sequences With Recurrent Neural Networks
- pid: e33cbb25a8c7390aec6a398e36381f4f7770c283
  title: Deep Neural Networks for Acoustic Modeling in Speech Recognition
- pid: 0894b06cff1cd0903574acaa7fcf071b144ae775
  title: Fast and Robust Neural Network Joint Models for Statistical Machine Translation
- pid: 162d958ff885f1462aeda91cd72582323fd6a1f4
  title: Gradient-based learning applied to document recognition
- pid: 398c296d0cc7f9d180f84969f8937e6d3a413796
  title: Multi-column deep neural networks for image classification
- pid: 97cedf99252026f58e8154bc61d49cf885d42030
  title: "Edinburgh\u2019s Phrase-based Machine Translation Systems for WMT-14"
- pid: 052b1d8ce63b07fec3de9dbb583772d860b7c769
  title: Learning representations by back-propagating errors
- pid: 84069287da0a6b488b8c933f3cb5be759cb6237e
  title: On the difficulty of training recurrent neural networks
- pid: aed054834e2c696807cc8b227ac7a4197196e211
  title: 'Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies'
- pid: 9f2efadf66817f1b38f58b3f50c7c8f34c69d89a
  title: 'DeepFace: Closing the Gap to Human-Level Performance in Face Verification'
- pid: b158a006bebb619e2ea7bf0a22c27d45c5d19004
  title: LSTM can Solve Hard Long Time Lag Problems
- pid: 1a3d22599028a05669e884f3eaf19a342e190a87
  title: 'Backpropagation Through Time: What It Does and How to Do It'
- pid: d7da009f457917aa381619facfa5ffae9329a6e9
  title: 'Bleu: a Method for Automatic Evaluation of Machine Translation'
- pid: 3d2218b17e7898a222e5fc2079a3f1531990708f
  title: I and J
- pid: 3f3d13e95c25a8f6a753e38dfce88885097cbd43
  title: Untersuchungen zu dynamischen neuronalen Netzen
slug: Sequence-to-Sequence-Learning-with-Neural-Networks-Sutskever-Vinyals
title: Sequence to Sequence Learning with Neural Networks
url: https://www.semanticscholar.org/paper/Sequence-to-Sequence-Learning-with-Neural-Networks-Sutskever-Vinyals/cea967b59209c6be22829699f05b8b1ac4dc092d?sort=total-citations
venue: NIPS
year: 2014
