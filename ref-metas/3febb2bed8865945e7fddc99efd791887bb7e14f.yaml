authors:
- Matthew E. Peters
- Mark Neumann
- Mohit Iyyer
- Matt Gardner
- Christopher Clark
- Kenton Lee
- Luke Zettlemoyer
badges:
- id: OPEN_ACCESS
corpusId: 3626819
fieldsOfStudy:
- Computer Science
numCitedBy: 7987
numCiting: 65
paperAbstract: We introduce a new type of deep contextualized word representation
  that models both (1) complex characteristics of word use (e.g., syntax and semantics),
  and (2) how these uses vary across linguistic contexts (i.e., to model polysemy).
  Our word vectors are learned functions of the internal states of a deep bidirectional
  language model (biLM), which is pre-trained on a large text corpus. We show that
  these representations can be easily added to existing models and significantly improve
  the state of the art across six challenging NLP problems, including question answering,
  textual entailment and sentiment analysis. We also present an analysis showing that
  exposing the deep internals of the pre-trained network is crucial, allowing downstream
  models to mix different types of semi-supervision signals.
ref_count: 65
references:
- pid: bc8fa64625d9189f5801837e7b133e7fe3c581f7
  title: 'Learned in Translation: Contextualized Word Vectors'
- pid: 0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38
  title: Semi-supervised sequence tagging with bidirectional language models
- pid: dac72f2c509aee67524d3321f77e97e8eff51de6
  title: 'Word Representations: A Simple and General Method for Semi-Supervised Learning'
- pid: 6dab1c6491929d396e9e5463bc2e87af88602aa2
  title: 'Finding Function in Form: Compositional Character Models for Open Vocabulary
    Word Representation'
- pid: 891ce1687e2befddd19f54e4eef1d3f39c8dbaf7
  title: Character-Aware Neural Language Models
- pid: 687bac2d3320083eb4530bf18bb8f8f721477600
  title: Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank
- pid: e2dba792360873aef125572812f3673b1a85d850
  title: Enriching Word Vectors with Subword Information
- pid: 12e9d005c77f76e344361f79c4b008034ae547eb
  title: 'Charagram: Embedding Words and Sentences via Character n-grams'
- pid: 452059171226626718eb677358836328f884298e
  title: 'Ask Me Anything: Dynamic Memory Networks for Natural Language Processing'
- pid: f37e1b62a767a307c046404ca96bc140b3e68cb5
  title: 'GloVe: Global Vectors for Word Representation'
- pid: 2f2d8f8072e5cc9b296fad551f65f183bdbff7aa
  title: Exploring the Limits of Language Modeling
- pid: f04df4e20a18358ea2f689b4c129781628ef7fc1
  title: A large annotated corpus for learning natural language inference
- pid: c34e41312b47f60986458759d5cc546c2b53f748
  title: End-to-end learning of semantic role labeling using recurrent neural networks
- pid: bc1022b031dc6c7019696492e8116598097a8c12
  title: Natural Language Processing (Almost) from Scratch
- pid: 87f40e6f3022adbc1f1905e3e506abad05a9964f
  title: Distributed Representations of Words and Phrases and their Compositionality
- pid: ade0c116120b54b57a91da51235108b75c28375a
  title: 'A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks'
- pid: 5d833331b0e22ff359db05c62a8bca18c4f04b68
  title: One billion word benchmark for measuring progress in statistical language
    modeling
- pid: 8dd6aae51e31a72752c4be5cddbdd76dfdc6cda4
  title: End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF
- pid: 3a7b63b50c64f4ec3358477790e84cbd6be2a0b4
  title: Bidirectional Attention Flow for Machine Comprehension
- pid: 10a4db59e81d26b2e0e896d3186ef81b4458b93f
  title: Named Entity Recognition with Bidirectional LSTM-CNNs
- pid: 05dd7254b632376973f3a1b4d39485da17814df5
  title: 'SQuAD: 100,000+ Questions for Machine Comprehension of Text'
- pid: 99d2dcdcf4cf05facaa101a48c7e31d140b4736d
  title: 'The Proposition Bank: An Annotated Corpus of Semantic Roles'
- pid: 3c78c6df5eb1695b6a399e346dde880af27d1016
  title: Simple and Effective Multi-Paragraph Reading Comprehension
- pid: 5b8364c21155d3d2cd38ea4c8b8580beba9a3250
  title: An Empirical Exploration of Recurrent Network Architectures
- pid: 0b44fcbeea9415d400c5f5789d6b892b6f98daff
  title: 'Building a Large Annotated Corpus of English: The Penn Treebank'
- pid: 03ad06583c9721855ccd82c3d969a01360218d86
  title: Deep multi-task learning with low level tasks supervised at lower layers
- pid: 0c1f9ca23f4f09ecfc44bcc8ca1c2736624f4652
  title: A Theoretically Grounded Application of Dropout in Recurrent Neural Networks
- pid: 1eb09fecd75eb27825dce4f964b97f4f5cc399d7
  title: "On the Properties of Neural Machine Translation: Encoder\u2013Decoder Approaches"
- pid: b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f
  title: Gated Self-Matching Networks for Reading Comprehension and Question Answering
- pid: 44d2abe2175df8153f465f6c39b68b76a0d40ab9
  title: Long Short-Term Memory
- pid: f4ba954b0412773d047dc41231c733de0c1f4926
  title: 'Conditional Random Fields: Probabilistic Models for Segmenting and Labeling
    Sequence Data'
- pid: b92aa7024b87f50737b372e5df31ef091ab54e62
  title: Training Very Deep Networks
- pid: 34f25a8704614163c4095b3ee2fc969b60de4698
  title: 'Dropout: a simple way to prevent neural networks from overfitting'
- pid: a6cb366736791bcccc5c8639de5a8f9636bf87e8
  title: 'Adam: A Method for Stochastic Optimization'
- pid: 97fb4e3d45bb098e27e0071448b6152217bd35a5
  title: Layer Normalization
- pid: 10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb
  title: 'Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity
    Recognition'
- pid: 24158c9fc293c8a998ac552b1188404a877da292
  title: Neural Architectures for Named Entity Recognition
- pid: 8729441d734782c3ed532a7d2d9611b438c0a09a
  title: 'ADADELTA: An Adaptive Learning Rate Method'
slug: Deep-Contextualized-Word-Representations-Peters-Neumann
title: Deep Contextualized Word Representations
url: https://www.semanticscholar.org/paper/Deep-Contextualized-Word-Representations-Peters-Neumann/3febb2bed8865945e7fddc99efd791887bb7e14f?sort=total-citations
venue: NAACL
year: 2018
