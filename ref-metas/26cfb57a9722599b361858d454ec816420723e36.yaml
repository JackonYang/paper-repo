authors:
- Jize Cao
- Zhe Gan
- Yu Cheng
- Licheng Yu
- Yen-Chun Chen
- Jingjing Liu
badges:
- id: OPEN_ACCESS
corpusId: 218665405
fieldsOfStudy:
- Computer Science
numCitedBy: 64
numCiting: 49
paperAbstract: 'Recent Transformer-based large-scale pre-trained models have revolutionized
  vision-and-language (V+L) research. Models such as ViLBERT, LXMERT and UNITER have
  significantly lifted state of the art across a wide range of V+L benchmarks with
  joint image-text pre-training. However, little is known about the inner mechanisms
  that destine their impressive success. To reveal the secrets behind the scene of
  these powerful models, we present VALUE (Vision-And-Language Understanding Evaluation),
  a set of meticulously designed probing tasks (e.g., Visual Coreference Resolution,
  Visual Relation Detection, Linguistic Probing Tasks) generalizable to standard pre-trained
  V+L models, aiming to decipher the inner workings of multimodal pre-training (e.g.,
  the implicit knowledge garnered in individual attention heads, the inherent cross-modal
  alignment learned through contextualized multimodal embeddings). Through extensive
  analysis of each archetypal model architecture via these probing tasks, our key
  observations are: (i) Pre-trained models exhibit a propensity for attending over
  text rather than images during inference. (ii) There exists a subset of attention
  heads that are tailored for capturing cross-modal interactions. (iii) Learned attention
  matrix in pre-trained models demonstrates patterns coherent with the latent alignment
  between image regions and textual words. (iv) Plotted attention patterns reveal
  visually-interpretable relations among image regions. (v) Pure linguistic knowledge
  is also effectively encoded in the attention heads. These are valuable insights
  serving to guide future work towards designing better model architecture and objectives
  for multimodal pre-training.'
ref_count: 49
references:
- pid: 2bc1c8bd00bbf7401afcb5460277840fd8bab029
  title: 'Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal
    Pre-training'
- pid: 6648b4db5f12c30941ea78c695e77aded19672bb
  title: Unified Vision-Language Pre-Training for Image Captioning and VQA
- pid: 54416048772b921720f19869ed11c2a360589d03
  title: 'UNITER: Learning UNiversal Image-TExt Representations'
- pid: 65a9c7b0800c86a196bc14e7621ff895cc6ab287
  title: 'ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language
    Tasks'
- pid: d8a305b9366608d54452ac30459ee57b4f5cf1c9
  title: 'UNITER: UNiversal Image-TExt Representation Learning'
- pid: 2f5f81bc516a6d085d39479378af1fc27104f91e
  title: Large-Scale Adversarial Training for Vision-and-Language Representation Learning
- pid: 2527626c11a84f15709e943fbfa2356e19930e3b
  title: 'VL-BERT: Pre-training of Generic Visual-Linguistic Representations'
- pid: 6548a60a6bcdf6c402d9de1c05ba7afe4f49fee9
  title: '12-in-1: Multi-Task Vision and Language Representation Learning'
- pid: 79c93274429d6355959f1e4374c2147bb81ea649
  title: 'LXMERT: Learning Cross-Modality Encoder Representations from Transformers'
- pid: 5aec474c31a2f4b74703c6f786c0a8ff85c450da
  title: 'VisualBERT: A Simple and Performant Baseline for Vision and Language'
- pid: c41a11c0e9b8b92b4faaf97749841170b760760a
  title: 'VideoBERT: A Joint Model for Video and Language Representation Learning'
- pid: df2b0e26d0599ce3e70df8a9da02e51594e0e992
  title: 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'
- pid: 6dfc2ff03534a4325d06c6f88c3144831996629b
  title: 'From Recognition to Cognition: Visual Commonsense Reasoning'
- pid: 3c54b796cc10cb530f77caa4d18e1c80ac863822
  title: 'Visual Entailment: A Novel Task for Fine-Grained Image Understanding'
- pid: a27ed1310b9c0832bde8f906e0fcd23d1f3aa79c
  title: 'Making the V in VQA Matter: Elevating the Role of Image Understanding in
    Visual Question Answering'
- pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  title: Attention is All you Need
- pid: 45dd2a3cd7c27f2e9509b023d702408f5ac11c9d
  title: Stacked Cross Attention for Image-Text Matching
- pid: a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8
  title: Bottom-Up and Top-Down Attention for Image Captioning and Visual Question
    Answering
- pid: b82153bf85d5d1edd3f170aace830e5328ca9ed0
  title: Fusion of Detected Objects in Text for Visual Question Answering
- pid: f259bc7ef31c4ec7dd041c94bfd6b2f93b99b47c
  title: Contrastive Bidirectional Transformer for Temporal Representation Learning
- pid: d78aed1dac6656affa4a04cbf225ced11a83d103
  title: Revealing the Dark Secrets of BERT
- pid: afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d
  title: 'Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image
    Annotations'
- pid: 0612745dbd292fc0a548a16d39cd73e127faedde
  title: 'Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer
    Image-to-Sentence Models'
- pid: c122fa378a774ba202d418cf71c5c356cf2f902f
  title: 'GQA: a new dataset for compositional question answering over real-world
    images'
- pid: 97ad70a9fa3f99adf18030e5e38ebe3d90daa2db
  title: 'VQA: Visual Question Answering'
- pid: e65142010431ffc089b272a1174214e00693e503
  title: Generation and Comprehension of Unambiguous Object Descriptions
- pid: 29efbe391950ae438c63d86ad5c82b2942efb0b4
  title: Modeling Context in Referring Expressions
- pid: cf336d272a30d6ad6141db67faa64deb8791cd61
  title: A Corpus for Reasoning about Natural Language Grounded in Photographs
- pid: 025a0dc4a2a98742f1b410b6318a46de2c854b22
  title: Learning Video Representations using Contrastive Bidirectional Transformer
- pid: 7113bd87c3e6f727efae24ee52f20c81358da761
  title: 'SentEval: An Evaluation Toolkit for Universal Sentence Representations'
- pid: 784da2a7b53a16d2243f747e14946cc5e3476af0
  title: 'VQA: Visual Question Answering'
slug: Behind-the-Scene:-Revealing-the-Secrets-of-Models-Cao-Gan
title: 'Behind the Scene: Revealing the Secrets of Pre-trained Vision-and-Language
  Models'
url: https://www.semanticscholar.org/paper/Behind-the-Scene:-Revealing-the-Secrets-of-Models-Cao-Gan/26cfb57a9722599b361858d454ec816420723e36?sort=total-citations
venue: ECCV
year: 2020
