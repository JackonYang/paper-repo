authors:
- Zihang Dai
- Zhilin Yang
- Yiming Yang
- J. Carbonell
- Quoc V. Le
- R. Salakhutdinov
badges:
- id: OPEN_ACCESS
corpusId: 57759363
fieldsOfStudy:
- Computer Science
numCitedBy: 1771
numCiting: 68
paperAbstract: Transformers have a potential of learning longer-term dependency, but
  are limited by a fixed-length context in the setting of language modeling. We propose
  a novel neural architecture Transformer-XL that enables learning dependency beyond
  a fixed length without disrupting temporal coherence. It consists of a segment-level
  recurrence mechanism and a novel positional encoding scheme. Our method not only
  enables capturing longer-term dependency, but also resolves the context fragmentation
  problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs
  and 450% longer than vanilla Transformers, achieves better performance on both short
  and long sequences, and is up to 1,800+ times faster than vanilla Transformers during
  evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to
  0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word,
  and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103,
  Transformer-XL manages to generate reasonably coherent, novel text articles with
  thousands of tokens. Our code, pretrained models, and hyperparameters are available
  in both Tensorflow and PyTorch.
ref_count: 68
references:
- pid: 88caa4a0253a8b0076176745ebc072864eab66e1
  title: Language Modeling with Gated Convolutional Networks
- pid: b9de9599d7241459db9213b5cdd7059696f5ef8d
  title: Character-Level Language Modeling with Deeper Self-Attention
- pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  title: Attention is All you Need
- pid: 2f2d8f8072e5cc9b296fad551f65f183bdbff7aa
  title: Exploring the Limits of Language Modeling
- pid: 5d833331b0e22ff359db05c62a8bca18c4f04b68
  title: One billion word benchmark for measuring progress in statistical language
    modeling
- pid: cd18800a0fe0b668a1cc19f2ec95b5003d0a5035
  title: Improving Language Understanding by Generative Pre-Training
- pid: 3febb2bed8865945e7fddc99efd791887bb7e14f
  title: Deep Contextualized Word Representations
- pid: efbd381493bb9636f489b965a2034d529cd56bcd
  title: Pointer Sentinel Mixture Models
- pid: 5522764282c85aea422f1c4dc92ff7e0ca6987bc
  title: A Clockwork RNN
- pid: d1275b2a2ab53013310e759e5c6878b96df643d4
  title: Context dependent recurrent neural network language model
- pid: c8efcc854d97dfc2a42b83316a2109f9d166e43f
  title: Self-Attention with Relative Position Representations
- pid: 71ae756c75ac89e2d731c9c79649562b5768ff39
  title: Memory Networks
- pid: 44d2abe2175df8153f465f6c39b68b76a0d40ab9
  title: Long Short-Term Memory
- pid: fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5
  title: Neural Machine Translation by Jointly Learning to Align and Translate
- pid: 98445f4172659ec5e891e031d8202c102135c644
  title: Neural Machine Translation in Linear Time
- pid: 921196c32213a229245a9705ee4768bc941e7a26
  title: An Empirical Evaluation of Generic Convolutional and Recurrent Networks for
    Sequence Modeling
- pid: 510e26733aaff585d65701b9f1be7ca9d5afc586
  title: 'Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts
    Layer'
- pid: 63e39cdf1ad884da6bc69096bb3413b5b1100559
  title: Using the Output Embedding to Improve Language Models
- pid: 952454718139dba3aafc6b3b67c4f514ac3964af
  title: Recurrent Batch Normalization
- pid: 4aa9f5150b46320f534de4747a2dd0cd7f3fe292
  title: Semi-supervised Sequence Learning
- pid: 9665247ea3421929f9b6ad721f139f11edb1dbb8
  title: Learning Longer Memory in Recurrent Neural Networks
- pid: 6c2b28f9354f667cd5bd07afc0471d8334430da7
  title: A Neural Probabilistic Language Model
- pid: d46b81707786d18499f911b4ab72bb10c65406ba
  title: A Simple Way to Initialize Recurrent Networks of Rectified Linear Units
- pid: c19fbefdeead6a4154a22a9c8551a18b1530033a
  title: Hierarchical Probabilistic Neural Network Language Model
- pid: 0c1f9ca23f4f09ecfc44bcc8ca1c2736624f4652
  title: A Theoretically Grounded Application of Dropout in Recurrent Neural Networks
- pid: 79baf48bd560060549998d7b61751286de062e2a
  title: Factorization tricks for LSTM networks
- pid: aed054834e2c696807cc8b227ac7a4197196e211
  title: 'Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies'
- pid: 136cf66392f1d6bf42da4cc070888996dc472b91
  title: On Multiplicative Integration with Recurrent Neural Networks
- pid: 9819b600a828a57e1cde047bbe710d3446b30da5
  title: Recurrent neural network based language model
- pid: 2270b8628fd8ca67ae39d277f45bc3c38ac63d5f
  title: 'Mesh-TensorFlow: Deep Learning for Supercomputers'
- pid: f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97
  title: Recurrent Neural Network Regularization
- pid: 89b1f4740ae37fd04f6ac007577bdd34621f0861
  title: Generating Sequences With Recurrent Neural Networks
- pid: 65eee67dee969fdf8b44c87c560d66ad4d78e233
  title: Hierarchical Multiscale Recurrent Neural Networks
- pid: c5145b1d15fea9340840cc8bb6f0e46e8934827f
  title: Understanding the exploding gradient problem
- pid: c3823aacea60bc1f2cabb9283144690a3d015db5
  title: Neural Turing Machines
- pid: 43f2ad297941db230c089ba353efc3f281ab678c
  title: "5\u5206\u3067\u5206\u304B\u308B!? \u6709\u540D\u8AD6\u6587\u30CA\u30CA\u30E1\
    \u8AAD\u307F\uFF1AJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional\
    \ Transformers for Language Understanding"
slug: Transformer-XL:-Attentive-Language-Models-beyond-a-Dai-Yang
title: 'Transformer-XL: Attentive Language Models beyond a Fixed-Length Context'
url: https://www.semanticscholar.org/paper/Transformer-XL:-Attentive-Language-Models-beyond-a-Dai-Yang/c4744a7c2bb298e4a52289a1e085c71cc3d37bc6?sort=total-citations
venue: ACL
year: 2019
