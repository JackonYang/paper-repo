authors:
- Zihang Dai
- Zhilin Yang
- Yiming Yang
- J. Carbonell
- Quoc V. Le
- R. Salakhutdinov
badges:
- id: OPEN_ACCESS
corpusId: 57759363
fieldsOfStudy:
- Computer Science
meta_key: transformer-xl-attentive-language-models-beyond-a-fixed-length-context
numCitedBy: 1771
numCiting: 68
paperAbstract: Transformers have a potential of learning longer-term dependency, but
  are limited by a fixed-length context in the setting of language modeling. We propose
  a novel neural architecture Transformer-XL that enables learning dependency beyond
  a fixed length without disrupting temporal coherence. It consists of a segment-level
  recurrence mechanism and a novel positional encoding scheme. Our method not only
  enables capturing longer-term dependency, but also resolves the context fragmentation
  problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs
  and 450% longer than vanilla Transformers, achieves better performance on both short
  and long sequences, and is up to 1,800+ times faster than vanilla Transformers during
  evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to
  0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word,
  and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103,
  Transformer-XL manages to generate reasonably coherent, novel text articles with
  thousands of tokens. Our code, pretrained models, and hyperparameters are available
  in both Tensorflow and PyTorch.
ref_count: 68
references:
- fieldsOfStudy:
  - Computer Science
  meta_key: regularizing-and-optimizing-lstm-language-models
  numCitedBy: 856
  pid: 58c6f890a1ae372958b7decf56132fe258152722
  show_ref_link: false
  title: Regularizing and Optimizing LSTM Language Models
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: language-modeling-with-gated-convolutional-networks
  numCitedBy: 1284
  pid: 88caa4a0253a8b0076176745ebc072864eab66e1
  show_ref_link: true
  title: Language Modeling with Gated Convolutional Networks
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: an-improved-relative-self-attention-mechanism-for-transformer-with-application-to-music-generation
  numCitedBy: 57
  pid: 2da824e19bd49a2e37739421fa003818c413946f
  show_ref_link: false
  title: An Improved Relative Self-Attention Mechanism for Transformer with Application
    to Music Generation
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: character-level-language-modeling-with-deeper-self-attention
  numCitedBy: 216
  pid: b9de9599d7241459db9213b5cdd7059696f5ef8d
  show_ref_link: true
  title: Character-Level Language Modeling with Deeper Self-Attention
  year: 2019
- fieldsOfStudy:
  - Computer Science
  - Environmental Science
  meta_key: learning-longer-term-dependencies-in-rnns-with-auxiliary-losses
  numCitedBy: 121
  pid: 608e4bbe7a2d6f04d68b5747d9d0778d5fce47df
  show_ref_link: false
  title: Learning Longer-term Dependencies in RNNs with Auxiliary Losses
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: sharp-nearby-fuzzy-far-away-how-neural-language-models-use-context
  numCitedBy: 177
  pid: fd5794fc63d5f19bf83cf7baa36e0aa62cbf6299
  show_ref_link: false
  title: 'Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context'
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: recurrent-highway-networks
  numCitedBy: 350
  pid: 7dba53e72c182e25e98e8f73a99d75ff69dda0c2
  show_ref_link: false
  title: Recurrent Highway Networks
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: larger-context-language-modelling
  numCitedBy: 52
  pid: 6c5325c2b67bf88f2b846cf5a6df6c2e6362d75b
  show_ref_link: false
  title: Larger-Context Language Modelling
  year: 2015
- fieldsOfStudy:
  - Computer Science
  meta_key: an-analysis-of-neural-language-modeling-at-multiple-scales
  numCitedBy: 155
  pid: 680aafd3d51e666b297e27b93d9554cc2caf1c4d
  show_ref_link: false
  title: An Analysis of Neural Language Modeling at Multiple Scales
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: attention-is-all-you-need
  numCitedBy: 35148
  pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  show_ref_link: true
  title: Attention is All you Need
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: topicrnn-a-recurrent-neural-network-with-long-range-semantic-dependency
  numCitedBy: 190
  pid: 7ab2166f6cdb1737e000df66d29c6538afc6811d
  show_ref_link: false
  title: 'TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency'
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: multiplicative-lstm-for-sequence-modelling
  numCitedBy: 148
  pid: 55cf59bfbb25d6363cab87cb747648ebe8a096e5
  show_ref_link: false
  title: Multiplicative LSTM for sequence modelling
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: tying-word-vectors-and-word-classifiers-a-loss-framework-for-language-modeling
  numCitedBy: 336
  pid: 424aef7340ee618132cc3314669400e23ad910ba
  show_ref_link: false
  title: 'Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling'
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: fast-slow-recurrent-neural-networks
  numCitedBy: 66
  pid: 87a913817503379547bec61a5f010abac5b0f76b
  show_ref_link: false
  title: Fast-Slow Recurrent Neural Networks
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: exploring-the-limits-of-language-modeling
  numCitedBy: 951
  pid: 2f2d8f8072e5cc9b296fad551f65f183bdbff7aa
  show_ref_link: true
  title: Exploring the Limits of Language Modeling
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: breaking-the-softmax-bottleneck-a-high-rank-rnn-language-model
  numCitedBy: 276
  pid: ef9ddbc35676ce8ffc2a8067044473727839dbac
  show_ref_link: false
  title: 'Breaking the Softmax Bottleneck: A High-Rank RNN Language Model'
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: one-billion-word-benchmark-for-measuring-progress-in-statistical-language-modeling
  numCitedBy: 902
  pid: 5d833331b0e22ff359db05c62a8bca18c4f04b68
  show_ref_link: false
  title: One billion word benchmark for measuring progress in statistical language
    modeling
  year: 2014
- fieldsOfStudy:
  - Computer Science
  meta_key: improving-language-understanding-by-generative-pre-training
  numCitedBy: 3533
  pid: cd18800a0fe0b668a1cc19f2ec95b5003d0a5035
  show_ref_link: true
  title: Improving Language Understanding by Generative Pre-Training
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: deep-contextualized-word-representations
  numCitedBy: 7987
  pid: 3febb2bed8865945e7fddc99efd791887bb7e14f
  show_ref_link: true
  title: Deep Contextualized Word Representations
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: pointer-sentinel-mixture-models
  numCitedBy: 1042
  pid: efbd381493bb9636f489b965a2034d529cd56bcd
  show_ref_link: true
  title: Pointer Sentinel Mixture Models
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: a-clockwork-rnn
  numCitedBy: 376
  pid: 5522764282c85aea422f1c4dc92ff7e0ca6987bc
  show_ref_link: false
  title: A Clockwork RNN
  year: 2014
- fieldsOfStudy:
  - Computer Science
  meta_key: context-dependent-recurrent-neural-network-language-model
  numCitedBy: 548
  pid: d1275b2a2ab53013310e759e5c6878b96df643d4
  show_ref_link: false
  title: Context dependent recurrent neural network language model
  year: 2012
- fieldsOfStudy:
  - Computer Science
  meta_key: self-attention-with-relative-position-representations
  numCitedBy: 923
  pid: c8efcc854d97dfc2a42b83316a2109f9d166e43f
  show_ref_link: true
  title: Self-Attention with Relative Position Representations
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: fast-parametric-learning-with-activation-memorization
  numCitedBy: 43
  pid: 69ac3b35887eb42e8fe554619fc7255e6e95a4cb
  show_ref_link: false
  title: Fast Parametric Learning with Activation Memorization
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: memory-networks
  numCitedBy: 1146
  pid: 71ae756c75ac89e2d731c9c79649562b5768ff39
  show_ref_link: true
  title: Memory Networks
  year: 2015
- fieldsOfStudy:
  - Computer Science
  meta_key: long-short-term-memory
  numCitedBy: 51691
  pid: 44d2abe2175df8153f465f6c39b68b76a0d40ab9
  show_ref_link: true
  title: Long Short-Term Memory
  year: 1997
- fieldsOfStudy:
  - Computer Science
  meta_key: topic-compositional-neural-language-model
  numCitedBy: 61
  pid: 01eb299fec9b9f5f499d0ce9702d5aeb77848d88
  show_ref_link: false
  title: Topic Compositional Neural Language Model
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: adaptive-input-representations-for-neural-language-modeling
  numCitedBy: 207
  pid: d170bd486e4c0fe82601e322b0e9e0dde63ab299
  show_ref_link: false
  title: Adaptive Input Representations for Neural Language Modeling
  year: 2019
- fieldsOfStudy:
  - Computer Science
  meta_key: skip-gram-language-modeling-using-sparse-non-negative-matrix-probability-estimation
  numCitedBy: 12
  pid: 0dc9eb7d17f2def56ad930945f2521653f04c3fa
  show_ref_link: false
  title: Skip-gram Language Modeling Using Sparse Non-negative Matrix Probability
    Estimation
  year: 2014
- fieldsOfStudy:
  - Computer Science
  meta_key: neural-machine-translation-by-jointly-learning-to-align-and-translate
  numCitedBy: 19339
  pid: fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5
  show_ref_link: true
  title: Neural Machine Translation by Jointly Learning to Align and Translate
  year: 2015
- fieldsOfStudy:
  - Computer Science
  meta_key: neural-machine-translation-in-linear-time
  numCitedBy: 454
  pid: 98445f4172659ec5e891e031d8202c102135c644
  show_ref_link: true
  title: Neural Machine Translation in Linear Time
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: independently-recurrent-neural-network-indrnn-building-a-longer-and-deeper-rnn
  numCitedBy: 402
  pid: 565ab57eede8bf6ef9c42df51216b9f85287c234
  show_ref_link: false
  title: 'Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper
    RNN'
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: an-empirical-evaluation-of-generic-convolutional-and-recurrent-networks-for-sequence-modeling
  numCitedBy: 1756
  pid: 921196c32213a229245a9705ee4768bc941e7a26
  show_ref_link: true
  title: An Empirical Evaluation of Generic Convolutional and Recurrent Networks for
    Sequence Modeling
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: outrageously-large-neural-networks-the-sparsely-gated-mixture-of-experts-layer
  numCitedBy: 862
  pid: 510e26733aaff585d65701b9f1be7ca9d5afc586
  show_ref_link: true
  title: 'Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts
    Layer'
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: neural-architecture-search-with-reinforcement-learning
  numCitedBy: 3426
  pid: 67d968c7450878190e45ac7886746de867bf673d
  show_ref_link: false
  title: Neural Architecture Search with Reinforcement Learning
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: using-the-output-embedding-to-improve-language-models
  numCitedBy: 568
  pid: 63e39cdf1ad884da6bc69096bb3413b5b1100559
  show_ref_link: true
  title: Using the Output Embedding to Improve Language Models
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: recurrent-batch-normalization
  numCitedBy: 341
  pid: 952454718139dba3aafc6b3b67c4f514ac3964af
  show_ref_link: true
  title: Recurrent Batch Normalization
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: sparse-attentive-backtracking-temporal-creditassignment-through-reminding
  numCitedBy: 60
  pid: fa0beb3f4d7f6e7e49b153af7e8a7c30f2937b60
  show_ref_link: false
  title: 'Sparse Attentive Backtracking: Temporal CreditAssignment Through Reminding'
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: semi-supervised-sequence-learning
  numCitedBy: 881
  pid: 4aa9f5150b46320f534de4747a2dd0cd7f3fe292
  show_ref_link: false
  title: Semi-supervised Sequence Learning
  year: 2015
- fieldsOfStudy:
  - Computer Science
  meta_key: learning-longer-memory-in-recurrent-neural-networks
  numCitedBy: 227
  pid: 9665247ea3421929f9b6ad721f139f11edb1dbb8
  show_ref_link: false
  title: Learning Longer Memory in Recurrent Neural Networks
  year: 2015
- fieldsOfStudy:
  - Computer Science
  meta_key: improving-neural-language-models-with-a-continuous-cache
  numCitedBy: 241
  pid: 2d7782c225e0fc123d6e227f2cb253e58279ac73
  show_ref_link: false
  title: Improving Neural Language Models with a Continuous Cache
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: document-context-language-models
  numCitedBy: 62
  pid: e8e76b1062918624e9904e0073e11794d7594593
  show_ref_link: false
  title: Document Context Language Models
  year: 2015
- fieldsOfStudy:
  - Computer Science
  meta_key: a-neural-probabilistic-language-model
  numCitedBy: 6009
  pid: 6c2b28f9354f667cd5bd07afc0471d8334430da7
  show_ref_link: false
  title: A Neural Probabilistic Language Model
  year: 2000
- fieldsOfStudy:
  - Computer Science
  meta_key: a-simple-way-to-initialize-recurrent-networks-of-rectified-linear-units
  numCitedBy: 572
  pid: d46b81707786d18499f911b4ab72bb10c65406ba
  show_ref_link: false
  title: A Simple Way to Initialize Recurrent Networks of Rectified Linear Units
  year: 2015
- fieldsOfStudy:
  - Computer Science
  meta_key: hierarchical-probabilistic-neural-network-language-model
  numCitedBy: 942
  pid: c19fbefdeead6a4154a22a9c8551a18b1530033a
  show_ref_link: false
  title: Hierarchical Probabilistic Neural Network Language Model
  year: 2005
- fieldsOfStudy:
  - Computer Science
  meta_key: efficient-neural-architecture-search-via-parameter-sharing
  numCitedBy: 1706
  pid: fe9b8aac9fa3bfd9724db5a881a578e471e612d7
  show_ref_link: false
  title: Efficient Neural Architecture Search via Parameter Sharing
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: a-theoretically-grounded-application-of-dropout-in-recurrent-neural-networks
  numCitedBy: 1315
  pid: 0c1f9ca23f4f09ecfc44bcc8ca1c2736624f4652
  show_ref_link: true
  title: A Theoretically Grounded Application of Dropout in Recurrent Neural Networks
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: efficient-softmax-approximation-for-gpus
  numCitedBy: 196
  pid: 9ec499af9b85f30bdbdd6cdfbb07d484808c526a
  show_ref_link: false
  title: Efficient softmax approximation for GPUs
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: factorization-tricks-for-lstm-networks
  numCitedBy: 91
  pid: 79baf48bd560060549998d7b61751286de062e2a
  show_ref_link: false
  title: Factorization tricks for LSTM networks
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: darts-differentiable-architecture-search
  numCitedBy: 2191
  pid: c1f457e31b611da727f9aef76c283a18157dfa83
  show_ref_link: false
  title: 'DARTS: Differentiable Architecture Search'
  year: 2019
- fieldsOfStudy:
  - Chemistry
  meta_key: gradient-flow-in-recurrent-nets-the-difficulty-of-learning-long-term-dependencies
  numCitedBy: 1567
  pid: aed054834e2c696807cc8b227ac7a4197196e211
  show_ref_link: false
  title: 'Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies'
  year: 2001
- fieldsOfStudy:
  - Computer Science
  meta_key: on-multiplicative-integration-with-recurrent-neural-networks
  numCitedBy: 129
  pid: 136cf66392f1d6bf42da4cc070888996dc472b91
  show_ref_link: true
  title: On Multiplicative Integration with Recurrent Neural Networks
  year: 2016
- fieldsOfStudy:
  - Computer Science
  meta_key: hypernetworks
  numCitedBy: 374
  pid: 563783de03452683a9206e85fe6d661714436686
  show_ref_link: false
  title: HyperNetworks
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: recurrent-neural-network-based-language-model
  numCitedBy: 4900
  pid: 9819b600a828a57e1cde047bbe710d3446b30da5
  show_ref_link: true
  title: Recurrent neural network based language model
  year: 2010
- fieldsOfStudy:
  - Computer Science
  meta_key: mesh-tensorflow-deep-learning-for-supercomputers
  numCitedBy: 209
  pid: 2270b8628fd8ca67ae39d277f45bc3c38ac63d5f
  show_ref_link: true
  title: 'Mesh-TensorFlow: Deep Learning for Supercomputers'
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: recurrent-neural-network-regularization
  numCitedBy: 1970
  pid: f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97
  show_ref_link: true
  title: Recurrent Neural Network Regularization
  year: 2014
- fieldsOfStudy:
  - Computer Science
  meta_key: generating-sequences-with-recurrent-neural-networks
  numCitedBy: 3151
  pid: 89b1f4740ae37fd04f6ac007577bdd34621f0861
  show_ref_link: true
  title: Generating Sequences With Recurrent Neural Networks
  year: 2013
- fieldsOfStudy:
  - Computer Science
  meta_key: hierarchical-multiscale-recurrent-neural-networks
  numCitedBy: 441
  pid: 65eee67dee969fdf8b44c87c560d66ad4d78e233
  show_ref_link: true
  title: Hierarchical Multiscale Recurrent Neural Networks
  year: 2017
- fieldsOfStudy:
  - Computer Science
  meta_key: pushing-the-bounds-of-dropout
  numCitedBy: 12
  pid: 717892acc8767a028218b5053ebe57a4f59685d1
  show_ref_link: false
  title: Pushing the bounds of dropout
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: sigsoftmax-reanalysis-of-the-softmax-bottleneck
  numCitedBy: 45
  pid: 7b9256b9fc59404b4cfe8c60b3943f4e38360122
  show_ref_link: false
  title: 'Sigsoftmax: Reanalysis of the Softmax Bottleneck'
  year: 2018
- fieldsOfStudy:
  - Computer Science
  meta_key: understanding-the-exploding-gradient-problem
  numCitedBy: 460
  pid: c5145b1d15fea9340840cc8bb6f0e46e8934827f
  show_ref_link: false
  title: Understanding the exploding gradient problem
  year: 2012
- fieldsOfStudy:
  - Computer Science
  meta_key: neural-turing-machines
  numCitedBy: 1634
  pid: c3823aacea60bc1f2cabb9283144690a3d015db5
  show_ref_link: true
  title: Neural Turing Machines
  year: 2014
- fieldsOfStudy:
  - Psychology
  meta_key: "5\u5206\u3067\u5206\u304B\u308B-\u6709\u540D\u8AD6\u6587\u30CA\u30CA\u30E1\
    \u8AAD\u307F-jacob-devlin-et-al-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding"
  numCitedBy: 1035
  pid: 43f2ad297941db230c089ba353efc3f281ab678c
  show_ref_link: false
  title: "5\u5206\u3067\u5206\u304B\u308B!? \u6709\u540D\u8AD6\u6587\u30CA\u30CA\u30E1\
    \u8AAD\u307F\uFF1AJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional\
    \ Transformers for Language Understanding"
  year: 2020
slug: Transformer-XL:-Attentive-Language-Models-beyond-a-Dai-Yang
title: 'Transformer-XL: Attentive Language Models beyond a Fixed-Length Context'
url: https://www.semanticscholar.org/paper/Transformer-XL:-Attentive-Language-Models-beyond-a-Dai-Yang/c4744a7c2bb298e4a52289a1e085c71cc3d37bc6?sort=total-citations
venue: ACL
year: 2019
