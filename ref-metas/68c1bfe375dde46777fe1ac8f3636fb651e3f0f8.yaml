authors:
- Y. Freund
- R. Schapire
badges:
- id: OPEN_ACCESS
corpusId: 1836349
fieldsOfStudy:
- Computer Science
numCitedBy: 8626
numCiting: 33
paperAbstract: In an earlier paper, we introduced a new "boosting" algorithm called
  AdaBoost which, theoretically, can be used to significantly reduce the error of
  any learning algorithm that con- sistently generates classifiers whose performance
  is a little better than random guessing. We also introduced the related notion of
  a "pseudo-loss" which is a method for forcing a learning algorithm of multi-label
  concepts to concentrate on the labels that are hardest to discriminate. In this
  paper, we describe experiments we carried out to assess how well AdaBoost with and
  without pseudo-loss, performs on real learning problems. We performed two sets of
  experiments. The first set compared boosting to Breiman's "bagging" method when
  used to aggregate various classifiers (including decision trees and single attribute-
  value tests). We compared the performance of the two methods on a collection of
  machine-learning benchmarks. In the second set of experiments, we studied in more
  detail the performance of boosting using a nearest-neighbor classifier on an OCR
  problem.
ref_count: 34
references:
- pid: ccf5208521cb8c35f50ee8873df89294b8ed7292
  title: A decision-theoretic generalization of on-line learning and an application
    to boosting
- pid: d414438926b73bde0313948d8b074cb5360a0e6f
  title: Bias, Variance , And Arcing Classifiers
- pid: a1dfeb731fc0c79e04523cd655413c223f6fa102
  title: Boosting Decision Trees
- pid: 807c1f19047f96083e13614f7ce20f2ac98c239a
  title: 'C4.5: Programs for Machine Learning'
- pid: 843ffb9898cedf899ddcdb9c4bdd10881c122429
  title: Boosting Performance in Neural Networks
- pid: 79ea6a5a68e05065f82acd11a478aa7eac5f6c06
  title: Bagging, Boosting, and C4.5
- pid: 7feb0fc888cd55360949554db032d7d1cba9e947
  title: Programs for Machine Learning
- pid: 6665e03447f989c9bdb3432d93e89b516b9d18a7
  title: Fast Effective Rule Induction
- pid: 7c3771fd6829630cf450af853df728ecd8da4ab2
  title: The Condensed Nearest Neighbor Rule
- pid: 25744dbb4294fe7abb2d9b1b0d39006482ebb4ab
  title: Error-Correcting Output Coding Corrects Bias and Variance
- pid: 8314dda1ec43ce57ff877f8f02ed89acb68ca035
  title: Efficient Pattern Recognition Using a New Transformation Distance
- pid: 4ba566223e426677d12a9a18418c023a4deec77e
  title: A Decision-Theoretic Generalization of On-Line Learning and an Application
    to Boosting
slug: Experiments-with-a-New-Boosting-Algorithm-Freund-Schapire
title: Experiments with a New Boosting Algorithm
url: https://www.semanticscholar.org/paper/Experiments-with-a-New-Boosting-Algorithm-Freund-Schapire/68c1bfe375dde46777fe1ac8f3636fb651e3f0f8?sort=total-citations
venue: ICML
year: 1996
