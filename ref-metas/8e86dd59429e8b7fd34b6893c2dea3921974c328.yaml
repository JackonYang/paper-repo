authors:
- Alane Suhr
- Yoav Artzi
badges:
- id: OPEN_ACCESS
corpusId: 202719370
fieldsOfStudy:
- Computer Science
numCitedBy: 5
numCiting: 5
paperAbstract: NLVR2 (Suhr et al., 2019) was designed to be robust for language bias
  through a data collection process that resulted in each natural language sentence
  appearing with both true and false labels. The process did not provide a similar
  measure of control for visual bias. This technical report analyzes the potential
  for visual bias in NLVR2. We show that some amount of visual bias likely exists.
  Finally, we identify a subset of the test data that allows to test for model performance
  in a way that is robust to such potential biases. We show that the performance of
  existing models (Li et al., 2019; Tan and Bansal 2019) is relatively robust to this
  potential bias. We propose to add the evaluation on this subset of the data to the
  NLVR2 evaluation protocol, and update the official release to include it. A notebook
  including an implementation of the code used to replicate this analysis is available
  at this http URL.
ref_count: 5
references:
- pid: 5aec474c31a2f4b74703c6f786c0a8ff85c450da
  title: 'VisualBERT: A Simple and Performant Baseline for Vision and Language'
- pid: cf336d272a30d6ad6141db67faa64deb8791cd61
  title: A Corpus for Reasoning about Natural Language Grounded in Photographs
- pid: c460bfcdd3cda8a787cbfe920dab41136f2dc129
  title: Weakly Supervised Semantic Parsing with Abstract Examples
- pid: 79c93274429d6355959f1e4374c2147bb81ea649
  title: 'LXMERT: Learning Cross-Modality Encoder Representations from Transformers'
slug: NLVR2-Visual-Bias-Analysis-Suhr-Artzi
title: NLVR2 Visual Bias Analysis
url: https://www.semanticscholar.org/paper/NLVR2-Visual-Bias-Analysis-Suhr-Artzi/8e86dd59429e8b7fd34b6893c2dea3921974c328?sort=total-citations
venue: ArXiv
year: 2019
