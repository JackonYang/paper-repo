authors:
- Ashish Vaswani
- Noam M. Shazeer
- Niki Parmar
- Jakob Uszkoreit
- Llion Jones
- Aidan N. Gomez
- Lukasz Kaiser
- Illia Polosukhin
badges:
- id: OPEN_ACCESS
corpusId: 13756489
fieldsOfStudy:
- Computer Science
numCitedBy: 35150
numCiting: 44
paperAbstract: The dominant sequence transduction models are based on complex recurrent
  or convolutional neural networks in an encoder-decoder configuration. The best performing
  models also connect the encoder and decoder through an attention mechanism. We propose
  a new simple network architecture, the Transformer, based solely on attention mechanisms,
  dispensing with recurrence and convolutions entirely. Experiments on two machine
  translation tasks show these models to be superior in quality while being more parallelizable
  and requiring significantly less time to train. Our model achieves 28.4 BLEU on
  the WMT 2014 English-to-German translation task, improving over the existing best
  results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation
  task, our model establishes a new single-model state-of-the-art BLEU score of 41.8
  after training for 3.5 days on eight GPUs, a small fraction of the training costs
  of the best models from the literature. We show that the Transformer generalizes
  well to other tasks by applying it successfully to English constituency parsing
  both with large and limited training data.
ref_count: 44
references:
- pid: b60abe57bc195616063be10638c6437358c81d1e
  title: Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation
- pid: cea967b59209c6be22829699f05b8b1ac4dc092d
  title: Sequence to Sequence Learning with Neural Networks
- pid: 98445f4172659ec5e891e031d8202c102135c644
  title: Neural Machine Translation in Linear Time
- pid: 032274e57f7d8b456bd255fe76b909b2c1d7458e
  title: A Deep Reinforced Model for Abstractive Summarization
- pid: 4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e
  title: End-To-End Memory Networks
- pid: d76c07211479e233f7c6a6f32d5346c983c5598f
  title: Multi-task Sequence to Sequence Learning
- pid: 43428880d75b3a14257c3ee9bda054e61eb869c0
  title: Convolutional Sequence to Sequence Learning
- pid: 510e26733aaff585d65701b9f1be7ca9d5afc586
  title: 'Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts
    Layer'
- pid: fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5
  title: Neural Machine Translation by Jointly Learning to Align and Translate
- pid: dbde7dfa6cae81df8ac19ef500c42db96c3d1edd
  title: 'Google''s Neural Machine Translation System: Bridging the Gap between Human
    and Machine Translation'
- pid: 23ffaa0fe06eae05817f527a47ac3291077f9e58
  title: Rethinking the Inception Architecture for Computer Vision
- pid: 1af68821518f03568f913ab03fc02080247a27ff
  title: Neural Machine Translation of Rare Words with Subword Units
- pid: 2f2d8f8072e5cc9b296fad551f65f183bdbff7aa
  title: Exploring the Limits of Language Modeling
- pid: 0b544dfe355a5070b60986319a3f51fb45d1348e
  title: "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical\
    \ Machine Translation"
- pid: adfcf065e15fd3bc9badf6145034c84dfb08f204
  title: Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling
- pid: 2c03df8b48bf3fa39054345bafabfeff15bfd11d
  title: Deep Residual Learning for Image Recognition
- pid: 5b6ec746d309b165f9f9def873a2375b6fb40f3d
  title: 'Xception: Deep Learning with Depthwise Separable Convolutions'
- pid: 34f25a8704614163c4095b3ee2fc969b60de4698
  title: 'Dropout: a simple way to prevent neural networks from overfitting'
- pid: 13fe71da009484f240c46f14d9330e932f8de210
  title: Long Short-Term Memory-Networks for Machine Reading
- pid: 5e4eb58d5b47ac1c73f4cf189497170e75ae6237
  title: Neural GPUs Learn Algorithms
- pid: 63e39cdf1ad884da6bc69096bb3413b5b1100559
  title: Using the Output Embedding to Improve Language Models
- pid: a6cb366736791bcccc5c8639de5a8f9636bf87e8
  title: 'Adam: A Method for Stochastic Optimization'
- pid: 44d2abe2175df8153f465f6c39b68b76a0d40ab9
  title: Long Short-Term Memory
- pid: 204a4a70428f3938d2c538a4d74c7ae0416306d8
  title: A Structured Self-attentive Sentence Embedding
- pid: 47570e7f63e296f224a0e7f9a0d08b0de3cbaf40
  title: Grammar as a Foreign Language
- pid: aed054834e2c696807cc8b227ac7a4197196e211
  title: 'Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies'
- pid: 79baf48bd560060549998d7b61751286de062e2a
  title: Factorization tricks for LSTM networks
- pid: 89b1f4740ae37fd04f6ac007577bdd34621f0861
  title: Generating Sequences With Recurrent Neural Networks
- pid: f52de7242e574b70410ca6fb70b79c811919fc00
  title: Learning Accurate, Compact, and Interpretable Tree Annotation
- pid: 7345843e87c81e24e42264859b214d26042f8d51
  title: Recurrent Neural Network Grammars
- pid: 0b44fcbeea9415d400c5f5789d6b892b6f98daff
  title: 'Building a Large Annotated Corpus of English: The Penn Treebank'
- pid: 78a9513e70f596077179101f6cb6eadc51602039
  title: Effective Self-Training for Parsing
slug: Attention-is-All-you-Need-Vaswani-Shazeer
title: Attention is All you Need
url: https://www.semanticscholar.org/paper/Attention-is-All-you-Need-Vaswani-Shazeer/204e3073870fae3d05bcbc2f6a8e263d9b72e776?sort=total-citations
venue: NIPS
year: 2017
