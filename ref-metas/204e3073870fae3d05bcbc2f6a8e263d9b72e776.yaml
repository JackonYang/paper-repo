authors:
- Ashish Vaswani
- Noam M. Shazeer
- Niki Parmar
- Jakob Uszkoreit
- Llion Jones
- Aidan N. Gomez
- Lukasz Kaiser
- Illia Polosukhin
badges:
- id: OPEN_ACCESS
corpusId: 13756489
fieldsOfStudy:
- Computer Science
numCitedBy: 35150
numCiting: 44
paperAbstract: The dominant sequence transduction models are based on complex recurrent
  or convolutional neural networks in an encoder-decoder configuration. The best performing
  models also connect the encoder and decoder through an attention mechanism. We propose
  a new simple network architecture, the Transformer, based solely on attention mechanisms,
  dispensing with recurrence and convolutions entirely. Experiments on two machine
  translation tasks show these models to be superior in quality while being more parallelizable
  and requiring significantly less time to train. Our model achieves 28.4 BLEU on
  the WMT 2014 English-to-German translation task, improving over the existing best
  results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation
  task, our model establishes a new single-model state-of-the-art BLEU score of 41.8
  after training for 3.5 days on eight GPUs, a small fraction of the training costs
  of the best models from the literature. We show that the Transformer generalizes
  well to other tasks by applying it successfully to English constituency parsing
  both with large and limited training data.
ref_count: 44
references:
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 174
  pid: b60abe57bc195616063be10638c6437358c81d1e
  title: Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation
  year: 2016
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 14880
  pid: cea967b59209c6be22829699f05b8b1ac4dc092d
  title: Sequence to Sequence Learning with Neural Networks
  year: 2014
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 454
  pid: 98445f4172659ec5e891e031d8202c102135c644
  title: Neural Machine Translation in Linear Time
  year: 2016
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1063
  pid: 032274e57f7d8b456bd255fe76b909b2c1d7458e
  title: A Deep Reinforced Model for Abstractive Summarization
  year: 2018
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1990
  pid: 4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e
  title: End-To-End Memory Networks
  year: 2015
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 683
  pid: d76c07211479e233f7c6a6f32d5346c983c5598f
  title: Multi-task Sequence to Sequence Learning
  year: 2016
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 2422
  pid: 43428880d75b3a14257c3ee9bda054e61eb869c0
  title: Convolutional Sequence to Sequence Learning
  year: 2017
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 862
  pid: 510e26733aaff585d65701b9f1be7ca9d5afc586
  title: 'Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts
    Layer'
  year: 2017
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 19339
  pid: fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5
  title: Neural Machine Translation by Jointly Learning to Align and Translate
  year: 2015
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 4645
  pid: dbde7dfa6cae81df8ac19ef500c42db96c3d1edd
  title: 'Google''s Neural Machine Translation System: Bridging the Gap between Human
    and Machine Translation'
  year: 2016
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 15542
  pid: 23ffaa0fe06eae05817f527a47ac3291077f9e58
  title: Rethinking the Inception Architecture for Computer Vision
  year: 2016
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 4793
  pid: 1af68821518f03568f913ab03fc02080247a27ff
  title: Neural Machine Translation of Rare Words with Subword Units
  year: 2016
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 951
  pid: 2f2d8f8072e5cc9b296fad551f65f183bdbff7aa
  title: Exploring the Limits of Language Modeling
  year: 2016
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 15050
  pid: 0b544dfe355a5070b60986319a3f51fb45d1348e
  title: "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical\
    \ Machine Translation"
  year: 2014
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 7376
  pid: adfcf065e15fd3bc9badf6145034c84dfb08f204
  title: Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling
  year: 2014
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 95314
  pid: 2c03df8b48bf3fa39054345bafabfeff15bfd11d
  title: Deep Residual Learning for Image Recognition
  year: 2016
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 6594
  pid: 5b6ec746d309b165f9f9def873a2375b6fb40f3d
  title: 'Xception: Deep Learning with Depthwise Separable Convolutions'
  year: 2017
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 28147
  pid: 34f25a8704614163c4095b3ee2fc969b60de4698
  title: 'Dropout: a simple way to prevent neural networks from overfitting'
  year: 2014
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 765
  pid: 13fe71da009484f240c46f14d9330e932f8de210
  title: Long Short-Term Memory-Networks for Machine Reading
  year: 2016
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 287
  pid: 5e4eb58d5b47ac1c73f4cf189497170e75ae6237
  title: Neural GPUs Learn Algorithms
  year: 2016
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 568
  pid: 63e39cdf1ad884da6bc69096bb3413b5b1100559
  title: Using the Output Embedding to Improve Language Models
  year: 2017
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 90052
  pid: a6cb366736791bcccc5c8639de5a8f9636bf87e8
  title: 'Adam: A Method for Stochastic Optimization'
  year: 2015
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 51691
  pid: 44d2abe2175df8153f465f6c39b68b76a0d40ab9
  title: Long Short-Term Memory
  year: 1997
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1501
  pid: 204a4a70428f3938d2c538a4d74c7ae0416306d8
  title: A Structured Self-attentive Sentence Embedding
  year: 2017
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 845
  pid: 47570e7f63e296f224a0e7f9a0d08b0de3cbaf40
  title: Grammar as a Foreign Language
  year: 2015
- fieldsOfStudy:
  - Chemistry
  numCitedBy: 1567
  pid: aed054834e2c696807cc8b227ac7a4197196e211
  title: 'Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies'
  year: 2001
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 91
  pid: 79baf48bd560060549998d7b61751286de062e2a
  title: Factorization tricks for LSTM networks
  year: 2017
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 3151
  pid: 89b1f4740ae37fd04f6ac007577bdd34621f0861
  title: Generating Sequences With Recurrent Neural Networks
  year: 2013
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 965
  pid: f52de7242e574b70410ca6fb70b79c811919fc00
  title: Learning Accurate, Compact, and Interpretable Tree Annotation
  year: 2006
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 424
  pid: 7345843e87c81e24e42264859b214d26042f8d51
  title: Recurrent Neural Network Grammars
  year: 2016
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 8175
  pid: 0b44fcbeea9415d400c5f5789d6b892b6f98daff
  title: 'Building a Large Annotated Corpus of English: The Penn Treebank'
  year: 1993
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 586
  pid: 78a9513e70f596077179101f6cb6eadc51602039
  title: Effective Self-Training for Parsing
  year: 2006
slug: Attention-is-All-you-Need-Vaswani-Shazeer
title: Attention is All you Need
url: https://www.semanticscholar.org/paper/Attention-is-All-you-Need-Vaswani-Shazeer/204e3073870fae3d05bcbc2f6a8e263d9b72e776?sort=total-citations
venue: NIPS
year: 2017
