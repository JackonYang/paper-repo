authors:
- Jiasen Lu
- Dhruv Batra
- Devi Parikh
- Stefan Lee
badges:
- id: OPEN_ACCESS
corpusId: 199453025
fieldsOfStudy:
- Computer Science
numCitedBy: 1266
numCiting: 53
paperAbstract: We present ViLBERT (short for Vision-and-Language BERT), a model for
  learning task-agnostic joint representations of image content and natural language.
  We extend the popular BERT architecture to a multi-modal two-stream model, pro-cessing
  both visual and textual inputs in separate streams that interact through co-attentional
  transformer layers. We pretrain our model through two proxy tasks on the large,
  automatically collected Conceptual Captions dataset and then transfer it to multiple
  established vision-and-language tasks -- visual question answering, visual commonsense
  reasoning, referring expressions, and caption-based image retrieval -- by making
  only minor additions to the base architecture. We observe significant improvements
  across tasks compared to existing task-specific models -- achieving state-of-the-art
  on all four tasks. Our work represents a shift away from learning groundings between
  vision and language only as part of task training and towards treating visual grounding
  as a pretrainable and transferable capability.
ref_count: 53
references:
- pid: 5aec474c31a2f4b74703c6f786c0a8ff85c450da
  title: 'VisualBERT: A Simple and Performant Baseline for Vision and Language'
- pid: 2527626c11a84f15709e943fbfa2356e19930e3b
  title: 'VL-BERT: Pre-training of Generic Visual-Linguistic Representations'
- pid: 6648b4db5f12c30941ea78c695e77aded19672bb
  title: Unified Vision-Language Pre-Training for Image Captioning and VQA
- pid: 2bc1c8bd00bbf7401afcb5460277840fd8bab029
  title: 'Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal
    Pre-training'
- pid: 79c93274429d6355959f1e4374c2147bb81ea649
  title: 'LXMERT: Learning Cross-Modality Encoder Representations from Transformers'
- pid: df2b0e26d0599ce3e70df8a9da02e51594e0e992
  title: 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'
- pid: fdce9cbe5c726201575b3c8a8c1af0752f1af53f
  title: 'MAttNet: Modular Attention Network for Referring Expression Comprehension'
- pid: c41a11c0e9b8b92b4faaf97749841170b760760a
  title: 'VideoBERT: A Joint Model for Video and Language Representation Learning'
- pid: 6dfc2ff03534a4325d06c6f88c3144831996629b
  title: 'From Recognition to Cognition: Visual Commonsense Reasoning'
- pid: afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d
  title: 'Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image
    Annotations'
- pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  title: Attention is All you Need
- pid: a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8
  title: Bottom-Up and Top-Down Attention for Image Captioning and Visual Question
    Answering
- pid: 6bd9642470ff8c2089427f7a6392cd17d213a334
  title: 'Vision-and-Language Navigation: Interpreting Visually-Grounded Navigation
    Instructions in Real Environments'
- pid: fc1b1c9364c58ec406f494dd944b609a6a038ba6
  title: Unsupervised Visual Representation Learning by Context Prediction
- pid: 97ad70a9fa3f99adf18030e5e38ebe3d90daa2db
  title: 'VQA: Visual Question Answering'
- pid: e5790afc079c6f36d6fe9235d6d253f3da631f51
  title: Embodied Question Answering
- pid: 7d0effebfa4bed19b6ba41f3af5b7e5b6890de87
  title: 'Context Encoders: Feature Learning by Inpainting'
- pid: 6d4e3616d0b27957c4107ae877dc0dd4504b69ab
  title: 'Shuffle and Learn: Unsupervised Learning Using Temporal Order Verification'
- pid: c426ba865e9158a0f7962a86a50575aa943051b1
  title: Learning Image Representations Tied to Ego-Motion
- pid: 90873a97aa9a43775e5aeea01b03aea54b28bfbd
  title: 'Don''t Just Assume; Look and Answer: Overcoming Priors for Visual Question
    Answering'
- pid: ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc
  title: Cross-lingual Language Model Pretraining
- pid: 45dd2a3cd7c27f2e9509b023d702408f5ac11c9d
  title: Stacked Cross Attention for Image-Text Matching
- pid: 3febb2bed8865945e7fddc99efd791887bb7e14f
  title: Deep Contextualized Word Representations
- pid: 8b55402ffee2734bfc7d5d7595500916e1ef04e8
  title: 'nocaps: novel object captioning at scale'
- pid: e9b13731027418ed38103d1dfc8a70f6881bc684
  title: Dynamic Fusion With Intra- and Inter-Modality Attention Flow for Visual Question
    Answering
- pid: 0e6824e137847be0599bb0032e37042ed2ef5045
  title: 'Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching
    Movies and Reading Books'
- pid: b4df354db88a70183a64dbc9e56cf14e7669a6c0
  title: 'Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic
    Image Captioning'
- pid: dbde7dfa6cae81df8ac19ef500c42db96c3d1edd
  title: 'Google''s Neural Machine Translation System: Bridging the Gap between Human
    and Machine Translation'
- pid: 2c03df8b48bf3fa39054345bafabfeff15bfd11d
  title: Deep Residual Learning for Image Recognition
- pid: ea99a5535388196d0d44be5b4d7dd02029a43bb2
  title: Mask R-CNN
- pid: 5d833331b0e22ff359db05c62a8bca18c4f04b68
  title: One billion word benchmark for measuring progress in statistical language
    modeling
- pid: 424561d8585ff8ebce7d5d07de8dbf7aae5e7270
  title: 'Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks'
- pid: 8201e6e687f2de477258e9be53ba7b73ee30d7de
  title: Colorful Image Colorization
- pid: 92c141447f51b6732242376164ff961e464731c8
  title: 'ReferItGame: Referring to Objects in Photographs of Natural Scenes'
- pid: 44040913380206991b1991daf1192942e038fe31
  title: 'From image descriptions to visual denotations: New similarity metrics for
    semantic inference over event descriptions'
- pid: d79ac7a7bafdc9a782fb8c53285ca11c7f2e3f18
  title: 'BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language
    Model'
- pid: e74f9b7f8eec6ba4704c206b93bc8079af3da4bd
  title: ImageNet Large Scale Visual Recognition Challenge
- pid: 696ca58d93f6404fea0fc75c62d1d7b378f47628
  title: 'Microsoft COCO Captions: Data Collection and Evaluation Server'
slug: ViLBERT:-Pretraining-Task-Agnostic-Visiolinguistic-Lu-Batra
title: 'ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language
  Tasks'
url: https://www.semanticscholar.org/paper/ViLBERT:-Pretraining-Task-Agnostic-Visiolinguistic-Lu-Batra/65a9c7b0800c86a196bc14e7621ff895cc6ab287?sort=total-citations
venue: NeurIPS
year: 2019
