authors:
- Ciprian Chelba
- Tomas Mikolov
- M. Schuster
- Qi Ge
- T. Brants
- P. Koehn
- T. Robinson
badges:
- id: OPEN_ACCESS
corpusId: 14136307
fieldsOfStudy:
- Computer Science
numCitedBy: 903
numCiting: 45
paperAbstract: We propose a new benchmark corpus to be used for measuring progress
  in statistical language modeling. With almost one billion words of training data,
  we hope this benchmark will be useful to quickly evaluate novel language modeling
  techniques, and to compare their contribution when combined with other advanced
  techniques. We show performance of several well-known types of language models,
  with the best results achieved with a recurrent neural network based language model.
  The baseline unpruned KneserNey 5-gram model achieves perplexity 67.6. A combination
  of techniques leads to 35% reduction in perplexity, or 10% reduction in cross-entropy
  (bits), over that baseline. The benchmark is available as a code.google.com project;
  besides the scripts needed to rebuild the training/held-out data, it also makes
  available log-probability values for each word in each of ten held-out data sets,
  for each of the baseline n-gram models.
ref_count: 45
references:
- pid: 96364af2d208ea75ca3aeb71892d2f7ce7326b55
  title: Statistical Language Models Based on Neural Networks
- pid: f9a1b3850dfd837793743565a8af95973d395a4e
  title: LSTM Neural Networks for Language Modeling
- pid: 0fcc184b3b90405ec3ceafd6a4007c749df7c363
  title: Continuous space language models
- pid: 9819b600a828a57e1cde047bbe710d3446b30da5
  title: Recurrent neural network based language model
- pid: 6c2b28f9354f667cd5bd07afc0471d8334430da7
  title: A Neural Probabilistic Language Model
- pid: 4af41f4d838daa7ca6995aeb4918b61989d1ed80
  title: Classes for fast maximum entropy training
- pid: 77dfe038a9bdab27c4505444931eaa976e9ec667
  title: Empirical Evaluation and Combination of Advanced Language Modeling Techniques
- pid: 0b26fa1b848ed808a0511db34bce2426888f0b68
  title: Adaptive Statistical Language Modeling; A Maximum Entropy Approach
- pid: c19fbefdeead6a4154a22a9c8551a18b1530033a
  title: Hierarchical Probabilistic Neural Network Language Model
- pid: 0687165a9f0360bde0469fd401d966540e0897c3
  title: A Dynamic Language Model for Speech Recognition
- pid: 29053eab305c2b585bcfbb713243b05646e7d62d
  title: Entropy-based Pruning of Backoff Language Models
- pid: cb45e9217fe323fbc199d820e7735488fca2a9b3
  title: Strategies for training large scale neural network language models
- pid: 9548ac30c113562a51e603dbbc8e9fa651cfd3ab
  title: Improved backing-off for M-gram language modeling
- pid: bd7d93193aad6c4b71cc8942e808753019e87706
  title: Three new graphical models for statistical language modelling
- pid: a1c3748820d6b5ab4e7334524815df9bb6d20aed
  title: Structured language modeling
- pid: 07ca885cb5cc4328895bfaec9ab752d5801b14cd
  title: Extensions of recurrent neural network language model
- pid: 3de5d40b60742e3dfa86b19e7f660962298492af
  title: Class-Based n-gram Models of Natural Language
- pid: ba786c46373892554b98df42df7af6f5da343c9d
  title: Large Language Models in Machine Translation
- pid: 668087f0ae7ce1de6e0bd0965dbb480c08103260
  title: Finding Structure in Time
- pid: b0130277677e5b915d5cd86b3afafd77fd08eb2e
  title: Estimation of probabilities from sparse data for the language model component
    of a speech recognizer
- pid: d4e8bed3b50a035e1eabad614fe4218a34b3b178
  title: An empirical study of smoothing techniques for language modeling
- pid: 3d2218b17e7898a222e5fc2079a3f1531990708f
  title: I and J
- pid: 1d453386011ef21285fa81fb4f87fdf811c6ad7a
  title: Learning internal representations by back-propagating errors
slug: One-billion-word-benchmark-for-measuring-progress-Chelba-Mikolov
title: One billion word benchmark for measuring progress in statistical language modeling
url: https://www.semanticscholar.org/paper/One-billion-word-benchmark-for-measuring-progress-Chelba-Mikolov/5d833331b0e22ff359db05c62a8bca18c4f04b68?sort=total-citations
venue: INTERSPEECH
year: 2014
