authors:
- Ciprian Chelba
- Tomas Mikolov
- M. Schuster
- Qi Ge
- T. Brants
- P. Koehn
- T. Robinson
badges:
- id: OPEN_ACCESS
corpusId: 14136307
fieldsOfStudy:
- Computer Science
numCitedBy: 903
numCiting: 45
paperAbstract: We propose a new benchmark corpus to be used for measuring progress
  in statistical language modeling. With almost one billion words of training data,
  we hope this benchmark will be useful to quickly evaluate novel language modeling
  techniques, and to compare their contribution when combined with other advanced
  techniques. We show performance of several well-known types of language models,
  with the best results achieved with a recurrent neural network based language model.
  The baseline unpruned KneserNey 5-gram model achieves perplexity 67.6. A combination
  of techniques leads to 35% reduction in perplexity, or 10% reduction in cross-entropy
  (bits), over that baseline. The benchmark is available as a code.google.com project;
  besides the scripts needed to rebuild the training/held-out data, it also makes
  available log-probability values for each word in each of ten held-out data sets,
  for each of the baseline n-gram models.
ref_count: 45
references:
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 575
  pid: 96364af2d208ea75ca3aeb71892d2f7ce7326b55
  title: Statistical Language Models Based on Neural Networks
  year: 2012
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1490
  pid: f9a1b3850dfd837793743565a8af95973d395a4e
  title: LSTM Neural Networks for Language Modeling
  year: 2012
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 554
  pid: 0fcc184b3b90405ec3ceafd6a4007c749df7c363
  title: Continuous space language models
  year: 2007
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 4900
  pid: 9819b600a828a57e1cde047bbe710d3446b30da5
  title: Recurrent neural network based language model
  year: 2010
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 6009
  pid: 6c2b28f9354f667cd5bd07afc0471d8334430da7
  title: A Neural Probabilistic Language Model
  year: 2000
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 233
  pid: 4af41f4d838daa7ca6995aeb4918b61989d1ed80
  title: Classes for fast maximum entropy training
  year: 2001
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 314
  pid: 77dfe038a9bdab27c4505444931eaa976e9ec667
  title: Empirical Evaluation and Combination of Advanced Language Modeling Techniques
  year: 2011
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 417
  pid: 0b26fa1b848ed808a0511db34bce2426888f0b68
  title: Adaptive Statistical Language Modeling; A Maximum Entropy Approach
  year: 1994
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 942
  pid: c19fbefdeead6a4154a22a9c8551a18b1530033a
  title: Hierarchical Probabilistic Neural Network Language Model
  year: 2005
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 146
  pid: 0687165a9f0360bde0469fd401d966540e0897c3
  title: A Dynamic Language Model for Speech Recognition
  year: 1991
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 347
  pid: 29053eab305c2b585bcfbb713243b05646e7d62d
  title: Entropy-based Pruning of Backoff Language Models
  year: 2000
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 474
  pid: cb45e9217fe323fbc199d820e7735488fca2a9b3
  title: Strategies for training large scale neural network language models
  year: 2011
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1792
  pid: 9548ac30c113562a51e603dbbc8e9fa651cfd3ab
  title: Improved backing-off for M-gram language modeling
  year: 1995
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 606
  pid: bd7d93193aad6c4b71cc8942e808753019e87706
  title: Three new graphical models for statistical language modelling
  year: 2007
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 316
  pid: a1c3748820d6b5ab4e7334524815df9bb6d20aed
  title: Structured language modeling
  year: 2000
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1423
  pid: 07ca885cb5cc4328895bfaec9ab752d5801b14cd
  title: Extensions of recurrent neural network language model
  year: 2011
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 3316
  pid: 3de5d40b60742e3dfa86b19e7f660962298492af
  title: Class-Based n-gram Models of Natural Language
  year: 1992
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 533
  pid: ba786c46373892554b98df42df7af6f5da343c9d
  title: Large Language Models in Machine Translation
  year: 2007
- fieldsOfStudy:
  - Psychology
  numCitedBy: 9861
  pid: 668087f0ae7ce1de6e0bd0965dbb480c08103260
  title: Finding Structure in Time
  year: 1990
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1907
  pid: b0130277677e5b915d5cd86b3afafd77fd08eb2e
  title: Estimation of probabilities from sparse data for the language model component
    of a speech recognizer
  year: 1987
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 2861
  pid: d4e8bed3b50a035e1eabad614fe4218a34b3b178
  title: An empirical study of smoothing techniques for language modeling
  year: 1999
- fieldsOfStudy: []
  numCitedBy: 151407
  pid: 3d2218b17e7898a222e5fc2079a3f1531990708f
  title: I and J
  year: 2012
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 524
  pid: 1d453386011ef21285fa81fb4f87fdf811c6ad7a
  title: Learning internal representations by back-propagating errors
  year: 1986
slug: One-billion-word-benchmark-for-measuring-progress-Chelba-Mikolov
title: One billion word benchmark for measuring progress in statistical language modeling
url: https://www.semanticscholar.org/paper/One-billion-word-benchmark-for-measuring-progress-Chelba-Mikolov/5d833331b0e22ff359db05c62a8bca18c4f04b68?sort=total-citations
venue: INTERSPEECH
year: 2014
