authors:
- Jonathan Baxter
badges:
- id: OPEN_ACCESS
corpusId: 9803204
fieldsOfStudy:
- Computer Science
numCitedBy: 972
numCiting: 85
paperAbstract: 'A major problem in machine learning is that of inductive bias: how
  to choose a learner''s hypothesis space so that it is large enough to contain a
  solution to the problem being learnt, yet small enough to ensure reliable generalization
  from reasonably-sized training sets. Typically such bias is supplied by hand through
  the skill and insights of experts. In this paper a model for automatically learning
  bias is investigated. The central assumption of the model is that the learner is
  embedded within an environment of related learning tasks. Within such an environment
  the learner can sample from multiple tasks, and hence it can search for a hypothesis
  space that contains good solutions to many of the problems in the environment. Under
  certain restrictions on the set of all hypothesis spaces available to the learner,
  we show that a hypothesis space that performs well on a sufficiently large number
  of training tasks will also perform well when learning novel tasks in the same environment.
  Explicit bounds are also derived demonstrating that learning multiple tasks within
  an environment of related tasks can potentially give much better generalization
  than learning a single task.'
ref_count: 85
references:
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 343
  pid: a24508e65e599b5b20c33af96dbe7017d5caca37
  title: Learning internal representations
  year: 1995
- fieldsOfStudy:
  - Mathematics
  - Computer Science
  numCitedBy: 979
  pid: fedfc9fbcfe46d50b81078560bce724678f90176
  title: Decision Theoretic Generalizations of the PAC Model for Neural Net and Other
    Learning Applications
  year: 1992
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 3259
  pid: 47aaeb6dc682162dfe5659c2cad64e5d825ad910
  title: Multitask Learning
  year: 1998
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 4191
  pid: 10ddb646feddc12337b5a755c72e153e37088c02
  title: A theory of the learnable
  year: 1984
- fieldsOfStudy:
  - Computer Science
  - Education
  numCitedBy: 685
  pid: 371c9dc680e916f79d9c78fcf6c894a2dd299095
  title: Is Learning The n-th Thing Any Easier Than Learning The First?
  year: 1995
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1198
  pid: 015999a72c70a960e59c51078b09c8f672af0d2c
  title: 'The Sample Complexity of Pattern Classification with Neural Networks: The
    Size of the Weights is More Important than the Size of the Network'
  year: 1998
- fieldsOfStudy:
  - Mathematics
  numCitedBy: 894
  pid: 788c6d1b1419a0f7b7695c0e7e9e41cf54fbfe1b
  title: On the Density of Families of Sets
  year: 1972
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 1909
  pid: e0b8fa3496283d4d808fba9ff62d5f024bcf23be
  title: Learnability and the Vapnik-Chervonenkis dimension
  year: 1989
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 38756
  pid: 8213dbed4db44e113af3ed17d6dad57471a0c048
  title: The Nature of Statistical Learning Theory
  year: 2000
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 3565
  pid: 43fcdee6c6d885ac2bd32e122dbf282f93720c22
  title: A Probabilistic Theory of Pattern Recognition
  year: 1996
- fieldsOfStudy:
  - Computer Science
  numCitedBy: 7326
  pid: b9dd05b69d6906fff6ea6c4ba3609a6d97c9b8a3
  title: Statistical Decision Theory and Bayesian Analysis
  year: 1988
- fieldsOfStudy:
  - Mathematics
  numCitedBy: 516
  pid: 78ecaabe915ba7df950671d36f92678192802df4
  title: 'Estimation of Dependences Based on Empirical Data: Springer Series in Statistics
    (Springer Series in Statistics)'
  year: 1982
slug: A-Model-of-Inductive-Bias-Learning-Baxter
title: A Model of Inductive Bias Learning
url: https://www.semanticscholar.org/paper/A-Model-of-Inductive-Bias-Learning-Baxter/727e1e16ede6eaad241bad11c525da07b154c688?sort=total-citations
venue: J. Artif. Intell. Res.
year: 2000
