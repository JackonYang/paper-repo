authors:
- Trieu H. Trinh
- Minh-Thang Luong
- Quoc V. Le
badges:
- id: OPEN_ACCESS
corpusId: 174801241
fieldsOfStudy:
- Computer Science
numCitedBy: 68
numCiting: 41
paperAbstract: We introduce a pretraining technique called Selfie, which stands for
  SELFie supervised Image Embedding. Selfie generalizes the concept of masked language
  modeling of BERT (Devlin et al., 2019) to continuous data, such as images, by making
  use of the Contrastive Predictive Coding loss (Oord et al., 2018). Given masked-out
  patches in an input image, our method learns to select the correct patch, among
  other "distractor" patches sampled from the same image, to fill in the masked location.
  This classification objective sidesteps the need for predicting exact pixel values
  of the target patches. The pretraining architecture of Selfie includes a network
  of convolutional blocks to process patches followed by an attention pooling network
  to summarize the content of unmasked patches before predicting masked ones. During
  finetuning, we reuse the convolutional weights found by pretraining. We evaluate
  Selfie on three benchmarks (CIFAR-10, ImageNet 32 x 32, and ImageNet 224 x 224)
  with varying amounts of labeled data, from 5% to 100% of the training sets. Our
  pretraining method provides consistent improvements to ResNet-50 across all settings
  compared to the standard supervised training of the same network. Notably, on ImageNet
  224 x 224 with 60 examples per class (5%), our method improves the mean accuracy
  of ResNet-50 from 35.6% to 46.7%, an improvement of 11.1 points in absolute accuracy.
  Our pretraining method also improves ResNet-50 training stability, especially on
  low data regime, by significantly lowering the standard deviation of test accuracies
  across different runs.
ref_count: 41
references:
- pid: df2b0e26d0599ce3e70df8a9da02e51594e0e992
  title: 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'
- pid: 2c03df8b48bf3fa39054345bafabfeff15bfd11d
  title: Deep Residual Learning for Image Recognition
- pid: 1e077413b25c4d34945cc2707e17e46ed4fe784a
  title: Universal Language Model Fine-tuning for Text Classification
- pid: 3febb2bed8865945e7fddc99efd791887bb7e14f
  title: Deep Contextualized Word Representations
- pid: aab368284210c1bb917ec2d31b84588e3d2d7eb4
  title: Unsupervised Representation Learning by Predicting Image Rotations
- pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  title: Attention is All you Need
- pid: 85f94d8098322f8130512b4c6c4627548ce4a6cc
  title: Unsupervised Pretraining for Sequence to Sequence Learning
- pid: dbde7dfa6cae81df8ac19ef500c42db96c3d1edd
  title: 'Google''s Neural Machine Translation System: Bridging the Gap between Human
    and Machine Translation'
- pid: 5694e46284460a648fe29117cbc55f6c9be3fa3c
  title: Densely Connected Convolutional Networks
- pid: 1c4e9156ca07705531e45960b7a919dc473abb51
  title: Wide Residual Networks
- pid: 7d0effebfa4bed19b6ba41f3af5b7e5b6890de87
  title: 'Context Encoders: Feature Learning by Inpainting'
- pid: 2ec8f7e0257a07d3914322b36072d1bbcd58a1e0
  title: Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles
- pid: 77f0a39b8e02686fd85b01971f8feb7f60971f80
  title: Identity Mappings in Deep Residual Networks
- pid: 4aa9f5150b46320f534de4747a2dd0cd7f3fe292
  title: Semi-supervised Sequence Learning
- pid: 6e795c6e9916174ae12349f5dc3f516570c17ce8
  title: Skip-Thought Vectors
- pid: fc1b1c9364c58ec406f494dd944b609a6a038ba6
  title: Unsupervised Visual Representation Learning by Context Prediction
- pid: f37e1b62a767a307c046404ca96bc140b3e68cb5
  title: 'GloVe: Global Vectors for Word Representation'
- pid: 24741d280869ad9c60321f5ab6e5f01b7852507d
  title: 'Deep Speech: Scaling up end-to-end speech recognition'
- pid: b3852f0113fcf8a3913c55ae92393ae6ccde347e
  title: 'Self-taught learning: transfer learning from unlabeled data'
- pid: 355d44f53428b1ac4fb2ab468d593c720640e5bd
  title: Greedy Layer-Wise Training of Deep Networks
- pid: e2b7f37cd97a7907b1b8a41138721ed06a0b76cd
  title: 'Stacked Denoising Autoencoders: Learning Useful Representations in a Deep
    Network with a Local Denoising Criterion'
- pid: 8978cf7574ceb35f4c3096be768c7547b28a35d0
  title: A Fast Learning Algorithm for Deep Belief Nets
- pid: f527bcfb09f32e6a4a8afc0b37504941c1ba2cee
  title: Distributed Representations of Sentences and Documents
- pid: 87f40e6f3022adbc1f1905e3e506abad05a9964f
  title: Distributed Representations of Words and Phrases and their Compositionality
slug: Selfie:-Self-supervised-Pretraining-for-Image-Trinh-Luong
title: 'Selfie: Self-supervised Pretraining for Image Embedding'
url: https://www.semanticscholar.org/paper/Selfie:-Self-supervised-Pretraining-for-Image-Trinh-Luong/5466ee5f16fc3c776fd1da667917592e5fd06720?sort=total-citations
venue: ArXiv
year: 2019
