authors:
- R. Caruana
- S. Lawrence
- C. Lee Giles
badges:
- id: OPEN_ACCESS
corpusId: 7365231
fieldsOfStudy:
- Computer Science
numCitedBy: 854
numCiting: 18
paperAbstract: 'The conventional wisdom is that backprop nets with excess hidden units
  generalize poorly. We show that nets with excess capacity generalize well when trained
  with backprop and early stopping. Experiments suggest two reasons for this: 1) Overfitting
  can vary significantly in different regions of the model. Excess capacity allows
  better fit to regions of high non-linearity, and backprop often avoids overfitting
  the regions of low non-linearity. 2) Regardless of size, nets learn task subcomponents
  in similar sequence. Big nets pass through stages similar to those learned by smaller
  nets. Early stopping can stop training the large net when it generalizes comparably
  to a smaller net. We also show that conjugate gradient can yield worse generalization
  because it overfits regions of low non-linearity when learning to fit regions of
  high non-linearity.'
ref_count: 18
references:
- pid: a34e35dbbc6911fa7b94894dffdc0076a261b6f0
  title: Neural Networks and the Bias/Variance Dilemma
- pid: 25406e6733a698bfc4ac836f8e74f458e75dad4f
  title: What Size Net Gives Valid Generalization?
- pid: de996c32045df6f7b404dda2a753b6a9becf3c08
  title: Parallel Networks that Learn to Pronounce English Text
- pid: 7e0dab4fe4299bc2f8b4b18f82702af717cf3924
  title: 'The Effective Number of Parameters: An Analysis of Generalization and Regularization
    in Nonlinear Learning Systems'
- pid: 70927dc841596f3e3dba07b472ccc565b944ddf0
  title: Recognizing Hand-Printed Letters and Digits Using Backpropagation Learning
- pid: f707a81a278d1598cd0a4493ba73f22dcdf90639
  title: Generalization by Weight-Elimination with Application to Forecasting
- pid: e7297db245c3feb1897720b173a59fe7e36babb7
  title: Optimal Brain Damage
slug: Overfitting-in-Neural-Nets:-Backpropagation,-and-Caruana-Lawrence
title: 'Overfitting in Neural Nets: Backpropagation, Conjugate Gradient, and Early
  Stopping'
url: https://www.semanticscholar.org/paper/Overfitting-in-Neural-Nets:-Backpropagation,-and-Caruana-Lawrence/072d756c8b17a78018298e67ff29e6d3a4fe5770?sort=total-citations
venue: NIPS
year: 2000
