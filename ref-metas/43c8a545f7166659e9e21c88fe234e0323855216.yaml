authors:
- "B. Sch\xF6lkopf"
- J. Platt
- T. Hofmann
badges: []
corpusId: 196065172
fieldsOfStudy:
- Computer Science
numCitedBy: 1216
numCiting: 0
paperAbstract: Complexity theory of circuits strongly suggests that deep architectures
  can be much more ef cient (sometimes exponentially) than shallow architectures,
  in terms of computational elements required to represent some functions. Deep multi-layer
  neural networks have many levels of non-linearities allowing them to compactly represent
  highly non-linear and highly-varying functions. However, until recently it was not
  clear how to train such deep networks, since gradient-based optimization starting
  from random initialization appears to often get stuck in poor solutions. Hinton
  et al. recently introduced a greedy layer-wise unsupervised learning algorithm for
  Deep Belief Networks (DBN), a generative model with many layers of hidden causal
  variables. In the context of the above optimization problem, we study this algorithm
  empirically and explore variants to better understand its success and extend it
  to cases where the inputs are continuous or where the structure of the input distribution
  is not revealing enough about the variable to be predicted in a supervised task.
  Our experiments also confirm the hypothesis that the greedy layer-wise unsupervised
  training strategy mostly helps the optimization, by initializing weights in a region
  near a good local minimum, giving rise to internal distributed representations that
  are high-level abstractions of the input, bringing better generalization.
ref_count: 0
references: []
slug: "Greedy-Layer-Wise-Training-of-Deep-Networks-Sch\xF6lkopf-Platt"
title: Greedy Layer-Wise Training of Deep Networks
url: "https://www.semanticscholar.org/paper/Greedy-Layer-Wise-Training-of-Deep-Networks-Sch\xF6\
  lkopf-Platt/43c8a545f7166659e9e21c88fe234e0323855216?sort=total-citations"
venue: ''
year: 2007
