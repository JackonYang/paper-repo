authors:
- Di Qi
- Lin Su
- Jianwei Song
- Edward Cui
- Taroon Bharti
- Arun Sacheti
badges:
- id: OPEN_ACCESS
corpusId: 210859480
fieldsOfStudy:
- Computer Science
numCitedBy: 101
numCiting: 32
paperAbstract: 'In this paper, we introduce a new vision-language pre-trained model
  -- ImageBERT -- for image-text joint embedding. Our model is a Transformer-based
  model, which takes different modalities as input and models the relationship between
  them. The model is pre-trained on four tasks simultaneously: Masked Language Modeling
  (MLM), Masked Object Classification (MOC), Masked Region Feature Regression (MRFR),
  and Image Text Matching (ITM). To further enhance the pre-training quality, we have
  collected a Large-scale weAk-supervised Image-Text (LAIT) dataset from Web. We first
  pre-train the model on this dataset, then conduct a second stage pre-training on
  Conceptual Captions and SBU Captions. Our experiments show that multi-stage pre-training
  strategy outperforms single-stage pre-training. We also fine-tune and evaluate our
  pre-trained ImageBERT model on image retrieval and text retrieval tasks, and achieve
  new state-of-the-art results on both MSCOCO and Flickr30k datasets.'
ref_count: 32
references:
- pid: 54416048772b921720f19869ed11c2a360589d03
  title: 'UNITER: Learning UNiversal Image-TExt Representations'
- pid: 6648b4db5f12c30941ea78c695e77aded19672bb
  title: Unified Vision-Language Pre-Training for Image Captioning and VQA
- pid: 2bc1c8bd00bbf7401afcb5460277840fd8bab029
  title: 'Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal
    Pre-training'
- pid: 45dd2a3cd7c27f2e9509b023d702408f5ac11c9d
  title: Stacked Cross Attention for Image-Text Matching
- pid: 48a7873681c6aa88b9e0e22a25c2a8245eaeb45f
  title: Position Focused Attention Network for Image-Text Matching
- pid: 79c93274429d6355959f1e4374c2147bb81ea649
  title: 'LXMERT: Learning Cross-Modality Encoder Representations from Transformers'
- pid: 2c03df8b48bf3fa39054345bafabfeff15bfd11d
  title: Deep Residual Learning for Image Recognition
- pid: 65a9c7b0800c86a196bc14e7621ff895cc6ab287
  title: 'ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language
    Tasks'
- pid: b4df354db88a70183a64dbc9e56cf14e7669a6c0
  title: 'Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic
    Image Captioning'
- pid: a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8
  title: Bottom-Up and Top-Down Attention for Image Captioning and Visual Question
    Answering
- pid: ad748d1772f893b3c8a3857a19292375be259daf
  title: Knowledge Aware Semantic Concept Expansion for Image-Text Matching
- pid: 1e077413b25c4d34945cc2707e17e46ed4fe784a
  title: Universal Language Model Fine-tuning for Text Classification
- pid: 204e3073870fae3d05bcbc2f6a8e263d9b72e776
  title: Attention is All you Need
- pid: 5aec474c31a2f4b74703c6f786c0a8ff85c450da
  title: 'VisualBERT: A Simple and Performant Baseline for Vision and Language'
- pid: 2527626c11a84f15709e943fbfa2356e19930e3b
  title: 'VL-BERT: Pre-training of Generic Visual-Linguistic Representations'
- pid: af1f7739283bdbd2b7a94903041f6d6afd991907
  title: Towards VQA Models That Can Read
- pid: df2b0e26d0599ce3e70df8a9da02e51594e0e992
  title: 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'
- pid: e0c6abdbdecf04ffac65c440da77fb9d66bb474c
  title: 'XLNet: Generalized Autoregressive Pretraining for Language Understanding'
- pid: 8e080b98efbe65c02a116439205ca2344b9f7cd4
  title: 'Im2Text: Describing Images Using 1 Million Captioned Photographs'
- pid: 0e6824e137847be0599bb0032e37042ed2ef5045
  title: 'Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching
    Movies and Reading Books'
- pid: b82153bf85d5d1edd3f170aace830e5328ca9ed0
  title: Fusion of Detected Objects in Text for Visual Question Answering
- pid: afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d
  title: 'Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image
    Annotations'
- pid: 5fc662287842e5cb2d23b5fa917354e957c573bf
  title: 'DenseNet: Implementing Efficient ConvNet Descriptor Pyramids'
- pid: 077f8329a7b6fa3b7c877a57b81eb6c18b5f87de
  title: 'RoBERTa: A Robustly Optimized BERT Pretraining Approach'
- pid: 97ad70a9fa3f99adf18030e5e38ebe3d90daa2db
  title: 'VQA: Visual Question Answering'
- pid: dbde7dfa6cae81df8ac19ef500c42db96c3d1edd
  title: 'Google''s Neural Machine Translation System: Bridging the Gap between Human
    and Machine Translation'
- pid: 6dfc2ff03534a4325d06c6f88c3144831996629b
  title: 'From Recognition to Cognition: Visual Commonsense Reasoning'
- pid: 696ca58d93f6404fea0fc75c62d1d7b378f47628
  title: 'Microsoft COCO Captions: Data Collection and Evaluation Server'
- pid: 44040913380206991b1991daf1192942e038fe31
  title: 'From image descriptions to visual denotations: New similarity metrics for
    semantic inference over event descriptions'
slug: ImageBERT:-Cross-modal-Pre-training-with-Image-Text-Qi-Su
title: 'ImageBERT: Cross-modal Pre-training with Large-scale Weak-supervised Image-Text
  Data'
url: https://www.semanticscholar.org/paper/ImageBERT:-Cross-modal-Pre-training-with-Image-Text-Qi-Su/a9fd5511b42206a27748f373e0fdb7eb76a23055?sort=total-citations
venue: ArXiv
year: 2020
