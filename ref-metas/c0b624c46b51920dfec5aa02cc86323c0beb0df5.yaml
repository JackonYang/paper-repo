authors:
- Vu Pham
- Christopher Kermorvant
- J. Louradour
badges:
- id: UNPAYWALL
- id: OPEN_ACCESS
corpusId: 9919769
fieldsOfStudy:
- Computer Science
numCitedBy: 466
numCiting: 41
paperAbstract: Recurrent neural networks (RNNs) with Long Short-Term memory cells
  currently hold the best known results in unconstrained handwriting recognition.
  We show that their performance can be greatly improved using dropout - a recently
  proposed regularization method for deep architectures. While previous works showed
  that dropout gave superior performance in the context of convolutional networks,
  it had never been applied to RNNs. In our approach, dropout is carefully used in
  the network so that it does not affect the recurrent connections, hence the power
  of RNNs in modeling sequences is preserved. Extensive experiments on a broad range
  of handwritten databases confirm the effectiveness of dropout on deep architectures
  even when the network mainly consists of recurrent and shared connections.
ref_count: 41
references:
- pid: 1a3c74c7b11ad5635570932577cdde2a3f7a6a5c
  title: Improving deep neural networks for LVCSR using rectified linear units and
    dropout
- pid: c08d0525bd42fa1c24f9f5df72f4c8fcf7063b22
  title: Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks
- pid: 96494e722f58705fa20302fe6179d483f52705b4
  title: 'Connectionist temporal classification: labelling unsegmented sequence data
    with recurrent neural networks'
- pid: d0be39ee052d246ae99c082a565aba25b811be2d
  title: Learning long-term dependencies with gradient descent is difficult
- pid: abd1c342495432171beb7ca8fd9551ef13cbd0ff
  title: ImageNet classification with deep convolutional neural networks
- pid: 1366de5bb112746a555e9c0cd00de3ad8628aea8
  title: Improving neural networks by preventing co-adaptation of feature detectors
- pid: 38f35dd624cd1cf827416e31ac5e0e0454028eca
  title: Regularization of Neural Networks using DropConnect
- pid: 98b098fb3fa3f270f8dbec444612e2e2acc9607d
  title: Improving Offline Handwritten Text Recognition with Hybrid HMM/ANN Models
- pid: 44d2abe2175df8153f465f6c39b68b76a0d40ab9
  title: Long Short-Term Memory
- pid: ec92efde21707ddf4b81f301cd58e2051c1a2443
  title: Fast dropout training
- pid: b7b915d508987b73b61eccd2b237e7ed099a2d29
  title: Maxout Networks
- pid: 8a56aa5ce50e6756206ec61c29c7bc1e1b579d8c
  title: An Off-Line Cursive Handwriting Recognition System
- pid: d12864a8acbab1830be755bfb9cb177e31ca5e20
  title: 'On-Line and Off-Line Handwriting Recognition: A Comprehensive Survey'
- pid: e15725948c2ea8b190b825a0887e430dc4898428
  title: Using a Statistical Language Model to Improve the Performance of an HMM-Based
    Cursive Handwriting Recognition System
- pid: 04a10e1b25f35a9ac1a4d4344bfbdb34b253cb59
  title: 'The IAM-database: an English sentence database for offline handwriting recognition'
- pid: 78e510627d3f28601e370212bf063bbfa539ebed
  title: Factorial Hidden Markov Models
- pid: 375214ac340226e23ec428e92ec499fb89f508b8
  title: A Novel Connectionist System for Unconstrained Handwriting Recognition
slug: Dropout-Improves-Recurrent-Neural-Networks-for-Pham-Kermorvant
title: Dropout Improves Recurrent Neural Networks for Handwriting Recognition
url: https://www.semanticscholar.org/paper/Dropout-Improves-Recurrent-Neural-Networks-for-Pham-Kermorvant/c0b624c46b51920dfec5aa02cc86323c0beb0df5?sort=total-citations
venue: 2014 14th International Conference on Frontiers in Handwriting Recognition
year: 2014
