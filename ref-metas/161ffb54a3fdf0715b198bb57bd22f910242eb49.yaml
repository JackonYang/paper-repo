authors:
- R. Caruana
badges:
- id: OPEN_ACCESS
corpusId: 45998148
fieldsOfStudy:
- Computer Science
numCitedBy: 3257
numCiting: 0
paperAbstract: Multitask Learning is an approach to inductive transfer that improves
  generalization by using the domain information contained in the training signals
  of related tasks as an inductive bias. It does this by learning tasks in parallel
  while using a shared representation; what is learned for each task can help other
  tasks be learned better. This paper reviews prior work on MTL, presents new evidence
  that MTL in backprop nets discovers task relatedness without the need of supervisory
  signals, and presents new results for MTL with k-nearest neighbor and kernel regression.
  In this paper we demonstrate multitask learning in three domains. We explain how
  multitask learning works, and show that there are many opportunities for multitask
  learning in real domains. We present an algorithm and results for multitask learning
  with case-based methods like k-nearest neighbor and kernel regression, and sketch
  an algorithm for multitask learning in decision trees. Because multitask learning
  works, can be applied to many different kinds of domains, and can be used with different
  learning algorithms, we conjecture there will be many opportunities for its use
  on real-world problems.
ref_count: 0
references: []
slug: Multitask-Learning-Caruana
title: Multitask Learning
url: https://www.semanticscholar.org/paper/Multitask-Learning-Caruana/161ffb54a3fdf0715b198bb57bd22f910242eb49?sort=total-citations
venue: Machine Learning
year: 2004
